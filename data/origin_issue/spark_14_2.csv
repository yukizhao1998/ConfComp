Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Container),Outward issue link (Container),Inward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Outward issue link (Required),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (dependent),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Sprint,Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
allCaseVersions function in  SqlLexical  leads to StackOverflow Exception,SPARK-5009,12764229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,OopsOutOfMemory,OopsOutOfMemory,30/Dec/14 17:49,06/May/15 05:59,14/Jul/23 06:25,21/Jan/15 21:06,1.1.1,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Recently I found a bug when I add new feature in SqlParser. Which is :
If I define a KeyWord that has a long name. Like：
 ```protected val ：SERDEPROPERTIES = Keyword(""SERDEPROPERTIES"")```

Since the all case version is implement by recursive function, so when  ```implicit asParser`` function is called  and the stack memory is very small, it will leads to SO Exception. 

java.lang.StackOverflowError
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
",,apachespark,marmbrus,OopsOutOfMemory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,345600,345600,,0%,345600,345600,,,,,,,,,SPARK-4208,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 21:06:45 UTC 2015,,,,,,,,,,"0|i23ur3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Jan/15 09:49;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/3909;;;","07/Jan/15 05:11;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3924;;;","07/Jan/15 06:23;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3926;;;","21/Jan/15 21:06;marmbrus;Issue resolved by pull request 3926
[https://github.com/apache/spark/pull/3926];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.port.maxRetries doesn't work,SPARK-5006,12764187,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,30/Dec/14 11:51,21/Jan/15 21:18,14/Jul/23 06:26,13/Jan/15 17:30,1.1.0,,,,,,,1.2.1,1.3.0,,,,,Deploy,,,,0,,,,,,We normally config spark.port.maxRetries in properties file or SparkConf. But in Utils.scala it read from SparkEnv's conf. As SparkEnv is an object whose env need to be set after JVM is launched and Utils.scala is also an object. So in most cases portMaxRetries will get the default value 16.,,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 12:18:04 UTC 2014,,,,,,,,,,"0|i23uhr:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"30/Dec/14 12:18;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3841;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No need to put WAL-backed block into block manager by default,SPARK-4999,12764129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jerryshao,jerryshao,30/Dec/14 05:04,13/Jan/15 04:20,14/Jul/23 06:26,13/Jan/15 04:20,1.2.0,,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,,,,,,"Currently WAL-backed block is read out from HDFS and put into BlockManger with storage level MEMORY_ONLY_SER by default, since WAL-backed block is already fault-tolerant, no need to put into BlockManger again by default.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 08:03:17 UTC 2015,,,,,,,,,,"0|i23u4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/14 05:15;jerryshao;{code}
          // Since storeInBlockManager = false, the storage level does not matter.
          new WriteAheadLogBackedBlockRDD[T](ssc.sparkContext,
            blockIds, logSegments, storeInBlockManager = true, StorageLevel.MEMORY_ONLY_SER)
        } else {
{code}

The code is in ReceiverInputDStream.scala  line 91. I think it would be more reasonable to set storeInBlockManager = false by default.;;;","30/Dec/14 07:44;jerryshao;cc [~tdas], what is your opinion ?;;;","06/Jan/15 08:03;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/3906;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""train"" methods in object DecisionTree cannot work when using java reflection",SPARK-4998,12764124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ljzzju,ljzzju,ljzzju,30/Dec/14 03:44,31/Dec/14 00:28,14/Jul/23 06:26,30/Dec/14 23:56,1.2.0,,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"When using the Java reflection, the several ""train"" methods defined in ""object DecisionTree"" cannot be identified due to the function with the same name in ""class DecisionTree"".

Since the namesake ""train"" method in class DecisionTree"" has already been a deprecated one, it should be commit out to make the ""train"" methods in ""object DecisionTree"" effective.",,apachespark,ljzzju,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 00:28:47 UTC 2014,,,,,,,,,,"0|i23u3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/14 04:45;ljzzju;I have make a PR to solve this issue.
https://github.com/apache/spark/pull/3836;;;","30/Dec/14 13:10;apachespark;User 'ljzzju' has created a pull request for this issue:
https://github.com/apache/spark/pull/3836;;;","30/Dec/14 23:56;mengxr;Issue resolved by pull request 3836
[https://github.com/apache/spark/pull/3836];;;","31/Dec/14 00:28;ljzzju;problem solved, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup removed executors' ShuffleInfo in yarn shuffle service,SPARK-4994,12764012,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,29/Dec/14 12:08,06/Feb/15 22:49,14/Jul/23 06:26,06/Feb/15 22:49,1.2.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"when the application is completed, yarn's nodemanager can remove application's local-dirs.but all executors' metadata of completed application havenot be removed. now it let yarn ShuffleService to have much more memory to store Executors' ShuffleInfo. so these metadata need to be removed.",,apachespark,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 29 12:44:06 UTC 2014,,,,,,,,,,"0|i23tfr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"29/Dec/14 12:44;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3828;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Search SPARK_CONF_DIR first when --properties-file is not specified,SPARK-4990,12763994,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,29/Dec/14 08:08,10/Jan/15 01:10,14/Jul/23 06:26,10/Jan/15 01:10,1.0.0,,,,,,,1.3.0,,,,,,Deploy,,,,0,,,,,,"In SparkSubmitArguments we first search SPARK_CONF_DIR then SPARK_HOME/conf directory to find ""spark-defaults.conf"" file.
I think in spark-submit script it should follow the same rule also.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 29 09:15:46 UTC 2014,,,,,,,,,,"0|i23tbr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"29/Dec/14 08:21;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3823;;;","29/Dec/14 09:15;srowen;As you say though, this case is already handled. Why also handle it in the script?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong application configuration cause cluster down in standalone mode,SPARK-4989,12763993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,29/Dec/14 08:04,30/Apr/15 12:51,14/Jul/23 06:26,30/Apr/15 12:51,1.0.0,1.1.0,1.2.0,,,,,1.1.2,1.2.1,1.3.0,,,,Deploy,Spark Core,,,0,,,,,,"when enabling eventlog in standalone mode, if give the wrong configuration, the standalone cluster will down (cause master restart, lose connection with workers). 

How to reproduce: just give an invalid value to ""spark.eventLog.dir"", for example: *spark.eventLog.dir=hdfs://tmp/logdir1, hdfs://tmp/logdir2*. This will throw illegalArgumentException, which will cause the *Master* restart. And the whole cluster is not available.

This is not acceptable that cluster is crashed by one application's wrong setting.",,apachespark,joshrosen,liyezhang556520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 16:39:56 UTC 2015,,,,,,,,,,"0|i23tbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/14 08:24;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/3824;;;","29/Dec/14 08:33;liyezhang556520;A following up JIRA is opened for resolving the Cluster's resume. Please see [SPARK-4891|https://issues.apache.org/jira/browse/SPARK-4991];;;","29/Dec/14 19:00;joshrosen;If you have a full stacktrace, can you past it into this issue's description?  This makes it easier to search for issues if users have seen that particular exception.;;;","30/Dec/14 02:02;liyezhang556520;Assume that we set *spark.eventLog.dir=hdfs://host:port/user/username/eventLog0, hdfs://host:port/user/username/eventLog1*, when starting the application, application will fail due to the exception, like following:
{code}
Exception in thread ""main"" java.net.URISyntaxException: Illegal character in path at index 41: hdfs//host:port/user/username/eventLog0, hdfs://host:port/user/username/eventLog1
        at java.net.URI$Parser.fail(URI.java:2829)
        at java.net.URI$Parser.checkChars(URI.java:3002)
        at java.net.URI$Parser.parseHierarchical(URI.java:3086)
        at java.net.URI$Parser.parse(URI.java:3034)
        at java.net.URI.<init>(URI.java:595)
        at org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:64)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:357)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
        at com.intel.benchmark.wordcount.Driver$.main(Driver.scala:44)
        at com.intel.benchmark.wordcount.Driver.main(Driver.scala)
{code}

This will cause the *Master* to remove the application and rebuild the sparkUI. In which will also throw the exception to *Master Actor*, which will lead to *Master*'s restart. The exception will be like the following:
{code}
14/12/30 09:48:18 ERROR OneForOneStrategy: Illegal character in path at index 41: hdfs://sr148:9000/user/liyezhan/eventLog, hdfs://sr148:9000/user/liyezhan/eventLog1
java.net.URISyntaxException: Illegal character in path at index 41: hdfs//host:port/user/username/eventLog0, hdfs://host:port/user/username/eventLog1
  at java.net.URI$Parser.fail(URI.java:2829)
  at java.net.URI$Parser.checkChars(URI.java:3002)
  at java.net.URI$Parser.parseHierarchical(URI.java:3086)
  at java.net.URI$Parser.parse(URI.java:3034)
  at java.net.URI.<init>(URI.java:595)
  at org.apache.spark.util.Utils$.resolveURI(Utils.scala:1561)
  at org.apache.spark.scheduler.EventLoggingListener$.getLogPath(EventLoggingListener.scala:262)
  at org.apache.spark.deploy.master.Master$$anonfun$11.apply(Master.scala:724)
  at org.apache.spark.deploy.master.Master$$anonfun$11.apply(Master.scala:724)
  at scala.Option.map(Option.scala:145)
  at org.apache.spark.deploy.master.Master.rebuildSparkUI(Master.scala:724)
  at org.apache.spark.deploy.master.Master.removeApplication(Master.scala:695)
  at org.apache.spark.deploy.master.Master.finishApplication(Master.scala:673)
  at org.apache.spark.deploy.master.Master$$anonfun$receiveWithLogging$1$$anonfun$applyOrElse$29.apply(Master.scala:419)
  at org.apache.spark.deploy.master.Master$$anonfun$receiveWithLogging$1$$anonfun$applyOrElse$29.apply(Master.scala:419)
  at scala.Option.foreach(Option.scala:236)
  at org.apache.spark.deploy.master.Master$$anonfun$receiveWithLogging$1.applyOrElse(Master.scala:419)
  at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
  at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
  at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
  at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:53)
  at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
  at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
  at org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
  at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
  at org.apache.spark.deploy.master.Master.aroundReceive(Master.scala:51)
  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
  at akka.actor.ActorCell.invoke(ActorCell.scala:487)
  at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
  at akka.dispatch.Mailbox.run(Mailbox.scala:220)
  at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code};;;","09/Jan/15 06:34;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/3969;;;","09/Jan/15 06:37;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/3970;;;","09/Jan/15 07:10;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/3971;;;","25/Mar/15 16:39;srowen;Just keeping score: this has been merged all the way back to 1.1.x; open question now is whether to merge the 1.0 edition: https://github.com/apache/spark/pull/3971

I bring it up again just because of the recent conversation on dev@ about back-porting, to maybe up to 2 minor versions ago. I am guessing 1.0.3 will not be released at this point, so this could be closed, but I don't feel strongly about it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Create table ..as select ..from..order by .. limit 10"" report error when one col is a Decimal",SPARK-4988,12763987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,guowei2,guowei2,29/Dec/14 07:48,06/Aug/15 23:20,14/Jul/23 06:26,06/Aug/15 23:20,,,,,,,,1.5.0,,,,,,SQL,,,,2,,,,,,"A table 'test' with a decimal type col.
create table test1 as select * from test order by a limit 10;

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.ClassCastException: scala.math.BigDecimal cannot be cast to org.apache.spark.sql.catalyst.types.decimal.Decimal
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$2.apply(HiveInspectors.scala:339)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$2.apply(HiveInspectors.scala:339)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:111)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:108)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:108)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:87)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:87)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)",,apachespark,davies,guowei2,kul,smolav,sthotaibeam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5166,,,,,,,,,,,,,,,,"26/Jun/15 23:26;sthotaibeam;spark-4988-1.txt;https://issues.apache.org/jira/secure/attachment/12742259/spark-4988-1.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 23:20:15 UTC 2015,,,,,,,,,,"0|i23ta7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/14 07:54;guowei2;in `ScalaReflection.scala`, method `convertToScala`  `case (d: Decimal, _: DecimalType) => d.toBigDecimal`.

so, `HiveShim.createDecimal(o.asInstanceOf[Decimal].toBigDecimal.underlying())` in `HiveInspectors.scala` report error

;;;","29/Dec/14 08:03;apachespark;User 'guowei2' has created a pull request for this issue:
https://github.com/apache/spark/pull/3821;;;","23/Jan/15 11:37;kul;This is also happening also for comparison
select * from tbl where decfield > 2

One more issue with decimal types is that when a udf is used against a decimal type the argument to udf comes as scala BigDecimal instead of JavaBigDecimal when using java api.;;;","26/Jun/15 23:26;sthotaibeam;SQL statements and data;;;","26/Jun/15 23:26;sthotaibeam;Both the spark-sql statements are working. 
Here is the listing if some one want to test it. The same is attached in a file too.

--------
1. Data file :

bash-3.2$ cat t1.txt
1,Barney,10.5
2,Nancy,7.5
3,Tony,4.5
5,Fred,3.5
6,Alok,12.5
7,Jan,23.5
8,Barbara,11.5
9,Mike,6.4
10,Deron,3.7
11,Glenn,9.9
12,Seth,7.8
13,Gerome,4.5
14,Alan,34.5
15,Rohan,33.7
16,clifford,3.5
17,Rosstin,1.5

2. Create table with decimal data type.

CREATE EXTERNAL TABLE user(id INT, name STRING, fico Decimal(4,2)) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION '/Users/sudhakarthota/tmp';

3. Check if all the data is appearing for select statement.

park-sql> select * from user;
1	Barney	10.5
2	Nancy	7.5
3	Tony	4.5
5	Fred	3.5
6	Alok	12.5
7	Jan	23.5
8	Barbara	11.5
9	Mike	6.4
10	Deron	3.7
11	Glenn	9.9
12	Seth	7.8
13	Gerome	4.5
14	Alan	34.5
15	Rohan	33.7
16	clifford	3.5
17	Rosstin	1.5
Time taken: 0.062 seconds, Fetched 16 row(s)

4. Create table test1 that was failing:
create table test1 as select * from user order by fico limit 10;

spark-sql> create table test1 as select * from user order by fico limit 10;
rmr: DEPRECATED: Please use 'rm -r' instead.
Deleted file:///user/hive/warehouse/test1
Time taken: 0.223 seconds
spark-sql> 

5. select records.

spark-sql> select * from  test1;                                           
17	Rosstin	1.5
16	clifford	3.5
5	Fred	3.5
10	Deron	3.7
3	Tony	4.5
13	Gerome	4.5
9	Mike	6.4
2	Nancy	7.5
12	Seth	7.8
11	Glenn	9.9
Time taken: 0.055 seconds, Fetched 10 row(s)
spark-sql> 


6. Do the second test that was failing

spark-sql> select * from user where fico >2;
1	Barney	10.5
2	Nancy	7.5
3	Tony	4.5
5	Fred	3.5
6	Alok	12.5
7	Jan	23.5
8	Barbara	11.5
9	Mike	6.4
10	Deron	3.7
11	Glenn	9.9
12	Seth	7.8
13	Gerome	4.5
14	Alan	34.5
15	Rohan	33.7
16	clifford	3.5
Time taken: 0.061 seconds, Fetched 15 row(s)
spark-sql> 
 
------

Thanks
Sudhakar Thota;;;","06/Aug/15 23:20;davies;This should be fixed after cleaning up Row and InternalRow stuff.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Graceful shutdown for Spark Streaming does not work in Standalone cluster mode,SPARK-4986,12763984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,koudelka,koudelka,koudelka,29/Dec/14 07:28,30/Apr/15 15:57,14/Jul/23 06:26,03/Feb/15 23:23,1.2.0,,,,,,,1.2.2,1.3.0,,,,,DStreams,,,,0,,,,,,"When using the graceful stop API of Spark Streaming in Spark Standalone cluster the stop signal never reaches the receivers. I have tested this with Spark 1.2 and Kafka receivers. 

ReceiverTracker will send StopReceiver message to ReceiverSupervisorImpl.
In local mode ReceiverSupervisorImpl receives this message but in Standalone cluster mode the message seems to be lost.

(I have modified the code to send my own string message as a stop signal from ReceiverTracker to ReceiverSupervisorImpl and it works as a workaround.)",,apachespark,joshrosen,koeninger,koudelka,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4545,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 14:46:40 UTC 2015,,,,,,,,,,"0|i23t9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/14 05:20;koudelka;I am currently using this patch as a work around.
https://gist.github.com/cleaton/dad2633ec56403fdd331

Still don't know why the original StopReceiver object can not be sent as a akka message in cluster mode.;;;","30/Dec/14 09:31;koudelka;Can anyone else confirm confirm that this is an issue?;;;","31/Dec/14 06:32;koudelka;I have updated the gist link with a modified patch. Added code to wait for the receiver job to be terminated before shutting down.;;;","31/Dec/14 09:19;joshrosen;I think I found one potential cause of why this works in local mode but not on a real cluster, which I've described at SPARK-5035.;;;","01/Jan/15 00:06;tdas;[~koudelka] I have merged SPARK-5035, could you check whether this issue is solved in your environment?
;;;","04/Jan/15 01:35;apachespark;User 'cleaton' has created a pull request for this issue:
https://github.com/apache/spark/pull/3868;;;","25/Jan/15 16:15;srowen;[~koudelka] This sounds pretty related to the issue I raised in SPARK-4545. I'm not sure, but your changes might address the problem too -- graceful shutdown takes a long time if no batch succeeds. If you have a moment would you mind taking a look? If it's really a subset of the problem you're solving we can close the other one.;;;","26/Jan/15 01:29;koudelka;[~srowen] The current patch I have does not address this issue, but looking at the code I think a check for eventActor == null might do the trick.

""  // eventActor is created when generator starts.
  // This not being null means the scheduler has been started and not stopped
  private var eventActor: ActorRef = null
"";;;","03/Feb/15 14:46;apachespark;User 'cleaton' has created a pull request for this issue:
https://github.com/apache/spark/pull/4338;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sleep() before tagging EC2 instances to allow instance metadata to propagate,SPARK-4983,12763963,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gen,nchammas,nchammas,29/Dec/14 03:46,03/Jun/15 18:35,14/Jul/23 06:26,06/Feb/15 21:29,1.2.0,,,,,,,1.2.2,1.3.0,,,,,EC2,,,,0,starter,,,,,"We launch EC2 instances in {{spark-ec2}} and then immediately tag them in a separate boto call. Sometimes, EC2 doesn't get enough time to propagate information about the just-launched instances, so when we go to tag them we get a server that doesn't know about them yet.

This yields the following type of error:

{code}
Launching instances...
Launched 1 slaves in us-east-1b, regid = r-cf780321
Launched master in us-east-1b, regid = r-da7e0534
Traceback (most recent call last):
  File ""./ec2/spark_ec2.py"", line 1284, in <module>
    main()
  File ""./ec2/spark_ec2.py"", line 1276, in main
    real_main()
  File ""./ec2/spark_ec2.py"", line 1122, in real_main
    (master_nodes, slave_nodes) = launch_cluster(conn, opts, cluster_name)
  File ""./ec2/spark_ec2.py"", line 646, in launch_cluster
    value='{cn}-master-{iid}'.format(cn=cluster_name, iid=master.id))
  File "".../spark/ec2/lib/boto-2.34.0/boto/ec2/ec2object.py"", line 80, in add_tag
    self.add_tags({key: value}, dry_run)
  File "".../spark/ec2/lib/boto-2.34.0/boto/ec2/ec2object.py"", line 97, in add_tags
    dry_run=dry_run
  File "".../spark/ec2/lib/boto-2.34.0/boto/ec2/connection.py"", line 4202, in create_tags
    return self.get_status('CreateTags', params, verb='POST')
  File "".../spark/ec2/lib/boto-2.34.0/boto/connection.py"", line 1223, in get_status
    raise self.ResponseError(response.status, response.reason, body)
boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request
<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidInstanceID.NotFound</Code><Message>The instance ID 'i-585219a6' does not exist</Message></Error></Errors><RequestID>b9f1ad6e-59b9-47fd-a693-527be1f779eb</RequestID></Response>
{code}

The solution is to tag the instances in the same call that launches them, or less desirably, tag the instances after some short wait.",,apachespark,gen,joshrosen,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7900,,,,,,SPARK-3332,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 18:35:39 UTC 2015,,,,,,,,,,"0|i23t4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/14 03:47;nchammas;FYI [~shivaram], [~joshrosen];;;","29/Dec/14 22:07;joshrosen;Did the original code tag them in a separate call because our older Boto version didn't support it?  Or was this just something that we had overlooked?;;;","30/Dec/14 02:02;nchammas;It's something we overlooked. I'm not sure if Boto supports tagging in the same call (whether in our current version or the old one), but I'll look into it since I was the one who added the tagging feature in the first place.;;;","08/Jan/15 21:36;gen;By boto, we can only tag instance after it launched, to my best knowledge. Therefore, I think that we can resolve this problem by using try except else statement. If you want, I can try to do this.
 ;;;","08/Jan/15 22:32;nchammas;Yeah, I took a quick look at the boto API and it looks like they have to be separate calls.

Sure, go ahead [~gen].  This should be a simple problem to solve.;;;","09/Jan/15 23:10;apachespark;User 'GenTang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3986;;;","06/Feb/15 19:02;nchammas;To summarize the discussion we had in [the PR for this issue|https://github.com/apache/spark/pull/3986], we found it was simpler to simply wait a few seconds after launching the instances for metadata to propagate, rather than insert this ugly try/except/wait/retry logic everywhere.;;;","06/Feb/15 21:29;joshrosen;Issue resolved by pull request 3986
[https://github.com/apache/spark/pull/3986];;;","03/Jun/15 18:35;nchammas;Per the discussion on [SPARK-7900], I think we should increase the wait time from the current 5 seconds to, say, 15 or 30 seconds.

An alternative proposed on [SPARK-7900] is to make fewer tagging calls, since the extra calls seem to make it more likely the we get metadata errors from AWS (like, ""instance ID not found"" right after AWS itself has given us the instance ID).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveInspectorSuite test failure,SPARK-4975,12763880,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,27/Dec/14 14:11,30/Dec/14 19:30,14/Jul/23 06:26,30/Dec/14 19:30,1.2.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"HiveInspectorSuite test failure：
[info] - wrap / unwrap null, constant null and writables *** FAILED *** (21 milliseconds)
[info]   1 did not equal 0 (HiveInspectorSuite.scala:136)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite.checkValues(HiveInspectorSuite.scala:136)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite$$anonfun$checkValues$1.apply(HiveInspectorSuite.scala:124)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite$$anonfun$checkValues$1.apply(HiveInspectorSuite.scala:123)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
[info]   at scala.collection.AbstractTraversable.map(Traversable.scala:105)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite.checkValues(HiveInspectorSuite.scala:123)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite$$anonfun$3.apply$mcV$sp(HiveInspectorSuite.scala:163)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite$$anonfun$3.apply(HiveInspectorSuite.scala:148)
[info]   at org.apache.spark.sql.hive.HiveInspectorSuite$$anonfun$3.apply(HiveInspectorSuite.scala:148)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 19:30:58 UTC 2014,,,,,,,,,,"0|i23sn3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"27/Dec/14 14:22;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3814;;;","30/Dec/14 19:30;marmbrus;Issue resolved by pull request 3814
[https://github.com/apache/spark/pull/3814];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local directory in the driver of client-mode continues remaining even if application finished when external shuffle is enabled,SPARK-4973,12763814,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,26/Dec/14 12:00,08/Jan/15 21:43,14/Jul/23 06:26,08/Jan/15 21:43,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Block Manager,Spark Core,,,0,,,,,,"When we enables external shuffle service, local directories in the driver of client-mode continue remaining even if application has finished.
I think local directories for drivers should be deleted.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 26 12:02:57 UTC 2014,,,,,,,,,,"0|i23s93:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"26/Dec/14 12:02;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SparkSQL] java.lang.UnsupportedOperationException when hive partition doesn't exist and order by and limit are used,SPARK-4968,12763774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sb58,sb58,25/Dec/14 16:20,29/Dec/14 21:50,14/Jul/23 06:26,29/Dec/14 21:50,1.1.1,,,,,,,1.2.1,1.3.0,,,,,SQL,,,,2,,,,,,"Create table with partitions
run query for partition which doesn't exist and contains order by and limit

I am running queries in hiveContext

1. Create hive table
{code}
create table if not exists testTable (ID1 BIGINT, ID2 BIGINT,Start_Time STRING, End_Time STRING) PARTITIONED BY (Region STRING,Market STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
{code}

2. Create data
{code}
1,2,""2014-11-01"",""2014-11-02""
2,3,""2014-11-01"",""2014-11-02""
3,4,""2014-11-01"",""2014-11-02""
{code}

3. Load data in hive
{code}
LOAD DATA LOCAL INPATH '/tmp/input.txt' OVERWRITE INTO TABLE testTable PARTITION (Region=""North"", market='market1');
{code}

4. run query
{code}
SELECT * FROM testTable WHERE market = 'market2' ORDER BY End_Time DESC LIMIT 100;


Error trace
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:863)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:863)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:863)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1136)
	at org.apache.spark.sql.execution.TakeOrdered.executeCollect(basicOperators.scala:171)
	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:438)
{code}","Spark 1.1.1
scala - 2.10.2
hive metastore db - pgsql
OS- Linux",apachespark,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 29 19:20:41 UTC 2014,,,,,,,,,,"0|i23s07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/14 19:20;apachespark;User 'saucam' has created a pull request for this issue:
https://github.com/apache/spark/pull/3830;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The MemoryOverhead value is not correct,SPARK-4966,12763744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,meiyoula,meiyoula,meiyoula,25/Dec/14 07:58,29/Dec/14 14:21,14/Jul/23 06:26,29/Dec/14 14:21,,,,,,,,1.2.1,1.3.0,,,,,YARN,,,,0,,,,,,"The value of amMemory and executorMemory is changed in parseArgs method. But the amMemoryOverhead and executorMemoryOverhead are setted before i. So the MemoryOverhead will not changed if amMemory changed, also “math.max” has no meaning.",,apachespark,meiyoula,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 25 08:01:58 UTC 2014,,,,,,,,,,"0|i23rtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Dec/14 08:01;apachespark;User 'XuTingjun' has created a pull request for this issue:
https://github.com/apache/spark/pull/3797;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaRDD.sample may return wrong results,SPARK-4963,12763739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,lian cheng,lian cheng,25/Dec/14 07:19,10/Jan/15 22:20,14/Jul/23 06:26,10/Jan/15 22:20,1.2.0,,,,,,,1.3.0,,,,,,SQL,,,,1,,,,,,"This {{sbt/sbt hive/console}} session can easily reproduce this issue:
{code}
sql(""SELECT * FROM src WHERE key % 2 = 0"").
  sample(withReplacement = false, fraction = 0.05).
  registerTempTable(""sampled"")

println(table(""sampled"").queryExecution)

val query = sql(""SELECT * FROM sampled WHERE key % 2 = 1"")
println(query.queryExecution)

// Should print `true'
println((1 to 10).map(_ => query.collect().isEmpty).reduce(_ && _))
{code}
Notice that when fraction is less than 0.4, {{GapSamplingIterator}} is used to do the sampling. My guess is that there’s something to do with the underlying mutable row objects used in {{HiveTableScan}}, but haven't figured out the root cause.",,apachespark,lian cheng,marmbrus,mengxr,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 10 22:20:20 UTC 2015,,,,,,,,,,"0|i23rsf:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"26/Dec/14 14:59;yanboliang;I also hit this bug.
In my experiment, last row of each partitions will be collect whether it pass predictive in Filter or not.
Further clue is different iterator behavior of  GenericRow & SpecificMutableRow.
I'm working on this issue,  [~lian cheng] could you assign to me and I will try to submit fix. ;;;","27/Dec/14 08:25;lian cheng;[~yanboliang] Unfortunately I don't have the privilege to assign JIRA ticket :( You can just start woking on it. And the fact that the last row of every partition will be collect clearly indicates mutable row is the root cause.;;;","29/Dec/14 09:45;apachespark;User 'yanbohappy' has created a pull request for this issue:
https://github.com/apache/spark/pull/3827;;;","29/Dec/14 09:58;yanboliang;SchemaRDD.sample() return wrong results due to GapSamplingIterator operating on mutable row.
HiveTableScan make RDD with SpecificMutableRow and SchemaRDD.sample() will return GapSamplingIterator for iterating.

override def next(): T = {
val r = data.next()
advance
r
}

GapSamplingIterator.next() return the current underlying element and assigned it to r.
However if the underlying iterator is mutable row just like what HiveTableScan returned, underlying iterator and r will point to the same object.
After advance operation, we drop some underlying elments and it also changed r which is not expected. Then we return the wrong value different from initial r.

To fix this issue, the most direct way is to make HiveTableScan return mutable row with copy just like the initial commit that I have made. This solution will make HiveTableScan can not get the full advantage of reusable MutableRow, but it can make sample operation return correct result.
Further more, we need to investigate GapSamplingIterator.next() and make it can implement copy operation inside it. To achieve this, we should define every elements that RDD can store implement the function like cloneable and it will make huge change.;;;","29/Dec/14 19:38;mengxr;[~yanboliang] Thanks for looking into this issue! I've assigned the JIRA to you.

1. What's the overhead of making HiveTableScan return mutable row with copy?
2. Is this issue also applies to other operations rather than sample? For example,  a user may operate directly on a SchemaRDD.;;;","30/Dec/14 04:30;lian cheng;[~yanboliang] Making {{HiveTableScan}} return copied mutable does fix this issue, but I'm afraid there can be noticeable performance regression. Would you mind to do a simple benchmark using code in [#758|https://github.com/apache/spark/pull/758]?

I'm thinking maybe we can introduce a new {{Copy}} physical operator which inserts {{_.copy}} whenever an operator that may cache mutable row(s) as intermediate result (like {{Sample}} and {{Sort}}) is found. I'd expect this operator to simplify and unify all ad-hoc mutable row copying code. [~marmbrus] What do you think?;;;","30/Dec/14 04:37;marmbrus;We could create a new operator, but the problem here is that we sometimes use operator specific logic to decide when to copy.  For example, we do this in exchange to avoid copies when the shuffle is going to be hash-based.  For that reason I think it might be okay to just do .map(_.copy()) before calling spark's sample method.;;;","30/Dec/14 04:48;lian cheng;OK I agree. However the {{\_.copy()}} call should be added in the SQL {{Sample}} operator rather than Spark's {{RDD.sample}} method, since the element type of an arbitrary RDD doesn't necessarily to be a {{Product}} and {{\_.copy()}} is not available.;;;","30/Dec/14 04:50;marmbrus;Yeah, agree.  Lets add it to SQL's Sample operator.;;;","30/Dec/14 08:26;yanboliang;[~mengxr] 
1. Just like Cheng & Michael's suggestion, add Copy operation at SQL's Sample operator is more reasonable. 
I had made a new commit to update at this PR and it have no effect on HiveTableScan.
2. This issue may not take effect on other SQL operators because of they are all iterating continuously without skipping or dropping.
But if the user want to skip or drop some elements when iterate over the SchemaRDD, it may produce wrong result. 
So users should pay more attention(eg. execute copy ahead) when using SchemaRDD if its underlying is based on mutable row.

As far as I know, ParquetFile/ParquetRelation is also hit this issue due to it also read data and deserialize to SpecificMutableRow.
However the new commit I have submit can also resolve it.
;;;","30/Dec/14 08:34;yanboliang;[~liancheng] & [~marmbrus]
Thank you for your suggestion.
I also agree to add copy to SQL's Sample operator and had submit the new patch.
Meantime I also found that ParquetFile/ParquetRelation is also hit this issue due to it also read data and deserialize to SpecificMutableRow. The new patch resolve it also. ;;;","30/Dec/14 18:33;marmbrus;Mutability is an internal optimization and we always copy at boundaries where we expose data to the user.  We should not remove it from parquet or hive table scan because it greatly improves performance.;;;","08/Jan/15 10:18;yanboliang;Can anyone verify and merge this patch? It's a bug appeared frequently and fix it asap will be better.;;;","08/Jan/15 13:58;lian cheng;Commented the PR, thanks for working on this!;;;","10/Jan/15 22:20;marmbrus;Issue resolved by pull request 3827
[https://github.com/apache/spark/pull/3827];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attributes are case sensitive when using a select query from a projection,SPARK-4959,12763725,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,chenghao,andyk,andyk,24/Dec/14 22:37,30/Jan/15 00:44,14/Jul/23 06:26,30/Jan/15 00:44,1.2.0,,,,,,,1.2.1,1.3.0,,,,,SQL,,,,0,backport-needed,,,,,"Per [~marmbrus], see this line of code, where we should be using an attribute map
 https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L147

To reproduce, i ran the following in the Spark shell:

{code}
import sqlContext._
sql(""drop table if exists test"")
sql(""create table test (col1 string)"")
sql(""""""insert into table test select ""hi"" from prejoined limit 1"""""")
val projection = ""col1"".attr.as(Symbol(""CaseSensitiveColName"")) :: ""col1"".attr.as(Symbol(""CaseSensitiveColName2"")) :: Nil
sqlContext.table(""test"").select(projection:_*).registerTempTable(""test2"")

# This succeeds.
sql(""select CaseSensitiveColName from test2"").first()

# This fails with java.util.NoSuchElementException: key not found: casesensitivecolname#23046
sql(""select casesensitivecolname from test2"").first()
{code}

The full stack trace printed for the final command that is failing: 
{code}
java.util.NoSuchElementException: key not found: casesensitivecolname#23046
	at scala.collection.MapLike$class.default(MapLike.scala:228)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:29)
	at scala.collection.MapLike$class.apply(MapLike.scala:141)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:29)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveTableScan.<init>(HiveTableScan.scala:57)
	at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$$anonfun$14.apply(HiveStrategies.scala:221)
	at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$$anonfun$14.apply(HiveStrategies.scala:221)
	at org.apache.spark.sql.SQLContext$SparkPlanner.pruneFilterProject(SQLContext.scala:378)
	at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$.apply(HiveStrategies.scala:217)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)
	at org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:285)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:418)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:416)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:422)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:422)
	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:444)
	at org.apache.spark.sql.SchemaRDD.take(SchemaRDD.scala:446)
	at org.apache.spark.sql.SchemaRDD.take(SchemaRDD.scala:108)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1093)
{code}",,andyk,apachespark,donnchadh,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 06:49:20 UTC 2015,,,,,,,,,,"0|i23rpb:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"25/Dec/14 06:44;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3796;;;","13/Jan/15 00:57;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4013;;;","21/Jan/15 06:49;pwendell;Excuse my last comment, it was on the wrong JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation doesn't work in YARN cluster mode,SPARK-4955,12763661,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lianhuiwang,chengxiang li,chengxiang li,24/Dec/14 09:16,28/Jan/15 20:54,14/Jul/23 06:26,28/Jan/15 20:54,1.2.0,,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"With executor dynamic scaling enabled, in yarn-cluster mode, after query finished and spark.dynamicAllocation.executorIdleTimeout interval, executor number is not reduced to configured min number.",,apachespark,chengxiang li,lianhuiwang,sandyr,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3145,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 09 02:28:40 UTC 2015,,,,,,,,,,"0|i23rbb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"24/Dec/14 09:29;chengxiang li;I verified this feature with Hive on Spark, it works well in yarn-client mode, while failed to reduce to min executor number after idle timeout in yarn-cluster mode, here is the related appmaster log:
{noformat}
14/12/24 16:12:50 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill executor(s) 2
14/12/24 16:12:50 WARN cluster.YarnClusterSchedulerBackend: Attempted to kill executors before the AM has registered!
14/12/24 16:12:50 WARN spark.ExecutorAllocationManager: Unable to reach the cluster manager to kill executor 2!
14/12/24 16:12:50 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill executor(s) 1
14/12/24 16:12:50 WARN cluster.YarnClusterSchedulerBackend: Attempted to kill executors before the AM has registered!
14/12/24 16:12:50 WARN spark.ExecutorAllocationManager: Unable to reach the cluster manager to kill executor 1!
14/12/24 16:12:50 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill executor(s) 4
14/12/24 16:12:50 WARN cluster.YarnClusterSchedulerBackend: Attempted to kill executors before the AM has registered!
14/12/24 16:12:50 WARN spark.ExecutorAllocationManager: Unable to reach the cluster manager to kill executor 4!
14/12/24 16:12:50 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill executor(s) 3
14/12/24 16:12:50 WARN cluster.YarnClusterSchedulerBackend: Attempted to kill executors before the AM has registered!
14/12/24 16:12:50 WARN spark.ExecutorAllocationManager: Unable to reach the cluster manager to kill executor 3!
{noformat};;;","24/Dec/14 09:30;chengxiang li;cc:[~andrewor14];;;","08/Jan/15 12:54;lianhuiwang;yes, because YarnSchedulerActor cannot connect with applicationMaster, i will look at how to resolve it.;;;","09/Jan/15 02:28;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3962;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the description of building Spark with YARN,SPARK-4953,12763643,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,24/Dec/14 07:07,25/Dec/14 15:06,14/Jul/23 06:26,25/Dec/14 15:06,1.3.0,,,,,,,1.3.0,,,,,,Documentation,,,,0,,,,,,"At the section ""Specifying the Hadoop Version"" In building-spark.md, there is description about building with YARN with Hadoop 0.23.
Spark 1.3.0 will not support Hadoop 0.23 so we should fix the description.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 24 07:09:10 UTC 2014,,,,,,,,,,"0|i23r7b:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"24/Dec/14 07:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3787;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle ConcurrentModificationExceptions in SparkEnv.environmentDetails ,SPARK-4952,12763642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,24/Dec/14 06:42,08/Mar/15 14:17,14/Jul/23 06:26,27/Dec/14 07:32,1.1.0,,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,YARN,,,1,,,,,,"the log:
{noformat}
Exception in thread ""main"" 14/12/24 12:00:25 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> tuan200, PROXY_URI_BASES -> http://host:9082/proxy/application_1414231702825_488625), /proxy/application_1414231702825_488625
java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1167)
        at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$3.next(Wrappers.scala:458)
        at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$3.next(Wrappers.scala:454)
        at scala.collection.Iterator$class.toStream(Iterator.scala:1143)
        at scala.collection.AbstractIterator.toStream(Iterator.scala:1157)
        at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1143)
        at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1143)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1085)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1077)
        at scala.collection.immutable.Stream$$anonfun$filteredTail$1.apply(Stream.scala:1149)
        at scala.collection.immutable.Stream$$anonfun$filteredTail$1.apply(Stream.scala:1149)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1085)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1077)
        at scala.collection.immutable.Stream.length(Stream.scala:284)
        at scala.collection.SeqLike$class.sorted(SeqLike.scala:608)
        at scala.collection.AbstractSeq.sorted(Seq.scala:40)
        at org.apache.spark.SparkEnv$.environmentDetails(SparkEnv.scala:324)
        at org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:1319)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:344)
        at com.zhe800.toona.als.computation.DealCF$.main(DealCF.scala:497)
        at com.zhe800.toona.als.computation.DealCF.main(DealCF.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:329)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
14/12/24 12:00:25 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
{noformat}",,apachespark,gq,jkleckner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3457,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 24 08:00:51 UTC 2014,,,,,,,,,,"0|i23r73:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,"24/Dec/14 08:00;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3788;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A busy executor may be killed when dynamicAllocation is enabled,SPARK-4951,12763639,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,24/Dec/14 05:29,12/Jan/15 00:24,14/Jul/23 06:26,12/Jan/15 00:24,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"If a task runs more than `spark.dynamicAllocation.executorIdleTimeout`, the executor which runs this task will be killed.

The following steps (yarn-client mode) can reproduce this bug:
1. Start `spark-shell` using
{code}
./bin/spark-shell --conf ""spark.shuffle.service.enabled=true"" \
    --conf ""spark.dynamicAllocation.minExecutors=1"" \
    --conf ""spark.dynamicAllocation.maxExecutors=4"" \
    --conf ""spark.dynamicAllocation.enabled=true"" \
    --conf ""spark.dynamicAllocation.executorIdleTimeout=30"" \
    --master yarn-client \
    --driver-memory 512m \
    --executor-memory 512m \
    --executor-cores 1
{code}

2. Wait more than 30 seconds until there is only one executor.
3. Run the following code (a task needs at least 50 seconds to finish)
{code}
val r = sc.parallelize(1 to 1000, 20).map{t => Thread.sleep(1000); t}.groupBy(_ % 2).collect()
{code}
4. Executors will be killed and allocated all the time, which makes the Job fail.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 24 05:35:55 UTC 2014,,,,,,,,,,"0|i23r6n:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"24/Dec/14 05:35;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3783;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shutdownCallback in SparkDeploySchedulerBackend should be enclosed by synchronized block.,SPARK-4949,12763627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,24/Dec/14 04:32,17/May/20 17:48,14/Jul/23 06:26,18/Feb/15 12:20,1.2.1,,,,,,,1.4.0,,,,,,Scheduler,Spark Core,,,0,,,,,,A variable `shutdownCallback` in SparkDeploySchedulerBackend can be accessed from multiple threads so it should be enclosed by synchronized block.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 12:20:23 UTC 2015,,,,,,,,,,"0|i23r47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/14 04:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3781;;;","18/Feb/15 12:20;srowen;Issue resolved by pull request 3781
[https://github.com/apache/spark/pull/3781];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing error for query with table name having dot,SPARK-4943,12763602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,alexliu68,alexliu68,24/Dec/14 01:19,15/Jan/15 14:17,14/Jul/23 06:26,10/Jan/15 21:42,1.2.0,,,,,,,1.2.1,1.3.0,,,,,SQL,,,,8,,,,,,"When integrating Spark 1.2.0 with Cassandra SQL, the following query is broken. It was working for Spark 1.1.0 version. Basically we use the table name having dot to include database name 

{code}
[info]   java.lang.RuntimeException: [1.29] failure: ``UNION'' expected but `.' found
[info] 

[info] SELECT test1.a FROM sql_test.test1 AS test1 UNION DISTINCT SELECT test2.a FROM sql_test.test2 AS test2
[info]                             ^
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:33)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
[info]   at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:174)
[info]   at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:173)
[info]   at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
[info]   at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
[info]   at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
[info]   at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
[info]   at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
[info]   at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
[info]   at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
[info]   at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
[info]   at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:83)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:83)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:83)
[info]   at org.apache.spark.sql.cassandra.CassandraSQLContext.cassandraSql(CassandraSQLContext.scala:53)
[info]   at org.apache.spark.sql.cassandra.CassandraSQLContext.sql(CassandraSQLContext.scala:56)
[info]   at com.datastax.spark.connector.sql.CassandraSQLSpec$$anonfun$20.apply$mcV$sp(CassandraSQLSpec.scala:169)
[info]   at com.datastax.spark.connector.sql.CassandraSQLSpec$$anonfun$20.apply(CassandraSQLSpec.scala:168)
[info]   at com.datastax.spark.connector.sql.CassandraSQLSpec$$anonfun$20.apply(CassandraSQLSpec.scala:168)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1647)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1683)
[info]   at org.scalatest.FlatSpecLike$class.invokeWithFixture$1(FlatSpecLike.scala:1644)
[info]   at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1656)
[info]   at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1656)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FlatSpecLike$class.runTest(FlatSpecLike.scala:1656)
[info]   at org.scalatest.FlatSpec.runTest(FlatSpec.scala:1683)
[info]   at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1714)
[info]   at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1714)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FlatSpecLike$class.runTests(FlatSpecLike.scala:1714)
[info]   at org.scalatest.FlatSpec.runTests(FlatSpec.scala:1683)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FlatSpec.org$scalatest$FlatSpecLike$$super$run(FlatSpec.scala:1683)
[info]   at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1760)
[info]   at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1760)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FlatSpecLike$class.run(FlatSpecLike.scala:1760)
[info]   at org.scalatest.FlatSpec.run(FlatSpec.scala:1683)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:466)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:677)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
[info] - should allow to select rows with union distinct clause *** FAILED *** (46 milliseconds)
[info]   java.lang.RuntimeException: [1.29] failure: ``UNION'' expected but `.' found
{code}",,alexliu68,apachespark,bcantoni,glenn.strycker@gmail.com,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 15 14:17:46 UTC 2015,,,,,,,,,,"0|i23qyv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/14 19:38;apachespark;User 'alexliu68' has created a pull request for this issue:
https://github.com/apache/spark/pull/3848;;;","31/Dec/14 20:25;marmbrus;One possible approach here is to change the signature of UnresolvedRelation as follows:

{code}
case class UnresolvedRelation(tableIdentifier: Seq[String], alias: Option[String])
{code}

This way we can leave parsing and handling of backticks up to the parser and let the catalogs interpret the identifiers in a system dependent way.;;;","31/Dec/14 21:26;alexliu68;The seq can be like 

{code}
tableName, databaseName, clusterName, catalog
{code}

First element is tableName, the next one is databaseName, then clusterName and catalog(option).


SQL query looks like

{code}
SELECT test_table.column1 FROM catalog.cluster.database.table AS test_table 
SELECT test_table.column1 FROM cluster.database.table AS test_table 
SELECT test_table.column1 FROM database.table AS test_table 
SELECT table.column1 FROM table
{code}

Potentially we should not use AS clause here.

Parser only allow maximum four levels in full table name. [catalog].[cluster].[database].[table];;;","02/Jan/15 20:55;alexliu68;Should we also change the signatures of Catalog methods to use {code}tableIdentifier: Seq[String] {code} instead of {code}db: Option[String], tableName: String{code}?

{code}

  def tableExists(db: Option[String], tableName: String): Boolean

  def lookupRelation(
    databaseName: Option[String],
    tableName: String,
    alias: Option[String] = None): LogicalPlan

  def registerTable(databaseName: Option[String], tableName: String, plan: LogicalPlan): Unit

  def unregisterTable(databaseName: Option[String], tableName: String): Unit

  def unregisterAllTables(): Unit

  protected def processDatabaseAndTableName(
      databaseName: Option[String],
      tableName: String): (Option[String], String)
{code};;;","05/Jan/15 20:59;alexliu68;The approach of {code}
case class UnresolvedRelation(tableIdentifier: Seq[String], alias: Option[String])
{code} is a little unclear about what's stored in tableIdentifier by simply reading the code.

Another approach is storing  catalog.cluster.database in databaseName and tableName in tableName and keep case class no change
{code}
case class UnresolvedRelation(databaseName: Option[String], tableName: String, alias: Option[String])
{code}
so no API changes.

If we keep clusterName as a separate parameter, then API changes to 
{code}
case class UnresolvedRelation(clusterName: Option[String], databaseName: Option[String], tableName: String, alias: Option[String])
{code}

Catalog API  needs change accordingly

;;;","05/Jan/15 21:44;marmbrus;I wouldn't say that the notion of {{tableIdentifier: Seq\[String\]}} is unclear.  Instead I would say that it is deliberately unspecified in order to be flexible.  Some systems have {{clusters}}, some systems have {{databases}}, some systems have {{schema}}, some have {{tables}}.  Thus, this API gives us one interface for communicating between the parser and the underlying datastore that makes no assumption about how that datastore is laid out.

If we do make this change then yes, I agree that we should also make it in the catalog as well.  In general our handling of this has always been a little clunky since there is a whole bunch of code that just ignores the database field.

One question is: what parts of the table identifier Spark SQL handles and what parts we pass on to the datasource?  A goal here should be to be able to connect to and join data from multiple sources.  Here is what I would propose as an addition to the current API, which only lets you register individual tables.

 - Users can register external catalogs which are responsible for producing {{BaseRelation}}s.
 - Each external catalog has a user specified name that is given when registering.
 - There is a notion of the current catalog, which can be changed with {{USE}}.  By default, we pass the all the {{tableIdentifiers}} to this default catalog and its up to it to determine what each part means.
 - Users can also specify fully qualified tables when joining multiple data sources.  We detect this case when the first {{tableIdentifier}} matches one of the registered catalogs.  In this case we strip of the catalog name and pass the remaining {{tableIdentifiers}} to the specified catalog.

What do you think?;;;","05/Jan/15 22:13;alexliu68;Catalog part of table identifier should be handled by Spark SQL which calls the registered catalog Context to connect to the underline datasources. cluster/database/scheme/table should be handled by datasource(Cassandra Spark SQL integration can handle cluster, database and table level join).

e.g.
{code}
SELECT test1.a, test1.b, test2.c FROM cassandra.cluster.database.table1 AS test1
    LEFT OUTER JOIN mySql.cluster.database.table2 AS test2 ON test1.a = test2.a
{code}

so cluster.database.table1 is passed to cassandra catalog datasource, cassandra is handled by Spark SQL to call cassandraContext which then call the underline datasource.

cluster.database.table2 is passed to mySql catalog datasource, mySql is handled by Spark SQL to call the mySqlContext which then call the underline datasource.


If USE command is used, then all tableIdentifiers are passed to datasource.  e.g.
{code}
USE cassandra
SELECT test1.a, test1.b, test2.c FROM cluster1.database.table AS test1
    LEFT OUTER JOIN cluster2.database.table AS test2 ON test1.a = test2.a
{code}

cluster1.database.table1 and cluster2.database.table are passed to cassandra datasource

;;;","05/Jan/15 22:19;alexliu68;For each catalog, the configuration settings should start with catalog name. e.g.
{noformat}
set cassandra.cluster.database.table.ttl = 1000
set cassandra.database.table.ttl =1000 (default cluster)
set mysql.cluster.database.table.xxx = 200
{noformat}

If there's no catalog in the setting string, use the default catalog.;;;","05/Jan/15 23:30;marmbrus;Thanks for your comments Alex.  Are you proposing any changes to what I said?

Another thing I'm confused about is your comment regarding joins.  As of now there is no public API for passing that kind of information down into a datasource.

Regarding the configuration.  We will pass the datasource a SQLContext and you can do {{.getConf}} using whatever arbitrary string you want.  I don't think Spark SQL needs to have any control here.;;;","05/Jan/15 23:42;alexliu68;No changes to your approach. 

Regarding cluster1.database.table1 and cluster2.database.table passing to datasources. they are set as tableIdentifier and tableIdentifier is passed to catalog.lookupRelation method where datasource can use it.;;;","08/Jan/15 05:13;apachespark;User 'alexliu68' has created a pull request for this issue:
https://github.com/apache/spark/pull/3941;;;","10/Jan/15 21:42;marmbrus;Issue resolved by pull request 3941
[https://github.com/apache/spark/pull/3941];;;","15/Jan/15 14:17;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/4062;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python updateStateByKey example hang in local mode,SPARK-4939,12763508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,23/Dec/14 18:19,21/May/15 23:34,14/Jul/23 06:26,04/Feb/15 22:25,1.2.1,,,,,,,1.2.2,1.3.0,,,,,DStreams,PySpark,Spark Core,,0,,,,,,,,apachespark,davies,kayousterhout,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4383,,SPARK-6232,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 23:34:03 UTC 2015,,,,,,,,,,"0|i23qdz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"23/Dec/14 20:05;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3779;;;","21/Jan/15 20:49;pwendell;[~tdas] [~davies] [~kayousterhout] Because there is still discussion about this, and this is modifying a very complex component in Spark, I'm not going to block on this for 1.2.1. Once we merge a patch we can decide whether to put it into 1.2 based on what the final patch looks like. It is definitely inconvenient that this doesn't work in local mode, but much less of a problem than introducing a bug in the scheduler for production cluster workloads.

As a workaround we could suggest running this example with local-cluster.;;;","21/Jan/15 20:50;kayousterhout;[~pwendell] just want to make sure you understand that there's a simple but slightly hacky change here that only modifies the local scheduler.  Just wanted to point that out in case that changes the dynamics of whether this should be fixed for 1.2.;;;","21/Jan/15 22:33;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4147;;;","21/Jan/15 22:34;davies;Sent out a PR to change the local scheduler to revive offers periodically, it should be safer to be merged into 1.2.;;;","21/Jan/15 23:21;davies;Another workaround may be change the spark.locality.wait to 0, but it's bad to have this in a example.;;;","04/Feb/15 22:25;kayousterhout;Fixed by https://github.com/apache/spark/commit/0a89b156850fc5ba93160987927f249a7e633d51;;;","21/May/15 23:34;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6337;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When hive.cli.print.header configured, spark-sql aborted if passed in a invalid sql",SPARK-4935,12763427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,23/Dec/14 09:53,30/Dec/14 21:44,14/Jul/23 06:26,30/Dec/14 21:44,1.1.0,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"When hive.cli.print.header configured, spark-sql aborted if passed in a invalid sql",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 21:44:37 UTC 2014,,,,,,,,,,"0|i23pw7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"23/Dec/14 09:55;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3761;;;","30/Dec/14 21:44;marmbrus;Issue resolved by pull request 3761
[https://github.com/apache/spark/pull/3761];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn Client mode can not support the HA after the exitcode change,SPARK-4929,12763373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,carlmartin,carlmartin,23/Dec/14 04:15,07/Jan/15 14:11,14/Jul/23 06:26,07/Jan/15 14:11,1.2.0,,,,,,,1.2.1,1.3.0,,,,,YARN,,,,0,,,,,,"Nowadays, yarn-client will exit directly when the HA change happens no matter how many times the am should retry.",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 04:22:45 UTC 2014,,,,,,,,,,"0|i23pkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/14 04:22;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3771;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Operator "">,<,>=,<="" with decimal between different precision report error",SPARK-4928,12763361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,guowei2,guowei2,23/Dec/14 02:33,30/Dec/14 20:37,14/Jul/23 06:26,30/Dec/14 20:37,1.3.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"create table test (a Decimal(10,1));
select * from test where a>1;

WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Types do not match DecimalType(10,1) != DecimalType(10,0), tree: (input[0] > 1)
	at org.apache.spark.sql.catalyst.expressions.Expression.c2(Expression.scala:249)
	at org.apache.spark.sql.catalyst.expressions.GreaterThan.eval(predicates.scala:204)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$apply$1.apply(predicates.scala:30)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$apply$1.apply(predicates.scala:30)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:794)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:794)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1324)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1324)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)",,apachespark,guowei2,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 20:37:43 UTC 2014,,,,,,,,,,"0|i23phr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/14 02:49;apachespark;User 'guowei2' has created a pull request for this issue:
https://github.com/apache/spark/pull/3767;;;","30/Dec/14 20:37;marmbrus;Issue resolved by pull request 3767
[https://github.com/apache/spark/pull/3767];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Developer API to REPL to allow re-publishing the REPL jar,SPARK-4923,12763321,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,senkwich,peng,peng,22/Dec/14 22:07,29/Jan/15 08:13,14/Jul/23 06:26,20/Jan/15 21:36,1.2.0,,,,,,,1.3.0,,,,,,Build,Spark Shell,,,6,shell,,,,,"Spark-repl installation and deployment has been discontinued (see SPARK-3452). But its in the dependency list of a few projects that extends its initialization process.
Please remove the 'skip' setting in spark-repl and make it an 'official' API to encourage more platform to integrate with it.",,apachespark,bzz,etmjansen,helena_e,jerryshao,jlewandowski,kkoster,moon,nchammas,noootsab,peng,pwendell,senkwich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,SPARK-5289,,,,,,,,,,,,,,"23/Dec/14 01:46;peng;SPARK-4923__Maven_build_should_keep_publishing_spark-repl.patch;https://issues.apache.org/jira/secure/attachment/12688770/SPARK-4923__Maven_build_should_keep_publishing_spark-repl.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 21:36:26 UTC 2015,,,,,,,,,,"0|i23p93:",9223372036854775807,,,,,pwendell,,,,,,,,,1.3.0,,,,,,,,,,,"22/Dec/14 23:20;pwendell;Hey [~pc175@uowmail.edu.au] - we removed this from Maven because it's not meant as a stable API in Spark. Could you talk about which parts of the repl API you are using and how you are using it?;;;","22/Dec/14 23:42;peng;Hey Patrick,

The following API has been integrated since 1.0.0, IMHO they are stable enough for daily prototyping, creating case class used to be defective but has been fixed long time ago.
SparkILoop.getAddedJars()
$SparkIMain.bind
$SparkIMain.quietBind
$SparkIMain.interpret
end of :)

At first I assume that further development on it has been moved to databricks cloud. But the JIRA ticket was already there in September. So maybe demand on this API from the community is indeed low enough.
However, I would still suggest keeping it, even promoting it into a Developer's API, this would encourage more projects to integrate in a more flexible way, and save prototyping/QA cost by customizing fixtures of REPL. People will still move to databricks cloud, which has far more features than that. Many influential projects already depends on the routinely published Scala-REPL (e.g. playFW), it would be strange for Spark not doing the same.
What do you think? ;;;","22/Dec/14 23:45;peng;Sorry my project is https://github.com/tribbloid/ISpark;;;","23/Dec/14 01:02;pwendell;Hey [~pc175@uowmail.edu.au], thanks for filling that in - I didn't even realize we had code in there that was bytecode public. By stable I meant that we are promising it is an unchanging API. This is what we usually think about when we release things. For 1.2.0 I refactored our build and found out that we were publishing a bunch of random internal build components, so I took them all out of the published artifacts (examples, our assembly jar, etc) in SPARK-4923.

Anyways - perhaps we could just annotate these as developer API's and be clear that they might change in the future. If you wanted to do that, and re-enable publishing them, I'd be happy to do that.;;;","23/Dec/14 01:46;peng;thank you so much! First patch uploaded;;;","25/Dec/14 07:24;jerryshao;Hey [~pwendell], for some projects like Zeppelin(http://zeppelin-project.org/) depend on this repl jar to work, it would be very nice to publish this jar publicly, as for 1.1 version Spark actually published it. This changes will make several similar projects fail to update to latest Spark version.;;;","25/Dec/14 14:53;pwendell;Yes - I can retro-actively publish this one for Spark 1.2.;;;","27/Dec/14 11:09;noootsab;Thanks [~pwendell], when done, I'll start supporting this release it in the [Spark Notebook|https://github.com/andypetrella/spark-notebook] as well.;;;","30/Dec/14 15:53;peng;Hey, a guy waiting for it was complaining that the version number of spark-repl_2.10 on typesafe repo is actually 1.2.0-SNAPSHOT, not the released 1.2.0
Is it done on purpose?;;;","30/Dec/14 16:04;srowen;The typesafe repo would not be where the artifacts are officially released. I don't see any 1.2.x artifacts for the REPL anywhere, including Maven Central. That's the point of this JIRA. If someone published a snapshot release to a different repo I think you'd best ask them.;;;","30/Dec/14 16:30;etmjansen;I am sorry for any misunderstanding. I did not find the SNAPSHOT in the typesafe repo, but indeed somewhere else on GitHub.;;;","30/Dec/14 23:24;helena_e;We are updating https://github.com/datastax/spark-cassandra-connector which integrates with the REPL, as does DSE, so this is a blocker for our upgrade to spark 1.2.0 as well.;;;","03/Jan/15 07:33;senkwich;FYI, this is a blocker for us as well: https://github.com/ibm-et/spark-kernel

Specific issue: https://github.com/ibm-et/spark-kernel/issues/12

We are using quite a few different public methods from SparkIMain (such as valueOfTerm to pull out variables from the interpreter), not just interpret and bind. The API markings suggested by [~peng] would not be enough for us, [~pwendell].;;;","03/Jan/15 14:00;bzz;Just FYI, same for us here https://github.com/NFLabs/zeppelin/issues/260;;;","04/Jan/15 00:03;peng;You are right, in fact 'Dev's API' simply means method is susceptible to changes without deprecation or notice, which the main 3 markings will be least likely to undergo.
Could you please edit the patch and add more markings?;;;","12/Jan/15 21:58;pwendell;Hey All,

Sorry this has caused a disruption. As I said in the earlier comment. if anyone on these projects can submit a patch that locks down the visibility in that package and opening up things that are specifically needed, I'm fine to keep publishing it (and do so retro-actively for 1.2). We just need to look closely at what we are exposing because this package currently violates Spark's API policy. Because the Scala repl does not itself offer any kind of API stability, it will be hard for Spark to do same. But I think it's fine to just annotate and expose unstable API's here, provided projects understand the implications of depending on them.

[~senkwich] - since you guys are probably the heaviest user, would you be willing to take a crack at this? Basically start by making everything private and then go and unlock things that you need as Developer API's.

- Patrick;;;","12/Jan/15 22:07;senkwich;[~pwendell], I can definitely do that. Would you prefer a patch in the same form as the one attached? Or would it be better to create a pull request on Github for this with the changes?;;;","12/Jan/15 22:10;pwendell;[~senkwich] definitely prefer github.;;;","12/Jan/15 22:12;senkwich;Okay, I'll do that and update this JIRA once I've submitted the pull request.;;;","14/Jan/15 06:33;apachespark;User 'rcsenkbeil' has created a pull request for this issue:
https://github.com/apache/spark/pull/4034;;;","14/Jan/15 06:36;senkwich;As the nice bot has stated, I created a pull request for this issue. I detailed why I marked each method/field public and provided Scaladocs for each of them to make the exposure of the REPL API a little nicer.

As stated in the pull request, I only tackled Scala 2.10 for now as the Scala 2.11 did not ""appear"" to be ready, although I could easily be mistaken. I merely glanced at the SparkIMain and noticed that it did not have the class server declaration to ship the compiled class files nor was it - or any of the other classes - in the org.apache.spark.repl package.;;;","20/Jan/15 21:36;pwendell;I updated the title of this to reflect the work that actually happened in Chip's patch. And SPARK-5289 is tracking publishing of the artifacts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Two sets of datanucleus versions left in lib_managed after running dev/run-tests,SPARK-4914,12763130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,21/Dec/14 16:24,23/Dec/14 20:54,14/Jul/23 06:26,23/Dec/14 20:54,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Build,,,,0,,,,,,"The {{dev/run-tests}} script first does a clean compile with Hive 0.12.0, and then builds assembly jar for unit testing with Hive 0.13.1 *without* cleaning. This left two sets of datanucleus jars under the {{lib_managed}} folder, one set depended by Hive 0.12.0, another by Hive 0.13.1.

This behavior sometimes messes up class paths and makes {{CliSuite}} and {{HiveThriftServer2Suite}} fail, because these two suites spawn external processes that depends on those datanucleus jars.",,apachespark,joshrosen,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 20:54:47 UTC 2014,,,,,,,,,,"0|i23o3z:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"21/Dec/14 16:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3756;;;","23/Dec/14 20:54;joshrosen;Issue resolved by pull request 3756
[https://github.com/apache/spark/pull/3756];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect event log path,SPARK-4913,12763129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,21/Dec/14 16:14,23/Dec/14 22:59,14/Jul/23 06:26,23/Dec/14 22:59,1.3.0,,,,,,,1.3.0,,,,,,,,,,0,,,,,,"SPARK-2261 uses a single file to log events for an app. `eventLogDir` in `ApplicationDescription` is replaced with `eventLogFile`. However, `ApplicationDescription` in `SparkDeploySchedulerBackend` is initialized with `SparkContext`'s `eventLogDir`. It is just the log directory, not the actual log file path. `Master.rebuildSparkUI` can not correctly rebuild a new SparkUI for the app.

Because the `ApplicationDescription` is remotely registered with `Master` and the app's id is then generated in `Master`, we can not get the app id in advance before registration. So the received description needs to be modified with correct `eventLogFile` value.",,apachespark,joshrosen,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4933,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 22:59:35 UTC 2014,,,,,,,,,,"0|i23o3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/14 16:16;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/3755;;;","23/Dec/14 22:59;joshrosen;Issue resolved by pull request 3755
[https://github.com/apache/spark/pull/3755];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Build failed with Hadoop 1.x profile: ""isFile is not a member of org.apache.hadoop.fs.FileStatus""",SPARK-4910,12763071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,xhudik,xhudik,20/Dec/14 14:08,21/Dec/14 21:20,14/Jul/23 06:26,21/Dec/14 21:20,1.3.0,,,,,,,1.3.0,,,,,,,,,,0,build-failure,,,,,"git clone
mvn  -DskipTests clean package

....
[ERROR] /home/x/sources/spark_git/core/src/test/scala/org/apache/spark/scheduler/EventLoggingListenerSuite.scala:68: value isFile is not a member of org.apache.hadoop.fs.FileStatus
[ERROR]     assert(logStatus.isFile)
[ERROR]                      ^
[ERROR] /home/x/sources/spark_git/core/src/test/scala/org/apache/spark/scheduler/EventLoggingListenerSuite.scala:72: value isFile is not a member of org.apache.hadoop.fs.FileStatus
[ERROR]     assert(fileSystem.getFileStatus(new Path(eventLogger.logPath)).isFile())
[ERROR]                                                                    ^
[ERROR] /home/x/sources/spark_git/core/src/test/scala/org/apache/spark/scheduler/ReplayListenerSuite.scala:115: value isFile is not a member of org.apache.hadoop.fs.FileStatus
[ERROR]     assert(eventLog.isFile)
[ERROR]                     ^
[ERROR] three errors found
[DEBUG] Compilation failed (CompilerInterface)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [ 13.440 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 23.752 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 18.842 s]
[INFO] Spark Project Core ................................. FAILURE [06:34 min]
[INFO] Spark Project Bagel ................................ SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED

even switch -DskipTests was provided it seems some tests are executed and failed","trunk: 1.3.0-SNAPSHOT
https://github.com/apache/spark/commit/a764960b3b6d842eef7fa4777c8fa99d3f60fa1e",apachespark,joshrosen,xhudik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 21 21:20:33 UTC 2014,,,,,,,,,,"0|i23nrj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/14 18:36;srowen;Confirmed, I think the commit for SPARK-2261 (https://github.com/apache/spark/commit/456451911d11cc0b6738f31b1e17869b1fb51c87) is actually using Hadoop 2-only methods.;;;","21/Dec/14 10:51;xhudik;@Sean - you are right. It is working for hadoop2:
mvn -Dhadoop.version=2.2.0 -Phadoop-2.2 -DskipTests clean package
;;;","21/Dec/14 11:14;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3754;;;","21/Dec/14 21:20;joshrosen;This was fixed by Sean's pull request.

For reference, this build failure was introduced in https://github.com/apache/spark/pull/1222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL built for Hive 13 fails under concurrent metadata queries,SPARK-4908,12762999,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,dyross,dyross,19/Dec/14 21:45,12/Jan/15 07:11,14/Jul/23 06:26,30/Dec/14 19:25,,,,,,,,1.2.1,1.3.0,,,,,SQL,,,,3,,,,,,"We are trunk: {{1.3.0-SNAPSHOT}}, as of this commit: 
https://github.com/apache/spark/commit/3d0c37b8118f6057a663f959321a79b8061132b6

We are using Spark built for Hive 13, using this option:
{{-Phive-0.13.1}}

In single-threaded mode, normal operations look fine. However, under concurrency, with at least 2 concurrent connections, metadata queries fail.

For example, {{USE some_db}}, {{SHOW TABLES}}, and the implicit {{USE}} statement when you pass a default schema in the JDBC URL, all fail.

{{SELECT}} queries like {{SELECT * FROM some_table}} do not have this issue.

Here is some example code:

{code}
object main extends App {
  import java.sql._
  import scala.concurrent._
  import scala.concurrent.duration._
  import scala.concurrent.ExecutionContext.Implicits.global

  Class.forName(""org.apache.hive.jdbc.HiveDriver"")

  val host = ""localhost"" // update this
  val url = s""jdbc:hive2://${host}:10511/some_db"" // update this

  val future = Future.traverse(1 to 3) { i =>
    Future {
      println(""Starting: "" + i)
      try {
        val conn = DriverManager.getConnection(url)
      } catch {
        case e: Throwable => e.printStackTrace()
        println(""Failed: "" + i)
      }
      println(""Finishing: "" + i)
    }
  }

  Await.result(future, 2.minutes)

  println(""done!"")
}
{code}

Here is the output:

{code}
Starting: 1
Starting: 3
Starting: 2
java.sql.SQLException: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Operation cancelled
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:121)
	at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:109)
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:231)
	at org.apache.hive.jdbc.HiveConnection.configureConnection(HiveConnection.java:451)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:195)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:270)
	at com.atscale.engine.connection.pool.main$$anonfun$30$$anonfun$apply$2.apply$mcV$sp(ConnectionManager.scala:896)
	at com.atscale.engine.connection.pool.main$$anonfun$30$$anonfun$apply$2.apply(ConnectionManager.scala:893)
	at com.atscale.engine.connection.pool.main$$anonfun$30$$anonfun$apply$2.apply(ConnectionManager.scala:893)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Failed: 3
Finishing: 3
java.sql.SQLException: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Operation cancelled
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:121)
	at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:109)
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:231)
	at org.apache.hive.jdbc.HiveConnection.configureConnection(HiveConnection.java:451)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:195)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:270)
	at com.atscale.engine.connection.pool.main$$anonfun$30$$anonfun$apply$2.apply$mcV$sp(ConnectionManager.scala:896)
	at com.atscale.engine.connection.pool.main$$anonfun$30$$anonfun$apply$2.apply(ConnectionManager.scala:893)
	at com.atscale.engine.connection.pool.main$$anonfun$30$$anonfun$apply$2.apply(ConnectionManager.scala:893)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Failed: 2
Finishing: 2
Finishing: 1
done!
{code}

Here are the errors from Spark Logs:
{code}
14/12/19 21:44:55 INFO thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO thriftserver.SparkExecuteStatementOperation: Running query 'use as_adventure'
14/12/19 21:44:55 INFO parse.ParseDriver: Parsing command: use as_adventure
14/12/19 21:44:55 INFO parse.ParseDriver: Parse Completed
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO parse.ParseDriver: Parsing command: use as_adventure
14/12/19 21:44:55 INFO parse.ParseDriver: Parse Completed
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=parse start=1419025495084 end=1419025495084 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Semantic Analysis Completed
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1419025495084 end=1419025495084 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=compile start=1419025495084 end=1419025495085 duration=1 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Starting command: use as_adventure
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1419025495084 end=1419025495085 duration=1 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=runTasks start=1419025495085 end=1419025495098 duration=13 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1419025495085 end=1419025495098 duration=13 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: OK
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1419025495098 end=1419025495098 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1419025495084 end=1419025495098 duration=14 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
14/12/19 21:44:55 INFO thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO thriftserver.SparkExecuteStatementOperation: Running query 'use as_adventure'
14/12/19 21:44:55 INFO thriftserver.SparkExecuteStatementOperation: Running query 'use as_adventure'
14/12/19 21:44:55 INFO thriftserver.SparkExecuteStatementOperation: Result Schema: List(result#274)
14/12/19 21:44:55 INFO parse.ParseDriver: Parsing command: use as_adventure
14/12/19 21:44:55 INFO parse.ParseDriver: Parse Completed
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO parse.ParseDriver: Parsing command: use as_adventure
14/12/19 21:44:55 INFO parse.ParseDriver: Parse Completed
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=parse start=1419025495165 end=1419025495165 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Semantic Analysis Completed
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1419025495165 end=1419025495166 duration=1 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=compile start=1419025495165 end=1419025495166 duration=1 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Starting command: use as_adventure
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1419025495165 end=1419025495166 duration=1 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO parse.ParseDriver: Parsing command: use as_adventure
14/12/19 21:44:55 INFO parse.ParseDriver: Parse Completed
14/12/19 21:44:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 WARN ql.Driver: Shutting down task : Stage-0:DDL
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO parse.ParseDriver: Parsing command: use as_adventure
14/12/19 21:44:55 INFO parse.ParseDriver: Parse Completed
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=parse start=1419025495173 end=1419025495174 duration=1 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Semantic Analysis Completed
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1419025495174 end=1419025495174 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=compile start=1419025495172 end=1419025495177 duration=5 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO ql.Driver: Starting command: use as_adventure
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1419025495172 end=1419025495177 duration=5 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=runTasks start=1419025495167 end=1419025495188 duration=21 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 ERROR ql.Driver: FAILED: Operation cancelled
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1419025495166 end=1419025495189 duration=23 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1419025495189 end=1419025495189 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 WARN ql.Driver: Shutting down task : Stage-0:DDL
14/12/19 21:44:55 ERROR hive.HiveContext:
======================
HIVE FAILURE OUTPUT
======================
RDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

FAILED: Hive Internal Error: org.apache.hadoop.hive.ql.metadata.HiveException(FAILED: Operation cancelled)
org.apache.hadoop.hive.ql.metadata.HiveException: FAILED: Operation cancelled
	at org.apache.hadoop.hive.ql.DriverContext.checkShutdown(DriverContext.java:125)
	at org.apache.hadoop.hive.ql.DriverContext.launching(DriverContext.java:91)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1497)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

OK
OK
OK
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1194)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled

======================
END HIVE FAILURE OUTPUT
======================

14/12/19 21:44:55 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query:
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Operation cancelled
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:309)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
14/12/19 21:44:55 WARN thrift.ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Operation cancelled
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:192)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=runTasks start=1419025495177 end=1419025495197 duration=20 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 ERROR ql.Driver: FAILED: Operation cancelled
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1419025495177 end=1419025495200 duration=23 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1419025495200 end=1419025495200 duration=0 from=org.apache.hadoop.hive.ql.Driver>
14/12/19 21:44:55 ERROR hive.HiveContext:
======================
HIVE FAILURE OUTPUT
======================
ache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

FAILED: Hive Internal Error: org.apache.hadoop.hive.ql.metadata.HiveException(FAILED: Operation cancelled)
org.apache.hadoop.hive.ql.metadata.HiveException: FAILED: Operation cancelled
	at org.apache.hadoop.hive.ql.DriverContext.checkShutdown(DriverContext.java:125)
	at org.apache.hadoop.hive.ql.DriverContext.launching(DriverContext.java:91)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1497)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

OK
OK
OK
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1194)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
OK
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled
OK
FAILED: Operation cancelled
FAILED: Operation cancelled

======================
END HIVE FAILURE OUTPUT
======================

14/12/19 21:44:55 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query:
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Operation cancelled
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:309)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
14/12/19 21:44:55 WARN thrift.ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Operation cancelled
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:192)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,dyross,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 07:11:26 UTC 2015,,,,,,,,,,"0|i23nbj:",9223372036854775807,,,,,,,,,,,,,,1.2.1,,,,,,,,,,,"22/Dec/14 20:43;dyross;Note that I noticed this line in the logs that seems to come from Hive logging (not spark code):

{code}
14/12/19 21:44:55 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager
{code}

It seems to be tied to this config:
https://github.com/apache/hive/blob/branch-0.13/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L719

I have this to our {{hive-site.xml}} in the spark {{conf}} directory:

{code}
<property>
  <name>hive.support.concurrency</name>
  <value>true</value>
</property>
{code}

And I still have the issue.

Perhaps there is more I need to do to support concurrency?;;;","29/Dec/14 22:29;marmbrus;You don't even need the JDBC server to cause the problem.  This seems to fail as well:

{code}
(1 to 100).par.map { _ => 
  sql(""USE default"")
  sql(""SHOW TABLES"")
}
{code};;;","30/Dec/14 03:11;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3834;;;","30/Dec/14 19:25;marmbrus;Issue resolved by pull request 3834
[https://github.com/apache/spark/pull/3834];;;","05/Jan/15 10:38;lian cheng;Would like to add a comment about the root cause of this issue.  When serving a HiveQL query, Spark SQL's {{HiveContext.runHive}} method gets a {{org.apache.hadoop.hive.ql.Driver}} instance via {{CommandProcessFactory.get}}, which creates and caches {{Driver}} instances. In the case of {{HiveThriftServer2}}, {{HiveContext.runHive}} is called by multiple threads owned by a threaded executor of the Thrift server. However, {{Driver}} is not thread safe, but cached {{Driver}} instance can be accessed by multiple threads, thus causes problem. PR #3834 fixes this issue by synchronizing {{HiveContext.runHive}}, which is valid.  On the other hand, HiveServer2 actually create a new {{Driver}} instance for every served SQL query when initializing a {{SQLOperation}}.

[~dyross] When built against Hive 0.12.0, Spark SQL 1.2.0 also suffers this issue. The snippet doesn't show this because Hive 0.12.0 JDBC driver doesn't execute a {{USE <db>}} statement to switch current database even if the JDBC connection URL specifies a database name. If you replace the lines in the {{try}} block with:
{code}
      val conn = DriverManager.getConnection(url)
      val stmt = conn.createStatement()
      stmt.execute(""use hello;"")
      stmt.close()
      println(""Finished: "" + i)
{code}
you'll see exactly the same exceptions.;;;","08/Jan/15 06:34;dyross;I've verified that this is fixed on trunk. Since his commit says ""just a quick fix"", I will let [~marmbrus] decide whether or not to keep this JIRA open.;;;","08/Jan/15 06:36;lian cheng;It was considered as a ""quick fix"" because we hadn't figured out the root cause when the PR was submitted. But now it turned out to be a valid fix :);;;","12/Jan/15 07:11;apachespark;User 'baishuo' has created a pull request for this issue:
https://github.com/apache/spark/pull/4001;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent loss and gradient in LeastSquaresGradient compared with R,SPARK-4907,12762988,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dbtsai,dbtsai,dbtsai,19/Dec/14 20:27,26/Dec/14 07:43,14/Jul/23 06:26,23/Dec/14 00:43,,,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"In most of the academic paper and algorithm implementations, people use L = 1/2n ||A weights-y||^2 instead of L = 1/n ||A weights-y||^2 for least-squared loss. See Eq. (1) in http://web.stanford.edu/~hastie/Papers/glmnet.pdf

Since MLlib uses different convention, this will result different residuals and all the stats properties will be different from GLMNET package in R. The model coefficients will be still the same under this change. ",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 26 07:43:27 UTC 2014,,,,,,,,,,"0|i23n9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/14 20:29;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3746;;;","22/Dec/14 17:26;srowen;PS I think you will want to update the docs too, for example, at http://spark.apache.org/docs/latest/mllib-linear-methods.html
There may be other places where the loss function formula is mentioned.;;;","23/Dec/14 00:43;mengxr;Issue resolved by pull request 3746
[https://github.com/apache/spark/pull/3746];;;","26/Dec/14 07:43;dbtsai;[~sowen] It seems that the existing document has 1/2 factor there in the formula. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.streaming.flume.FlumeStreamSuite.flume input stream,SPARK-4905,12762881,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,hshreedharan,joshrosen,joshrosen,19/Dec/14 16:20,19/Feb/15 02:48,14/Jul/23 06:26,09/Feb/15 22:17,1.3.0,,,,,,,1.2.2,1.3.0,,,,,DStreams,,,,0,flaky-test,,,,,"It looks like the ""org.apache.spark.streaming.flume.FlumeStreamSuite.flume input stream"" test might be flaky ([link|https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/24647/testReport/junit/org.apache.spark.streaming.flume/FlumeStreamSuite/flume_input_stream/]):

{code}
Error Message

The code passed to eventually never returned normally. Attempted 106 times over 10.045097243 seconds. Last failure message: ArrayBuffer("""", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """") was not equal to Vector(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"", ""47"", ""48"", ""49"", ""50"", ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""69"", ""70"", ""71"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""80"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""90"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""98"", ""99"", ""100"").
Stacktrace

sbt.ForkMain$ForkError: The code passed to eventually never returned normally. Attempted 106 times over 10.045097243 seconds. Last failure message: ArrayBuffer("""", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """") was not equal to Vector(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"", ""47"", ""48"", ""49"", ""50"", ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""69"", ""70"", ""71"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""80"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""90"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""98"", ""99"", ""100"").
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:420)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:438)
	at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
	at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
	at org.apache.spark.streaming.flume.FlumeStreamSuite.writeAndVerify(FlumeStreamSuite.scala:142)
	at org.apache.spark.streaming.flume.FlumeStreamSuite.org$apache$spark$streaming$flume$FlumeStreamSuite$$testFlumeStream(FlumeStreamSuite.scala:74)
	at org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$2.apply$mcV$sp(FlumeStreamSuite.scala:62)
	at org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$2.apply(FlumeStreamSuite.scala:62)
	at org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$2.apply(FlumeStreamSuite.scala:62)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.flume.FlumeStreamSuite.org$scalatest$BeforeAndAfter$$super$runTest(FlumeStreamSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.flume.FlumeStreamSuite.runTest(FlumeStreamSuite.scala:46)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.flume.FlumeStreamSuite.org$scalatest$BeforeAndAfter$$super$run(FlumeStreamSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.flume.FlumeStreamSuite.run(FlumeStreamSuite.scala:46)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: ArrayBuffer("""", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """", """") was not equal to Vector(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"", ""47"", ""48"", ""49"", ""50"", ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""69"", ""70"", ""71"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""80"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""90"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""98"", ""99"", ""100"")
	at org.scalatest.MatchersHelper$.newTestFailedException(MatchersHelper.scala:160)
	at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6231)
	at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6265)
	at org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$writeAndVerify$2.apply$mcV$sp(FlumeStreamSuite.scala:149)
	at org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$writeAndVerify$2.apply(FlumeStreamSuite.scala:142)
	at org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$writeAndVerify$2.apply(FlumeStreamSuite.scala:142)
	at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:394)
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:408)
	... 52 more
{code}


",,apachespark,hshreedharan,joshrosen,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 22:38:22 UTC 2015,,,,,,,,,,"0|i23mlj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/15 00:10;tdas;[~hshreedharan] Can you take a look at this please!! I have been seeing this once in a while. I has seen this when the test sent one message at a time, and to increase the chances of success, i modified the test to send one whole bunch at a time, repeatedly, until all got through or nothing got through. But it still seems to failing. I have no idea why empty string are being send when i trying to send ""1"", 2"", ""3"", etc. Please take a look.;;;","05/Jan/15 19:14;hshreedharan;Taking a look now.;;;","05/Jan/15 22:22;hshreedharan;I can't reproduce this - but once Jenkins is back, I will look at the logs to see if there is any info there.;;;","05/Jan/15 23:11;tdas;What is the reason behind such a behavior where the number of records received is same as sent, but all the records are empty?
;;;","05/Jan/15 23:25;hshreedharan;I am not sure. It might have something to do with the encoding/decoding. The events even have the headers, but the body is empty (there is comparison of the headers for each event);;;","06/Jan/15 08:32;tdas;Any insights yet?;;;","03/Feb/15 18:51;tdas;Can you please take care of this. This must be fixed before 1.3;;;","03/Feb/15 18:58;hshreedharan;Is there a recent instance of failure? I can't reproduce this at all.;;;","04/Feb/15 21:06;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4371;;;","09/Feb/15 22:17;joshrosen;Issue resolved by pull request 4371
[https://github.com/apache/spark/pull/4371];;;","09/Feb/15 22:18;joshrosen;I've merged [~hshreedharan]'s PR, so let's see if that fixes this issue.  I'm going to resolve this issue for now, but let's re-open it if we still see flakiness in this test.;;;","09/Feb/15 22:38;hshreedharan;Thanks [~joshrosen]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RDD remains cached after ""DROP TABLE""",SPARK-4903,12762847,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,evertlammerts,evertlammerts,19/Dec/14 13:44,18/Feb/15 05:26,14/Jul/23 06:26,13/Feb/15 17:49,1.3.0,,,,,,,,,,,,,SQL,,,,0,,,,,,"In beeline, when I run:
{code:sql}
CREATE TABLE test AS select col from table;
CACHE TABLE test
DROP TABLE test
{code}
The the table is removed but the RDD is still cached. Running UNCACHE is not possible anymore (table not found from metastore).",Spark master @ Dec 17 (3cd516191baadf8496ccdae499771020e89acd7e),apachespark,evertlammerts,guowei2,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 05:26:31 UTC 2015,,,,,,,,,,"0|i23mdz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"22/Dec/14 03:17;guowei2;uncache table test

it is OK on my workspace;;;","13/Feb/15 17:45;yhuai;I believe that it has been resolved in 1.3 ([see this|https://github.com/apache/spark/blob/v1.3.0-snapshot1/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala#L61]). I tried the following snippet in ""build/sbt -Phive sparkShell""and verified the cached RDD was unpersisted after I dropped it. 

{code}
sqlContext.jsonRDD(sc.parallelize(""""""{""a"":1}""""""::Nil)).registerTempTable(""test"")
sqlContext.sql(""create table jt as select a from test"")
sqlContext.sql(""cache table jt"").collect
sqlContext.sql(""select * from jt"").collect
sqlContext.sql(""drop table jt"").collect
{code};;;","13/Feb/15 17:49;yhuai;I am resolving it. It has been fixed by SPARK-4912 ([commit|https://github.com/apache/spark/commit/6463e0b9e8067cce70602c5c9006a2546856a9d6#diff-050cd38349dde9a736dfa182ba481f21R52]).;;;","18/Feb/15 05:26;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4671;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hot fix for the BytesWritable.copyBytes not exists in Hadoop1,SPARK-4901,12762825,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,19/Dec/14 11:34,19/Dec/14 16:05,14/Jul/23 06:26,19/Dec/14 16:05,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"HiveInspectors.scala failed in compiling with Hadoop 1, as the BytesWritable.copyBytes is not available in Hadoop 1. ",,apachespark,chenghao,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 16:05:04 UTC 2014,,,,,,,,,,"0|i23m93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/14 11:37;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3742;;;","19/Dec/14 16:05;joshrosen;Issue resolved by pull request 3742
[https://github.com/apache/spark/pull/3742];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLlib SingularValueDecomposition ARPACK IllegalStateException ,SPARK-4900,12762822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,mbofb,mbofb,19/Dec/14 11:23,04/Apr/15 15:55,14/Jul/23 06:26,04/Apr/15 15:54,1.1.1,1.2.0,1.2.1,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"java.lang.reflect.InvocationTargetException
        ...
Caused by: java.lang.IllegalStateException: ARPACK returns non-zero info = 3 Please refer ARPACK user guide for error message.
        at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:120)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:235)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:171)
		...","Ubuntu 1410, Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
spark local mode",apachespark,mbofb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1782,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 04 15:54:51 UTC 2015,,,,,,,,,,"0|i23m8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/14 11:01;mbofb;this exception occurs for various numbers of rows, columns, and k;;;","06/Feb/15 21:43;srowen;Do you have any more info, like how to reproduce this? what were you computing?;;;","09/Feb/15 20:05;mbofb;put a snapshot test data 1000x1000 matrix to https://dl.dropboxusercontent.com/u/8489998/test_matrix_1.zip

calling: 
			String filename = ""/custompath/27637/test_matrix_1"";
			RDD<Vector> vectorRDD = MLUtils.loadVectors(javaSparkContext.sc(), filename);
			vectorRDD.cache();
			System.out.println(""trtRowRDD.count():\t"" + vectorRDD.count());
			RowMatrix rowMatrix = new RowMatrix(vectorRDD);
			System.out.println(""rowMatrix.numRows():\t"" + rowMatrix.numRows());
			System.out.println(""rowMatrix.numCols():\t"" + rowMatrix.numCols());
			{
				int k = 10;
				boolean computeU = true;
				double rCond = 1.0E-9d;
				SingularValueDecomposition<RowMatrix, Matrix> svd = rowMatrix.computeSVD(k, computeU, rCond);
				RowMatrix u = svd.U();
				RDD<Vector> uRowsRDD = u.rows();
				System.out.println(""uRowsRDD.count():\t"" + uRowsRDD.count());
				Vector s = svd.s();
				System.out.println(""s.size():\t"" + s.size());
				Matrix v = svd.V();
				System.out.println(""v.numRows():\t"" + v.numRows());
				System.out.println(""v.numCols():\t"" + v.numCols());
			}

results in:


maxFeatureSpaceTermNumber:      1000
trtRowRDD.count():      1000
rowMatrix.numRows():    1000
rowMatrix.numCols():    1000
15/02/09 19:56:59 WARN PrimaryRunnerSpark:
java.lang.IllegalStateException: ARPACK returns non-zero info = 3 Please refer ARPACK user guide for error message.
        at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:120)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:258)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:190)
        at com.example.processing.spark.SVDProcessing2.createSVD_2(SVDProcessing2.java:184)
        at com.example.processing.spark.RunnerSpark.main(PrimaryRunnerSpark.java:27)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at sbt.Run.invokeMain(Run.scala:67)
        at sbt.Run.run0(Run.scala:61)
        at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
        at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Logger$$anon$4.apply(Logger.scala:85)
        at sbt.TrapExit$App.run(TrapExit.scala:248)
        at java.lang.Thread.run(Thread.java:745)
15/02/09 19:56:59 INFO TimeCounter: TIMER [com.example.processing.spark.PrimaryRunnerSpark] : 13.0 Seconds
TIMER [com.example.processing.spark.PrimaryRunnerSpark] : 13.0 Seconds
15/02/09 19:56:59 ERROR ContextCleaner: Error cleaning broadcast 20
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:137)
        at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:227)
        at org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)
        at org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:66)
        at org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:185)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(
ContextCleaner.scala:147)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(
ContextCleaner.scala:138)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.sc
ala:138)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134
)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134
)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1550)
        at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
        at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
15/02/09 19:56:59 ERROR Utils: Uncaught exception in thread SparkListenerBus
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1550)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)
[success] Total time: 20 s, completed Feb 9, 2015 7:56:59 PM;;;","09/Feb/15 20:11;mbofb;BTW: Matrix pc = rowMatrix.computePrincipalComponents(3); yields also an exception:

breeze.linalg.NotConvergedException:
        at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:60)
        at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:32)
        at breeze.generic.UFunc$class.apply(UFunc.scala:48)
        at breeze.linalg.svd$.apply(svd.scala:17)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponents(RowMatrix.scala:375)
        at com.example.processing.spark.SVDProcessing2.createSVD_2(SVDProcessing2.java:236)
        at com.example.processing.spark.RunnerSpark.main(PrimaryRunnerSpark.java:27)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at sbt.Run.invokeMain(Run.scala:67)
        at sbt.Run.run0(Run.scala:61)
        at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
        at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Logger$$anon$4.apply(Logger.scala:85)
        at sbt.TrapExit$App.run(TrapExit.scala:248)
        at java.lang.Thread.run(Thread.java:745);;;","09/Feb/15 20:19;srowen;So I think there is at least a small problem in the error reporting:

{code}
      info.`val` match {
        case 1 => throw new IllegalStateException(""ARPACK returns non-zero info = "" + info.`val` +
            "" Maximum number of iterations taken. (Refer ARPACK user guide for details)"")
        case 2 => throw new IllegalStateException(""ARPACK returns non-zero info = "" + info.`val` +
            "" No shifts could be applied. Try to increase NCV. "" +
            ""(Refer ARPACK user guide for details)"")
        case _ => throw new IllegalStateException(""ARPACK returns non-zero info = "" + info.`val` +
            "" Please refer ARPACK user guide for error message."")
      }
{code}

Really, what's called case 2 here corresponds to return value 3, which is what you get.

{code}
            =  0: Normal exit.
            =  1: Maximum number of iterations taken.
                  All possible eigenvalues of OP has been found. IPARAM(5)  
                  returns the number of wanted converged Ritz values.
            =  2: No longer an informational error. Deprecated starting
                  with release 2 of ARPACK.
            =  3: No shifts could be applied during a cycle of the 
                  Implicitly restarted Arnoldi iteration. One possibility 
                  is to increase the size of NCV relative to NEV. 
                  See remark 4 below.
{code}

I can fix the error message. Remark 4 that it refers to is:

{code}
    4. At present there is no a-priori analysis to guide the selection
       of NCV relative to NEV.  The only formal requrement is that NCV > NEV.
       However, it is recommended that NCV .ge. 2*NEV.  If many problems of
       the same type are to be solved, one should experiment with increasing
       NCV while keeping NEV fixed for a given test problem.  This will 
       usually decrease the required number of OP*x operations but it
       also increases the work and storage required to maintain the orthogonal
       basis vectors.   The optimal ""cross-over"" with respect to CPU time
       is problem dependent and must be determined empirically.
{code}

So I think that translates to ""k is too big"". Is the matrix low-rank?
In any event this is ultimately breeze code and I'm not sure if there's much that will done in Spark itself.;;;","09/Feb/15 20:26;srowen;I think the other message means what it says; the SVD couldn't finish finding the 3rd principal component. The implementation you're calling for PCA is simple but may not be the most accurate. It's either a function of your input, or that you'd need a different algo or set of settings when calling into Breeze to run PCA.;;;","09/Feb/15 23:15;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4485;;;","10/Feb/15 05:14;mengxr;Issue resolved by pull request 4485
[https://github.com/apache/spark/pull/4485];;;","10/Feb/15 09:12;srowen;Yeah, that fixes the error at least. I imagine the rest is a function of the input and ARPACK, but if there's evidence that something is wrong in the Spark in between, you can reopen.;;;","27/Feb/15 14:35;mbofb;i traced the problem back due to Double.NaN, Double.POSITIVE_INFINITY or  Double.NEGATIVE_INFINITY values of the java double values. In my opinion the user should care to have not such values in the data, but when an exception is thrown it should give a hint that this is a likely cause. If also other allgorithms in MLLib are not pretecting against failing due to Double.NaN, Double.POSITIVE_INFINITY or  Double.NEGATIVE_INFINITY it should be mentioned in the MLlib doc that the users has to ensure only proper double numbers.;;;","27/Feb/15 14:39;srowen;Hm, I tend to agree that upfront error checking is good, and infinite / NaN values are not valid as input. There's a bit of a question of where you check, and whether it slows things down a lot to be always checking. Would you care to propose error checking for this case? I suppose at least the error message for this ARPACK error could hint at this possible cause.;;;","27/Feb/15 14:47;mbofb;in my opinion there should be no automatic error checking but only hints on exception messages and in general MLlib Vector and Matrix doc. Maybe this should hint to an utility method (to be put somewhere) which would do such an check on vectors and matrix and which can be invoked by someone getting exceptions on his data.;;;","27/Feb/15 15:03;srowen;This is still mostly counting on the user to validate their input. How about augmenting the error message here to suggest this as a cause - would that resolve the proximate issue more completely?;;;","27/Feb/15 15:07;mbofb;			
I would suggest a printValueStatistics(Vector/Matrix) method e.g. on MLUtils which should be hinted at the SVD exeption message (and on all exception methods from algorithms which are not able to process data with such double values - maybee also the PCA exception ?)
 
which says something like

values of the vector/matrix: 
x 0 values
y values are different from 0
sparsity is z %;

(an enhanced method could report more detailed stats one the distribution which can be costly to obtain but helpfull to grasp what one actully wants to process)

Non finite values (might cause problems on some (all?) algorithms):
m values are Double.NaN 
n values are Double.POSITIVE_INFINITY
o values are Double.NEGATIVE_INFINITY
;;;","27/Feb/15 15:31;srowen;[~mengxr] what do you think on this one? I/we could implement whatever sounds good. We could keep this narrower by just implementing an explicit check on the argument to LAPACK, and also expanding its error message. I hesitate to build out more debugging utilities, even if they sound a bit useful.;;;","04/Apr/15 15:54;srowen;Given lack of follow-up I think this is as resolved as we're going to make it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a bad unittest in LogisticRegressionSuite,SPARK-4887,12762666,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dbtsai,dbtsai,dbtsai,18/Dec/14 19:46,18/Dec/14 21:56,14/Jul/23 06:26,18/Dec/14 21:56,,,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"The original test doesn't make sense since if you step in, the lossSum already NaN, and the coefficients are diverging. That's because the step size is too large for SGD, so it doesn't work. The correct behavior is that you should get smaller coefficients than the one without regularization. Comparing the values using 20000.0 relative error doesn't make sense as well. ",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 18 21:56:00 UTC 2014,,,,,,,,,,"0|i23la7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/14 19:48;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3735;;;","18/Dec/14 21:56;mengxr;Issue resolved by pull request 3735
[https://github.com/apache/spark/pull/3735];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark broadcast breaks when using KryoSerializer,SPARK-4882,12762508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,coderfi,coderfi,18/Dec/14 08:41,13/Feb/15 22:41,14/Jul/23 06:26,30/Dec/14 17:34,1.2.0,1.3.0,,,,,,1.2.1,1.3.0,,,,,PySpark,,,,0,,,,,,"When KryoSerializer is used, PySpark will throw NullPointerException when trying to send broadcast variables to workers.  This issue does not occur when the master is {{local}}, or when using the default JavaSerializer.

*Reproduction*:

Run

{code}
SPARK_LOCAL_IP=127.0.0.1 ./bin/pyspark --master local-cluster[2,2,512] --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
{code}

then run

{code}
b = sc.broadcast(""hello"")
sc.parallelize([0]).flatMap(lambda x: b.value).collect()
{code}

This job fails because all tasks throw the following exception:

{code}
14/12/28 14:26:08 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 8, localhost): java.lang.NullPointerException
	at org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:589)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(PythonRDD.scala:232)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(PythonRDD.scala:228)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:228)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:203)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:203)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1515)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:202)
{code}

KryoSerializer may be enabled in the {{spark-defaults.conf}} file, so users may hit this error and be confused.

*Workaround*:

Override the {{spark.serializer}} setting to use the default Java serializer.",,apachespark,coderfi,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5779,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 01:06:44 UTC 2014,,,,,,,,,,"0|i23kbj:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"28/Dec/14 22:33;joshrosen;I've edited this issue's description to use a reproduction which does not involve Mesos or changes to spark-defaults.conf, and to trim down the description to its bare essentials (I hope you don't mind; this makes the description easier to follow for reviewers, I think).  I suspect that the bug here involves code paths that aren't hit in {{local}} mode, which is why you weren't able to reproduce this except on an actual cluster.

To address a question raised in the original description:

{quote}
I don't even know if KryoSerializer is an appropriate setting for a pyspark program (seems like no?).
{quote}

If you're using MLlib or SparkSQL with PySpark, then Spark will sometimes need to serialize Java objects rather than byte arrays, so in principle PySpark jobs can benefit from KryoSerializer, too.
;;;","29/Dec/14 03:03;coderfi;No problem about updating the description, it's more concise and addresses the core problem.

Thanks for the KryoSerializer and PySpark clarification, the docs didn't make it obvious if it was beneficial or a NO-OP.
I figured it may help and it didn't seem to break anything, so I went ahead left set that in the spark defaults, and thus ran into this problem.

At least the workaround works and lets me leverage the broadcast feature, and in my use cases, I saw no apparent degradation in performance.

;;;","29/Dec/14 20:27;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3831;;;","30/Dec/14 17:32;joshrosen;Removing 1.0.x and 1.1.x as affected versions, since PythonBroadcast was added in 1.2.;;;","30/Dec/14 17:34;joshrosen;This was resolved by my PR for 1.3.0 and 1.2.1.;;;","31/Dec/14 01:06;coderfi;Thanks for the quick turnaround on this fix!

I'll try it out when it gets merged in to the trunk (and reenable the Kryo serializer).

thanks,
FI
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing output partitions after job completes with speculative execution,SPARK-4879,12762466,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,18/Dec/14 02:15,26/Sep/15 20:28,14/Jul/23 06:26,26/Sep/15 20:28,1.0.2,1.1.1,1.2.0,1.3.0,,,,1.3.0,,,,,,Input/Output,Spark Core,,,3,,,,,,"When speculative execution is enabled ({{spark.speculation=true}}), jobs that save output files may report that they have completed successfully even though some output partitions written by speculative tasks may be missing.

h3. Reproduction

This symptom was reported to me by a Spark user and I've been doing my own investigation to try to come up with an in-house reproduction.

I'm still working on a reliable local reproduction for this issue, which is a little tricky because Spark won't schedule speculated tasks on the same host as the original task, so you need an actual (or containerized) multi-host cluster to test speculation.  Here's a simple reproduction of some of the symptoms on EC2, which can be run in {{spark-shell}} with {{--conf spark.speculation=true}}:

{code}
    // Rig a job such that all but one of the tasks complete instantly
    // and one task runs for 20 seconds on its first attempt and instantly
    // on its second attempt:
    val numTasks = 100
    sc.parallelize(1 to numTasks, numTasks).repartition(2).mapPartitionsWithContext { case (ctx, iter) =>
      if (ctx.partitionId == 0) {  // If this is the one task that should run really slow
        if (ctx.attemptId == 0) {  // If this is the first attempt, run slow
         Thread.sleep(20 * 1000)
        }
      }
      iter
    }.map(x => (x, x)).saveAsTextFile(""/test4"")
{code}

When I run this, I end up with a job that completes quickly (due to speculation) but reports failures from the speculated task:

{code}
[...]
14/12/11 01:41:13 INFO scheduler.TaskSetManager: Finished task 37.1 in stage 3.0 (TID 411) in 131 ms on ip-172-31-8-164.us-west-2.compute.internal (100/100)
14/12/11 01:41:13 INFO scheduler.DAGScheduler: Stage 3 (saveAsTextFile at <console>:22) finished in 0.856 s
14/12/11 01:41:13 INFO spark.SparkContext: Job finished: saveAsTextFile at <console>:22, took 0.885438374 s
14/12/11 01:41:13 INFO scheduler.TaskSetManager: Ignoring task-finished event for 70.1 in stage 3.0 because task 70 has already completed successfully

scala> 14/12/11 01:41:13 WARN scheduler.TaskSetManager: Lost task 49.1 in stage 3.0 (TID 413, ip-172-31-8-164.us-west-2.compute.internal): java.io.IOException: Failed to save output of task: attempt_201412110141_0003_m_000049_413
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:160)
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172)
        org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132)
        org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:109)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:991)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

One interesting thing to note about this stack trace: if we look at {{FileOutputCommitter.java:160}} ([link|http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hadoop/hadoop-core/2.5.0-mr1-cdh5.2.0/org/apache/hadoop/mapred/FileOutputCommitter.java#160]), this point in the execution seems to correspond to a case where a task completes, attempts to commit its output, fails for some reason, then deletes the destination file, tries again, and fails:

{code}
 if (fs.isFile(taskOutput)) {
152      Path finalOutputPath = getFinalPath(jobOutputDir, taskOutput, 
153                                          getTempTaskOutputPath(context));
154      if (!fs.rename(taskOutput, finalOutputPath)) {
155        if (!fs.delete(finalOutputPath, true)) {
156          throw new IOException(""Failed to delete earlier output of task: "" + 
157                                 attemptId);
158        }
159        if (!fs.rename(taskOutput, finalOutputPath)) {
160          throw new IOException(""Failed to save output of task: "" + 
161        		  attemptId);
162        }
163      }
{code}

This could explain why the output file is missing: the second copy of the task keeps running after the job completes and deletes the output written by the other task after failing to commit its own copy of the output.

There are still a few open questions about how exactly we get into this scenario:

*Why is the second copy of the task allowed to commit its output after the other task / the job has successfully completed?*

To check whether a task's temporary output should be committed, SparkHadoopWriter calls {{FileOutputCommitter.needsTaskCommit()}}, which returns {{true}} if the tasks's temporary output exists ([link|http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hadoop/hadoop-core/2.5.0-mr1-cdh5.2.0/org/apache/hadoop/mapred/FileOutputCommitter.java#206]).  Tihs does not seem to check whether the destination already exists.  This means that {{needsTaskCommit}} can return {{true}} for speculative tasks.

*Why does the rename fail?*

I think that what's happening is that the temporary task output files are being deleted once the job has completed, which is causing the {{rename}} to fail because {{FileOutputCommitter.commitTask}} doesn't seem to guard against missing output files.

I'm not sure about this, though, since the stack trace seems to imply that the temporary output file existed.  Maybe the filesystem methods are returning stale metadata?  Maybe there's a race?  I think a race condition seems pretty unlikely, since the time-scale at which it would have to happen doesn't sync up with the scale of the timestamps that I saw in the user report.

h3. Possible Fixes:

The root problem here might be that speculative copies of tasks are somehow allowed to commit their output.  We might be able to fix this by centralizing the ""should this task commit its output"" decision at the driver.

(I have more concrete suggestions of how to do this; to be posted soon)",,aash,apachespark,darabos,darose,DjvuLee,Earne,glenn.strycker,igor.berman,joshrosen,lian cheng,lianhuiwang,litao1990,liushaohui,mcheah,mkim,mridulm80,rdub,romi-totango,stevel@apache.org,tgraves,zfry,zhpengg,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/15 23:47;zfry;speculation.txt;https://issues.apache.org/jira/secure/attachment/12690660/speculation.txt","07/Jan/15 23:47;zfry;speculation2.txt;https://issues.apache.org/jira/secure/attachment/12690661/speculation2.txt",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 13 12:01:18 UTC 2015,,,,,,,,,,"0|i23k27:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"07/Jan/15 23:47;zfry;Hey Josh, 

I have been playing around with your repro above and I think I can consistently trigger the bad behavior by just tweaking the value of {{spark.speculation.multiplier}} and {{spark.speculation.quantile}}.

I set the {{multiplier}} to be 1 and the {{quantile}} to 0.01 so that only 1% of tasks have to finish before any task that takes longer than those 1% of tasks should speculate. 
As expected, I see a lot of tasks getting speculated. 
After running the repro about 5 times, I have seen 2 errors (stack traces at the bottom and the full run from the REPL is attached with this comment). 

One thing I do notice is that the part-00000 associated with Stage 1 was always where I expected it to be in HDFS, and all lines were present (checked using a {{wc -l}})


{code}
scala> 15/01/07 13:44:26 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 119, <redacted-host-02>): java.io.IOException: The temporary job-output directory hdfs://<redacted-host-01>:8020/test6/_temporary doesn't exist!
        org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
        org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:240)
        org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:116)
        org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:980)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

{code}
15/01/07 15:17:39 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 120, <redacted-host-03>): org.apache.hadoop.ipc.RemoteException: No lease on /test7/_temporary/_attempt_201501071517_0000_m_000000_120/part-00000: File does not exist. Holder DFSClient_NONMAPREDUCE_-469253416_73 does not have any open files.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2609)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2426)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2339)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:501)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:299)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44954)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1752)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1748)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1746)

        org.apache.hadoop.ipc.Client.call(Client.java:1238)
        org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
        com.sun.proxy.$Proxy9.addBlock(Unknown Source)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
        org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
        com.sun.proxy.$Proxy9.addBlock(Unknown Source)
        org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:291)
        org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1177)
        org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1030)
        org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:488)
{code}
;;;","09/Jan/15 19:57;joshrosen;Hey Zach,

From my last round of attempts (maybe a week ago), I was able to reproduce some sort of ""No lease on ...."" error but not the missing output partitions symptom.  I'd still like to try to find a reproduction for the missing files but this could be tricky if it involves a very quick race condition; I don't think that's the case, though, since it's pretty unlikely that the speculated and original task are finishing at the exact same time.;;;","10/Jan/15 02:50;zfry;Hey Josh,

I was able to reproduce the missing file using the speculation settings in my previous comment:

{code}
scala> 15/01/09 18:33:28 WARN scheduler.TaskSetManager: Lost task 42.1 in stage 0.0 (TID 113, <redacted-03>): java.io.IOException: Failed to save output of task: attempt_201501091833_0000_m_000042_113
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:160)
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172)
        org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132)
        org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:109)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:991)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
15/01/09 18:33:47 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, <redacted-03>): java.io.IOException: The temporary job-output directory hdfs://<redacted-01>:8020/test2/_temporary doesn't exist!
        org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
        org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:240)
        org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:116)
        org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:980)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

Notice here that there are only 99 part files and part-00042 is missing (as seen in the stacktrace above)
{code}
  $ hadoop fs -ls /test2 | grep part | wc -l
99
 $ hadoop fs -ls /test2 | grep part-0004
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00040
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00041
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00043
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00044
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00045
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00046
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00047
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00048
-rw-r--r--   3 <redacted> supergroup          8 2015-01-09 18:33 /test2/part-00049
{code}

;;;","10/Jan/15 02:51;zfry;For clarity, here is the scala code I used in the REPL:
{code}
scala>     val numTasks = 100
numTasks: Int = 100

scala>     sc.parallelize(1 to numTasks, numTasks).mapPartitionsWithContext { case (ctx, iter) =>
     |       if (ctx.partitionId == 0) {  // If this is the one task that should run really slow
     |         if (ctx.attemptId == 0) {  // If this is the first attempt, run slow
     |          Thread.sleep(20 * 1000)
     |         }
     |       }
     |       iter
     |     }.map(x => (x, x)).saveAsTextFile(""/test2"")
{code};;;","12/Jan/15 19:31;joshrosen;I think that part of the reproduction issues that I had might have been due to {{attemptId}} returning a unique task attempt ID rather than the attempt number, meaning that only the _first_ run of that test in the REPL would be capable of uncovering the bug.

See https://github.com/apache/spark/pull/3849 / SPARK-4014 for more context.  I'm going to try to merge that patch today, which will let me write a reliable regression test.;;;","15/Jan/15 22:29;joshrosen;I'm not sure that SparkHadoopWriter's use of FileOutputCommitter properly obeys the OutputCommitter contracts in Hadoop.  According to the [OutputCommitter Javadoc|https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/OutputCommitter.html]

{quote}
The methods in this class can be called from several different processes and from several different contexts. It is important to know which process and which context each is called from. Each method should be marked accordingly in its documentation. It is also important to note that not all methods are guaranteed to be called once and only once. If a method is not guaranteed to have this property the output committer needs to handle this appropriately. Also note it will only be in rare situations where they may be called multiple times for the same task.
{quote}

Based on the documentation, `needsTaskCommit` "" is called from each individual task's process that will output to HDFS, and it is called just for that task."", so it seems like it should be safe to call this from SparkHadoopWriter.

However, maybe we're misusing the `commitTask` method:

{quote}
If needsTaskCommit(TaskAttemptContext) returns true and this task is the task that the AM determines finished first, this method is called to commit an individual task's output. This is to mark that tasks output as complete, as commitJob(JobContext) will also be called later on if the entire job finished successfully. This is called from a task's process. This may be called multiple times for the same task, but different task attempts. It should be very rare for this to be called multiple times and requires odd networking failures to make this happen. In the future the Hadoop framework may eliminate this race. 
{quote}

I think that we're missing the ""this task is the task that the AM determines finished first"" part of the equation here.  If `needsTaskCommit` is false, then we definitely shouldn't commit (e.g. if it's an original task that lost to a speculated copy), but if it's true then I don't think it's safe to commit; we need some central authority to pick a winner.

Let's see how Hadoop does things, working backwards from actual calls of `commitTask` to see whether they're guarded by some coordination through the AM.  It looks like `OutputCommitter` is part of the `mapred` API, so I'll only look at classes in that package:

In `Task.java`, `committer.commitTask` is only performed after checking `canCommit` through `TaskUmbilicalProtocol`: https://github.com/apache/hadoop/blob/a655973e781caf662b360c96e0fa3f5a873cf676/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java#L1185.  According to the Javadocs for TaskAttemptListenerImpl.canCommit (the actual concrete implementation of this method):

{code}
  /**
   * Child checking whether it can commit.
   * 
   * <br/>
   * Commit is a two-phased protocol. First the attempt informs the
   * ApplicationMaster that it is
   * {@link #commitPending(TaskAttemptID, TaskStatus)}. Then it repeatedly polls
   * the ApplicationMaster whether it {@link #canCommit(TaskAttemptID)} This is
   * a legacy from the centralized commit protocol handling by the JobTracker.
   */
  @Override
  public boolean canCommit(TaskAttemptID taskAttemptID) throws IOException {
{code}

This ends up delegating to `Task.canCommit()`:

{code}
  /**
   * Can the output of the taskAttempt be committed. Note that once the task
   * gives a go for a commit, further canCommit requests from any other attempts
   * should return false.
   * 
   * @param taskAttemptID
   * @return whether the attempt's output can be committed or not.
   */
  boolean canCommit(TaskAttemptId taskAttemptID);
{code}

There's a bunch of tricky logic that involves communication with the AM (see AttemptCommitPendingTransition and the other transitions in TaskImpl), but it looks like the gist is that the ""winner"" is picked by the AM through some central coordination process. 

So, it looks like the right fix is to implement these same state transitions ourselves.  It would be nice if there was a clean way to do this that could be easily backported to maintenance branches.  ;;;","16/Jan/15 02:18;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4066;;;","22/Jan/15 02:02;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/4155;;;","10/Feb/15 22:21;joshrosen;This issue is _really_ hard to reproduce, but I managed to trigger the original bug as part of the testing for my patch.  Here's what I ran:

{code}
~/spark-1.3.0-SNAPSHOT-bin-1.0.4/bin/spark-shell --conf spark.speculation.multiplier=1 --conf spark.speculation.quantile=0.01 --conf spark.speculation=true --conf  spark.hadoop.outputCommitCoordination.enabled=false
{code}

{code}
val numTasks = 100
val numTrials = 100
val outputPath = ""/output-committer-bug-""
val sleepDuration = 1000

for (trial <- 0 to (numTrials - 1)) {
  val outputLocation = outputPath + trial
  sc.parallelize(1 to numTasks, numTasks).mapPartitionsWithContext { case (ctx, iter) =>
    if (ctx.partitionId % 5 == 0) {
      if (ctx.attemptNumber == 0) {  // If this is the first attempt, run slow
       Thread.sleep(sleepDuration)
      }
    }
    iter
  }.map(identity).saveAsTextFile(outputLocation)
  Thread.sleep(sleepDuration * 2)
  println(""TESTING OUTPUT OF TRIAL "" + trial)
  val savedData = sc.textFile(outputLocation).map(_.toInt).collect()
  if (savedData.toSet != (1 to numTasks).toSet) {
    println(""MISSING: "" + ((1 to numTasks).toSet -- savedData.toSet))
    assert(false)
  }
  println(""-"" * 80)
}
{code}

It took 22 runs until I actually observed missing output partitions (several of the earlier runs threw spurious exceptions and didn't have missing outputs):

{code}
[...]
15/02/10 22:17:21 INFO scheduler.DAGScheduler: Job 66 finished: saveAsTextFile at <console>:39, took 2.479592 s
15/02/10 22:17:21 WARN scheduler.TaskSetManager: Lost task 75.0 in stage 66.0 (TID 6861, ip-172-31-1-124.us-west-2.compute.internal): java.io.IOException: Failed to save output of task: attempt_201502102217_0066_m_000075_6861
        at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:160)
        at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172)
        at org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132)
        at org.apache.spark.SparkHadoopWriter.performCommit$1(SparkHadoopWriter.scala:113)
        at org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:150)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1082)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1059)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/02/10 22:17:21 WARN scheduler.TaskSetManager: Lost task 80.0 in stage 66.0 (TID 6866, ip-172-31-11-151.us-west-2.compute.internal): java.io.IOException: The temporary job-output directory hdfs://ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn't exist!
        at org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
        at org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:244)
        at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:116)
        at org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:91)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1068)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1059)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/02/10 22:17:21 INFO scheduler.TaskSetManager: Lost task 85.0 in stage 66.0 (TID 6871) on executor ip-172-31-1-124.us-west-2.compute.internal: java.io.IOException (The temporary job-output directory hdfs://ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn't exist!) [duplicate 1]
15/02/10 22:17:21 INFO scheduler.TaskSetManager: Lost task 90.0 in stage 66.0 (TID 6876) on executor ip-172-31-1-124.us-west-2.compute.internal: java.io.IOException (The temporary job-output directory hdfs://ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn't exist!) [duplicate 2]
15/02/10 22:17:21 INFO scheduler.TaskSetManager: Lost task 95.0 in stage 66.0 (TID 6881) on executor ip-172-31-11-151.us-west-2.compute.internal: java.io.IOException (The temporary job-output directory hdfs://ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn't exist!) [duplicate 3]
15/02/10 22:17:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool
TESTING OUTPUT OF TRIAL 22
[...]
15/02/10 22:17:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
15/02/10 22:17:23 INFO scheduler.DAGScheduler: Stage 67 (collect at <console>:42) finished in 0.158 s
15/02/10 22:17:23 INFO scheduler.DAGScheduler: Job 67 finished: collect at <console>:42, took 0.165580 s
MISSING: Set(76)
java.lang.AssertionError: assertion failed
[...]
{code}

And I confirmed that it's missing in HDFS:

{code}
~/ephemeral-hdfs/bin/hadoop fs -ls /output-committer-bug-22 | grep part | wc -l
Warning: $HADOOP_HOME is deprecated.

99
{code}

To test my patch, I'm going to remove the flag that disables it and run this for a huge number of trials to ensure that we don't hit the missing output bug.;;;","10/Feb/15 22:27;aash;This is really great work [~joshrosen]!  I really appreciate the effort you're putting into getting this one figured out since these kind of non-deterministic bugs are the most painful for both users and devs to figure out.;;;","11/Feb/15 09:59;mcheah;Thanks for picking this up [~joshrosen], I see that the hotly-debated patch has finally merged in!

I'm a bit backed up but there will be others from my side that will be testing this. We'll comment on the bug once we have verified the issue is fixed on our side.;;;","17/Feb/15 15:14;romi-totango;Could this happen very very rarely when not using speculative execution?
Once in a long while, I have a situation where the OutputCommitter says it wrote the file successfully, but the output file doesn't appear there.;;;","17/Feb/15 16:40;aash;[~romi-totango] what filesystem are you writing to?  HDFS / S3 / other ?  If this was HDFS then I think you may have a bug, but if it was S3 then there are loose eventual consistency guarantees that might be surprising you.;;;","18/Feb/15 08:28;romi-totango;[~aash] I'm writing to S3. I'm aware of eventual consistency issues, but is it possible that 99.999% files are written and available immediately, and then one file doesn't appear for very long? (couldn't afford to wait until it was available so I ran the process again and it wrote the file immediately).
Still isn't it somehow possibly related to this bug or similar, that Spark's output writing mechanism thinks it wrote the file, but actually it did not?;;;","18/Feb/15 14:52;aash;That sounds exactly like what I'd expect to happen in S3.  The eventual consistency bug is that S3 reports something is written, Spark then believes it has written the file, but when you go to access it the file isn't there yet (at least in the us standard region.  If you use another region, there is read-after-write consistency).

In short, it doesn't sound like you're encountering this particular issue with speculative execution (SPARK-4879), just normal S3 consistency trickiness.;;;","05/Mar/15 10:08;mridulm80;
With 1.3 RC, we are still seeing this issue.;;;","05/Mar/15 10:13;mridulm80;Just to add - we are seeing this on hdfs.
Out of the 75k output files to be written, about 53k did not get moved.

But on waiting for a really long time (about 20 minutes) all files eventually got moved.
Is this something to do with FileOutputCommitter itself or side-effect of the patch committed ?

We have speculative execution turned on; even though the saveAsHadoopFile taskset completes (as per logs), the call itself does not return until entire 'move' is done - just that it took more time to commit the output than to generate it in first place !;;;","05/Mar/15 10:52;joshrosen;[~mridulm80], when you say that ""the call itself does not return until entire 'move' is done"", do you mean that the {{saveAsHadoopFile()}} call blocks in {{saveAsHadoopDataset()}}'s final {{   writer.commitJob()}} call?  This ticket addresses a bug where output partitions are missing _after_ control returns from the {{saveAsHadoop*()}} call.

In the issue addressed by this ticket, partitions were deleted after the job and save action had completed and output remained missing.  It sounds like you might be describing a different issue where jobs take a long time to commit.  For your workload, is the behavior that you are observing a performance regression in Spark 1.3?  If you'd like to run 1.3 without the effects of my patch, just set {{spark.hadoop.outputCommitCoordination.enabled=false}} in your SparkConf to bypass the new output commit coordination logic.;;;","05/Mar/15 16:03;mridulm80;[~joshrosen] The former - the call blocks on saveAsHadoopFile (the very next line was sc.stop() and then exit).

We cannot use 'spark.hadoop.outputCommitCoordination.enabled=false' since we need to have speculative execution on (turning it off drastically affects job runtime) and disabling commit coordination causes other issues (as reported in this jira).

I was not sure if 
- the dramatically increased to commit was due to the PR from this jira.
- is orthogonal to this jira and an issue with FileOutputCommitter (as was seen which searching for cause online).

Given the central coordination blocking on master, I wanted to eliminate causes from our end before digging into FIleOutputCommitter and/or Namenode code.;;;","05/Mar/15 18:14;mcheah;Can you perhaps jstack or profile the driver and the executors to see if they're blocked in OutputCommitCoordinator logic?;;;","04/Jun/15 09:18;zhpengg;Hi, is there any updates for this issue?;;;","15/Jun/15 16:28;igor.berman;I'm experiencing this issue. Sometimes rdd with 4 partitions is written with 3 parts and _SUCCESS marker is there.;;;","09/Jul/15 14:26;darabos;I have a fairly reliable reproduction with Spark 1.4.0 and HDFS. I'm running on 10 EC2 m3.2xlarge instances using the ephemeral HDFS. If {{spark.speculation}} is true, I get hit by this 50% of the time or more. It's a fairly complex workload, not something you can test in a {{spark-shell}}. What I saw was that I saved a 400-partition RDD with {{saveAsNewAPIHadoopFile}} (which returned without error) and when I tried to read it back, the files for partitions 323 and 324 were missing. (In the case that I took a closer look at.) I don't have the logs at hand now, but it's like you describe I think ({{Failed to save output of task}}). I can add them later if it would be useful.

I turned off {{spark.speculation}} and haven't seen the issue since.

Is there anything I could do to help debug this issue?;;;","10/Jul/15 12:24;darabos;I wonder if this issue is serious enough to note in the documentation. What do you think about adding a big fat warning for speculative execution until it is fixed? ""Enabling speculative execution may lead to missing output files""? Or perhaps add a verification pass that checks if all the outputs are present and raises an exception if not.

Silently dropping output files is a horrible bug. We've been debugging a somewhat mythological data corruption issue for about a month, and now we realize that this issue (SPARK-4879) is a very plausible explanation. We have never been able to reproduce it, but we have a log file, and it shows a speculative task for a {{saveAsNewAPIHadoopFile}} stage.;;;","12/Jul/15 19:14;joshrosen;[~darabos], do you think that this issue might have been resolved in an earlier Spark version but inadvertently broken in the upgrade to 1.4.0?  If you have an easy reproduction, it might be helpful to see whether the problem occurs on 1.3.1.;;;","12/Jul/15 20:40;darabos;Good idea! I'll try with 1.3.1 next week.;;;","15/Jul/15 14:01;darabos;I've managed to reproduce on Spark 1.3.1 too (pre-built for Hadoop 1). I ran on EC2 with the {{spark-ec2}} script and used the ephemeral HDFS. I used a 5-machine cluster and repeatedly ran a complex test suite for about 30 minutes until the error was triggered. Here are the relevant logs:

{noformat}
I2015-07-15 13:45:19,954 TaskSetManager:[task-result-getter-2] Finished task 198.0 in stage 320.0 (TID 13290) in 568 ms on ip-10-153-188-224.ec2.internal (195/200)
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 197 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 176 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 194 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 196 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 195 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 197.1 in stage 320.0 (TID 13292, ip-10-153-188-224.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 176.1 in stage 320.0 (TID 13293, ip-10-63-27-248.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 194.1 in stage 320.0 (TID 13294, ip-10-154-1-239.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 195.1 in stage 320.0 (TID 13295, ip-10-228-67-34.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 196.1 in stage 320.0 (TID 13296, ip-10-153-188-224.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,402 TaskSetManager:[task-result-getter-3] Finished task 176.0 in stage 320.0 (TID 13268) in 2223 ms on ip-10-231-214-6.ec2.internal (196/200)
I2015-07-15 13:45:21,445 TaskSetManager:[task-result-getter-0] Finished task 195.0 in stage 320.0 (TID 13287) in 2113 ms on ip-10-231-214-6.ec2.internal (197/200)
I2015-07-15 13:45:21,461 TaskSetManager:[task-result-getter-1] Finished task 196.1 in stage 320.0 (TID 13296) in 285 ms on ip-10-153-188-224.ec2.internal (198/200)
I2015-07-15 13:45:21,464 TaskSetManager:[task-result-getter-2] Finished task 194.1 in stage 320.0 (TID 13294) in 287 ms on ip-10-154-1-239.ec2.internal (199/200)
I2015-07-15 13:45:21,465 TaskSetManager:[task-result-getter-3] Ignoring task-finished event for 176.1 in stage 320.0 because task 176 has already completed successfully
I2015-07-15 13:45:21,468 TaskSetManager:[task-result-getter-0] Finished task 197.1 in stage 320.0 (TID 13292) in 292 ms on ip-10-153-188-224.ec2.internal (200/200)
I2015-07-15 13:45:21,468 DAGScheduler:[dag-scheduler-event-loop] Stage 320 (saveAsNewAPIHadoopFile at HadoopFile.scala:208) finished in 4.802 s
I2015-07-15 13:45:21,468 DAGScheduler:[DataManager-5] Job 46 finished: saveAsNewAPIHadoopFile at HadoopFile.scala:208, took 4.836626 s
W2015-07-15 13:45:21,478 TaskSetManager:[task-result-getter-1] Lost task 195.1 in stage 320.0 (TID 13295, ip-10-228-67-34.ec2.internal): java.io.IOException: Failed to save output of task: attempt_201507151345_0628_r_000195_1
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:203)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:214)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:167)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:1009)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

I checked the output directory and indeed {{part-r-00195}} is missing.

Our speculative execution configuration was:

{noformat}
spark.speculation true
spark.speculation.interval 1000
spark.speculation.quantile 0.90
spark.speculation.multiplier 2
{noformat}

We've been running production code with this setup for almost a year. Until now we hadn't checked for missing files. Such a painful bug.;;;","15/Jul/15 16:17;joshrosen;I think that this is going to be a complex issue to investigate / fix, so I'd like to break down that task into a series of smaller subtasks.  Here's my initial proposal (each of these could be done as a separate JIRA / patch / TODO so that we make incremental progress):

- Add automated post-save-action checks to assert that the expected number of output files are present.  This will make debugging much easier.  There are some corner-cases to address related to S3 and _temporary / _SUCCESS files, but I'm sure we can come up with a solution that addresses them.  The error message raised by this assert could have a pointer to this issue and could suggest disabling speculation as a workaround.
- Write a standalone program which is capable of reproducing the most recently reported set of symptoms for this bug.  The symptoms that originally prompted this ticket were cases where a speculative / duplicated task would continue running after the original job had completed, then fail to commit and delete the destination file.  It's possible that this issue has resurfaced / wasn't properly fixed, but it's also possible that there is a different race which wasn't addressed by the original patch.
  -- One subtask of this may be logging improvements: we might not be able to come up with a reasonably minimal reproduction until we have more granular logs to help us better pinpoint the race.
  -- If this is non-deterministic, then we can rig our program to try to expose the race, then have it re-try itself hundreds of times in a loop and stop once a failure occurs.
  -- Given the inherent non-determinism, external dependencies, and slow nature of this type of test, I don't think that this can be an automated test in Jenkins.  However, I think that the code for this test should live in the Spark codebase. We should create a pre-release manual QA checklist and make sure that it includes a task to manually run this test on EC2.
- If we gain a better understanding of the races / problems that lead to this bug, we may be able to mock / stub / simulate / interpose in a way that lets us write a unit test to reproduce this bug.  If that's possible, then we should do it in order to have a fast-to-run test that can run regularly.
- Fix the underlying bug.;;;","15/Jul/15 17:17;darabos;Thanks, Josh! I wonder when the post-save-action check you suggest would run. Based on the above log snippet I think it's quite likely that at the point when the stage finishes the files are all there. I suspect it's the speculative task which fails after the stage has finished that deletes the file. It may be hard to check at the right point in time.

I'll try to find a smaller reproduction. First it would be great if I could reproduce on my local machine instead of starting EC2 clusters. Then I just need to dig out the key operations from our -entangled mess of a- _highly sophisticated_ codebase.

One more thing that occurs to me is that perhaps there should never be a line of code that deletes an output file. I haven't had a chance to dig into the code yet, and I'm sure there is a reason for it, but perhaps the same goal could be accomplished without deleting output files. What do you think?;;;","17/Jul/15 15:22;joshrosen;That's a good point about the post-save-action check: code internal to Spark can only perform the check right before we return control back to the user so it's possible that the check will say that everything's fine even if the files are later deleted. This check still might have some value, but it's certainly prone to false-negatives and wouldn't be able to catch all manifestations of this bug.

In the original issue that prompted this patch, the deletion of the output file occurred inside of Hadoop code: if the renaming of the temporary file to the final output file fails, Hadoop's FileOutputCommitter will attempt to delete the final output file before re-attempting the rename (this is explained in a bit more detail, including a code walkthrough, in the description of this JIRA ticket).;;;","23/Aug/15 09:54;igor.berman;there is blog post http://tech.grammarly.com/blog/posts/Petabyte-Scale-Text-Processing-with-Spark.html with a link to gist by Aaron Davidson: https://gist.github.com/aarondav/c513916e72101bbe14ec which mentions in comments that when using speculation it should be DirectOutputCommitter

might be somebody will find it useful (imho, it should be in documentation)
;;;","12/Sep/15 13:02;srowen;I'm clearing ""backport-needed"" since it's virtually certain that there will be no more 1.2.x or earlier releases, and so the fix that was committed won't go back further at this point.

Is it something to leave open pending the ongoing conversation here? sounds like there may be more to the fix? ;;;","13/Sep/15 12:01;stevel@apache.org;What about splitting the issue into HDFS commits (interesting its mostly EC2 reports), which is probably fixed, and eventually consistent object stores (S3 on the Apache Hadoop releases (but not amazon's own), which need a different committer (no rename), better checks for file presence (direct stat() is more reliable than a directory listing), and some dedicated test suite which could be targeted straight at s3 —yet still runnable remotely by someone (not jenkins) from their own desktop & build servers. That's essentially what we do in core hadoop to qualify the object stores' base API compatibility.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
userClassPathFirst doesn't handle user classes inheriting from parent,SPARK-4877,12762375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,click_stephen,stephen,stephen,17/Dec/14 20:18,06/Feb/15 19:07,14/Jul/23 06:26,06/Feb/15 19:05,1.2.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"We're trying out userClassPathFirst.

To do so, we make an uberjar that does not contain Spark or Scala classes (because we want those to load from the parent classloader, otherwise we'll get errors like scala.Function0 != scala.Function0 since they'd load from different class loaders).

(Tangentially, some isolation classloaders like Jetty whitelist certain packages, like spark/* and scala/*, to only come from the parent classloader, so that technically if the user still messes up and leaks the Scala/Spark jars into their uberjar, it won't blow up; this would be a good enhancement, I think.)

Anyway, we have a custom Kryo registrar, which ships in our uberjar, but since it ""extends spark.KryoRegistrator"", which is not in our uberjar, we get a ClassNotFoundException.",,apachespark,holden,joshrosen,matt.whelan,MattWhelan,stephen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5358,,,,,,SPARK-5358,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 19:07:32 UTC 2015,,,,,,,,,,"0|i23jhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/14 20:19;stephen;Stack trace:

{code}
2014-12-17 05:07:36,811 WARN  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logWarning(71)) - Lost task 0.0 in stage 0.0 (TID 0, ip-10-153-171-58.ec2.internal): java.lang.NoClassDefFoundError: org/apache/spark/serializer/KryoRegistrator
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at org.apache.spark.executor.ChildExecutorURLClassLoader$userClassLoader$.findClass(ExecutorURLClassLoader.scala:42)
	at org.apache.spark.executor.ChildExecutorURLClassLoader.findClass(ExecutorURLClassLoader.scala:50)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$3.apply(KryoSerializer.scala:101)
	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$3.apply(KryoSerializer.scala:101)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:101)
	at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:157)
	at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:119)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:214)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:177)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1000)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:87)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:61)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.serializer.KryoRegistrator
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at org.apache.spark.executor.ChildExecutorURLClassLoader$userClassLoader$.findClass(ExecutorURLClassLoader.scala:42)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
{code}

Also note that I have a fix and will be submitting a PR soon.;;;","17/Dec/14 20:25;apachespark;User 'stephenh' has created a pull request for this issue:
https://github.com/apache/spark/pull/3725;;;","24/Dec/14 02:33;stephen;FWIW two reviewers have okay'd this PR; can someone else take a look + commit it?;;;","22/Jan/15 19:12;MattWhelan;Looking at the PR, you're still overriding findClass, not loadClass.  You can't change the delegation strategy successfully without changing loadClass.  ;;;","23/Jan/15 08:00;stephen;Hi Matt,

I don't doubt you are right, but can you clarify?

I had noticed that the Jetty/Hadoop implementations do overload loadClass, but I was going for a minimal amount of changes for this initial PR. It also passes the tests and works for us in production, so AFAICT was okay.

I think more fully porting the Jetty/Hadoop code over is a good idea, I was just assuming moving from findClass to loadClass wouldn't be required until taking that on.;;;","23/Jan/15 17:30;MattWhelan;The best thing to do is look at the source for ClassLoader.loadClass, and think about the implications.  Much has been written on the topic as well.

The short version is that loadClass controls the delegation process, and findClass is a fallback that gets called after delegation fails.  You can't effectively change delegation with just findClass.

The Java Language Spec, the docs for ClassLoader, and http://www.ibm.com/developerworks/library/j-dclp1/index.html (actually, that whole series) are all good references.  It's a really complicated topic, and is terribly easy to mess it up in subtle ways if you don't consider all the possible implications of what you're doing.;;;","04/Feb/15 19:37;holden;Hi Matt,

I don't believe we need to override loadClass, this would be true if we created a class loader with the parent in the normal sense, but when we extend the classloader we keep the parent class loader as null and do our own resolution routing, so loadClass shouldn't be able to trigger a resolution on the parent since it has no knowledge of the parent.;;;","04/Feb/15 22:12;matt.whelan;Overriding only findClass ignores caching, which causes LinkageErrors when you try to load the same class twice.  

Really getting into the details of class loading is beyond the scope of a Jira issue thread.  I posted links to a couple really good sources above.  If you read those and familiarize yourself with the details of the default loadClass implementation, I think you'll come to the same conclusion.;;;","04/Feb/15 22:21;stephen;Hi Matt,

I know about the caching/LinkageError issue, because I saw it running a job; my patch has a test that reproduced it and fixes it.

I'm still willing to believe loadClass might be preferred, but AFAICT it's not required, and the current findClass approach is working fine (with this patch) in our production jobs.

(Note that I'm very open to refactoring this approach further in the future, especially to introduce the Jetty/Hadoop concept of system classes, but our jobs have not ran into any issues.);;;","06/Feb/15 19:05;joshrosen;Issue resolved by pull request 3725
[https://github.com/apache/spark/pull/3725];;;","06/Feb/15 19:07;joshrosen;I've gone ahead and committed this PR because it fixes a known bug and adds a new test case.  Both the old and new code overloaded findClass; I think the findClass vs. loadClass change is related to the this JIRA, but kind of orthogonal to the fix here.  If you think that we should re-work our classloader to change its overriding strategy, let's do that in a separate followup PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show sql statement in spark ui when run sql use spark-sql,SPARK-4871,12762209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,17/Dec/14 06:22,11/Jan/15 01:07,14/Jul/23 06:26,11/Jan/15 01:07,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Show sql statement in spark ui when run sql use spark-sql.,,apachespark,marmbrus,nisgoel,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 11 01:07:21 UTC 2015,,,,,,,,,,"0|i23ihb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"17/Dec/14 06:23;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3718;;;","11/Jan/15 01:07;marmbrus;Issue resolved by pull request 3718
[https://github.com/apache/spark/pull/3718];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support StructType as key in MapType,SPARK-4866,12762122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,davies,davies,16/Dec/14 21:18,05/May/15 15:34,14/Jul/23 06:26,17/Dec/14 05:26,,,,,,,,1.3.0,,,,,,PySpark,SQL,,,0,,,,,,"http://apache-spark-user-list.1001560.n3.nabble.com/Error-when-Applying-schema-to-a-dictionary-with-a-Tuple-as-key-td20716.html

Hi Guys, 

Im running a spark cluster in AWS with Spark 1.1.0 in EC2 

I am trying to convert a an RDD with tuple 

(u'string', int , {(int, int): int, (int, int): int}) 

to a schema rdd using the schema: 

{code}
fields = [StructField('field1',StringType(),True), 
                StructField('field2',IntegerType(),True), 
                StructField('field3',MapType(StructType([StructField('field31',IntegerType(),True), 
                        StructField('field32',IntegerType(),True)]),IntegerType(),True),True) 
                ] 

schema = StructType(fields) 
# generate the schemaRDD with the defined schema 
schemaRDD = sqc.applySchema(RDD, schema) 
{code}

But when I add ""field3"" to the schema, it throws an execption: 

{code}
Traceback (most recent call last): 
  File ""<stdin>"", line 1, in <module>
  File ""/root/spark/python/pyspark/rdd.py"", line 1153, in take 
    res = self.context.runJob(self, takeUpToNumLeft, p, True) 
  File ""/root/spark/python/pyspark/context.py"", line 770, in runJob 
    it = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions, allowLocal) 
  File ""/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__ 
  File ""/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value 
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob. 
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 4 times, most recent failure: Lost task 0.3 in stage 28.0 (TID 710, ip-172-31-29-120.ec2.internal): net.razorvine.pickle.PickleException: couldn't introspect javabean: java.lang.IllegalArgumentException: wrong number of arguments 
        net.razorvine.pickle.Pickler.put_javabean(Pickler.java:603) 
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:299) 
        net.razorvine.pickle.Pickler.save(Pickler.java:125) 
        net.razorvine.pickle.Pickler.put_map(Pickler.java:321) 
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:286) 
        net.razorvine.pickle.Pickler.save(Pickler.java:125) 
        net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:412) 
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:195) 
        net.razorvine.pickle.Pickler.save(Pickler.java:125) 
        net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:412) 
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:195) 
        net.razorvine.pickle.Pickler.save(Pickler.java:125) 
        net.razorvine.pickle.Pickler.dump(Pickler.java:95) 
        net.razorvine.pickle.Pickler.dumps(Pickler.java:80) 
        org.apache.spark.sql.SchemaRDD$$anonfun$javaToPython$1$$anonfun$apply$2.apply(SchemaRDD.scala:417) 
        org.apache.spark.sql.SchemaRDD$$anonfun$javaToPython$1$$anonfun$apply$2.apply(SchemaRDD.scala:417) 
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:331) 
        org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:209) 
        org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:184) 
        org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:184) 
        org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311) 
        org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:183) 
Driver stacktrace: 
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185) 
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174) 
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173) 
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) 
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) 
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173) 
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688) 
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688) 
        at scala.Option.foreach(Option.scala:236) 
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688) 
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391) 
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) 
        at akka.actor.ActorCell.invoke(ActorCell.scala:456) 
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) 
        at akka.dispatch.Mailbox.run(Mailbox.scala:219) 
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) 
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) 
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 
{code}
",,apachespark,davies,lisbethron,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 15:34:05 UTC 2015,,,,,,,,,,"0|i23hyf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"16/Dec/14 21:43;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3714;;;","17/Dec/14 05:26;marmbrus;Issue resolved by pull request 3714
[https://github.com/apache/spark/pull/3714];;;","05/May/15 15:34;lisbethron;Hi Guys, 

I'm dealing with a situation, working on Spark 1.3.1 with dataframes to train a random forest algorithm with Mmlib and Python Spark. And I have this error, I don't know where its comes from... somebody can Help me.?

Thanks 

Lisbeth


  File ""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o58.sql.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include temporary tables in SHOW TABLES,SPARK-4865,12762107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,chernetsov,chernetsov,16/Dec/14 20:37,22/Apr/15 20:34,14/Jul/23 06:26,16/Feb/15 23:59,1.2.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,abhietc31,apachespark,bbejeck,chernetsov,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3299,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 20:34:00 UTC 2015,,,,,,,,,,"0|i23hv3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"19/Dec/14 20:30;marmbrus;Temporary tables are tied to a specific SQLContext and thus can't be seen or queried across different JVMs.  Is that the issue you are reporting?  This is a fundamental design thing that we are not going to change.

Or are you creating a JDBC server with an existing HiveContext and then not seeing the tables (a separate issue that I do want to fix).;;;","19/Dec/14 20:46;chernetsov;> Or are you creating a JDBC server with an existing HiveContext and then not seeing the tables (a separate issue that I do want to fix).
I am reporting that one.;;;","09/Jan/15 12:48;bbejeck;I'm done with SPARK-3299.  If no one is currently working this, could I have this assigned to me?;;;","13/Feb/15 21:41;yhuai;I will start to work on it based on SPARK-3299.;;;","16/Feb/15 02:28;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4618;;;","16/Feb/15 23:59;marmbrus;Issue resolved by pull request 4618
[https://github.com/apache/spark/pull/4618];;;","22/Apr/15 18:50;abhietc31;Hi, 
It seems like issue is resolved but where  can someone help to understand how to read Temporary Spark table through Spark SQL/Thrift server ?
 I'm using Spark 1.2.0, create Spark temp table and registered it. but in Spark sql, it does not show temp table.

Can some one please elaborate how to resolve the issue?

Thanks,
Abhishek;;;","22/Apr/15 20:34;marmbrus;The fix version on the issue is set to 1.3.0, which means that it is not fixed until that version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation to Netty-based configs,SPARK-4864,12762086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,16/Dec/14 19:55,22/Dec/14 21:10,14/Jul/23 06:26,22/Dec/14 21:10,,,,,,,,1.2.1,1.3.0,,,,,Documentation,,,,0,,,,,,Currently there is no public documentation for the NettyBlockTransferService or various configuration options of the network package. We should add some.,,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 16 20:07:55 UTC 2014,,,,,,,,,,"0|i23hqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/14 20:07;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3713;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null & empty string should not be considered as StringType at begining in Json schema inferring,SPARK-4856,12761923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,16/Dec/14 01:26,12/Feb/15 18:03,14/Jul/23 06:26,17/Dec/14 23:02,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"We have data like:
{noformat}
TestSQLContext.sparkContext.parallelize(
  """"""{""ip"":""27.31.100.29"",""headers"":{""Host"":""1.abc.com"",""Charset"":""UTF-8""}}"""""" ::
  """"""{""ip"":""27.31.100.29"",""headers"":{}}"""""" ::
  """"""{""ip"":""27.31.100.29"",""headers"":""""}"""""" :: Nil)
{noformat}

As empty string (the ""headers"") will be considered as String, and it ignores the real nested data type (struct type ""headers"" in line 1), and then we will get the ""headers"" (in line 1) as String Type, which is not our expectation.",,apachespark,chenghao,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 18:03:19 UTC 2015,,,,,,,,,,"0|i23gqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/14 01:31;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3708;;;","17/Dec/14 23:02;marmbrus;Issue resolved by pull request 3708
[https://github.com/apache/spark/pull/3708];;;","12/Feb/15 17:58;yhuai;[~chenghao] I think it is fine to use NullType for an empty string during the process of inferring schema. However, I think we should not always treat an empty string as null (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/json/JsonRDD.scala#L402). If the inferred data type is StringType, we should return the empty string. Otherwise, we are destroying information. If the inferred data type is any other data type, I think it is reasonable to return a null.;;;","12/Feb/15 18:03;yhuai;Oh, please ignore my comment above. I missed the line of matching StringType. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Uninitialized staticmethod object"" error in PySpark",SPARK-4851,12761759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,nadavg,nadavg,15/Dec/14 14:57,11/Nov/16 01:15,14/Jul/23 06:26,08/Oct/16 00:03,1.0.2,1.2.0,1.3.0,,,,,,,,,,,PySpark,,,,1,,,,,,"*Reproduction:*

{code}
class A:
    @staticmethod
    def foo(self, x):
        return x

sc.parallelize([1]).map(lambda x: A.foo(x)).count()
{code}

This gives

{code}
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/worker.py"", line 107, in main
    process()
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/worker.py"", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 2070, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 2070, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 2070, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 247, in func
    return f(iterator)
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 818, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 818, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""<stdin>"", line 1, in <lambda>
RuntimeError: uninitialized staticmethod object

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:136)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:173)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745){code}",,arnabguin,holden,joshrosen,nadavg,otoomet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 11 01:14:54 UTC 2016,,,,,,,,,,"0|i23frb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/14 22:44;joshrosen;I've edited this issue to include an updated reproduction and more complete stacktrace.

I'm pretty sure that this is due to limitations in our pickling library.  Dill, another Python pickling extension, also has trouble with this case and it doesn't appear that they have a fix either: https://github.com/uqfoundation/dill/issues/13;;;","08/Oct/16 00:03;holden;The provided repro now runs (although we need to provide it with the correct number of args).;;;","11/Nov/16 01:14;otoomet;In which version is this fixed?  I still get ""uninitialized staticmethod object"" when running the example on 2.1.0 snapshot (compiled today, Nov 10th);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow different Worker configurations in standalone cluster,SPARK-4848,12761693,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nkronenfeld,nkronenfeld,nkronenfeld,15/Dec/14 07:07,14/Apr/15 01:21,14/Jul/23 06:26,14/Apr/15 01:21,1.0.0,,,,,,,1.4.0,,,,,,Deploy,,,,0,,,,,,"On a stand-alone spark cluster, much of the determination of worker specifics, especially one has multiple instances per node, is done only on the master.

The master loops over instances, and starts a worker per instance on each node.

This means, if your workers have different values of SPARK_WORKER_INSTANCES or SPARK_WORKER_WEBUI_PORT from each other (or from the master), all values are ignored except the one on the master.

SPARK_WORKER_PORT looks like it is unread in scripts, but read in code - I'm not sure how it will behave, since all instances will read the same value from the environment.",stand-alone spark cluster,apachespark,nkronenfeld,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 17:35:51 UTC 2015,,,,,,,,,,"0|i23fcv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,"15/Dec/14 07:14;apachespark;User 'nkronenfeld' has created a pull request for this issue:
https://github.com/apache/spark/pull/3699;;;","23/Mar/15 17:35;apachespark;User 'nkronenfeld' has created a pull request for this issue:
https://github.com/apache/spark/pull/5140;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
extraStrategies cannot take effect in SQLContext,SPARK-4847,12761691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jerryshao,jerryshao,15/Dec/14 06:39,16/Dec/14 22:12,14/Jul/23 06:26,16/Dec/14 22:12,1.2.0,,,,,,,1.2.1,,,,,,SQL,,,,0,,,,,,"Because strategies is initialized when SparkPlanner is created, so later added extraStrategies cannot be added into strategies.",,apachespark,jerryshao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 16 22:12:27 UTC 2014,,,,,,,,,,"0|i23fcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/14 06:57;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/3698;;;","16/Dec/14 22:12;marmbrus;Issue resolved by pull request 3698
[https://github.com/apache/spark/pull/3698];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the vocabulary size is large, Word2Vec may yield ""OutOfMemoryError: Requested array size exceeds VM limit""",SPARK-4846,12761684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephtang,josephtang,josephtang,15/Dec/14 05:26,29/Jan/16 02:15,14/Jul/23 06:26,30/Jan/15 18:07,1.1.1,1.2.0,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"Exception in thread ""Driver"" java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:162)
Caused by: java.lang.OutOfMemoryError: Requested array size exceeds VM limit 
    at java.util.Arrays.copyOf(Arrays.java:2271)
    at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
    at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
    at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1870)
    at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1779)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1186)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
    at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)
    at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:164)
    at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)
    at org.apache.spark.SparkContext.clean(SparkContext.scala:1242)
    at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:610)
    at org.apache.spark.mllib.feature.Word2Vec$$anonfun$fit$1.apply$mcVI$sp(Word2Vec.scala:291)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:290)","Use Word2Vec to process a corpus(sized 3.5G) with one partition.
The corpus contains about 300 million words and its vocabulary size is about 10 million.",3cham,apachespark,josephkb,josephtang,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5261,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 02:15:40 UTC 2016,,,,,,,,,,"0|i23fav:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,"15/Dec/14 05:38;apachespark;User 'jinntrance' has created a pull request for this issue:
https://github.com/apache/spark/pull/3697;;;","15/Dec/14 14:11;srowen;I think you're just running out of memory on your driver. It fails to have enough memory to copy and serialize two data structures, syn0Global and syn1Global which contain (vocab size * vector length) floats. With a default vector length of 100, and 10M vocab, that's at least 8GB of RAM, and the default for the driver isn't nearly that big.

I think this is just a matter of increasing your driver memory. I imagine you will need 16GB+;;;","15/Dec/14 15:06;josephtang;In the very beginning, either I thought it's an issue of memory shortage in the driver or in the executors . But after I increase the driver's memory to 16G and all the executors' memory to 15G, this issue occurred in the same way.

Actually in the RDD.mapPartitionsWithIndex(higher-order-function), firstly SparkContext.clean() is invoked and then ClosureCleaner.ensureSerializable() tries to serialize the higher-order-function(including the initialized syn0Global and syn1Global) using ByteArrayOutputStream before it is executed. 

Because the ByteArrayOutputStream has a maximum size limit of Int.MaxValue(2^31), which means the higher-order-function can not be larger than 2GB.  So in this case, I'm trying to make syn0Global and syn1Global lazy, avoiding being serialized.;;;","15/Dec/14 15:21;srowen;But being lazy doesn't really change whether it is serialized, right? one way or the other the recipients of the higher-order function have to get the same data. The function does use the data structures; it's not a question of simply keeping something out of the closure that shouldn't be there.

Is the problem that only part of this large data structure should go to each partition?;;;","16/Dec/14 00:24;josephkb;I agree with [~srowen] that the current implementation has to serialize those big data structures, no matter what.  Splitting the big syn0Global and syn1Global data structures across partitions sounds possible, but I would guess that the Word2VecModel itself would then need to be distributed as well since it occupies the same order of memory.  A distributed Word2VecModel sounds like a much bigger PR.

In the meantime, a simpler & faster solution might be nice.  The easiest would be to catch the error and print a warning.  A fancier but better solution might be to automatically minCount as much as necessary (and print a warning about this automatic change).;;;","16/Dec/14 13:51;josephtang;Hi  [~srowen] and [~josephkb].

You are right. I had another test about the 'lazy' and then found my misunderstanding of the object serialization in JVM. The 'lazy' does not work. I'll remove the 'lazy' later. 

[~josephkb]'s good advice about automatic minCount should work, meanwhile it costs less memory.  However, one potential issue may be that in the worst case we have to try many times to find the best minCount when changing the minCount step by step.
Also I think a quick fix can be automatically decreasing the vectorSize, but in this case it has lower accuracy. 

Or else, if a Spark user changes one of minCount and vectorSize, we automatically change the other one.  If  a user change neither, we just auto change vectorSize by default.  How do you think? 

;;;","16/Dec/14 19:50;josephkb;Changing vectorSize sounds too aggressive to me.  I'd vote for either the simple solution (throw a nice error), or an efficient method which chooses minCount automatically.  For the latter, this might work:

1. Try the current method.  Catch errors during the collect() for vocab and during the array allocations.  If there is no error, skip step 2.
2. If there is an error, do 1 pass over the data to collect stats (e.g., a histogram).  Use those stats to choose a reasonable minCount.  Choose the vocab again, etc.
3. After the big array allocations, the algorithm can continue as before.
;;;","23/Dec/14 11:05;josephtang;It sounds accomplishable.

I'll try this and make a PR later if it works pretty well .;;;","30/Dec/14 23:11;mengxr;We merged `setMinCount()` in PR #3693. For this issue, throwing an error and asking users to try a larger minCount or a smaller vectorSize should be sufficient for now.;;;","27/Jan/15 00:29;mengxr;[~josephtang] Are you working on this issue? If not, do you mind me sending a PR that throwing an exception if vectorSize is beyond limit?;;;","27/Jan/15 02:14;josephtang;Sorry about the procrastination. I just thought you meant there is no need to implement a dynamic strategy. I'm still working on it and I'd like to quickly fix this issue.

Regarding your previous comment, should I throw a customized error in Spark or just an OOM besides the hint about minCount and vectorSize? ;;;","27/Jan/15 02:43;josephtang;Hi Xiangrui, here is a problem.

PR #3693 that added the `setMinCount ` was merged to the branch `master`, while my PR #3697 was sent to `branch-1.1`.

Should I better close  PR #3697 and send a new PR based on PR #3693?;;;","27/Jan/15 03:35;josephtang;I've added some code at https://github.com/jinntrance/spark/compare/w2v-fix?diff=split&name=w2v-fix

If it's OK, I would send a new PR to the branch `master`.

BTW, sorry for the horrible readability of the difference because of the space indent.;;;","28/Jan/15 08:45;mengxr;We should throw a RuntimeException before allocating memory. Yes, it is easier to close #3697 and send a new PR to `master`.;;;","28/Jan/15 11:14;apachespark;User 'jinntrance' has created a pull request for this issue:
https://github.com/apache/spark/pull/4247;;;","28/Jan/15 11:23;josephtang;OK. I've added a piece of RuntimeException code and have sent a new PR as below.;;;","30/Jan/15 18:07;mengxr;Issue resolved by pull request 4247
[https://github.com/apache/spark/pull/4247];;;","02/Dec/15 07:55;3cham;I have a question regarding this issue: as far as I understand, word2vec.fit(input) will return a Word2VecModel object which will be stored on the driver's memory only? 

This leads to a painful consequence when experimenting on yarn-client mode, my driver (my computer) has limited memory and the object cannot fit into it. How could I improve the situation?

;;;","07/Dec/15 20:56;josephkb;This sounds like a limitation of using yarn-client mode, if the client/driver has limited memory.

Eventually, we could support a distributed representation of the model, but that would make the algorithm slower for many use cases.  It'd be worth adding at some point, though, perhaps with a switching mechanism.  But I don't think it would be high priority unless there were many such problems.

Is this something you can get around by changing modes, or by using a different client computer?;;;","28/Jan/16 13:59;3cham;[~josephkb]: I have changed the mode to yarn-cluster, however it seems that the implementation of word2vec has some problem with memory management. I give you some details about my experiment:

The dataset is only 2.8GB big with about 700K different words and vector length is only 200, so syn0Global and syn1Global should be around 1.2GB. For spark 1.5.1, I contantly receive this exception even with 100GB for driver (-Xmx80G), 120GB for each worker (10 total). I then switched to 1.6.0, it worked with just 8G for driver and 20GB for each worker (what I expected). However, if I increase the vector length to 400, I receive this exception again even with 100GB driver and 120GB worker.

The word2vec model should not be that big. Could you please give me some hint how I could solve this problem?;;;","29/Jan/16 02:15;josephtang;Hi Tung,

As far as I can remember, the data is serialized by ByteArray that has the
length limit Integer.MAX_VALUE, which means ByteArray can only serialize
data less than 2GB.

May this piece of information help.

Joseph


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch serializer bug in PySpark's RDD.zip,SPARK-4841,12761596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,mengxr,mengxr,14/Dec/14 04:12,24/Feb/15 18:57,14/Jul/23 06:26,17/Dec/14 20:22,1.2.0,,,,,,,1.2.1,1.3.0,,,,,PySpark,,,,0,,,,,,"{code}
t = sc.textFile(""README.md"")
t.zip(t).count()
{code}

{code}
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-6-60fdeb8339fd> in <module>()
----> 1 readme.zip(readme).count()

/Users/meng/src/spark/python/pyspark/rdd.pyc in count(self)
    817         3
    818         """"""
--> 819         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
    820
    821     def stats(self):

/Users/meng/src/spark/python/pyspark/rdd.pyc in sum(self)
    808         6.0
    809         """"""
--> 810         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
    811
    812     def count(self):

/Users/meng/src/spark/python/pyspark/rdd.pyc in reduce(self, f)
    713             yield reduce(f, iterator, initial)
    714
--> 715         vals = self.mapPartitions(func).collect()
    716         if vals:
    717             return reduce(f, vals)

/Users/meng/src/spark/python/pyspark/rdd.pyc in collect(self)
    674         """"""
    675         with SCCallSiteSync(self.context) as css:
--> 676             bytesInJava = self._jrdd.collect().iterator()
    677         return list(self._collect_iterator_through_file(bytesInJava))
    678

/Users/meng/src/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539
    540         for temp_arg in temp_args:

/Users/meng/src/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o69.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/Users/meng/src/spark/python/pyspark/worker.py"", line 107, in main
    process()
  File ""/Users/meng/src/spark/python/pyspark/worker.py"", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/Users/meng/src/spark/python/pyspark/serializers.py"", line 198, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/Users/meng/src/spark/python/pyspark/serializers.py"", line 81, in dump_stream
    raise NotImplementedError
NotImplementedError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:204)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:204)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:203)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,apachespark,donnchadh,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5973,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:22:55 UTC 2014,,,,,,,,,,"0|i23err:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/14 17:12;mengxr;This is the commit that caused the bug: 786e75b33f0bc1445bfc289fe4b62407cb79026e (SPARK-3886);;;","15/Dec/14 22:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3706;;;","16/Dec/14 06:59;joshrosen;I've merged https://github.com/apache/spark/pull/3706 into master; adding a {{backport-needed}} label so that this makes it into 1.2.1.;;;","17/Dec/14 20:22;joshrosen;I've merged this into {{branch-1.2}}, so it will be included in Spark 1.2.1.  Since this was the last backport, I'm marking this as Fixed.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyBlockTransferService does not abide by spark.blockManager.port config option,SPARK-4837,12761535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ilikerps,ilikerps,ilikerps,13/Dec/14 03:57,19/Dec/14 00:44,14/Jul/23 06:26,19/Dec/14 00:44,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"The NettyBlockTransferService always binds to a random port, and does not use the spark.blockManager.port config as specified.",,aash,apachespark,ilikerps,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2157,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 00:44:14 UTC 2014,,,,,,,,,,"0|i23ee7:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"13/Dec/14 04:01;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3688;;;","13/Dec/14 19:36;aash;This is a regression from features that landed in 1.1.0 (SPARK-2157) so I think should be marked as a blocker to releasing 1.2.0;;;","15/Dec/14 18:42;pwendell;Hey [~aash] because there is a work around (you can simply switch back to the old IO mode) we probably won't block on it, but I can include it in the release notes as a known issue. We can also spin a bug-fix release to address this in a week or two. It is indeed an annoying issue and will be bad for usability if someone upgrades.;;;","15/Dec/14 18:55;aash;Ok that's fair -- a release note and targeting 1.2.1 sounds good.

Draft language for that release note could be:

- Spark 1.2.0 changes the default block transfer service to NettyBlockTransferService, a higher performance block transfer service than the old XYZBlockTransferService.  The new transfer service does not yet respect `spark.blockManager.port` so deployments needing full control of Spark's network ports in 1.2.0 should temporarily set `spark.abc=xyz` and watch SPARK-4837;;;","19/Dec/14 00:44;joshrosen;Issue resolved by pull request 3688
[https://github.com/apache/spark/pull/3688];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming saveAs*HadoopFiles() methods may throw FileAlreadyExistsException during checkpoint recovery,SPARK-4835,12761518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,joshrosen,joshrosen,13/Dec/14 01:55,03/Feb/15 05:47,14/Jul/23 06:26,05/Jan/15 04:28,1.3.0,,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,,,,,,"While running (a slightly modified version of) the ""recovery with saveAsHadoopFiles operation"" test in the streaming CheckpointSuite, I noticed the following error message in the streaming driver log:

{code}
14/12/12 17:42:50.687 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO JobScheduler: Added jobs for time 1500 ms
14/12/12 17:42:50.687 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO RecurringTimer: Started timer for JobGenerator at time 2000
14/12/12 17:42:50.688 sparkDriver-akka.actor.default-dispatcher-3 INFO JobScheduler: Starting job streaming job 1500 ms.0 from job set of time 1500 ms
14/12/12 17:42:50.688 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO JobGenerator: Restarted JobGenerator at 2000 ms
14/12/12 17:42:50.688 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO JobScheduler: Started JobScheduler
14/12/12 17:42:50.689 sparkDriver-akka.actor.default-dispatcher-3 INFO JobScheduler: Starting job streaming job 1500 ms.1 from job set of time 1500 ms
14/12/12 17:42:50.689 sparkDriver-akka.actor.default-dispatcher-3 ERROR JobScheduler: Error running job streaming job 1500 ms.0
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/var/folders/0k/2qp2p2vs7bv033vljnb8nk1c0000gn/T/1418434967213-0/-1500.result already exists
	at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1045)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:944)
	at org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9.apply(PairDStreamFunctions.scala:677)
	at org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9.apply(PairDStreamFunctions.scala:675)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:32)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:171)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/12/12 17:42:50.691 pool-12-thread-1 INFO SparkContext: Starting job: apply at Transformer.scala:22
{code}

Spark Streaming's {{saveAsHadoopFiles}} method calls Spark's {{rdd.saveAsHadoopFile}} method.  The Spark method, in turn, called {{PairRDDFunctions.saveAsHadoopDataset()}}, which has error-checking to ensure that the output directory does not already exist:

{code}
    if (self.conf.getBoolean(""spark.hadoop.validateOutputSpecs"", true)) {
      // FileOutputFormat ignores the filesystem parameter
      val ignoredFs = FileSystem.get(hadoopConf)
      hadoopConf.getOutputFormat.checkOutputSpecs(ignoredFs, hadoopConf)
    }
{code}

If Spark Streaming recovers from a checkpoint and re-runs the last batch in the checkpoint, then {{saveAsHadoopDataset}} will have been called twice with the same output path.  If the output path exists from the first, pre-recovery run, then the recovery will fail.

This seems like it could be a pretty serious issue: imagine that a streaming job fails partway through a save() operation, then recovers: in this case, the existing directory will prevent us from ever recovering and finishing the save().

Fortunately, this should be simple to fix: we should disable the existing directory checks for output operations called by streaming jobs.",,apachespark,dyzhou,joshrosen,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5545,,,,,,SPARK-5545,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 05 04:42:20 UTC 2015,,,,,,,,,,"0|i23eaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/14 05:44;joshrosen;One subtlety here: we probably shouldn't rely on the SparkConf having {{spark.hadoop.validateOutputSpecs}} set to {{false}}, since this is the SparkContext's {{conf}} and that context might be shared with other non-streaming jobs / tasks.  We also shouldn't mutate it, since, in general, mutating SparkConf is a serious anti-pattern (even in internal code).

Instead, we could add some plumbing so that every {{saveAs*}} RDD method accepts an optional parameter to disable output spec validation.  This solution also kind of messy, though, since it ends up touching a lot of code: if we don't change this everywhere, then there's the possibility that we'll miss some corner-case and re-introduce the bug.

Instead, we might be able to use DynamicVariable to dynamically bypass the output spec checking for jobs that are launched by the streaming scheduler.  This would have a somewhat minimal impact on the source code and would avoid merge-conflict hell when backporting this code, but might be hard to understand.  However, this might be nicer from a user-facing point-of-view since we wouldn't end up cluttering up the {{saveAs*}} methods with a {{bypassOutputSpecValidation}} boolean that's only used by streaming.;;;","13/Dec/14 08:25;joshrosen;Alright, pushed an experimental version of the {{DynamicVariable}} approach as part of my streaming test flakiness PR: https://github.com/apache/spark/pull/3687.

Here's the actual commit: https://github.com/JoshRosen/spark/commit/3db335f01e01986c412ae2a1de6fbe6b8c3a7a32;;;","13/Dec/14 10:07;tdas;I wonder whether there is a semantically cleaner way of doing this. In streaming we know which batches might have been already processed earlier because we store in the checkpoint the batches that were generated and queued. So upon recovery, the validation could be disabled only for those batches that were queue before failure, and not any of the subsequent batches. 
What do you think?
;;;","13/Dec/14 17:36;joshrosen;I thought about the ""figure out whether the batch has already been processed"" approach, but I was worried about various types of partial failure.  The processing of a batch might involve an arbitrary number of Spark jobs, so it seems like it could be possible that an entire {{saveAs*}} operation succeeds even though the batch itself is marked as a failure during a crash.  Also, I was worried about partial failure of the {{saveAs*}} call itself: is it possible for half of the partitions' saving to succeed before the crash such that the output directory is created but half-full?  Or is the output directory created only once the entire output operation has succeeded and committed (e.g. is success / failure atomic from a directory-existence point-of-view)?;;;","24/Dec/14 21:39;tdas;That is a very good point. Lets brainstorm on this offline once I get to your PR.;;;","30/Dec/14 01:31;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3832;;;","30/Dec/14 01:41;joshrosen;[~tdas],

After some more thought, I think that the ""figure out which batch has already been processed"" approach might actually be fine in those partial-failure cases, since there should be a record in the checkpoint that says whether a batch has successfully completed.  I think the crux of the problem is that a job might complete successfully but this might not be reflected in the checkpoint, so we'd _think_ that it was the first time that we've seen a batch even thought it was already processed.

I suppose that we could work around this if we used a write-ahead log to store a record before starting to process a batch, which would avoid this issue.  Can we do this with the existing WAL machinery?  If we already have this functionality, then I think we should use that.;;;","30/Dec/14 02:09;joshrosen;I guess what we'd really want is something akin to a ""batch attempt"" number, but it doesn't look like the current WAL gives us this.;;;","31/Dec/14 19:49;tdas;Lets keep this simple for now, just disable the validity checks for streaming by default. We can look into this later. ;;;","05/Jan/15 04:29;tdas;[~joshrosen] Even if this resolved, there probably should be another JIRA opened regarding the spark.streaming.hadoop.validate , isnt it?;;;","05/Jan/15 04:42;joshrosen;Yeah, let's open a second followup JIRA for that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark fails to clean up cache / lock files in local dirs,SPARK-4834,12761496,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,12/Dec/14 23:56,28/May/15 16:33,14/Jul/23 06:26,23/Dec/14 20:02,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"This issue was caused by https://github.com/apache/spark/commit/7aacb7bfa.

That change shares downloaded jar / files among multiple executors running on the same host by using a lock file and a cache file for each file the executor needs to download. The problem is that these lock and cache files are never deleted.

On Yarn, the app's dir is automatically deleted when the app ends, so no files are left behind. But on standalone, there's no such thing as ""the app's dir""; files will end up in ""/tmp"" or in whatever place the user configure in ""SPARK_LOCAL_DIRS"", and will eventually start to fill that volume.

We should add a way to clean up these files. It's not as simple as ""hey, just call File.deleteOnExit()!"" because we're talking about multiple processes accessing these files, so to maintain the efficiency gains of the original change, the files should only be deleted when the application is finished.",,apachespark,barrybecker4,brett_s_r,joshrosen,vanzin,zfry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-9017,SPARK-7917,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 16:32:19 UTC 2015,,,,,,,,,,"0|i23e5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/14 00:12;vanzin;For example, after running a job that ends up uploading 3 files (the job's jar + two files using ""sc.addJar()""), I end up with this:

{noformat}
/tmp/8760687361418428261220_lock
/tmp/-15888563551418428259097_lock
/tmp/16304071011418428261208_lock
/tmp/8760687361418428261220_cache
/tmp/-15888563551418428259097_cache
/tmp/16304071011418428261208_cache
{noformat}

My first idea was to somehow delete these files when the executors go down. But for that, you'd need to keep a ref count somewhere, turning the locking into something way more complicated than it is now.

So my second solution would change the way local dirs are assigned. Basically, each Worker would create one app-specific dir for each directory in SPARK_LOCAL_DIRS, and set that variable when starting the executor. After the app is done, the Worker would clean up that directory. I haven't looked at whether this would require protocol changes, but it should be sort of simple to do.

I'll start looking at the above solution, but feel free to suggest different approaches in the meantime.;;;","15/Dec/14 22:28;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3705;;;","23/Dec/14 20:02;joshrosen;Issue resolved by pull request 3705
[https://github.com/apache/spark/pull/3705];;;","28/May/15 16:12;zfry;[~joshrosen], 

We are seeing behavior on Spark 1.3.0 where the _files_ in the {{spark.local.dir}} directories are getting cleaned up, but not the _directories_ themselves. 

Its a pretty simple repro: 
Run a job that does some shuffling, wait for the shuffle files to get cleaned up, go and look on disk at {{spark.local.dir}} and notice that the directory(s) are still there, but there are no files in them. 

Should we open another ticket for this? Or can we reopen this one? 

;;;","28/May/15 16:13;joshrosen;Hey [~zfry],

Could you open a separate issue for this?  That will make it easier for us to track where the fix is applied, whether it introduces any new regressions, etc.  Thanks!.;;;","28/May/15 16:32;zfry;Thanks Josh! 

I've filed SPARK-7917 to address. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some other processes might take the daemon pid,SPARK-4832,12761262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,12/Dec/14 07:42,16/Apr/15 00:06,14/Jul/23 06:26,13/Feb/15 10:28,,,,,,,,1.2.2,1.3.0,,,,,Deploy,,,,0,,,,,,Some other processes might use the pid saved in pid file. In that case we should ignore it and launch daemons.,,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5889,SPARK-6952,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 10:28:15 UTC 2015,,,,,,,,,,"0|i23csn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/14 07:43;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3683;;;","13/Feb/15 10:28;srowen;Issue resolved by pull request 3683
[https://github.com/apache/spark/pull/3683];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Current directory always on classpath with spark-submit,SPARK-4831,12761091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,darabos,darabos,darabos,11/Dec/14 15:42,24/Mar/15 10:38,14/Jul/23 06:26,24/Jan/15 23:09,1.1.1,1.2.0,,,,,,1.3.0,,,,,,Deploy,,,,0,,,,,,"We had a situation where we were launching an application with spark-submit, and a file (play.plugins) was on the classpath twice, causing problems (trying to register plugins twice). Upon investigating how it got on the classpath twice, we found that it was present in one of our jars, and also in the current working directory. But the one in the current working directory should not be on the classpath. We never asked spark-submit to put the current directory on the classpath.

I think this is caused by a line in [compute-classpath.sh|https://github.com/apache/spark/blob/v1.2.0-rc2/bin/compute-classpath.sh#L28]:

{code}
CLASSPATH=""$SPARK_CLASSPATH:$SPARK_SUBMIT_CLASSPATH""
{code}

Now if SPARK_CLASSPATH is empty, the empty string is added to the classpath, which means the current working directory.

We tried setting SPARK_CLASSPATH to a bogus value, but that is [not allowed|https://github.com/apache/spark/blob/v1.2.0-rc2/core/src/main/scala/org/apache/spark/SparkConf.scala#L312].

What is the right solution? Only add SPARK_CLASSPATH if it's non-empty? I can send a pull request for that I think. Thanks!",,darabos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6491,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 24 23:09:23 UTC 2015,,,,,,,,,,"0|i23bm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 16:54;srowen;Hm, so I made a quick test, where I put a class {{Foo.class}} inside {{Foo.jar}} and then ran {{java -cp :otherstuff.jar Foo}}. It does not find the class, which suggests to me that it does not interpret that empty entry as meaning ""local directory too"". 

It doesn't work even if I put ""."" on the classpath. That makes sense. The working directory contains JARs, in your case, not classes.

However it finds it if I leave {{Foo.class}} in the working directory, *if* I have an empty entry in the classpath. Is it perhaps finding and exploded directory of classes? Otherwise, I can't repro this directly I suppose, in Java.;;;","11/Dec/14 20:17;darabos;bq. Is it perhaps finding and exploded directory of classes?

Yes, that is exactly the situation. One instance of the file is in a jar, another is just there (""free-floating"") in the directory. It is a configuration file. (Actually it's in a ""conf"" directory, but Play looks for both ""play.plugins"" and ""conf/play.plugins"" with getResources in the classpath. So it finds the copy inside the generated jar, also in the ""conf"" directory of the project. We can of course work around this in numerous ways.)

I think there is no reason for spark-submit to add an empty entry to the classpath. It will just lead to accidents like ours. If the user wants to add an empty entry, they can easily do so.

I've sent https://github.com/apache/spark/pull/3678 as a possible fix. Thanks for investigating!;;;","24/Jan/15 23:09;srowen;Looks like this was merged in https://github.com/apache/spark/commit/7cb3f54793124c527d62906c565aba2c3544e422;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sum and avg over empty table should return null,SPARK-4828,12761030,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,adrian-wang,adrian-wang,11/Dec/14 11:13,12/Dec/14 06:50,14/Jul/23 06:26,12/Dec/14 06:50,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 06:50:03 UTC 2014,,,,,,,,,,"0|i23azb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 11:14;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3675;;;","12/Dec/14 06:50;marmbrus;Issue resolved by pull request 3675
[https://github.com/apache/spark/pull/3675];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Max iterations (100) reached for batch Resolution with deeply nested projects and project *s,SPARK-4827,12760956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,11/Dec/14 07:21,16/Dec/14 23:33,14/Jul/23 06:26,16/Dec/14 23:33,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,nitin2goyal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 16 23:33:12 UTC 2014,,,,,,,,,,"0|i23b13:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"11/Dec/14 07:26;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3674;;;","16/Dec/14 23:33;marmbrus;Issue resolved by pull request 3674
[https://github.com/apache/spark/pull/3674];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Possible flaky tests in WriteAheadLogBackedBlockRDDSuite: ""java.lang.IllegalStateException: File exists and there is no append support!""",SPARK-4826,12760954,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,11/Dec/14 07:08,15/Dec/14 22:34,14/Jul/23 06:26,15/Dec/14 22:34,1.2.0,1.3.0,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,flaky-test,,,,,"I saw a recent master Maven build failure in WriteHeadLogBackedBlockRDDSuite where four tests failed with the same exception.

[Link to test result (this will eventually break)|https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/1156/].  In case that link breaks:

The failed tests:

{code}
org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.Read data available only in block manager, not in write ahead log
org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.Read data available only in write ahead log, not in block manager
org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.Read data available only in write ahead log, and test storing in block manager
org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.Read data with partially available in block manager, and rest in write ahead log
{code}

The error messages are all (essentially) the same:

{code}
     java.lang.IllegalStateException: File exists and there is no append support!
      at org.apache.spark.streaming.util.HdfsUtils$.getOutputStream(HdfsUtils.scala:33)
      at org.apache.spark.streaming.util.WriteAheadLogWriter.org$apache$spark$streaming$util$WriteAheadLogWriter$$stream$lzycompute(WriteAheadLogWriter.scala:34)
      at org.apache.spark.streaming.util.WriteAheadLogWriter.org$apache$spark$streaming$util$WriteAheadLogWriter$$stream(WriteAheadLogWriter.scala:34)
      at org.apache.spark.streaming.util.WriteAheadLogWriter.<init>(WriteAheadLogWriter.scala:42)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.writeLogSegments(WriteAheadLogBackedBlockRDDSuite.scala:140)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.org$apache$spark$streaming$rdd$WriteAheadLogBackedBlockRDDSuite$$testRDD(WriteAheadLogBackedBlockRDDSuite.scala:95)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite$$anonfun$4.apply$mcV$sp(WriteAheadLogBackedBlockRDDSuite.scala:67)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite$$anonfun$4.apply(WriteAheadLogBackedBlockRDDSuite.scala:67)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite$$anonfun$4.apply(WriteAheadLogBackedBlockRDDSuite.scala:67)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
      at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
      at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:318)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$class.run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.org$scalatest$BeforeAndAfterAll$$super$run(WriteAheadLogBackedBlockRDDSuite.scala:31)
      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
      at org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.run(WriteAheadLogBackedBlockRDDSuite.scala:31)
      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$class.run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:318)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)
{code}",,apachespark,hshreedharan,joshrosen,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 15 22:34:22 UTC 2014,,,,,,,,,,"0|i23b1j:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"11/Dec/14 07:09;joshrosen;Actually, it turns out that this has happened twice.  Here's a link to another occurrence of the same failure pattern: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/1147/;;;","14/Dec/14 16:40;joshrosen;I committed a documentation typo fix to {{master}} and {{branch-1.2}} at the same time, which caused a huge number of Maven builds to kick off simultaneously in Jenkins (since it was otherwise idle), and alll of these builds failed due to tests in WriteAheadLogBackedBlockRDDSuite; ; it also broke the master SBT build.

I wonder if there's some kind of sharing / contention where multiple copies of the test are attempting to write to the same directory.

[~hshreedharan], it would be great to get your help with this to see if you can spot any potential problems in that test suite.;;;","14/Dec/14 18:32;hshreedharan;It looks like there is some issue with the directories/files existing (though we use random names for files/dirs). I will see try to get something ready later today;;;","14/Dec/14 20:21;nchammas;This raises an interesting test infrastructure question: Do we have a way of invoking multiple copies of the same test (on the same or across multiple JVMs) to check a test's level of isolation? If not, that might be a good thing to look into.;;;","14/Dec/14 20:57;nchammas;I just cooked up a quick way of invoking this test multiple times in parallel using [GNU parallel|http://www.gnu.org/software/parallel/]:

{code}
parallel 'sbt/sbt -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl -Phive -Phive-thriftserver ""testOnly org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite""' ::: '' '' '' ''
{code}

This will fire up 4 copies of that one test in parallel. I ran it a couple of times on my laptop without issue, but appears to be due to some sbt locking that prevents the tests from actually running in parallel.

I've [posted a question on Stack Overflow|http://stackoverflow.com/questions/27474000/how-can-i-run-multiple-copies-of-the-same-test-in-parallel] about this.;;;","15/Dec/14 03:10;hshreedharan;I suspect that the nextString is conflicting and producing strings that are likely conflicting (since createTempDir is atomic). Using monotonically increasing names for the file counter will likely fix the issue.;;;","15/Dec/14 03:37;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3695;;;","15/Dec/14 19:00;pwendell;I pushed a hotfix disabling these tests, but let's re-enable them once things are working.;;;","15/Dec/14 19:26;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3704;;;","15/Dec/14 22:34;joshrosen;Issue resolved by pull request 3704
[https://github.com/apache/spark/pull/3704];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS fails to resolve when created using saveAsTable,SPARK-4825,12760946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chenghao,marmbrus,marmbrus,11/Dec/14 06:03,12/Dec/14 06:53,14/Jul/23 06:26,12/Dec/14 06:53,1.2.0,,,,,,,1.2.1,,,,,,SQL,,,,0,,,,,,"While writing a test for a different issue, I found that saveAsTable seems to be broken:

{code}
  test(""save join to table"") {
    val testData = sparkContext.parallelize(1 to 10).map(i => TestData(i, i.toString))
    sql(""CREATE TABLE test1 (key INT, value STRING)"")
    testData.insertInto(""test1"")
    sql(""CREATE TABLE test2 (key INT, value STRING)"")
    testData.insertInto(""test2"")
    testData.insertInto(""test2"")
    sql(""SELECT COUNT(a.value) FROM test1 a JOIN test2 b ON a.key = b.key"").saveAsTable(""test"")
    checkAnswer(
      table(""test""),
      sql(""SELECT COUNT(a.value) FROM test1 a JOIN test2 b ON a.key = b.key"").collect().toSeq)
  }
​
    sql(""SELECT COUNT(a.value) FROM test1 a JOIN test2 b ON a.key = b.key"").saveAsTable(""test"")
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved plan found, tree:
'CreateTableAsSelect None, test, false, None
 Aggregate [], [COUNT(value#336) AS _c0#334L]
  Join Inner, Some((key#335 = key#339))
   MetastoreRelation default, test1, Some(a)
   MetastoreRelation default, test2, Some(b)
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 11 07:14:19 UTC 2014,,,,,,,,,,"0|i23b1b:",9223372036854775807,,,,,,,,,,,,,,1.2.1,,,,,,,,,,,"11/Dec/14 07:14;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3673;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.mllib.rand docs not generated correctly,SPARK-4821,12760878,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,10/Dec/14 22:24,12/Jan/15 22:51,14/Jul/23 06:26,17/Dec/14 22:13,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Documentation,MLlib,PySpark,,0,,,,,,"spark/python/docs/pyspark.mllib.rst needs to be updated to reflect the change in package names from pyspark.mllib.random to .rand

Otherwise, the Python API docs are empty.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 22:51:40 UTC 2015,,,,,,,,,,"0|i23au7:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"10/Dec/14 22:36;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/3669;;;","17/Dec/14 22:13;mengxr;Issue resolved by pull request 3669
[https://github.com/apache/spark/pull/3669];;;","12/Jan/15 22:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4011;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join operation should use iterator/lazy evaluation,SPARK-4818,12760804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,johannes.simon,johannes.simon,10/Dec/14 16:57,22/Dec/14 22:28,14/Jul/23 06:26,22/Dec/14 22:28,1.1.1,,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,,,,0,,,,,,"The current implementation of the join operation does not use an iterator (i.e. lazy evaluation), causing it to explicitly evaluate the co-grouped values. In big data applications, these value collections can be very large. This causes the *cartesian product of all co-grouped values* for a specific key of both RDDs to be kept in memory during the flatMapValues operation, resulting in an *O(size(pair._1)*size(pair._2))* memory consumption instead of *O(1)*. Very large value collections will therefore cause ""GC overhead limit exceeded"" exceptions and fail the task, or at least slow down execution dramatically.

{code:title=PairRDDFunctions.scala|borderStyle=solid}
//...
def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = {
  this.cogroup(other, partitioner).flatMapValues( pair =>
    for (v <- pair._1; w <- pair._2) yield (v, w)
  )
}
//...
{code}

Since cogroup returns an Iterable instance of an Array, the join implementation could be changed to the following, which uses lazy evaluation instead, and has almost no memory overhead:
{code:title=PairRDDFunctions.scala|borderStyle=solid}
//...
def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = {
  this.cogroup(other, partitioner).flatMapValues( pair =>
    for (v <- pair._1.iterator; w <- pair._2.iterator) yield (v, w)
  )
}
//...
{code}

Alternatively, if the current implementation is intentionally not using lazy evaluation for some reason, there could be a *lazyJoin()* method next to the original join implementation that utilizes lazy evaluation. This of course applies to other join operations as well.

Thanks! :)",,apachespark,johannes.simon,joshrosen,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4824,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 22 22:28:15 UTC 2014,,,,,,,,,,"0|i23ae7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 03:18;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3671;;;","22/Dec/14 22:28;joshrosen;Issue resolved by pull request 3671
[https://github.com/apache/spark/pull/3671];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven profile netlib-lgpl does not work,SPARK-4816,12760697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,guillaumepitel,guillaumepitel,10/Dec/14 10:33,14/Dec/15 20:38,14/Jul/23 06:26,14/Dec/15 20:17,1.1.0,,,,,,,1.1.1,1.4.2,,,,,Build,,,,0,,,,,,"When doing what the documentation recommends to recompile Spark with Netlib Native system binding (i.e. to bind with openblas or, in my case, MKL), 

mvn -Pnetlib-lgpl -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -DskipTests clean package

The resulting assembly jar still lacked the netlib-system class. (I checked the content of spark-assembly...jar)

When forcing the netlib-lgpl profile in MLLib package to be active, the jar is correctly built.

So I guess it's a problem with the way maven passes profiles activitations to children modules.

Also, despite the documentation claiming that if the job's jar contains netlib with necessary bindings, it should works, it does not. The classloader must be unhappy with two occurrences of netlib ?",maven 3.0.5 / Ubuntu,guillaumepitel,rnowling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 20:38:04 UTC 2015,,,,,,,,,,"0|i239q7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/14 10:57;srowen;No, you can definitely see that the profile is activated in the child module if you try {{mvn -Pnetlib-lgpl dependency:tree}} and see that {{com.github.fommil.netlib:all}} is there.

If I build and {{grep netlib}} on the assembly, I see loads of netlib items, including the expected native libraries. Earlier you showed that you also grepped for ""Native"" but I don't see why this is to be expected and there is no entry with that name.

I don't think there's an issue then?

{code}
...
org/netlib/arpack/Sstatn.class
org/netlib/arpack/Sstats.class
org/netlib/arpack/Sstqrb.class
netlib-native_ref-osx-x86_64.jnilib
netlib-native_ref-osx-x86_64.jnilib.asc
netlib-native_ref-osx-x86_64.pom
netlib-native_ref-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.properties
com/github/fommil/netlib/NativeRefARPACK.class
com/github/fommil/netlib/NativeRefBLAS.class
com/github/fommil/netlib/NativeRefLAPACK.class
...
{code};;;","10/Dec/14 12:21;guillaumepitel;Nevermind, the problem does not occur in the v1.1.1 tag, only in the sources distributed as spark-1.1.1.tgz

I should have used the git to check, sorry

;;;","09/Dec/15 21:24;rnowling;I ran into the same issue with Spark 1.4.  If I download the tarball from {{spark.apache.org}} and build with {{-Pnetlib-lgpl}}, the native libraries are excluded from the jar by the shader.  However, if I check out the branch-1.4 from github and build with that, the appropriate libraries are included.

I don't know much about the source release processes, but it is possible that something in that process is resulting in different maven builds?;;;","10/Dec/15 14:42;srowen;I don't know why it would be different -- are you sure you did a clean build? it's easy to accidentally use artifacts from ~/.m2 sometimes and get a wrench in the works that way, but I don't know how you built.

What about master? I don't think this is an issue since I have been building with netlib successfully recently in 1.5.x.;;;","10/Dec/15 14:50;rnowling;Hi [~srowen],

I haven't tried master yet but that wouldn't address the problem I'm seeing.  As I said, I downloaded the source tarball from the spark.apache.org web site vs checkout out branch-1.4.  I think it has something to do with the release process (but saying this with ignorance of what is involved).  I ran the same build command with both the source tarball (which reported excluding the native libs in the shading) and the branch-1.4 head from git (which reported including the native libs in the shading).

The .m2 repo shouldn't be an issue.  Normally, Spark pulls in the {{core}} artifact ID, which excludes the native libraries.  When the {{netlib-lpgp}} profile is enabled, the Spark MLLib pom.xml adds the {{all}} artifact ID which pulls in the native libs.  ({{all}} is really just a pom.xml file that pulls in {{core}} + native libs).

I get that this is weird.  I also get that my lack of knowledge of the release process is basically zero.  But I shouldn't have different results from git vs the released source tarball.  Maybe it's not the release process -- maybe something has changed in the mean time.  I'll search through the commits on the branch-1.4 for something to related to shading.;;;","10/Dec/15 21:43;srowen;I just tried building the 1.4.1 tarball with -Pnetlib-lgpl and I see the ""netlib-native*"" items in the assembly JAR as expected. How are you building and what are you looking at?;;;","14/Dec/15 14:44;srowen;I'm re-resolving this since I've tested this exact branch and master recently and observe the right native libs in the right place in the assembly.;;;","14/Dec/15 15:31;rnowling;I tested it again to make sure and ran into the same issue:

{code}
$ mvn -version
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)
Maven home: /usr/share/apache-maven
Java version: 1.7.0_85, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.85-2.6.1.2.el7_1.x86_64/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""3.10.0-229.1.2.el7.x86_64"", arch: ""amd64"", family: ""unix""

$ export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m""
$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.1.tgz
$ tar -xzvf spark-1.4.1.tgz
$ cd spark-1.4.1
$ mvn -Pnetlib-lgpl -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
$ zipinfo -1 assembly/target/scala-2.10/spark-assembly-1.4.1-hadoop2.4.0.jar | grep netlib-native

(No output)
{code}

If I build the head from git {{branch-1.4}} and run {{zipinfo}}:

{code}
$ git clone https://github.com/apache/spark.git spark-1.4-netlib
$ cd spark-1.4-netlib
$ git checkout origin/branch-1.4
$ git log | head
commit c7c99857d47e4ca8373ee9ac59e108a9c443dd05
Author: Sean Owen <sowen@cloudera.com>
Date:   Tue Dec 8 14:34:47 2015 +0000

    [SPARK-11652][CORE] Remote code execution with InvokerTransformer

    Fix commons-collection group ID to commons-collections for version 3.x

    Patches earlier PR at https://github.com/apache/spark/pull/9731

$ zipinfo -1 assembly/target/scala-2.10/spark-assembly-1.4.3-SNAPSHOT-hadoop2.4.0.jar | grep netlib-native
netlib-native_ref-osx-x86_64.jnilib
netlib-native_ref-osx-x86_64.jnilib.asc
netlib-native_ref-osx-x86_64.pom
netlib-native_ref-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.properties
netlib-native_ref-linux-x86_64.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.properties
netlib-native_ref-linux-i686.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/pom.properties
netlib-native_ref-win-x86_64.dll
netlib-native_ref-win-x86_64.dll.asc
netlib-native_ref-win-x86_64.pom
netlib-native_ref-win-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/pom.properties
netlib-native_ref-win-i686.dll
netlib-native_ref-win-i686.dll.asc
netlib-native_ref-win-i686.pom
netlib-native_ref-win-i686.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/pom.properties
netlib-native_ref-linux-armhf.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/pom.properties
netlib-native_system-osx-x86_64.jnilib
netlib-native_system-osx-x86_64.jnilib.asc
netlib-native_system-osx-x86_64.pom
netlib-native_system-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/pom.properties
netlib-native_system-linux-x86_64.pom.asc
netlib-native_system-linux-x86_64.pom
netlib-native_system-linux-x86_64.so
netlib-native_system-linux-x86_64.so.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/pom.properties
netlib-native_system-linux-i686.pom
netlib-native_system-linux-i686.so.asc
netlib-native_system-linux-i686.pom.asc
netlib-native_system-linux-i686.so
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/pom.properties
netlib-native_system-linux-armhf.pom
netlib-native_system-linux-armhf.so.asc
netlib-native_system-linux-armhf.pom.asc
netlib-native_system-linux-armhf.so
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/pom.properties
netlib-native_system-win-x86_64.dll
netlib-native_system-win-x86_64.dll.asc
netlib-native_system-win-x86_64.pom
netlib-native_system-win-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/pom.properties
netlib-native_system-win-i686.dll
netlib-native_system-win-i686.dll.asc
netlib-native_system-win-i686.pom
netlib-native_system-win-i686.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/pom.properties
{code}

I'm testing on CentOS 7 but using a custom installed Maven version instead of the EPEL default.;;;","14/Dec/15 15:36;srowen;I did the same pretty much down to the letter and found the netlib-native artifacts in the assembly JAR. I'll try your exact steps later to see what i can see.;;;","14/Dec/15 15:57;rnowling;I think [SPARK-9507] fixed the issue. I checked out git commit {{5ad9f950c4bd0042d79cdccb5277c10f8412be85}} (the commit before [https://github.com/apache/spark/commit/b53ca247d4a965002a9f31758ea2b28fe117d45f]) and found that the {{netlib-native}} libraries were missing:

{code}
$ git checkout 5ad9f950c4bd0042d79cdccb5277c10f8412be85
$ mvn -Pnetlib-lgpl -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
$ zipinfo -1 assembly/target/scala-2.10/spark-assembly-1.4.2-SNAPSHOT-hadoop2.4.0.jar | grep netlib-native

(No output)
{code}

I then checked out {{b53ca247d4a965002a9f31758ea2b28fe117d45f}} and built it to test:

{code}
$ zipinfo -1 assembly/target/scala-2.10/spark-assembly-1.4.2-SNAPSHOT-hadoop2.4.0.jar | grep netlib-native
netlib-native_ref-osx-x86_64.jnilib
netlib-native_ref-osx-x86_64.jnilib.asc
netlib-native_ref-osx-x86_64.pom
netlib-native_ref-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.properties
netlib-native_ref-linux-x86_64.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.properties
netlib-native_ref-linux-i686.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/pom.properties
netlib-native_ref-win-x86_64.dll
netlib-native_ref-win-x86_64.dll.asc
netlib-native_ref-win-x86_64.pom
netlib-native_ref-win-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/pom.properties
netlib-native_ref-win-i686.dll
netlib-native_ref-win-i686.dll.asc
netlib-native_ref-win-i686.pom
netlib-native_ref-win-i686.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/pom.properties
netlib-native_ref-linux-armhf.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/pom.properties
netlib-native_system-osx-x86_64.jnilib
netlib-native_system-osx-x86_64.jnilib.asc
netlib-native_system-osx-x86_64.pom
netlib-native_system-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/pom.properties
netlib-native_system-linux-x86_64.pom.asc
netlib-native_system-linux-x86_64.pom
netlib-native_system-linux-x86_64.so
netlib-native_system-linux-x86_64.so.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/pom.properties
netlib-native_system-linux-i686.pom
netlib-native_system-linux-i686.so.asc
netlib-native_system-linux-i686.pom.asc
netlib-native_system-linux-i686.so
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/pom.properties
netlib-native_system-linux-armhf.pom
netlib-native_system-linux-armhf.so.asc
netlib-native_system-linux-armhf.pom.asc
netlib-native_system-linux-armhf.so
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/pom.properties
netlib-native_system-win-x86_64.dll
netlib-native_system-win-x86_64.dll.asc
netlib-native_system-win-x86_64.pom
netlib-native_system-win-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/pom.properties
netlib-native_system-win-i686.dll
netlib-native_system-win-i686.dll.asc
netlib-native_system-win-i686.pom
netlib-native_system-win-i686.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/pom.properties
{code}

As such, I can confirm that [SPARK-9507] resolved the issue that was probably created by  the changes in [SPARK-8819].;;;","14/Dec/15 17:41;srowen;I tried your exact build above and I get lots of hits for netlib-native:

{code}
$ zipinfo -1 assembly/target/scala-2.10/spark-assembly-1.4.1-hadoop2.4.0.jar | grep netlib-native
netlib-native_ref-osx-x86_64.jnilib
netlib-native_ref-osx-x86_64.jnilib.asc
netlib-native_ref-osx-x86_64.pom
netlib-native_ref-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.properties
netlib-native_ref-linux-x86_64.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.properties
netlib-native_ref-linux-i686.so
...
{code}

Still not sure what to make of the difference here, though it seems to work here. You're saying that at worst you think it's fixed for 1.4.2 by SPARK-9507. In that case I suppose it's already resolved, just not in the 1.4.1 release. But as I say it does seem to work for me.;;;","14/Dec/15 17:51;rnowling;I want to push for two things (a) some sort of documentation for users (e.g., release notes in the next releases) and (b) make sure it's fixed in the latest releases.  I want users to be able to find documentation (like this JIRA) so they don't have to spend time tracking it down like I did.  

Spark 1.4.2 hasn't been released yet and git has moved to a 1.4.3 SNAPSHOT.  You mention adding the commit to the 1.5.x branch in the commit -- has this been done?

Until 1.4.3 and a 1.5.x release are out with your change, this could still hit certain users, even if it's rare because it's tied to a specific Maven version or such.;;;","14/Dec/15 17:55;rnowling;Also, what version of Maven are you running?;;;","14/Dec/15 18:13;srowen;I'm on Maven 3.3.x. I wonder if that could be a difference -- can you try 3.3.x just to check?

If you're correct, this is already fixed for the next 1.4 which should be 1.4.2. I don't know if/when that will be released though. (I also don't know why the branch shows 1.4.3-SNAPSHOT) It's as fixed as it would be for this branch though. But then yes it would be listed as fixed as part of any release notes, automatically.

I think finding a relevant JIRA may be as good as it gets in the general case for finding whether something's already known as an issue and fixed. This one ought to be easy to find by keyword. Of course -- if there is a problem -- just having it work in later releases is even better.

I'm not aware of any additional fix that needs to be made though. As I say I can't even reproduce it.;;;","14/Dec/15 18:15;rnowling;Happy to try Maven 3.3.x and report back.  Would certainly confirm if it's a Maven bug or regression in behavior.;;;","14/Dec/15 20:05;rnowling;Tried with Maven 3.3.9.  I see no issues with the newer version of Maven:

{code}
$ mvn -version
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T16:41:47+00:00)
Maven home: /root/apache-maven-3.3.9
Java version: 1.7.0_85, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.85-2.6.1.2.el7_1.x86_64/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""3.10.0-229.1.2.el7.x86_64"", arch: ""amd64"", family: ""unix""
$ zipinfo -1 assembly/target/scala-2.10/spark-assembly-1.4.1-hadoop2.4.0.jar | grep netlib-native
netlib-native_ref-osx-x86_64.jnilib
netlib-native_ref-osx-x86_64.jnilib.asc
netlib-native_ref-osx-x86_64.pom
netlib-native_ref-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-osx-x86_64/pom.properties
netlib-native_ref-linux-x86_64.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-x86_64/pom.properties
netlib-native_ref-linux-i686.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-i686/pom.properties
netlib-native_ref-win-x86_64.dll
netlib-native_ref-win-x86_64.dll.asc
netlib-native_ref-win-x86_64.pom
netlib-native_ref-win-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-x86_64/pom.properties
netlib-native_ref-win-i686.dll
netlib-native_ref-win-i686.dll.asc
netlib-native_ref-win-i686.pom
netlib-native_ref-win-i686.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-win-i686/pom.properties
netlib-native_ref-linux-armhf.so
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_ref-linux-armhf/pom.properties
netlib-native_system-osx-x86_64.jnilib
netlib-native_system-osx-x86_64.jnilib.asc
netlib-native_system-osx-x86_64.pom
netlib-native_system-osx-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-osx-x86_64/pom.properties
netlib-native_system-linux-x86_64.pom.asc
netlib-native_system-linux-x86_64.pom
netlib-native_system-linux-x86_64.so
netlib-native_system-linux-x86_64.so.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-x86_64/pom.properties
netlib-native_system-linux-i686.pom
netlib-native_system-linux-i686.so.asc
netlib-native_system-linux-i686.pom.asc
netlib-native_system-linux-i686.so
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-i686/pom.properties
netlib-native_system-linux-armhf.pom
netlib-native_system-linux-armhf.so.asc
netlib-native_system-linux-armhf.pom.asc
netlib-native_system-linux-armhf.so
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-linux-armhf/pom.properties
netlib-native_system-win-x86_64.dll
netlib-native_system-win-x86_64.dll.asc
netlib-native_system-win-x86_64.pom
netlib-native_system-win-x86_64.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-x86_64/pom.properties
netlib-native_system-win-i686.dll
netlib-native_system-win-i686.dll.asc
netlib-native_system-win-i686.pom
netlib-native_system-win-i686.pom.asc
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/pom.xml
META-INF/maven/com.github.fommil.netlib/netlib-native_system-win-i686/pom.properties
{code}

Thus, it's likely only a problem with older versions of Maven or its plugins.  Maven 3.0+ is support for Spark 1.4.1 according to the docs.  Even without the commit that fixed the issue, this probably wouldn't be an issue with 1.5.x+ since they only support Maven 3.3.x+.;;;","14/Dec/15 20:17;srowen;I see, so it's re-fixed for older (but supported) versions of Maven by a commit already in the branch. Elsewhere, it's a moot point. I guess we can consider it fixed as a better resolution here.;;;","14/Dec/15 20:38;rnowling;Agreed.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Enable assertions in SBT, Maven tests / AssertionError from Hive's LazyBinaryInteger",SPARK-4814,12760689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,10/Dec/14 09:42,24/Mar/15 10:23,14/Jul/23 06:26,24/Mar/15 10:23,1.1.0,,,,,,,1.1.2,1.2.2,1.3.0,,,,Spark Core,SQL,,,0,,,,,,"Follow up to SPARK-4159, wherein we noticed that Java tests weren't running in Maven, in part because a Java test actually fails with {{AssertionError}}. That code/test was fixed in SPARK-4850.

The reason it wasn't caught by SBT tests was that they don't run with assertions on, and Maven's surefire does.

Turning on assertions in the SBT build is trivial, adding one line:

{code}
    javaOptions in Test += ""-ea"",
{code}

This reveals a test failure in Scala test suites though:

{code}
[info] - alter_merge_2 *** FAILED *** (1 second, 305 milliseconds)
[info]   Failed to execute query using catalyst:
[info]   Error: Job aborted due to stage failure: Task 1 in stage 551.0 failed 1 times, most recent failure: Lost task 1.0 in stage 551.0 (TID 1532, localhost): java.lang.AssertionError
[info]   	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryInteger.init(LazyBinaryInteger.java:51)
[info]   	at org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase$FieldInfo.uncheckedGetField(ColumnarStructBase.java:110)
[info]   	at org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.getField(ColumnarStructBase.java:171)
[info]   	at org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.getStructFieldData(ColumnarStructObjectInspector.java:166)
[info]   	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$1.apply(TableReader.scala:318)
[info]   	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$1.apply(TableReader.scala:314)
[info]   	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
[info]   	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$6.apply(Aggregate.scala:132)
[info]   	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$6.apply(Aggregate.scala:128)
[info]   	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:615)
[info]   	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:615)
[info]   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
[info]   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
[info]   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
[info]   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
[info]   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
[info]   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
[info]   	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
[info]   	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
[info]   	at org.apache.spark.scheduler.Task.run(Task.scala:56)
[info]   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
[info]   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   	at java.lang.Thread.run(Thread.java:745)
{code}

The items for this JIRA are therefore:

- Enable assertions in SBT
- Fix this failure
- Figure out why Maven scalatest didn't trigger it - may need assertions explicitly turned on too.",,apachespark,donnchadh,joshrosen,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4159,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 10:23:08 UTC 2015,,,,,,,,,,"0|i239of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/14 14:52;srowen;CC [~lian cheng] as someone who might understand the {{AssertionError}} above. It's coming from Hive and I don't know if it's anything we are doing wrong. But unfortunately the test fails when assertions are on as a result of this. The changes to enable assertions themselves are trivial.;;;","11/Dec/14 01:49;lian cheng;Thanks [~srowen]! I'll take a look.;;;","11/Dec/14 02:31;marmbrus;Hive throws a lot of warning here for some reason even when reading data from their own test files.  One option here would be to just override the configuration only for the hive subproject.

{code}
// Hive throws assertion errors for tests that pass.
javaOptions in Test := (javaOptions in Test).value.filterNot(_ == ""-ea""),
{code};;;","11/Dec/14 14:38;lian cheng;This assertion failure seems to be related to details of the RCFile format. I haven't found the root cause. Tested the original {{alter_merge_2.q}} test case under Hive 0.13.1 by running
{code}
$ cd itest/qtest
$ mvn -Dtest=TestCliDriver -Phadoop-2 -Dqfile=alter_merge_2.q test
{code}
 didn't observe similar {{AssertionError}}. Keep digging. Although in this case the assertion failure doesn't affect correctness, I'm not sure whether it's generally safe to ignore it...;;;","14/Dec/14 18:03;marmbrus;Either way, I don't think we should block turning on assertions for the rest of Spark.;;;","14/Dec/14 21:28;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3692;;;","16/Dec/14 01:18;joshrosen;Alright, I've merged Sean's PR into master, branch-1.2, and branch-1.1.  There's a large-ish merge conflict that we'll have to fix to get this into branch-1.0 (or I can just manually fix things up there).  Tagging this as {{backport-needed}} so I remember to come back and do that.;;;","27/Feb/15 15:29;srowen;General question on back-ports: at this stage, is it realistic to expect another 0.9.x release? 1.0.x? 1.1.x? That is I'm wondering if something marked for back-port for 0.9, 1.0, 1.1 can be simply resolved now or not.;;;","24/Mar/15 10:23;srowen;Provisionally deciding that it's not worth back-porting to 1.0.x at this stage, now 3 minor versions behind.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContextWaiter didn't handle 'spurious wakeup',SPARK-4813,12760679,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zsxwing,zsxwing,10/Dec/14 08:33,23/Jan/15 06:57,14/Jul/23 06:26,30/Dec/14 22:42,,,,,,,,1.0.3,1.1.2,1.2.1,1.3.0,,,DStreams,,,,0,,,,,,"According to [javadocs|https://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#wait(long)],
{quote}
A thread can also wake up without being notified, interrupted, or timing out, a so-called spurious wakeup. While this will rarely occur in practice, applications must guard against it by testing for the condition that should have caused the thread to be awakened, and continuing to wait if the condition is not satisfied. In other words, waits should always occur in loops, like this one:

     synchronized (obj) {
         while (<condition does not hold>)
             obj.wait(timeout);
         ... // Perform action appropriate to condition
     }
{quote}
`wait` should always occur in loops.

But now ContextWaiter.waitForStopOrError doesn't.
{code}
  def waitForStopOrError(timeout: Long = -1) = synchronized {
    // If already had error, then throw it
    if (error != null) {
      throw error
    }

    // If not already stopped, then wait
    if (!stopped) {
      if (timeout < 0) wait() else wait(timeout)
      if (error != null) throw error
    }
  }
{code}",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5379,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 10 08:49:29 UTC 2014,,,,,,,,,,"0|i239m7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/14 08:49;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkPlan.codegenEnabled may be initialized to a wrong value,SPARK-4812,12760665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,10/Dec/14 07:26,16/Dec/14 22:38,14/Jul/23 06:26,16/Dec/14 22:38,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"The problem is `codegenEnabled` is `val`, but it uses a `val` `sqlContext`, which can be override by subclasses. Here is a simple example to show this issue.

{code}
scala> :paste
// Entering paste mode (ctrl-D to finish)

abstract class Foo {

  protected val sqlContext = ""Foo""

  val codegenEnabled: Boolean = {
    println(sqlContext) // it will call subclass's `sqlContext` which has not yet been initialized.
    if (sqlContext != null) {
      true
    } else {
      false
    }
  }
}

class Bar extends Foo {
  override val sqlContext = ""Bar""
}

println(new Bar().codegenEnabled)

// Exiting paste mode, now interpreting.

null
false
defined class Foo
defined class Bar

scala> 
{code}

We should make `sqlContext` `final` to prevent subclasses from overriding it incorrectly.",,apachespark,marmbrus,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 16 22:38:12 UTC 2014,,,,,,,,,,"0|i239j3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"10/Dec/14 07:32;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3660;;;","16/Dec/14 22:38;marmbrus;Issue resolved by pull request 3660
[https://github.com/apache/spark/pull/3660];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark fails to spill with small number of large objects,SPARK-4808,12760619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mkim,dlawler,dlawler,10/Dec/14 01:50,19/May/15 16:11,14/Jul/23 06:26,19/May/15 16:11,1.0.2,1.1.0,1.2.0,1.2.1,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Spillable's maybeSpill does not allow spill to occur until at least 1000 elements have been spilled, and then will only evaluate spill every 32nd element thereafter.  When there is a small number of very large items being tracked, out-of-memory conditions may occur.

I suspect that this and the every-32nd-element behavior was to reduce the impact of the estimateSize() call.  This method was extracted into SizeTracker, which implements its own exponential backup for size estimation, so now we are only avoiding using the resulting estimated size.",,aash,andrewor14,apachespark,dlawler,michaelmalak,mkim,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5915,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 13:16:18 UTC 2015,,,,,,,,,,"0|i2398v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,"10/Dec/14 01:53;apachespark;User 'lawlerd' has created a pull request for this issue:
https://github.com/apache/spark/pull/3656;;;","06/Feb/15 06:06;apachespark;User 'mingyukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/4420;;;","22/Feb/15 08:57;srowen;[~andrewor14] Is this resolved for 1.2.2 / 1.3.0?
https://github.com/apache/spark/pull/4420#issuecomment-75178396;;;","22/Feb/15 17:41;andrewor14;[~sowen] not fully. The PR you linked to is just a temporary fix. The real fix probably won't make it into 1.3;;;","16/Mar/15 21:41;mkim;[~andrewor14], should this now be closed with the fix version 1.4? What's the next step?;;;","15/May/15 13:16;srowen;I think this is considered resolved now for 1.4 after https://github.com/apache/spark/commit/3be92cdac30cf488e09dbdaaa70e5c4cdaa9a099 ? but not 1.3.
Maybe [~andrewor14] can confirm.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockTransferMessage.toByteArray() trips assertion,SPARK-4805,12760587,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,09/Dec/14 22:49,17/May/20 18:30,14/Jul/23 06:26,10/Dec/14 00:39,1.1.0,,,,,,,,,,,,,Shuffle,Spark Core,,,0,,,,,,"The {{ByteBuf}} below is allocated to have enough room to encode just the message, but a type byte is written too. The buffer resizes and is left with capacity, which is a minor waste. But when assertions are on, it fails the assertion below. This is triggered by unit tests, but the Java test for this wasn't running. You can verify that a simple ""+1"" in the allocated size fixes it.

{code}
  public byte[] toByteArray() {
    ByteBuf buf = Unpooled.buffer(encodedLength());
    buf.writeByte(type().id);
    encode(buf);
    assert buf.writableBytes() == 0 : ""Writable bytes remain: "" + buf.writableBytes();
    return buf.array();
  }
{code}

[~ilikerps]",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 22:51:41 UTC 2014,,,,,,,,,,"0|i2391z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 22:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3650;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate RegisterReceiver messages sent from ReceiverSupervisor,SPARK-4803,12760550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,ilayaperumalg,ilayaperumalg,09/Dec/14 21:14,20/Jan/15 09:48,14/Jul/23 06:26,20/Jan/15 09:48,,,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,,,,,,"The ReceiverTracker receivers `RegisterReceiver` messages two times
     1) When the actor at `ReceiverSupervisorImpl`'s preStart is invoked
     2) After the receiver is started at the executor `onReceiverStart()` at `ReceiverSupervisorImpl`

Though the 'RegisterReceiver' message uses the same streamId and the receiverInfo gets updated every time the message is processed at the `ReceiverTracker`, it makes sense to call register receiver only after the
receiver is started.

or, am I missing something here?",,apachespark,ilayaperumalg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 21:16:00 UTC 2014,,,,,,,,,,"0|i238uf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 21:16;apachespark;User 'ilayaperumalg' has created a pull request for this issue:
https://github.com/apache/spark/pull/3648;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReceiverInfo removal at ReceiverTracker upon deregistering receiver,SPARK-4802,12760533,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ilayaperumalg,ilayaperumalg,09/Dec/14 20:13,23/Dec/14 23:24,14/Jul/23 06:26,23/Dec/14 23:24,1.1.0,,,,,,,1.1.2,1.2.1,1.3.0,,,,DStreams,,,,0,,,,,,"When the streaming receiver is deregistered, the ReceiverTracker doesn't remove the corresponding receiverInfo entry for the receiver.

When the receiver is stopped at the executor and the ReceiverTrackerActor that processes the 'DeregisterReceiver' message. Shouldn't it remove the receiverInfo entry for that receiver as the receiver is actually deregistered?

Not sure if there is any specific reason for not removing it.
Currently, I see this warning if we don't remove it:

WARN main-EventThread scheduler.ReceiverTracker - All of the receivers have not deregistered, Map(0 -> ReceiverInfo(0,MyReceiver-0,null,false,localhost,Stopped by driver,))

",,apachespark,ilayaperumalg,joshrosen,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2892,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 23:12:42 UTC 2014,,,,,,,,,,"0|i238qv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 20:15;ilayaperumalg;Submitted PR: https://github.com/apache/spark/pull/3647

Let me know if that makes sense.
Thanks!;;;","09/Dec/14 20:15;apachespark;User 'ilayaperumalg' has created a pull request for this issue:
https://github.com/apache/spark/pull/3647;;;","23/Dec/14 23:12;tdas;SPARK-2892 is not a duplicate of this though they have same symptoms, the reason are different but related. SPARK-2892 affects only socket receiver and is probably because the socket receiver does not stop cleanly. SPARK-4802 affects all receivers, and prevents the driver from realizing that all receivers have been closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test in ReceivedBlockTrackerSuite: ""block addition, block to batch allocation, and cleanup with write ahead log""",SPARK-4790,12760281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,joshrosen,joshrosen,08/Dec/14 22:04,31/Dec/14 22:37,14/Jul/23 06:26,31/Dec/14 22:37,1.3.0,,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,flaky-test,,,,,"Found another flaky streaming test, ""org.apache.spark.streaming.ReceivedBlockTrackerSuite.block addition, block to batch allocation and cleanup with write ahead log"":

{code}
Error Message

File /tmp/1418069118106-0/receivedBlockMetadata/log-0-1000 does not exist.

Stacktrace

sbt.ForkMain$ForkError: File /tmp/1418069118106-0/receivedBlockMetadata/log-0-1000 does not exist.
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:397)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:324)
	at org.apache.spark.streaming.util.WriteAheadLogSuite$.getLogFilesInDirectory(WriteAheadLogSuite.scala:344)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite.getWriteAheadLogFiles(ReceivedBlockTrackerSuite.scala:248)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite$$anonfun$4.apply$mcV$sp(ReceivedBlockTrackerSuite.scala:173)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite$$anonfun$4.apply(ReceivedBlockTrackerSuite.scala:96)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite$$anonfun$4.apply(ReceivedBlockTrackerSuite.scala:96)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite.org$scalatest$BeforeAndAfter$$super$runTest(ReceivedBlockTrackerSuite.scala:41)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite.runTest(ReceivedBlockTrackerSuite.scala:41)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite.org$scalatest$BeforeAndAfter$$super$run(ReceivedBlockTrackerSuite.scala:41)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.ReceivedBlockTrackerSuite.run(ReceivedBlockTrackerSuite.scala:41)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,hshreedharan,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 21:39:15 UTC 2014,,,,,,,,,,"0|i2377r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/14 07:03;joshrosen;Curiously, it looks like this test hasn't failed recently in the pull request builder ([link|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/24351/testReport/junit/org.apache.spark.streaming/ReceivedBlockTrackerSuite/block_addition__block_to_batch_allocation_and_cleanup_with_write_ahead_log/history/]).  However, it looks like it has failed on-and-off in the Maven builds ([link|https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/1132/hadoop.version=1.0.4,label=centos/testReport/junit/org.apache.spark.streaming/ReceivedBlockTrackerSuite/block_addition__block_to_batch_allocation_and_cleanup_with_write_ahead_log/history/]).  Perhaps the problem could be related to test suite initialization working differently in Maven than SBT?  Just a theory; maybe it actually _has_ failed in the PRB and the recent test history is just a lucky streak.;;;","11/Dec/14 07:11;joshrosen;Actually, scratch that theory: here's a failure in the Master SBT build: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/1206/;;;","16/Dec/14 07:45;joshrosen;/cc [~hshreedharan], you might want to take a look at this issue, too.  This test seems to fail intermittently on Jenkins.  Since this is a new test, I think we should fix its flakiness in its own PR.;;;","17/Dec/14 20:49;hshreedharan;Looks like an HDFS bug - and this looks like it affects a really old version of HDFS (1.0.2). This code is all within RawLocalFileSystem.java is listing the files and the calling getFileStatus on each one of them - and for some reason a file which was returned from the listing itself is missing in the getFileStatus call (newer versions of HDFS simply eat the FileNotFoundException).;;;","17/Dec/14 20:53;hshreedharan;Ah, so this is the issue:
tracker3.cleanupOldBatches(batchTime2) eventually ends up calling deleteFiles() in WriteAheadLogManager which is run in another thread. The thread may or may not be done before the getWrittenLogData call is made. So if the deletion happens in between the list call and the getFileStatus call happening in HDFS, this race conditions gets hit.;;;","17/Dec/14 21:39;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet filter pushdown for BYTE and SHORT types,SPARK-4786,12760154,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,lian cheng,lian cheng,08/Dec/14 14:12,29/Jan/15 23:42,14/Jul/23 06:26,29/Jan/15 23:42,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Among all integral types, currently only INT and LONG predicates can be converted to Parquet filter predicate. BYTE and SHORT predicates can be covered by INT.",,apachespark,lian cheng,marmbrus,saucam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 29 23:42:46 UTC 2015,,,,,,,,,,"0|i236hb:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"22/Jan/15 06:14;apachespark;User 'saucam' has created a pull request for this issue:
https://github.com/apache/spark/pull/4156;;;","29/Jan/15 23:42;marmbrus;Issue resolved by pull request 4156
[https://github.com/apache/spark/pull/4156];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When called with arguments referring column fields, PMOD throws NPE",SPARK-4785,12760079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,lian cheng,lian cheng,08/Dec/14 07:19,09/Dec/14 18:35,14/Jul/23 06:26,09/Dec/14 18:35,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Reproducible when compiled with {{-Phive-0.13.1}}, {{-Phive0.12.0}} is OK.

Reproduction steps with {{hive/console}}:
{code}
scala> loadTestTable(""src"")
scala> sql(""SELECT PMOD(key, 10) FROM src LIMIT 1"").collect()
...
14/12/08 15:11:31 INFO DAGScheduler: Job 0 failed: runJob at basicOperators.scala:141, took 0.235788 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.initialize(GenericUDFBaseNumeric.java:109)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:116)
        at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector$lzycompute(hiveUdfs.scala:156)
        at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector(hiveUdfs.scala:155)
        at org.apache.spark.sql.hive.HiveGenericUdf.eval(hiveUdfs.scala:174)
        at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:92)
        at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)
        at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:312)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.Limit$$anonfun$4.apply(basicOperators.scala:141)
        at org.apache.spark.sql.execution.Limit$$anonfun$4.apply(basicOperators.scala:141)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

This issue is introduced in [PR #3109|https://github.com/apache/spark/pull/3109/files#diff-010a66b2d9b5e8a991c7b23f666a2036R156], where {{GenericUDF.initialize}} was replaced by {{GenericUDF.initializeAndFoldConstants}}, which then calls {{GenericUDFBaseNumeric.initialize}} in case of {{PMOD}}. However, {{GenericUDFBaseNumeric.initialize}} needs to access the current {{SessionState}} [\[1\]|https://github.com/apache/hive/blob/release-0.13.1/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBaseNumeric.java#L109], which only exists on the driver side. Thus, when executed on executor side, an NPE is thrown.",,apachespark,lian cheng,marmbrus,nemccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 18:35:22 UTC 2014,,,,,,,,,,"0|i23613:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"08/Dec/14 09:10;lian cheng;Looked into this a bit. Seems that this issue is caused by a UDF contract change in Hive 0.13.1. In Hive 0.12.0, it's always safe to construct and initialize a fresh UDF object on worker side, while in Hive 0.13.1, UDF objects should only be initialized on driver side and then serialized to the worker side.

Thanks [~chenghao] for helping investigating this issue!;;;","09/Dec/14 03:55;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3640;;;","09/Dec/14 18:35;marmbrus;Issue resolved by pull request 3640
[https://github.com/apache/spark/pull/3640];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System.exit() calls in SparkContext disrupt applications embedding Spark,SPARK-4783,12760021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,hymanroth,hymanroth,07/Dec/14 12:25,02/Jun/16 11:27,14/Jul/23 06:26,16/Apr/15 09:45,,,,,,,,1.4.0,,,,,,Spark Core,,,,2,,,,,,"A common architectural choice for integrating Spark within a larger application is to employ a gateway to handle Spark jobs. The gateway is a server which contains one or more long-running sparkcontexts.

A typical server is created with the following pseudo code:

var continue = true
while (continue){
 try {
    server.run() 
  } catch (e) {
  continue = log_and_examine_error(e)
}

The problem is that sparkcontext frequently calls System.exit when it encounters a problem which means the server can only be re-spawned at the process level, which is much more messy than the simple code above.

Therefore, I believe it makes sense to replace all System.exit calls in sparkcontext with the throwing of a fatal error. ",,apachespark,ardlema,hymanroth,jahubba,michalklos,pwendell,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15685,SPARK-6804,SPARK-9687,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 11:37:02 UTC 2015,,,,,,,,,,"0|i235o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/14 01:10;pwendell;For code cleanliness, we should go through and look everywhere we call System.exit() and see which ones can be converted safely to exceptions.  Most of our use of System.exit is on the executor side, but there may be a few on the driver/SparkContext side.

That said, if there is a fatal exception in the SparkContext, I don't think your app can safely just catch the exception, log it, and create a new SparkContext. Is that what you are trying to do? In that case there could be static state around that is not properly cleaned up and will cause the new context to be buggy.

;;;","08/Dec/14 10:13;hymanroth;The key idea with this proposal is that the server at least gets a chance to log the error (remotely) and decide what to do next. If the server also provides other services beyond Spark, these can continue to be available even if the Spark part is down and cannot be safely restarted.

As regards a possibly buggy static state, yes that would be a problem.

As an interim solution, I suggest SparkContext would throw an exception with extends either ""SparkRecoverable"" or ""SparkUnrecoverable"" (for example).

In the longer term, I can't think of a reason why SparkContext could not clean and recreate the static state upon each invocation.
;;;","07/Apr/15 17:57;michalklos;We are running into this exact issue. We have a driver application that has other responsibilities beyond submitting spark work and we don't want it to die if there is an issue with the cluster. The cluster can be recovered or a new one can be spun up with the same DNS, and we can start fresh with a new context. But, in the meantime, we want the driver app to continue running and carry on its other business. 

Specifically for us, we are having issues with this exit:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L409-L411

We are considering patching it, but not sure whether or not this will cause other problems or if there was a good reason it hasn't been removed yet.;;;","10/Apr/15 06:37;ardlema;We are having exactly the same issue. In fact yesterday I opened a new jira ticket to report this (I wasn't aware of this one): https://issues.apache.org/jira/browse/SPARK-6804

In our case we are building a web application that is using Spark under the hood and we are also having issues with the same exit invocation at the TaskSchedulerImpl class. As that call is killing the JVM our web app is also killed.

I'm totally agree with the throwing exception approach proposal.;;;","10/Apr/15 08:58;stevel@apache.org;There's a class {{org.apache.hadoop.util.ExitUtil}} that wraps all of Hadoop's inner exit codes; lets you optionall download the feature. It's tagged as hadoop private, but that is fixable; HADOOP-9626 would be the place to do it. It was primarily written for testing, but with that extra patch, would allow exceptions to serve up the exit code.;;;","10/Apr/15 12:01;srowen;How about narrowly addressing what appears to be the pain point here, which is {{TaskSchedulerImpl}}? I don't immediately see why that has to exit instantly. Scanning the other occurrences they look mostly ""OK"" in that they are in CLI programs, examples, tests, daemon processes, rather than something that would be embedded. Go for a PR.;;;","13/Apr/15 07:03;ardlema;Does it mean that you guys are going to create a PR with a fix/change proposal for this? Or just asking someone to create that PR? If so I am willing to create it.;;;","13/Apr/15 12:38;srowen;I have a PR ready, but am testing it. I am seeing test failures but am not sure if they're related. You are also welcome to go ahead with a PR if you think you have a handle on it and I can chime in with what I know.;;;","13/Apr/15 13:31;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5492;;;","16/Apr/15 09:45;srowen;Issue resolved by pull request 5492
[https://github.com/apache/spark/pull/5492];;;","06/Aug/15 11:37;ardlema;Still having this issue. We've found out that the exception throw by TaskSchedulerImpl is being caught by SparkUncaughtException which is calling System.exit() again.

Would it make sense just logging the error and not throwing the exception? See https://github.com/apache/spark/pull/7993;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some block memory after unrollSafely not count into used memory(memoryStore.entrys or unrollMemory),SPARK-4777,12759963,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,SuYan,SuYan,SuYan,06/Dec/14 11:40,03/Mar/15 00:54,14/Jul/23 06:26,03/Mar/15 00:54,1.1.0,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Some memory not count into memory used by memoryStore or unrollMemory.

Thread A after unrollsafely memory, it will release 40MB unrollMemory(40MB will used by other threads). then ThreadA wait get accountingLock to tryToPut blockA(30MB). before Thread A get accountingLock, blockA memory size is not counting into unrollMemory or memoryStore.currentMemory.
  
 IIUC, freeMemory should minus that block memory",,apachespark,SuYan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 02:25:57 UTC 2014,,,,,,,,,,"0|i235bb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,"06/Dec/14 12:08;apachespark;User 'suyanNone' has created a pull request for this issue:
https://github.com/apache/spark/pull/3629;;;","16/Dec/14 11:50;srowen;[~SuYan] Given your comment in your PR, do you intend this to be considered a duplicate of SPARK-3000?;;;","17/Dec/14 02:25;SuYan;Sean Owen, Hi, I intended to close that patch, but after having discussed with SPARK-3000 author, he says, current SPARK-3000 is have not merge into spark yet, so the problem is still in current code, so let's keep my patch still be open. 
If there are not a need to keep it open, tell me, I will close it, thanks.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make HiveFromSpark example more portable,SPARK-4774,12759926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kostas,kostas,kostas,06/Dec/14 00:32,08/Dec/14 23:45,14/Jul/23 06:26,08/Dec/14 23:45,,,,,,,,1.2.0,,,,,,Examples,SQL,,,0,,,,,,"The HiveFromSpark example needs the kv1.txt file to run in a specific location: SPARK_HOME/examples/src/main/resources/kv1.txt which assumes you have the source tree checked out. 

A more portable way is to copy the kv1.txt to a temporary file that gets cleaned up when the jvm shutsdown. This would allow us to run this example outside of a source tree.",,apachespark,kostas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 06 01:20:01 UTC 2014,,,,,,,,,,"0|i2353b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/14 01:20;apachespark;User 'ksakellis' has created a pull request for this issue:
https://github.com/apache/spark/pull/3628;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS Doesn't Use the Current Schema,SPARK-4773,12759925,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dyross,dyross,06/Dec/14 00:30,09/Dec/14 06:34,14/Jul/23 06:26,09/Dec/14 06:34,,,,,,,,,,,,,,,,,,1,,,,,,"In a CTAS  (CREATE TABLE __ AS SELECT __), the current schema isn't used. For example, this all works:

{code}
CREATE DATABASE test_db;
USE test_db;
CREATE TABLE test_table_1(s string);
SELECT * FROM test_table_1;
CREATE TABLE test_table_2 AS SELECT * FROM test_db.test_table_1;
SELECT * FROM test_table_2;
{code}

But this fails:

{code}
CREATE TABLE test_table_3 AS SELECT * FROM test_table_1;
{code}

Message:
{code}
14/12/06 00:28:57 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query:
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:43 Table not found 'test_table_1'
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1324)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1053)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8342)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:284)
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim12.scala:190)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:193)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:175)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:150)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:207)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:58)
	at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:55)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:526)
	at org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:43 Table not found 'test_table_1'
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1079)
	... 33 more
14/12/06 00:28:57 WARN thrift.ThriftCLIService: Error fetching results:
org.apache.hive.service.cli.HiveSQLException: org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:43 Table not found 'test_table_1'
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim12.scala:221)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:193)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:175)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:150)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:207)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:58)
	at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:55)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:526)
	at org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,dyross,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 06:34:53 UTC 2014,,,,,,,,,,"0|i23533:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 06:34;dyross;Looks like this was broken by: https://github.com/apache/spark/commit/4b55482abf899c27da3d55401ad26b4e9247b327

And fixed by: https://github.com/apache/spark/commit/51b1fe1426ffecac6c4644523633ea1562ff9a4e

Thanks for quick turnaround!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Accumulators leak memory, both temporarily and permanently",SPARK-4772,12759894,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nkronenfeld,nkronenfeld,nkronenfeld,05/Dec/14 21:49,17/Dec/14 20:19,14/Jul/23 06:26,17/Dec/14 20:19,1.0.0,,,,,,,1.0.3,1.1.2,1.2.1,1.3.0,,,Spark Core,,,,0,accumulators,,,,,"Accumulators.localAccums is cleared at the beginning of a task, and not at the end.

This means that any locally accumulated values hang around until another task is run on that thread.

If for some reason, the thread dies, said values hang around indefinitely.

This is really only a big issue with very large accumulators.",,apachespark,joshrosen,nkronenfeld,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:18:56 UTC 2014,,,,,,,,,,"0|i234w7:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.2,1.2.1,1.3.0,,,,,,,,"05/Dec/14 21:55;apachespark;User 'nkronenfeld' has created a pull request for this issue:
https://github.com/apache/spark/pull/3570;;;","10/Dec/14 08:03;joshrosen;This was fixed by https://github.com/apache/spark/pull/3570, which I committed for 1.0.3, 1.1.2, and 1.3.0.  I've added the {{backport-needed}} tag so that we remember to backport this into {{branch-1.2}} after the 1.2.0 release.;;;","17/Dec/14 20:18;joshrosen;I've merged this into {{branch-1.2}}, so this will be included in 1.2.1.  Since that was the last remaining backport, I'm marking this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document standalone --supervise feature,SPARK-4771,12759889,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,05/Dec/14 21:40,10/Dec/14 20:44,14/Jul/23 06:26,10/Dec/14 20:44,1.1.0,,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,,,,0,,,,,,We need this especially for streaming.,,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 22:49:45 UTC 2014,,,,,,,,,,"0|i234v3:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,"05/Dec/14 22:49;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.scheduler.minRegisteredResourcesRatio documented default is incorrect for YARN,SPARK-4770,12759861,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,05/Dec/14 20:51,09/Dec/14 00:29,14/Jul/23 06:26,09/Dec/14 00:29,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Documentation,YARN,,,0,,,,,,,,apachespark,joshrosen,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 00:29:25 UTC 2014,,,,,,,,,,"0|i234p3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/14 20:58;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3624;;;","09/Dec/14 00:29;joshrosen;Issue resolved by pull request 3624
[https://github.com/apache/spark/pull/3624];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS does not work when reading from temporary tables,SPARK-4769,12759858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chenghao,marmbrus,marmbrus,05/Dec/14 20:50,09/Dec/14 01:42,14/Jul/23 06:26,09/Dec/14 01:41,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"{code}
test(""double nested data"") {
    sparkContext.parallelize(Nested1(Nested2(Nested3(1))) ::
Nil).registerTempTable(""nested"")
    checkAnswer(
      sql(""SELECT f1.f2.f3 FROM nested""),
      1)
    checkAnswer(sql(""CREATE TABLE test_ctas_1234 AS SELECT * from nested""),
Seq.empty[Row])
    checkAnswer(
      sql(""SELECT * FROM test_ctas_1234""),
      sql(""SELECT * FROM nested"").collect().toSeq)
  }
{code}

{code}
11:57:15.974 ERROR org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:45 Table not found
'nested'
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1243)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
        at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
        at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
        at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
        at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
        at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
        at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
        at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)
        at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:103)
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4469,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 01:41:33 UTC 2014,,,,,,,,,,"0|i234of:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"07/Dec/14 04:22;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3336;;;","09/Dec/14 01:41;marmbrus;Issue resolved by pull request 3336
[https://github.com/apache/spark/pull/3336];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"With JDBC server, set Kryo as default serializer and disable reference tracking",SPARK-4761,12759624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,pwendell,pwendell,05/Dec/14 08:36,04/Jun/15 08:30,14/Jul/23 06:26,05/Dec/14 18:28,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"I saw some regression in 1.2 Spark SQL due to our broadcast change which requires significantly more serialization. It would be good if when the JDBC server is running it automatically sets the serializer to Kryo and disables Kryo reference tracking.

Doing this while still honoring a user-provided serializer is tricky. I think the best way is to expose an internal (private) constructor to SparkConf that takes a set of lowest-precedence defaults. Those could get merged in with the lowest precedence before the config file is read.",,apachespark,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 08:30:04 UTC 2015,,,,,,,,,,"0|i2338n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Dec/14 15:17;lian cheng;The JDBC Thrift server is started by {{spark-submit}}, which handles {{spark-defaults.conf}} properly. We don't need to modify {{SparkConf}}, instead we can just use {{SparkConf.getOption}} to check whether users have already set related properties explicitly.;;;","05/Dec/14 15:18;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3621;;;","04/Jun/15 08:30;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6639;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in complex spark job in local mode,SPARK-4759,12759611,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,dgshep,dgshep,05/Dec/14 07:43,21/Jan/15 18:49,14/Jul/23 06:26,21/Jan/15 18:49,1.1.1,1.2.0,1.3.0,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,,,,0,backport-needed,,,,,"The attached test class runs two identical jobs that perform some iterative computation on an RDD[(Int, Int)]. This computation involves 
  # taking new data merging it with the previous result
  # caching and checkpointing the new result
  # rinse and repeat

The first time the job is run, it runs successfully, and the spark context is shut down. The second time the job is run with a new spark context in the same process, the job hangs indefinitely, only having scheduled a subset of the necessary tasks for the final stage.

Ive been able to produce a test case that reproduces the issue, and I've added some comments where some knockout experimentation has left some breadcrumbs as to where the issue might be.  
","Java version ""1.7.0_51""
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

Mac OSX 10.10.1
Using local spark context",andrewor14,apachespark,dgshep,glenn.strycker@gmail.com,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/14 07:53;dgshep;SparkBugReplicator.scala;https://issues.apache.org/jira/secure/attachment/12685280/SparkBugReplicator.scala",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 23:00:39 UTC 2014,,,,,,,,,,"0|i2335r:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,"05/Dec/14 14:15;srowen;Can you dump the thread state with ""kill -QUIT"" and attach the stack traces of the deadlocked threads? Or at least that would help determine whether it's really a deadlock.;;;","05/Dec/14 15:47;dgshep;Here is a thread dump with superfluous threads omitted (namely jetty qtp threads)

{noformat}
""sparkDriver-akka.actor.default-dispatcher-18"" daemon prio=5 tid=7fc3853ef800 nid=0x119e87000 waiting on condition [119e86000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-17"" daemon prio=5 tid=7fc38c007000 nid=0x119d84000 waiting on condition [119d83000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-14"" daemon prio=5 tid=7fc38b003800 nid=0x119b96000 waiting on condition [119b95000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:1626)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1579)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-15"" daemon prio=5 tid=7fc38b003000 nid=0x119a93000 waiting on condition [119a92000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-16"" daemon prio=5 tid=7fc38c006000 nid=0x11971d000 waiting on condition [11971c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-13"" daemon prio=5 tid=7fc38681f000 nid=0x1193d1000 waiting on condition [1193d0000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""task-result-getter-3"" daemon prio=5 tid=7fc3871a7800 nid=0x1192ce000 waiting on condition [1192cd000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbeaace0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:957)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""task-result-getter-2"" daemon prio=5 tid=7fc3871a6800 nid=0x1191cb000 waiting on condition [1191ca000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbeaace0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:957)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""task-result-getter-1"" daemon prio=5 tid=7fc3871a6000 nid=0x1190c8000 waiting on condition [1190c7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbeaace0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:957)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""task-result-getter-0"" daemon prio=5 tid=7fc38d164800 nid=0x118fc5000 waiting on condition [118fc4000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbeaace0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:957)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-8"" daemon prio=5 tid=7fc385409800 nid=0x118ec2000 waiting on condition [118ec1000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-7"" daemon prio=5 tid=7fc386b4b800 nid=0x118dbf000 waiting on condition [118dbe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-6"" daemon prio=5 tid=7fc388013800 nid=0x118cbc000 waiting on condition [118cbb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-5"" daemon prio=5 tid=7fc3869e3800 nid=0x118bb9000 waiting on condition [118bb8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-4"" daemon prio=5 tid=7fc38689b800 nid=0x118ab6000 waiting on condition [118ab5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-3"" daemon prio=5 tid=7fc38c005000 nid=0x1189b3000 waiting on condition [1189b2000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-2"" daemon prio=5 tid=7fc38b001800 nid=0x1188b0000 waiting on condition [1188af000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-1"" daemon prio=5 tid=7fc3871a5000 nid=0x1187ad000 waiting on condition [1187ac000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Executor task launch worker-0"" daemon prio=5 tid=7fc38d164000 nid=0x1146c9000 waiting on condition [1146c8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7bbed5360> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:955)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
	at java.lang.Thread.run(Thread.java:695)

""Driver Heartbeater"" daemon prio=5 tid=7fc385572800 nid=0x114223000 waiting on condition [114222000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:383)

""Spark Context Cleaner"" daemon prio=5 tid=7fc38d163000 nid=0x114120000 in Object.wait() [11411f000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7bbea0748> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
	- locked <7bbea0748> (a java.lang.ref.ReferenceQueue$Lock)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:136)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1364)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)

""Timer-1"" daemon prio=5 tid=7fc38689a800 nid=0x11401d000 in Object.wait() [11401c000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7bbec0758> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:485)
	at java.util.TimerThread.mainLoop(Timer.java:483)
	- locked <7bbec0758> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:462)

""SparkListenerBus"" daemon prio=5 tid=7fc387085800 nid=0x113f1a000 waiting on condition [113f19000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c3360168> (a java.util.concurrent.Semaphore$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:286)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1364)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)

""SPARK_CONTEXT cleanup timer"" daemon prio=5 tid=7fc387201000 nid=0x1135ff000 in Object.wait() [1135fe000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c330c2d0> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:485)
	at java.util.TimerThread.mainLoop(Timer.java:483)
	- locked <7c330c2d0> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:462)

""BROADCAST_VARS cleanup timer"" daemon prio=5 tid=7fc385423800 nid=0x112bad000 in Object.wait() [112bac000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c330c2e8> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:485)
	at java.util.TimerThread.mainLoop(Timer.java:483)
	- locked <7c330c2e8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:462)

""BLOCK_MANAGER cleanup timer"" daemon prio=5 tid=7fc3868a8000 nid=0x112aaa000 in Object.wait() [112aa9000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c330c300> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:485)
	at java.util.TimerThread.mainLoop(Timer.java:483)
	- locked <7c330c300> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:462)

""connection-manager-thread"" daemon prio=5 tid=7fc3869ea800 nid=0x1129a7000 runnable [1129a6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <7c332e678> (a sun.nio.ch.Util$2)
	- locked <7c332e690> (a java.util.Collections$UnmodifiableSet)
	- locked <7c33228f8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
	at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:332)
	at org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:145)

""SHUFFLE_BLOCK_MANAGER cleanup timer"" daemon prio=5 tid=7fc3871d7000 nid=0x1128a4000 in Object.wait() [1128a3000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c330c318> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:485)
	at java.util.TimerThread.mainLoop(Timer.java:483)
	- locked <7c330c318> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:462)

""MAP_OUTPUT_TRACKER cleanup timer"" daemon prio=5 tid=7fc3871b1800 nid=0x1127a1000 in Object.wait() [1127a0000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c330c330> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:485)
	at java.util.TimerThread.mainLoop(Timer.java:483)
	- locked <7c330c330> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:462)

""sparkDriver-akka.actor.default-dispatcher-5"" daemon prio=5 tid=7fc3871d4000 nid=0x11208c000 waiting on condition [11208b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-4"" daemon prio=5 tid=7fc38d088000 nid=0x111f89000 waiting on condition [111f88000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-3"" daemon prio=5 tid=7fc385541800 nid=0x111e86000 waiting on condition [111e85000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-akka.actor.default-dispatcher-2"" daemon prio=5 tid=7fc385579800 nid=0x111d83000 waiting on condition [111d82000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7c33229d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:1594)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

""sparkDriver-scheduler-1"" daemon prio=5 tid=7fc387086800 nid=0x111acf000 waiting on condition [111ace000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:695)

""main"" prio=5 tid=7fc385001000 nid=0x1052ca000 in Object.wait() [1052c7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7bbed5f38> (a org.apache.spark.scheduler.JobWaiter)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.spark.scheduler.JobWaiter.awaitResult(JobWaiter.scala:73)
	- locked <7bbed5f38> (a org.apache.spark.scheduler.JobWaiter)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:511)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1110)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1129)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1143)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1157)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:774)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:595)
	at com.conviva.go.test.SparkBugReplicator$$anonfun$runJob$1.apply(SparkBugReplicator.scala:73)
	at com.conviva.go.test.SparkBugReplicator$$anonfun$runJob$1.apply(SparkBugReplicator.scala:47)
	at scala.collection.immutable.Range.foreach(Range.scala:141)
	at com.conviva.go.test.SparkBugReplicator$.runJob(SparkBugReplicator.scala:47)
	at com.conviva.go.test.SparkBugReplicator.job2(SparkBugReplicator.scala:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{noformat};;;","05/Dec/14 16:12;srowen;I don't see a deadlock here, so maybe that's not the cause or the right term here, but of course may still be some issue.;;;","05/Dec/14 16:37;dgshep;Fair enough. As far as I can work out, it appears that there are 9 expected tasks in the final stage, but only 6 ever get scheduled.  I suppose that leaves only one thread waiting on something that will never happen. ;);;;","05/Dec/14 22:50;pwendell;Thanks [~dgshep] a ton for creating a repro of this. I've asked [~andrewor14] to take a look at it and see if we can reproduce it on our side.;;;","05/Dec/14 23:30;andrewor14;Hey just wanted to let you know that I am able to reproduce this locally. It is stuck at task 6/9 exactly as you pointed out. Investigating.;;;","06/Dec/14 02:41;andrewor14;Quick update, I was only able to reproduce this in local mode when multiple cores are used. This doesn't happen if I only use 1 core in local mode, in local-cluster mode, or in standalone mode. It probably has something to do with how we allocate cores to executors in local mode. Still investigating.;;;","07/Dec/14 20:44;dgshep;It is possible to reproduce the issue with single core local mode.

Simply change the partitions parameter to > 1 (in the attached version it uses the defaultParallelism of the spark context, which in single core mode is 1) in either of the coalesce calls.;;;","07/Dec/14 23:51;andrewor14;Hey I came up with a much smaller reproduction for this from your program.

1. Start spark-shell with --master local[N] where N can be anything (or simply local with 1 core)
2. Copy and paste the following into your REPL
{code}
    def runMyJob(): Unit = {
      val rdd = sc.parallelize(1 to 100).repartition(5).cache()
      rdd.count()
      val rdd2 = sc.parallelize(1 to 100).repartition(12)
      rdd.union(rdd2).count()
    }
{code}
3. runMyJob()

It should be stuck at task 5/17. Note that with local-cluster and (local) standalone mode, it pauses a little at 5/17 too, but finishes shortly afterwards.

=== EDIT ===
This seems to reproduce it only on the master branch, but not 1.1.;;;","08/Dec/14 02:29;dgshep;This doesn't seem to reproduce the issue for me. The job finishes regardless of how many times I call runMyJob()
;;;","08/Dec/14 02:31;andrewor14;Found the issue. The task scheduler schedules tasks based on the preferred locations specified by the partition. In CoalescedRDD's partitions, we use the empty string as the default preferred location, even though this does not actually represent a real host: https://github.com/apache/spark/blob/e895e0cbecbbec1b412ff21321e57826d2d0a982/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala#L41

As a result, the task scheduler doesn't schedule a subset of the tasks on the local executor because these tasks are supposed to be scheduled on the host """" (empty string) that doesn't actually exist. I have not dug into the details of PartitionCoalescer as to why this is only specific to local mode.

I'll submit a fix shortly.;;;","08/Dec/14 02:33;andrewor14;[~dgshep] That's strange. I am able to reproduce this every time, and I only need to call ""runMyJob"" once. What master are you running?

I just tried local, local[6], and local[*] and they all reproduced the deadlock. I am running the master branch with this commit: 6eb1b6f6204ea3c8083af3fb9cd990d9f3dac89d;;;","08/Dec/14 02:34;dgshep;local[2] on the latest commit of branch 1.1;;;","08/Dec/14 02:37;andrewor14;Hm I'll try branch 1.1 again later tonight. There might very well be more than one issue that causes this.;;;","08/Dec/14 02:52;dgshep;Ok your version does reproduce the issue against the spark-core 1.1.1 artifact if I copy and paste your code into the original SparkBugReplicator, but it only seems to hang on the second time the job is run :P. This smells of a race condition...;;;","08/Dec/14 03:14;dgshep;I still cannot reproduce the issue with your snippet in the spark shell on tag v1.1.1;;;","08/Dec/14 07:26;andrewor14;Ok yeah you're right, I can't reproduce it from the code snippet in branch 1.1 either. There seems to be at least two issues going on here... Can you confirm that the snippet does reproduce the lock in master branch?;;;","08/Dec/14 08:06;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3633;;;","08/Dec/14 08:16;andrewor14;Hey I have opened the following PR to fix the symptom I described earlier: https://github.com/apache/spark/pull/3633. I will spend some more time trying to understand why there is a discrepancy in reproducibility between master and 1.1, but in both cases the patch should be sufficient in preventing this from happening again. Can you try it out?;;;","08/Dec/14 18:40;dgshep;The fix appears to resolve the issue in master for both repros.;;;","08/Dec/14 19:13;andrewor14;I have a smaller reproduction for branch-1.1. It seems that we need to run the two jobs in two different SparkContexts in tandem to reproduce it here:

1. Run bin/spark-shell. The master doesn't matter here.
2. Copy and paste the following into the REPL
{code}
  import org.apache.spark.{SparkConf, SparkContext}
  sc.stop()

  def setup(): SparkContext = {
    val conf = new SparkConf
    conf.setMaster(""local[8]"")
    conf.setAppName(""test"")
    new SparkContext(conf)
  }

  def runMyJob(sc: SparkContext): Unit = {
    val rdd = sc.parallelize(1 to 100).repartition(5).cache()
    rdd.count()
    val rdd2 = sc.parallelize(1 to 100).repartition(12)
    rdd.union(rdd2).count()
  }

  def test(): Unit = {
    var sc = setup()
    runMyJob(sc)
    sc.stop()
    println(""\n========== FINISHED FIRST JOB ==========\n"")
    sc = setup()
    runMyJob(sc) // This will get stuck at task 5/17 and never finish
    sc.stop()
    println(""\n========== FINISHED SECOND JOB ==========\n"")
  }
{code}
3. Call test().;;;","09/Dec/14 21:14;joshrosen;[~andrewor14],

If you wanted to run the reproduction in the shell without the ""multiple SparkContexts"" warning, couldn't you just call sc.stop() to stop the shell's default SparkContext prior to running your reproduction code?;;;","09/Dec/14 23:00;andrewor14;Aha, that is a great idea. Thanks [~joshrosen];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn-client failed to start due to Wrong FS error in distCacheMgr.addResource,SPARK-4757,12759593,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,huangjs,huangjs,05/Dec/14 05:46,05/Jan/15 19:40,14/Jul/23 06:26,05/Dec/14 05:58,1.3.0,,,,,,,1.2.0,1.3.0,,,,,YARN,,,,0,,,,,,"I got the following error during Spark startup (Yarn-client mode):

14/12/04 19:33:58 INFO Client: Uploading resource file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
java.lang.IllegalArgumentException: Wrong FS: hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
        at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
        at org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
        at org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
        at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
        at $iwC$$iwC.<init>(<console>:9)
        at $iwC.<init>(<console>:18)
        at <init>(<console>:20)
        at .<init>(<console>:24)


According to Liancheng and Andrew, this hotfix might be the root cause:

 https://github.com/apache/spark/commit/38cb2c3a36a5c9ead4494cbc3dde008c2f0698ce


Jianshi",,andrewor14,ChrisAlbright,huangjs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 05 19:40:41 UTC 2015,,,,,,,,,,"0|i2331r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/14 05:59;andrewor14;Hey I just reverted the offending patches. Thanks for bringing this up.;;;","05/Jan/15 19:40;ChrisAlbright;Is there an ETA on when this might make it into a release? Or, can we checkout a commit and build locally? I don't see a commit hash for the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sessionToActivePool  grow infinitely, even as sessions expire",SPARK-4756,12759588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,guowei2,guowei2,05/Dec/14 05:12,19/Dec/14 04:11,14/Jul/23 06:26,19/Dec/14 04:11,1.3.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"sessionToActivePool  in SparkSQLOperationManager grow infinitely, even as sessions expire.

",,apachespark,guowei2,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 04:11:22 UTC 2014,,,,,,,,,,"0|i2330n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/14 05:33;apachespark;User 'guowei2' has created a pull request for this issue:
https://github.com/apache/spark/pull/3617;;;","19/Dec/14 04:11;marmbrus;Issue resolved by pull request 3617
[https://github.com/apache/spark/pull/3617];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQRT(negative value) should return null,SPARK-4755,12759585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,05/Dec/14 04:49,17/Dec/14 20:51,14/Jul/23 06:26,17/Dec/14 20:51,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:51:39 UTC 2014,,,,,,,,,,"0|i232zz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/14 04:52;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3616;;;","17/Dec/14 20:51;marmbrus;Issue resolved by pull request 3616
[https://github.com/apache/spark/pull/3616];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet2 does not prune based on OR filters on partition columns,SPARK-4753,12759575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,05/Dec/14 02:45,05/Dec/14 20:45,14/Jul/23 06:26,05/Dec/14 20:45,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 02:47:15 UTC 2014,,,,,,,,,,"0|i232xr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Dec/14 02:47;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation - we need to synchronize kills,SPARK-4750,12759550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,05/Dec/14 00:28,17/Dec/14 20:13,14/Jul/23 06:26,17/Dec/14 20:12,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"https://github.com/apache/spark/blob/ab8177da2defab1ecd8bc0cd5a21f07be5b8d2c5/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L337

Simple omission on my part.",,andrewor14,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:12:58 UTC 2014,,,,,,,,,,"0|i232s7:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"05/Dec/14 00:30;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3612;;;","09/Dec/14 00:03;joshrosen;Merged into {{master}} and waiting to backport into {{branch-1.2}}.;;;","17/Dec/14 20:12;joshrosen;I've backported this into {{branch-1.2}} for inclusion in 1.2.1, so I'm marking this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_existing_cluster() doesn't work with additional security groups,SPARK-4745,12759433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,alexdebrie,alexdebrie,alexdebrie,04/Dec/14 14:35,04/Dec/14 22:21,14/Jul/23 06:26,04/Dec/14 22:17,1.1.0,,,,,,,1.1.2,1.2.1,,,,,EC2,,,,0,,,,,,"The spark-ec2 script has a flag that allows you to add additional security groups to clusters when you launch. However, the get_existing_cluster() function cycles through active instances and only returns instances whose group_names == cluster_name + ""-master"" (or + ""-slaves""), which is the group created by default.  The get_existing_cluster() function is used to login to, stop, and destroy existing clusters, among other actions.

This is a pretty simple fix for which I've already submitted a [pull request|https://github.com/apache/spark/pull/3596]. It checks if cluster_name + ""-master"" is in the list of groups for each active instance. This means the cluster group can be one among many groups, rather than the sole group for an instance.

",,alexdebrie,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 04 22:17:25 UTC 2014,,,,,,,,,,"0|i2322v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/14 14:36;apachespark;User 'alexdebrie' has created a pull request for this issue:
https://github.com/apache/spark/pull/3596;;;","04/Dec/14 22:17;joshrosen;Issue resolved by pull request 3596
[https://github.com/apache/spark/pull/3596];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent serialization errors from ever crashing the DAG scheduler,SPARK-4737,12759359,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mccheah,pwendell,pwendell,04/Dec/14 05:20,02/Feb/15 06:04,14/Jul/23 06:26,09/Jan/15 22:16,1.0.2,1.1.1,1.2.0,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"Currently in Spark we assume that when tasks are serialized in the TaskSetManager that the serialization cannot fail. We assume this because upstream in the DAGScheduler we attempt to catch any serialization errors by serializing a single partition. However, in some cases this upstream test is not accurate - i.e. an RDD can have one partition that can serialize cleanly but not others.

Do do this in the proper way we need to catch and propagate the exception at the time of serialization. The tricky bit is making sure it gets propagated in the right way.",,aash,andyk,apachespark,marmbrus,mcheah,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4349,,,,,,,,SPARK-4349,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 10 01:32:09 UTC 2015,,,,,,,,,,"0|i231mf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"04/Dec/14 19:44;marmbrus;I think another big problem here is that the DAGScheduler restarts (somewhat silently) and comes back in a bad state.  Perhaps if the DAGScheduler crashes we should kill the whole process if we aren't actually resilient to restarts.;;;","05/Dec/14 19:19;pwendell;[~marmbrus] this is a good idea. I'm sure if there are any cases where the DAG scheduler can really legitimately recover from these.;;;","06/Dec/14 02:12;mcheah;Forgot to mention that I'm actively working on this now - you can mark as in progress.

https://github.com/mccheah/spark/commit/097e7a21e15d3adf45687bd58ff095088f0282f7 <-- this will become a pull request once I get some of my internal mentors to review it.;;;","08/Dec/14 20:44;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/3638;;;","10/Jan/15 01:32;pwendell;It's great to see this go in. Thanks [~mcheah]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL UDF doesn't support 0 arguments.,SPARK-4735,12759352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,04/Dec/14 04:06,19/Dec/14 20:25,14/Jul/23 06:26,19/Dec/14 20:25,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"To reproduce that with:

    val udf = () => {Seq(1,2,3)}
    sqlCtx.registerFunction(""myudf"", udf)
    sqlCtx.sql(""select myudf() from tbl limit 1"").collect.foreach(println)",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 04 11:56:44 UTC 2014,,,,,,,,,,"0|i231kv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/14 11:56;apachespark;User 'potix2' has created a pull request for this issue:
https://github.com/apache/spark/pull/3604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.1.1 launches broken EC2 clusters,SPARK-4731,12759302,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,jey,jey,03/Dec/14 22:47,13/Dec/14 22:12,14/Jul/23 06:26,04/Dec/14 22:22,1.1.1,,,,,,,1.1.1,,,,,,EC2,,,,0,,,,,,"EC2 clusters launched using Spark 1.1.1's `spark-ec2` script with the `-v 1.1.1` flag fail to initialize the master and workers correctly. The `/root/spark` directory contains only the `conf` directory and doesn't have the `bin` and other directories. 

[~joshrosen] suggested that [spark-ec2 #81](https://github.com/mesos/spark-ec2/pull/81) might have fixed it, but I still see this problem after that was merged.",Spark 1.1.1 on MacOS X,andrewor14,jey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4718,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 04 21:37:02 UTC 2014,,,,,,,,,,"0|i2319r:",9223372036854775807,,,,,,,,,,,,,,1.1.1,,,,,,,,,,,"04/Dec/14 21:37;andrewor14;This should work once https://github.com/mesos/spark-ec2/pull/82 is merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warn against deprecated YARN settings,SPARK-4730,12759292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,03/Dec/14 22:09,23/Dec/14 22:29,14/Jul/23 06:26,23/Dec/14 22:29,1.2.0,,,,,,,1.2.1,1.3.0,,,,,YARN,,,,0,,,,,,"Yarn currently reads from SPARK_MASTER_MEMORY and SPARK_WORKER_MEMORY. If you have these settings leftover from a standalone cluster setup and you try to run Spark on Yarn on the same cluster, then your executors suddenly get the amount of memory specified through SPARK_WORKER_MEMORY.

This behavior is due in large part to backward compatibility. However, we should log a warning against the use of these variables at the very least.",,andrewor14,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 22:29:17 UTC 2014,,,,,,,,,,"0|i2317j:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"03/Dec/14 22:09;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3590;;;","23/Dec/14 22:29;joshrosen;Issue resolved by pull request 3590
[https://github.com/apache/spark/pull/3590];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remainder should also return null if the divider is 0.,SPARK-4720,12759133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,03/Dec/14 10:08,17/Dec/14 05:20,14/Jul/23 06:26,17/Dec/14 05:20,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,This is a follow-up of SPARK-4593.,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4593,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 05:20:41 UTC 2014,,,,,,,,,,"0|i2308v:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"03/Dec/14 10:16;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3581;;;","17/Dec/14 05:20;marmbrus;Issue resolved by pull request 3581
[https://github.com/apache/spark/pull/3581];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consolidate various narrow dep RDD classes with MapPartitionsRDD,SPARK-4719,12759125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,03/Dec/14 09:24,04/Dec/14 08:46,14/Jul/23 06:26,04/Dec/14 08:46,1.2.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"Seems like we don't really need MappedRDD, MappedValuesRDD, FlatMappedValuesRDD, FilteredRDD, GlommedRDD. They can all be implemented directly using MapPartitionsRDD.",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 09:29:31 UTC 2014,,,,,,,,,,"0|i23073:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"03/Dec/14 09:29;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3578;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShuffleMemoryManager.tryToAcquire may return a negative value,SPARK-4715,12759102,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,03/Dec/14 07:58,03/Dec/14 20:21,14/Jul/23 06:26,03/Dec/14 20:21,1.1.0,,,,,,,1.1.2,1.2.0,,,,,Spark Core,,,,0,,,,,,"Here is a unit test to demonstrate it:

{code}
  test(""threads should not be granted a negative size"") {
    val manager = new ShuffleMemoryManager(1000L)
    manager.tryToAcquire(700L)

    val latch = new CountDownLatch(1)
    startThread(""t1"") {
       manager.tryToAcquire(300L)
      latch.countDown()
    }
    latch.await() // Wait until `t1` calls `tryToAcquire`

    val granted = manager.tryToAcquire(300L)
    assert(0 === granted, ""granted is negative"")
  }
{code}

It outputs ""0 did not equal -200 granted is negative""",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 08:02:48 UTC 2014,,,,,,,,,,"0|i2301z:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.0,,,,,,,,,,"03/Dec/14 08:02;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3575;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaRDD.unpersist() should not raise exception if it is not cached.,SPARK-4713,12759080,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,chenghao,chenghao,03/Dec/14 05:12,12/Dec/14 06:42,14/Jul/23 06:26,12/Dec/14 06:42,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Unpersist a uncached RDD, will not raise exception, for example:
{panel}
val data = Array(1, 2, 3, 4, 5)
val distData = sc.parallelize(data)
distData.unpersist(true)
{panel}

But the SchemaRDD will throws exception if the SchemaRDD is not cached. Since SchemaRDD is the subclasses of the RDD, we should follow the same behavior.",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 06:42:08 UTC 2014,,,,,,,,,,"0|i22zxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/14 05:15;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3572;;;","12/Dec/14 06:42;marmbrus;Issue resolved by pull request 3572
[https://github.com/apache/spark/pull/3572];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reliable Kafka Receiver can lose data if the block generator fails to store data,SPARK-4707,12759026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,hshreedharan,hshreedharan,03/Dec/14 00:41,04/Feb/15 22:21,14/Jul/23 06:26,04/Feb/15 22:21,1.2.0,,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"The Reliable Kafka Receiver commits offsets only when events are actually stored, which ensures that on restart we will actually start where we left off. But if the failure happens in the store() call, and the block generator reports an error the receiver does not do anything and will continue reading from the current offset and not the last commit. This means that messages between the last commit and the current offset will be lost. 

I will send a PR for this soon - I have a patch which needs some minor fixes, which I need to test.",,apachespark,hshreedharan,jerryshao,jongyoul,koeninger,liqusha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 10 02:03:32 UTC 2014,,,,,,,,,,"0|i22zin:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"03/Dec/14 01:10;jerryshao;Thanks Hari for this fix. So a basic question, how to treat the data when store() is failed, shall we stop receiving the data from Kafka?;;;","03/Dec/14 01:21;hshreedharan;No, not really. There is really only one case where we'd lose data - that is when the store fails and the receiver is still active. There are two ways of working around this:
* Kill the consumer without committing the offsets and start a new consumer which will start reading data from the last commit (this is the easiest one, but is sort of expensive to create new consumers and also causes duplicates due to rebalancing).
* In the second option, store all of the pending messages in an ordered buffer locally in the receiver and try to push the data again on failure (on success just clear the buffer and commit). Finally, once the data is pushed commit the offset and start reading from Kafka again (commit offsets only when there are no pending messages). To make this smarter, we can keep track of how many messages are each block for each topic and partition and commit

;;;","03/Dec/14 01:48;jerryshao;Seems the second choice is cool, looking forward to your patch:).;;;","10/Dec/14 01:46;hshreedharan;TD and I discussed this and decided that the second option can be implemented with only a limited number of retries, which can make it quite readable and less complex.;;;","10/Dec/14 02:03;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3655;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove FakeParquetSerDe,SPARK-4706,12759012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,marmbrus,marmbrus,02/Dec/14 23:05,12/Apr/15 18:48,14/Jul/23 06:26,12/Apr/15 18:48,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-12-02 23:05:14.0,,,,,,,,,,"0|i22zfr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Driver retries in cluster mode always fail if event logging is enabled,SPARK-4705,12759008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,02/Dec/14 22:49,01/May/15 19:32,14/Jul/23 06:26,01/May/15 14:52,1.2.0,,,,,,,1.4.0,,,,,,Spark Core,YARN,,,2,,,,,,"yarn-cluster mode will retry to run the driver in certain failure modes. If even logging is enabled, this will most probably fail, because:

{noformat}
Exception in thread ""Driver"" java.io.IOException: Log directory hdfs://vanzin-krb-1.vpc.cloudera.com:8020/user/spark/applicationHistory/application_1417554558066_0003 already exists!
        at org.apache.spark.util.FileLogger.createLogDir(FileLogger.scala:129)
        at org.apache.spark.util.FileLogger.start(FileLogger.scala:115)
        at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:74)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:353)
{noformat}

The even log path should be ""more unique"". Or perhaps retries of the same app should clean up the old logs first.",,andrewor14,apachespark,ashwinshankar77,bcwalrus,cheolsoo,emres,glenn.strycker@gmail.com,hshreedharan,irashid,lewuathe,neelesh77,oarmand,twinkle,vanzin,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6751,,,,,,,,,,,,,,,,,,,,,,"10/Feb/15 13:01;twinkle;Screen Shot 2015-02-10 at 6.27.49 pm.png;https://issues.apache.org/jira/secure/attachment/12697770/Screen+Shot+2015-02-10+at+6.27.49+pm.png","24/Feb/15 06:38;twinkle;Updated UI - II.png;https://issues.apache.org/jira/secure/attachment/12700361/Updated+UI+-+II.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 14:52:40 UTC 2015,,,,,,,,,,"0|i22zev:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,"08/Dec/14 11:43;WangTaoTheTonic;We have an configuration item ""spark.eventLog.overwrite"", is it enough to solve this issue?;;;","08/Dec/14 18:06;vanzin;It doesn't sound right to force the user to enable another setting just to get basic functionality working. The way it is, the default settings make Yarn retries useless.;;;","22/Dec/14 07:36;twinkle;hi,

Can you please assign this to me? I would like to work on this.

Thanks,
Twinkle;;;","03/Jan/15 10:57;WangTaoTheTonic;[~twinkle] You could just fix this issue and create a pull request on github with title prefixed with [SPARK-4975].;;;","03/Jan/15 10:58;WangTaoTheTonic;It is SPARK-4705, sorry for the typo.;;;","01/Feb/15 22:11;bcwalrus;[~twinkle], have you had a chance to worked on this?;;;","02/Feb/15 04:18;twinkle;Hi,

Sorry, i saw just this update of yours. Missed last two comments, will work on it.

Thanks,;;;","02/Feb/15 15:38;apachespark;User 'twinkle-sachdeva' has created a pull request for this issue:
https://github.com/apache/spark/pull/4311;;;","03/Feb/15 10:12;twinkle;Hi [~bcwalrus],

( please ignore above pull request as of now, as that is not complete )

This is the overall strategy, i am thinking of taking regarding this issue.

There will be two kinds of applications ( based on the cluster manager and mode being used )
1. Applications which will have only on attempt possible 
    I am thinking of leaving the folder structure of event logs  as well as history server UI intact
2. Applications which will have more than one attempt tried
   In this case, I am thinking of changing folder structure of event logs to be inside <application_id>/<attempt_id> , so as to make sure that logDir is different for each attempt, while keeping on application's all attempts logs inside one directory.
  Regarding History server UI, there will be two cases:
   2.1 Application got succeeded in one attempt. Here we can keep the UI intact from current, but this will make it different to look at if somebody is using yarn-cluster and some applications got completed in multiple attempts.
   2.2 Application got completed in more than one attempts. Here we can have two options:
           2.2.1 Here if somebody clicks at application id, then another page gets loaded, which shows another table, which lists all the attempts of the application. On clicking one of the attempts, we will show the UI as we show today specific to that attempt.
           2.2.2 Here if somebody clicks at application id, then on the same page, we show some subtable or some kind of list, which has links for all attempts. On clicking this attempt id link, we will show the UI as we show today specific to that attempt.
      In both of these two options, we will need to change the header to show the attempt id value also.

Please provide your suggestions.

Thanks,
Twinkle;;;","03/Feb/15 18:47;vanzin;Hi [~twinkle], a few comments.

I'm not sure it's worth it to differentiate applications that only have one try from those that allow multiple tries. The former is just a special case of the latter, where max number of tries = 1.

Also note that in the current master, there isn't a ""folder structure"". All logs are in a single file. You could play with the file names, or create a new folder structure for handling app logs. The former is a tiny bit easier on the HDFS NameNode.

As for the history server UI, I'd like to throw a different suggestion: list app attempts in the existing listing tables. For example,  the row listing the application would have multiple ""sub rows"" with the different attemps for that particular application. That means we wouldn't need a separate page to list application attempts, and we could have filters on the listing page to only list successful attempts, for example.
;;;","04/Feb/15 14:04;twinkle;Hi [~vanzin],

Currently, inside the event log directory, a directory is created with application id, which contains following files:
APPLICATION_COMPLETE
EVENT_LOG_1 
SPARK_VERSION_1.2.0 ( for 1.2.0 version )

This is what I have planned ( and partially implemented )
 <eventlog_dir>/<application_id>/<attempt_id>/All the three files mentioned above for that specific attempt.

This will cause minimum noise with the current way of logging the events, as well as rendering the same too.
Please note that as of now, I am doing this change only for yarn-cluster mode. Though whole of it ( including UI ) can be availed by overriding applicationAttemptId() inside the SchedulerBackend implementation for that particular mode/ scheduler.

Regarding UI:
Showing multiple attempts in different subrows within the same page looks good to me too. There are two points regarding the same:
1. As of now, we don't show any status regarding Succeeded or failed, so probably, that can be taken later on. I hope, I am not missing something here.
2. As of now, stats are available for each attempt level ( stats includes: start time, end time, duration and last updated time ), should we aggregate some or all of these to be shown at application level, or should we just leave these stats blank for the main row?

As multiple attempts are specific to scheduler being used, if we just leave the current UI intact for those who don't have multiple attempts, that will leave their UI intact. In case of yarn cluster, we can show attempts in the sub rows, irrespective of the number of attempts tried, that will make it consistent. 

Please provide your suggestions. 

Just an update regarding the coding part : So far, i have implemented the folder structure and rendering of the same for multiple attempts separately. As of now, I am waiting to have the UI stuff to get finalised.

Thanks,
Twinkle;;;","04/Feb/15 14:11;twinkle;Hi,

If there is any folder structure change, then please suggest that too.

Thanks,
Twinkle;;;","04/Feb/15 18:07;vanzin;Hi [~twinkle],

bq. Please note that as of now, I am doing this change only for yarn-cluster mode

Is there any limitation with other cluster managers that prevent you from also supporting them? I know I filed the bug with ""yarn-cluster"" in the summary, but standalone cluster most probably suffers from the same issue if you run with the ""--supervise"" flag.

bq.  leave the current UI intact for those who don't have multiple attempts

I think that's good, but it doesn't require yarn-cluster-specific logic. All you need to check is whether some application has one attempt or multiple attemps, and render things slightly different. For example, with a single attempt:

|| App Id || App Name || Attempt Id || Started || ... ||
| app-1 | MyApp | | 201500204 | ...|

With multiple attempts (sorry don't know how to do it in jira markup):

{code}
<table border=""1"">
  <tr><th>App Id</th><th>App Name</th><th>Attempt Id</th><th>Started</th><th>...</th></tr>
  <tr><td rowspan=""2"">app-2</td><td rowspan=""2"">MyApp</td><td>2</td><td>201500205</td><td>...</td></tr>
  <tr><td>1</td><td>201500204</td><td>...</td></tr>
</table>
{code}

(You can paste that at http://htmledit.squarefree.com/ to see it, or load it in your browser somehow.)

You'd have that new ""attempt id"" column, but I think that's ok. We can look at exposing other things like the final status separately.;;;","05/Feb/15 15:28;twinkle;Hi [~vanzin],

Regarding adding that for other modes, I just need to override an API, after figuring a bit about getting attempt id. I will plan for that.

Thanks for the html stuff, will upload the UI snapshot too.



;;;","09/Feb/15 15:07;twinkle;Hi [~vanzin]

Please take a look at the screenshot. I will make NA to be non-anchored element.

It shows the UI for a history server, where some of the applications has been run on a scheduler where multiple attempts are not supported, whereas some of the applications has multiple attempts.

Should we introduce a property, which will show multi-attempt UI by default?;;;","10/Feb/15 12:58;twinkle;UI - 2;;;","10/Feb/15 12:59;twinkle;Hi,

So here is the final approach I have taken regarding UI.

If there is no application, where logging of event is happening per attempt, then previous UI will continue to appear. As soon as there is one or more application, whose events has been logged per attempt ( even if there is only one attempt), then UI will change to per attempt UI ( please see the attachment).

By logging per attempt, I meant the changed folder structure.

Please note that in case of no attempt specific UI, anchor was on application id value. In the new UI( UI - 2 ) , anchor will appear for attempt ID.

Thanks,;;;","10/Feb/15 13:01;twinkle;UI-2;;;","10/Feb/15 21:53;vanzin;Hi [~twinkle],

I think the UI on the latest screenshot is a little too cluttered. How about:

- Keep the app id as the main link to the application's UI, pointing at the last attempt in the case of multiple attempts
- Have the attempt column list the attempt IDs only for those apps that have multiple attempts. Those with a single attempt would have an empty cell.

This would result in a redundant link (app id link + link to the last attempt pointing at the same place), but I think it looks better. And it's probably less confusing for those used to the current UI.;;;","16/Feb/15 04:49;twinkle;Hi,


+1. I will upload the screenshot with these changes.

Thanks,
Twinkle;;;","24/Feb/15 06:47;twinkle;Hi [~vanzin],

Please have a look at the Update UI- II.

Thanks,
Twinkle;;;","24/Feb/15 20:00;vanzin;Latest screenshot looks great to me. Did you have a chance to address the review comments on the PR?;;;","25/Feb/15 05:39;twinkle;Hi [~vanzin]

Working on it.

Thanks,
Twinkle;;;","01/Mar/15 15:16;apachespark;User 'twinkle-sachdeva' has created a pull request for this issue:
https://github.com/apache/spark/pull/4845;;;","09/Apr/15 00:45;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5432;;;","24/Apr/15 08:15;emres;Hello,

Any plans on resolving this issue? ;;;","24/Apr/15 11:04;srowen;Looks like it's still active and in progress: https://github.com/apache/spark/pull/5432;;;","24/Apr/15 11:18;emres;Great! From the conversation on Github, it seems like it'll be resolved soon. I'm looking forward to it.;;;","01/May/15 14:52;irashid;Issue resolved by pull request 5432
[https://github.com/apache/spark/pull/5432];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmitDriverBootstrap doesn't flush output,SPARK-4704,12758997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,stephen,stephen,02/Dec/14 21:59,13/Mar/15 17:44,14/Jul/23 06:26,13/Mar/15 17:44,1.2.0,,,,,,,1.2.2,1.3.1,1.4.0,,,,Spark Core,,,,0,,,,,,"When running spark-submit with a job that immediately blows up (say due to init errors in the job code), there is no error output from spark-submit on the console.

When I ran spark-class directly, then I do see the error/stack trace on the console.

I believe the issue is in SparkSubmitDriverBootstrapper (I had spark.driver.memory set in spark-defaults.conf) not waiting for the  RedirectThreads to flush/complete before exiting.

E.g. here:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmitDriverBootstrapper.scala#L143

I believe around line 165 or so, stdoutThread.join() and
stderrThread.join() calls are necessary to make sure the other threads
have had a chance to flush process.getInputStream/getErrorStream to
System.out/err before the process exits.

I've been tripped up by this in similar RedirectThread/process code, hence suspecting this is the problem.
",1.2.0-rc1,apachespark,joshrosen,stephen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 11:51:09 UTC 2015,,,,,,,,,,"0|i22zcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/14 02:42;stephen;Note that PR 3655 (from the previous comment) is for a separate issue (it referenced this ticket number as a typo).;;;","24/Dec/14 02:44;joshrosen;I've removed the link / comment to the unrelated PR.;;;","26/Feb/15 11:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4788;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in sbt/sbt,SPARK-4701,12758972,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tsudukim,tsudukim,tsudukim,02/Dec/14 19:41,03/Dec/14 20:09,14/Jul/23 06:26,03/Dec/14 20:09,,,,,,,,1.1.2,1.2.0,,,,,Build,,,,0,,,,,,"in sbt/sbt
{noformat}
  -S-X               add -X to sbt's scalacOptions (-J is stripped)
{noformat}
but {{(-S is stripped)}} is correct.",,apachespark,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 19:45:41 UTC 2014,,,,,,,,,,"0|i22z7j:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"02/Dec/14 19:45;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/3560;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System properties should override environment variables,SPARK-4697,12758913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,02/Dec/14 14:56,24/Jan/15 23:05,14/Jul/23 06:26,24/Jan/15 23:05,1.0.0,,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"I found some arguments in yarn module take environment variables before system properties while the latter override the former in core module.
This should be changed in org.apache.spark.deploy.yarn.ClientArguments and org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.",,andrewor14,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 24 23:05:46 UTC 2015,,,,,,,,,,"0|i22yuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/14 15:10;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3557;;;","03/Dec/14 19:19;andrewor14;Hey did you search for a duplicate or related JIRA in the history? I'm pretty sure there's another one that suggests something similar;;;","08/Dec/14 12:47;WangTaoTheTonic;I took a quick look at the results filtered by ""project = SPARK AND component = YARN AND status = Open"" and found no similar issues.;;;","19/Jan/15 11:10;WangTaoTheTonic;Should this be closed?;;;","24/Jan/15 23:05;srowen;This looks like it was fixed in https://github.com/apache/spark/commit/9dea64e53ad8df8a3160c0f4010811af1e73dd6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long-run user thread(such as HiveThriftServer2) causes the 'process leak' in yarn-client mode,SPARK-4694,12758884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,carlmartin,carlmartin,02/Dec/14 12:56,29/Dec/14 22:13,14/Jul/23 06:26,17/Dec/14 20:24,,,,,,,,1.3.0,,,,,,YARN,,,,0,backport-needed,,,,,"Recently when I use the Yarn HA mode to test the HiveThriftServer2 I found a problem that the driver can't exit by itself.
To reappear it, you can do as fellow:
1.use yarn HA mode and set am.maxAttemp = 1for convenience
2.kill the active resouce manager in cluster

The expect result is just failed, because the maxAttemp was 1.

But the actual result is that: all executor was ended but the driver was still there and never close.",,apachespark,carlmartin,marmbrus,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:24:23 UTC 2014,,,,,,,,,,"0|i22yof:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"02/Dec/14 13:07;carlmartin;The reason was that Yarn had reported the status to the RM and the YarnClientSchedulerBackend would detect the status to stop sc in function 'asyncMonitorApplication'.
But the HiveThriftServer2 is a long-run user thread. JVM will never exit until all the no-demo threads have ended or using System.exit().
It cause such problem.
The easiest way to reslove this problem is using System.exit(0) instead of sc.stop in funciton 'asyncMonitorApplication' .
But system.exit is not recommended in https://issues.apache.org/jira/browse/SPARK-4584
Do you have any ideas about this problem? [~vanzin];;;","02/Dec/14 18:17;vanzin;I'm not sure I understand the bug or the context, but there must be some code that manages both the SparkContext and the HiveThriftServer2 thread. That code is responsible for stopping the context and shutting down the HiveThriftServer2 thread; if it can't do it cleanly because of some deficiency of the API, it can use Thread.stop() or some other less kosher approach.

Using {{System.exit()}} is not recommended because there's no way for the backend to detect that without severe performance implications. Apps will always be reported as ""succeeded"" when using that approach.;;;","03/Dec/14 07:42;carlmartin;Thanks for reply. [~vanzin] the problem is very sure: the scheduler backend was aware of the AM had been exited so it call sc.stop to exit the driver process but there was a user thread which was still alive and cause this problem.
To fix this, just using System.exit(-1) instead of the sc.stop so that jvm will not wait all the user threads being down and exit clearly.
Can I use System.exit() in spark code?;;;","03/Dec/14 08:40;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3576;;;","03/Dec/14 18:45;vanzin;To answer your question, you can call System.exit() if you want. It's just recommended that it's done after you properly shutdown the SparkContext, otherwise Yarn won't report your app status correctly. But it seems your patch doesn't use System.exit(), so this is kinda moot.;;;","17/Dec/14 20:24;marmbrus;Issue resolved by pull request 3576
[https://github.com/apache/spark/pull/3576];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PruningPredicates may be wrong if predicates contains an empty AttributeSet() references,SPARK-4693,12758879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,yantangzhai,yantangzhai,02/Dec/14 12:22,19/Dec/14 04:14,14/Jul/23 06:26,19/Dec/14 04:14,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"The sql ""select * from spark_test::for_test where abs(20141202) is not null"" has predicates=List(IS NOT NULL HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFAbs(20141202)) and 
partitionKeyIds=AttributeSet(). PruningPredicates is List(IS NOT NULL HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFAbs(20141202)). Then the exception ""java.lang.IllegalArgumentException: requirement failed: Partition pruning predicates only supported for partitioned tables."" is thrown.
The sql ""select * from spark_test::for_test_partitioned_table where abs(20141202) is not null and type_id=11 and platform = 3"" with partitioned key insert_date has predicates=List(IS NOT NULL HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFAbs(20141202), (type_id#12 = 11), (platform#8 = 3)) and partitionKeyIds=AttributeSet(insert_date#24). PruningPredicates is List(IS NOT NULL HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFAbs(20141202)).",,apachespark,marmbrus,yantangzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 04:14:06 UTC 2014,,,,,,,,,,"0|i22ynb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/14 12:30;apachespark;User 'YanTangZhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3556;;;","19/Dec/14 04:14;marmbrus;Issue resolved by pull request 3556
[https://github.com/apache/spark/pull/3556];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restructure a few lines in shuffle code,SPARK-4691,12758851,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maji2014,maji2014,maji2014,02/Dec/14 09:45,17/May/20 18:29,14/Jul/23 06:26,17/Dec/14 20:16,1.1.0,,,,,,,1.2.1,1.3.0,,,,,Shuffle,Spark Core,,,0,,,,,,"aggregator and mapSideCombine judgement in 
HashShuffleWriter.scala 
SortShuffleWriter.scala
HashShuffleReader.scala",,apachespark,joshrosen,maji2014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:16:20 UTC 2014,,,,,,,,,,"0|i22yh3:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"02/Dec/14 10:00;apachespark;User 'maji2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/3553;;;","17/Dec/14 20:16;joshrosen;I've merged this into branch-1.2 for inclusion in 1.2.1, so I'm marking this as Fixed.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext#addFile doesn't keep file folder information,SPARK-4687,12758801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,jxiang,jxiang,02/Dec/14 01:11,04/Mar/15 20:43,14/Jul/23 06:26,05/Feb/15 18:16,1.2.0,,,,,,,1.3.0,1.4.0,,,,,Spark Core,,,,0,,,,,,"Files added with SparkContext#addFile are loaded with Utils#fetchFile before a task starts. However, Utils#fetchFile puts all files under the Spart root on the worker node. We should have an option to keep the folder information. ",,apachespark,joshrosen,jxiang,pwendell,sandyr,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6144,,,,,SPARK-3145,HIVE-8851,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 18:16:33 UTC 2015,,,,,,,,,,"0|i22y67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/14 02:40;xuefuz;[~jxiang], alternatively, would a new method, SparkContext.addFolder(), better fit to this use case?

CC [~sandyr], [~rxin].;;;","02/Dec/14 03:07;jxiang;If it can add files under the folder recursively, it will be great.;;;","03/Dec/14 19:21;sandyr;[~pwendell], do you think this is a reasonable API addition?  If so, I'll try and add it.;;;","11/Dec/14 01:18;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3670;;;","11/Dec/14 01:33;pwendell;I commented a bit on the JIRA after seeing this. Instead of having this can Hive just add individual files with a prefix denoting what the folder is? Maintaining addFile has been nontrivial (we found a lot of corner cases over time) so it would be good to understand what the alternative is for Hive.;;;","11/Dec/14 03:02;sandyr;I think [~xuefuz] can probably motivate this better, but from what I understand, the main use case is Hive's map joins and map bucket joins, in which a smaller table needs to be distributed to every node.  The smaller table typically resides in HDFS, and is the output of a separate job.  For map joins, the smaller table is composed of a bunch of files in a single folder.  For map bucket joins, the smaller table is composed of a single folder with a bunch of bucket folders underneath, each containing a set of data files.  At the very least, doing the prefixing would require a bunch of extra FS operations to rename all the subfiles.  Though that might make them difficult to read from other Hive implementations?

Another totally separate situation I encountered a while ago where this kind of thing would have been useful was calling http://ctakes.apache.org/ in a distributed fashion.  Calling into it requires letting it load a bunch of files from a particular directory structure.  We ultimately had to go with a workaround that required installing the directory on every node.

Beyond the issues I outlined in my patch, are there particular edge cases you're worried about where we wouldn't be able to copy the behavior from addFile?
;;;","11/Dec/14 03:49;xuefuz;I concur with [~sandyr]'s account for the need of addFolder() as it helps any Spark application from doing the same thing over and over again, possibly in a less performant way. Folder is such an natural, indispensible to some extent, extension to file in any system that deals with bits on at storage level. I'd also contend that being hard to get it right shouldn't prevent us from trying and perfecting it on the way if we believe functionally it's right thing to add.;;;","06/Jan/15 00:37;pwendell;I spent some more time looking at this and talking with [~sandyr] and [~joshrosen]. I think having some limited version of this is fine given that, from what I can tell, this is pretty difficult to implement outside of Spark. I am going to post further comments on the JIRA.;;;","05/Feb/15 18:16;joshrosen;Issue resolved by pull request 3670
[https://github.com/apache/spark/pull/3670];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Link to ""allowed master URLs"" is broken in configuration documentation",SPARK-4686,12758792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,02/Dec/14 00:34,02/Dec/14 17:09,14/Jul/23 06:26,02/Dec/14 17:09,1.0.2,1.1.0,1.2.0,,,,,1.1.2,1.2.0,,,,,Documentation,,,,0,,,,,,"The link points to the old scala programming guide; it should point to the submitting applications page.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 00:37:06 UTC 2014,,,,,,,,,,"0|i22y47:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.0,,,,,,,,,,"02/Dec/14 00:37;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/3542;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaSchemaRDD.schema may throw NullType MatchError if sql has null,SPARK-4676,12758644,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,yantangzhai,yantangzhai,01/Dec/14 13:19,02/Dec/14 22:17,14/Jul/23 06:26,02/Dec/14 22:17,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"val jsc = new org.apache.spark.api.java.JavaSparkContext(sc)
val jhc = new org.apache.spark.sql.hive.api.java.JavaHiveContext(jsc)
val nrdd = jhc.hql(""select null from spark_test.for_test"")
println(nrdd.schema)
Then the error is thrown as follows:
scala.MatchError: NullType (of class org.apache.spark.sql.catalyst.types.NullType$)
at org.apache.spark.sql.types.util.DataTypeConversions$.asJavaDataType(DataTypeConversions.scala:43)",,apachespark,marmbrus,yantangzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 22:17:47 UTC 2014,,,,,,,,,,"0|i22x7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 13:24;apachespark;User 'YanTangZhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3538;;;","02/Dec/14 22:17;marmbrus;Issue resolved by pull request 3538
[https://github.com/apache/spark/pull/3538];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimizing limit using coalesce,SPARK-4673,12758582,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,01/Dec/14 07:57,09/Jan/15 12:39,14/Jul/23 06:26,09/Jan/15 12:39,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Now limit used ShuffledRDD and HashPartitioner to repartition 1 which leads to shuffle.,,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 09 12:39:04 UTC 2015,,,,,,,,,,"0|i22wtz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"01/Dec/14 07:58;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3531;;;","09/Jan/15 12:39;scwf;since coalesce (1) will lead to run with a single thread， not always speed up limit so close this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cut off the super long serialization chain in GraphX to avoid the StackOverflow error,SPARK-4672,12758581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,jerrylead,jerrylead,01/Dec/14 07:53,03/Dec/14 10:11,14/Jul/23 06:26,03/Dec/14 07:54,1.1.0,,,,,,,1.2.0,,,,,,GraphX,Spark Core,,,0,,,,,,"While running iterative algorithms in GraphX, a StackOverflow error will stably occur in the serialization phase at about 300th iteration. In general, these kinds of algorithms have two things in common:

# They have a long computing chain.
{code:borderStyle=solid}
(e.g., “degreeGraph=>subGraph=>degreeGraph=>subGraph=>…=>”)
{code}
# They will iterate many times to converge. An example:
{code:borderStyle=solid}
//K-Core Algorithm
val kNum = 5

var degreeGraph = graph.outerJoinVertices(graph.degrees) {
		(vid, vd, degree) => degree.getOrElse(0)
}.cache()
	
do {
	val subGraph = degreeGraph.subgraph(
		vpred = (vid, degree) => degree >= KNum
	).cache()

	val newDegreeGraph = subGraph.degrees

	degreeGraph = subGraph.outerJoinVertices(newDegreeGraph) {
		(vid, vd, degree) => degree.getOrElse(0)
	}.cache()

	isConverged = check(degreeGraph)
} while(isConverged == false)
{code}

After about 300 iterations, StackOverflow will definitely occur with the following stack trace:

{code:borderStyle=solid}
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.StackOverflowError
java.io.ObjectOutputStream.writeNonProxyDesc(ObjectOutputStream.java:1275)
java.io.ObjectOutputStream.writeClassDesc(ObjectOutputStream.java:1230)
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1426)
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
{code}

It is a very tricky bug, which only occurs with enough iterations. Since it took us a long time to find out its causes, we will detail the causes in the following 3 paragraphs. 
 
h3. Phase 1: Try using checkpoint() to shorten the lineage

It's easy to come to the thought that the long lineage may be the cause. For some RDDs, their lineages may grow with the iterations. Also, for some magical references,  their lineage lengths never decrease and finally become very long. As a result, the call stack of task's serialization()/deserialization() method will be very long too, which finally exhausts the whole JVM stack.

In deed, the lineage of some RDDs (e.g., EdgeRDD.partitionsRDD) increases 3 OneToOne dependencies in each iteration in the above example. Lineage length refers to the  maximum length of OneToOne dependencies (e.g., from the finalRDD to the ShuffledRDD) in each stage.

To shorten the lineage, a checkpoint() is performed every N (e.g., 10) iterations. Then, the lineage will drop down when it reaches a certain length (e.g., 33). 

However, StackOverflow error still occurs after 300+ iterations!

h3. Phase 2:  Abnormal f closure function leads to a unbreakable serialization chain

After a long-time debug, we found that an abnormal _*f*_ function closure and a potential bug in GraphX (will be detailed in Phase 3) are the ""Suspect Zero"". They together build another serialization chain that can bypass the broken lineage cut by checkpoint() (as shown in Figure 1). In other words, the serialization chain can be as long as the original lineage before checkpoint().

Figure 1 shows how the unbreakable serialization chain is generated. Yes, the OneToOneDep can be cut off by checkpoint(). However, the serialization chain can still access the previous RDDs through the (1)->(2) reference chain. As a result, the checkpoint() action is meaningless and the lineage is as long as that before. 

!https://raw.githubusercontent.com/JerryLead/Misc/master/SparkPRFigures/g1.png|width=100%!

The (1)->(2) chain can be observed in the debug view (in Figure 2).

{code:borderStyle=solid}
_rdd (i.e., A in Figure 1, checkpointed) -> f -> $outer (VertexRDD) -> partitionsRDD:MapPartitionsRDD -> RDDs in  the previous iterations
{code}

!https://raw.githubusercontent.com/JerryLead/Misc/master/SparkPRFigures/g2.png|width=100%!


More description: While a RDD is being serialized, its f function 
{code:borderStyle=solid}
e.g., f: (Iterator[A], Iterator[B]) => Iterator[V]) in ZippedPartitionsRDD2
{code}

will be serialized too. This action will be very dangerous if the f closure has a member “$outer” that references its outer class (as shown in Figure 1). This reference will be another way (except the OneToOneDependency) that a RDD (e.g., PartitionsRDD) can reference the other RDDs (e.g., VertexRDD). Note that checkpoint() only cuts off the direct lineage, while the function reference is still kept. So, serialization() can still access the other RDDs along the f references. 

h3. Phase 3: Non-transient member variable of VertexRDD makes things worse

""Reference (1)"" in Figure 1 is caused by the abnormal f clousre, while ""Reference (2)"" is caused by the potential bug in GraphX: *PartitionsRDD is a non-transient member variable of VertexRDD*. 

With this _small_ bug, the f closure itself (without OneToOne dependency) can cause StackOverflow error, as shown in the red box in Figure 3:

# While _vertices:VertexRDD_ is being serialized, its member _PartitionsRDD_ will be serialized too.
# Next, while serializing this _partitionsRDD_, serialization() will simultaneously serialize its f’s referenced $outer. Here, it is another _partitionsRDD_.
# Finally, the chain 
{code:borderStyle=solid}
""f => f$3 => f$3 => $outer => vertices: VertexRDD => partitionsRDD => … => ShuffledRDD""
{code}

comes into shape. As a result, the serialization chain can be as long as the original lineage and finally triggers StackOverflow error.
  
!https://raw.githubusercontent.com/JerryLead/Misc/master/SparkPRFigures/g3.png|width=100%!


h2. Conclusions

In conclusion, the root cause of StackOverflow error is the long serialization chain, which cannot be cut off by _checkpoint()_. This long chain is caused by the multiple factors, including:

# long lineage
# $outer reference in the f closure
# non-transient member variable

h2. How to fix this error

We propose three pull requests as follows to solve this problem thoroughly.

# PR-3544
In this pr, we change the ""val PartitionsRDD"" to be transient in EdgeRDDImpl and VertexRDDImpl. As a result, while _vertices:VertexRDD_ is being serialized, its member _PartitionsRDD_ will not be serialized. In other words, the ""Reference (2)"" in Figure 1 will be cut off.
# PR-3545
In this pr, we set ""f = null"" if ZippedPartitionsRDD is checkpointed. As a result, when PartitionsRDD is checkpointed, its f closure will be cleared and the ""Reference (1)"" (i.e., f => $outer) in Figure 1 will no exist.
# PR-3549
To cut off the long lineage, we need to perform checkpoint()  on PartitionsRDD. However, current checkpoint() is performed on VertexRDD and EdgeRDD themselves. As a result, we need to override the checkpoint() methods in VertexRDDImpl and EdgeRDDImpl to perform checkpoint() on PartitionsRDD.







",,adeandrade,ankurd,apachespark,glenn.strycker@gmail.com,jason.dai,jerrylead,lianhuiwang,liyezhang556520,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3623,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 10:11:11 UTC 2014,,,,,,,,,,"0|i22wtr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/14 03:51;apachespark;User 'JerryLead' has created a pull request for this issue:
https://github.com/apache/spark/pull/3545;;;","02/Dec/14 06:31;apachespark;User 'JerryLead' has created a pull request for this issue:
https://github.com/apache/spark/pull/3549;;;","02/Dec/14 18:40;rxin;cc [~ankurdave]

Can you take a look at this ASAP? Would be great to fix for 1.2.
;;;","03/Dec/14 03:35;jason.dai;We ran into the same issue, and this is a nice summary for the bug analysis. On the other hand, while this may fix the specific GraphX issue, I don't think it is generally applicable for dealing with super long lineage that can be generated in GraphX or other iterative algorithms. 

In particular, the user can define arbitrary functions, which can be called in RDD.compute() and refer to an arbitrary member variable that is an RDD, or can be used to construct another RDD, such as:
{noformat}
  class MyRDD (val rdd1, val rdd2, func1) extends RDD {
    val func2 = (f, iter1, iter2) => iter1– f(iter2)
    …
    override def compute(part, sc) {
      func2(func1, rdd1.iterator(part, sc), rdd2.iterator(part, sc))
    }
    …
   define newRDD(val rdd3, func3) = {
     val func4 = func2(func3)
     new AnotherRDD() {
       override def compute(part, sc) {
         func4(rdd1.iterator(part, sc) + rdd2.iterator(part, sc), rdd3.iterator(part, sc))
       }  
    }
  }
}
{noformat}
In this case, we will need to serialize rdd1 and rdd2 before MyRDD is checkpointed; after MyRDD is checkpointed, we don’t need to serialize rdd1 or rdd2, but we cannot clear func2 either. 

I think we can fix this more general issues as follows:
# As only RDD.compute(or RDD.iterator) should be called at the worker side, we only need to serialize anything that is referenced in that function (no matter it’s a member variable or not)
# After the RDD is checkpointed, the RDD.compute should be changed to read the checkpint file, which will not reference other variables – again, we only need to serialize whatever is referenced in that function now
;;;","03/Dec/14 03:37;rxin;Yea it makes sense to remove all the function closure f from an RDD if it is checkpointed.
;;;","03/Dec/14 03:44;jason.dai;[~rxin] what exactly do you mean by ""remove all the function closure f from an RDD if it is checkpointed""?

In my previous example, we should not clear func2 even if MyRDD is checkpointed, otherwise newRDD() will be no longer correct. Instead, we should make sure we only include RDD.compute(or RDD.iterator) in the closure (no matter whether it is checkpointed or not), and change RDD.compute to reading checkpoint files once it is checkpointed.;;;","03/Dec/14 03:50;rxin;Ok I admit I wasn't reading your comment too carefully :)

Is there a concrete way you are proposing that would solve this problem for arbitrarily defined RDDs? I don't think it is solvable at this point.

That said, we can solve this for most of the built-in RDDs.


;;;","03/Dec/14 04:08;jason.dai;I can see two possible ways to fix this:
# Define customized closure serialization mechanisms in task serializations, which can use reflections to carefully choose which to serialize (i.e., only those referenced by RDD.iterator); this potentially needs to deal with many details and can be error prone.
# In task serialization, each ""base"" RDD can generate a dual, ""shippable"" RDD, which only has transient member variables, and only implements the compute() function (which in turn calls the compute() function of the ""base"" RDD through ClosureCleaner.clean()); we can then probably rely on the Java serializer to handle this correctly.;;;","03/Dec/14 07:54;ankurd;Issue resolved by pull request 3545
[https://github.com/apache/spark/pull/3545];;;","03/Dec/14 07:55;ankurd;[~jerrylead] Thanks for investigating this bug and the excellent explanation. Now that the PRs are merged, can you confirm that the bug is fixed for you? I haven't yet been able to reproduce it locally.;;;","03/Dec/14 10:11;jerrylead;Thank you [~ankurdave]. Yes, the StackOverflow error disappears once the PRs are merged and checkpoint() is performed every N iterations. However, If the lineage of iterative algorithms are too long, we still need to do checkpoint() manually to cut off the lineage to avoid this error. Moreover, the fix suggestion given by [~jason.dai] is fine since it is a general problem. We need more elegant methods to avoid the long chain of task's serialization().;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming block need not to replicate 2 copies when WAL is enabled,SPARK-4671,12758580,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jerryshao,jerryshao,01/Dec/14 07:42,23/Dec/14 23:46,14/Jul/23 06:26,23/Dec/14 23:46,1.2.0,,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,,,,,,"Generated streaming blocks should not be replicated to another node when WAL is enabled, since WAL is already fault tolerant, this will hurt the throughput of streaming application.",,apachespark,huitseeker,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 08:46:53 UTC 2014,,,,,,,,,,"0|i22wtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 08:46;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/3534;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bitwise NOT has a wrong `toString` output,SPARK-4670,12758573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,adrian-wang,adrian-wang,01/Dec/14 06:35,02/Dec/14 22:27,14/Jul/23 06:26,02/Dec/14 22:27,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 22:27:03 UTC 2014,,,,,,,,,,"0|i22wrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 06:36;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3528;;;","02/Dec/14 22:27;marmbrus;Issue resolved by pull request 3528
[https://github.com/apache/spark/pull/3528];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation typos,SPARK-4668,12758570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdub,rdub,rdub,01/Dec/14 06:05,15/Dec/14 22:54,14/Jul/23 06:26,15/Dec/14 22:54,,,,,,,,1.2.0,,,,,,Documentation,,,,0,,,,,,There are a few typos in tuning.md and configuration.md that should be fixed.,,apachespark,hammer,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 06:06:31 UTC 2014,,,,,,,,,,"0|i22wrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 06:06;apachespark;User 'ryan-williams' has created a pull request for this issue:
https://github.com/apache/spark/pull/3523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overflow of `maxFrameSizeBytes`,SPARK-4664,12758566,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,01/Dec/14 05:26,01/Dec/14 08:33,14/Jul/23 06:26,01/Dec/14 08:33,,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"If `spark.akka.frameSize` > 2047, it will overflow and become negative. Should have some assertion in `maxFrameSizeBytes` to warn people.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 05:29:37 UTC 2014,,,,,,,,,,"0|i22wqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 05:29;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3527;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
close() function is not surrounded by finally in ParquetTableOperations.scala,SPARK-4663,12758561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,baishuo,baishuo,01/Dec/14 04:00,02/Dec/14 20:39,14/Jul/23 06:26,02/Dec/14 20:39,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,baishuo,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 20:39:22 UTC 2014,,,,,,,,,,"0|i22wpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 04:49;apachespark;User 'baishuo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3526;;;","02/Dec/14 20:39;marmbrus;Issue resolved by pull request 3526
[https://github.com/apache/spark/pull/3526];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whitelist more Hive unittest,SPARK-4662,12758555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chenghao,chenghao,01/Dec/14 02:45,12/Dec/14 06:43,14/Jul/23 06:26,12/Dec/14 06:43,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Whitelist more hive unit test:

""create_like_tbl_props""
""udf5""
""udf_java_method""
""decimal_1""
""udf_pmod""
""udf_to_double""
""udf_to_float""
""udf7"" (this will fail in Hive 0.12)",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 06:43:14 UTC 2014,,,,,,,,,,"0|i22wnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 02:48;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3522;;;","12/Dec/14 06:43;marmbrus;Issue resolved by pull request 3522
[https://github.com/apache/spark/pull/3522];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaSerializer uses wrong classloader,SPARK-4660,12758500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,pkolaczk,pkolaczk,pkolaczk,30/Nov/14 14:24,04/Apr/15 16:00,14/Jul/23 06:26,20/Jan/15 20:38,1.1.0,1.1.1,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,,,,1,,,,,,"During testing we found failures when trying to load some classes of the user application:

{noformat}
ERROR 2014-11-29 20:01:56 org.apache.spark.storage.BlockManagerWorker: Exception handling buffer message
java.lang.ClassNotFoundException: org.apache.spark.demo.HttpReceiverCases$HttpRequest
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.spark.serializer.JavaDeseriali
zationStream$$anon$1.resolveClass(JavaSerializer.scala:59)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:235)
	at org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:126)
	at org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:104)
	at org.apache.spark.storage.MemoryStore.putBytes(MemoryStore.scala:76)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:748)
	at org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:639)
	at org.apache.spark.storage.BlockManagerWorker.putBlock(BlockManagerWorker.scala:92)
	at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:73)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:48)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:48)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)
	at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:48)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:38)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:38)
	at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:682)
	at org.apache.spark.network.ConnectionManager$$anon$10.run(ConnectionManager.scala:520)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}
",,apachespark,bcantoni,jlewandowski,matei,pkolaczk,sams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4830,,,,,,,,,,,,,,,,,,,,,,"30/Nov/14 14:24;pkolaczk;spark-serializer-classloader.patch;https://issues.apache.org/jira/secure/attachment/12684298/spark-serializer-classloader.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 16:56:08 UTC 2015,,,,,,,,,,"0|i22wbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/14 14:24;pkolaczk;Attaching a patch against 1.1 branch.;;;","29/Dec/14 23:31;matei;[~pkolaczk] mind sending a pull request against http://github.com/apache/spark for this? It will allow us to run it through the automated tests. It looks like a good fix but this stuff can be tricky.;;;","30/Dec/14 11:19;apachespark;User 'pkolaczk' has created a pull request for this issue:
https://github.com/apache/spark/pull/3840;;;","20/Jan/15 06:03;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4114;;;","20/Jan/15 06:03;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4113;;;","20/Jan/15 06:04;jlewandowski;Pull requests for 1.2 and master branches respectively:
https://github.com/apache/spark/pull/4114
https://github.com/apache/spark/pull/4113
;;;","25/Mar/15 19:57;sams;I'm getting this exception in executor logs, behaviour seems different to https://issues.apache.org/jira/browse/SPARK-4830 this time.

Our cluster is kind of stuck on 1.0.0 for a while.  Could someone please explain what this is caused by and how one might work around it?;;;","27/Mar/15 16:56;sams;Furthermore it seems this issue is more likely to happen when I try to process more data.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code documentation issue in DDL of datasource,SPARK-4658,12758475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ravi.pesala,ravi.pesala,30/Nov/14 03:48,01/Dec/14 21:32,14/Jul/23 06:26,01/Dec/14 21:32,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The syntax mentioned to create table for datasource in ddl.scala file is documented with wrong syntax like
{code}
  /**
   * CREATE FOREIGN TEMPORARY TABLE avroTable
   * USING org.apache.spark.sql.avro
   * OPTIONS (path ""../hive/src/test/resources/data/files/episodes.avro"")
   */
{code}

but the correct syntax is 
{code}
 /**
   * CREATE TEMPORARY TABLE avroTable
   * USING org.apache.spark.sql.avro
   * OPTIONS (path ""../hive/src/test/resources/data/files/episodes.avro"")
   */
{code}

Wrong syntax is documented in newParquet.scala like
{code}
`CREATE TABLE ... USING org.apache.spark.sql.parquet`.  
{code} 
 but the correct syntax is 
{code}
`CREATE TEMPORARY TABLE ... USING org.apache.spark.sql.parquet`.
{code}",,apachespark,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 21:32:20 UTC 2014,,,,,,,,,,"0|i22w6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/14 03:54;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/3516;;;","01/Dec/14 21:32;marmbrus;Issue resolved by pull request 3516
[https://github.com/apache/spark/pull/3516];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in Programming Guide markdown,SPARK-4656,12758473,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,lewuathe,lewuathe,30/Nov/14 02:18,01/Dec/14 01:19,14/Jul/23 06:26,01/Dec/14 01:19,1.1.0,,,,,,,1.2.1,,,,,,Documentation,,,,0,,,,,,Grammatical error in Programming Guide document,,apachespark,joshrosen,lewuathe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 01:19:36 UTC 2014,,,,,,,,,,"0|i22w5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/14 02:19;lewuathe;Created the patch. Please review it.
https://github.com/apache/spark/pull/3412;;;","30/Nov/14 02:19;apachespark;User 'Lewuathe' has created a pull request for this issue:
https://github.com/apache/spark/pull/3412;;;","01/Dec/14 01:19;joshrosen;Issue resolved by pull request 3412
[https://github.com/apache/spark/pull/3412];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Supporting multi column support in countDistinct function like count(distinct c1,c2..) in Spark SQL",SPARK-4650,12758387,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ravi.pesala,ravi.pesala,28/Nov/14 19:46,01/Dec/14 21:30,14/Jul/23 06:26,01/Dec/14 21:30,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Support  multi column support inside count(distinct c1,c2..) which is not working in Spark SQL. ",,apachespark,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 21:30:22 UTC 2014,,,,,,,,,,"0|i22vnj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/14 19:50;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/3511;;;","01/Dec/14 21:30;marmbrus;Issue resolved by pull request 3511
[https://github.com/apache/spark/pull/3511];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Asynchronous execution in HiveThriftServer2 with Hive 0.13.1 doesn't play well with Simba ODBC driver,SPARK-4645,12758313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,28/Nov/14 07:22,28/Nov/14 17:47,14/Jul/23 06:26,28/Nov/14 17:47,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Hive 0.13.1 enables asynchronous execution for {{SQLOperation}} by default. So does Spark SQL HiveThriftServer2 when built with Hive 0.13.1. This works well for normal JDBC clients like BeeLine, but throws exception when using Simba ODBC driver v1.0.0.1000.

Simba ODBC driver tries to execute two statement while connecting to Spark SQL HiveThriftServer2:

- {{use `default`}}
- {{set -v}}

However, HiveThriftServer2 throws exception when executing them:
{code}
14/11/28 15:18:37 ERROR SparkExecuteStatementOperation: Error executing query:
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Java heap space
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:309)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$runInternal(Shim13.scala:84)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(Shim13.scala:224)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(Shim13.scala:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/11/28 15:18:37 ERROR SparkExecuteStatementOperation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Java heap space
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$runInternal(Shim13.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(Shim13.scala:224)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(Shim13.scala:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 28 07:52:47 UTC 2014,,,,,,,,,,"0|i22v7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/14 07:30;lian cheng;I haven't figure out the exact reason why asynchronous execution doesn't play well with Simba ODBC driver, probably there is a bug in the Hive 13 shim layer. But resorting to normal synchronous execution fixes the problem.

Since 1.2.0 release is so close, I'd like to fix this issue by disabling asynchronous execution in 1.2.0, and re-enable it after fixing the potential bug in 1.2.1.;;;","28/Nov/14 07:52;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL uses Hadoop Configuration in a thread-unsafe manner when writing Parquet files,SPARK-4629,12758072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,michael,michael,26/Nov/14 21:21,15/Sep/15 21:16,14/Jul/23 06:26,15/Sep/15 21:16,1.1.0,,,,,,,,,,,,,SQL,,,,0,,,,,,"The method {{ParquetRelation.createEmpty}} mutates its given Hadoop {{Configuration}} instance to set the Parquet writer compression level (cf. https://github.com/apache/spark/blob/v1.1.0/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala#L149). This can lead to a {{ConcurrentModificationException}} when running concurrent jobs sharing a single {{SparkContext}} which involve saving Parquet files.

Our ""fix"" was to simply remove the line in question and set the compression level in the hadoop configuration before starting our jobs.",,boyork,chenghao,glenn.strycker@gmail.com,joshrosen,lewuathe,lian cheng,michael,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4229,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 14:15:01 UTC 2015,,,,,,,,,,"0|i22trb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 08:53;chenghao;Are they relate issues with SPARK-4229?;;;","30/Apr/15 08:58;lian cheng;I don't think so. SPARK-4229 is about configuration instance creation, while this one is about mutating configurations.;;;","22/May/15 20:33;joshrosen;This is possibly related to https://issues.apache.org/jira/browse/SPARK-2546 or https://issues.apache.org/jira/browse/SPARK-1097;;;","27/May/15 23:39;yhuai;[~michael] Can you try our 1.4 branch and see if it is still the problem? Seems we only us it for our old parquet support. We are not using it any more.

cc [~lian cheng];;;","28/May/15 13:08;michael;[~yhuai] I apologize but it's not practical for us to deploy another instance of Spark concurrently with our existing deployment, or to take down our production instance and run a different branch. FWIW, we haven't had this problem using the new parquet support in spark 1.3. If the old code is officially dead, then I'd say it's safe to close this ticket.

Cheers.;;;","28/May/15 14:15;yhuai;[~michael] Thank you for the information! That's is really helpful. Since the parquet support in 1.4 is basically based on that in 1.3, I think we can resolve this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many TripletFields,SPARK-4627,12758043,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,jegonzal,jegonzal,26/Nov/14 18:58,26/Nov/14 19:00,14/Jul/23 06:26,26/Nov/14 19:00,,,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,"The `TripletFields` class defines a set of constants for all possible configurations of the triplet fields.  However, many are not useful and as result the API is slightly confusing.  ",,apachespark,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 19:00:25 UTC 2014,,,,,,,,,,"0|i22tlb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 19:00;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/3472;;;","26/Nov/14 19:00;jegonzal;Fixed in PR #3472.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElementException in CoarseGrainedSchedulerBackend,SPARK-4626,12758014,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,silvermast,silvermast,silvermast,26/Nov/14 17:08,28/Nov/14 08:37,14/Jul/23 06:26,27/Nov/14 23:55,1.0.2,,,,,,,1.1.2,1.2.0,,,,,Spark Core,,,,0,,,,,,"{code}
26 Nov 2014 06:38:21,330 ERROR [spark-akka.actor.default-dispatcher-22] OneForOneStrategy - key not found: 0
java.util.NoSuchElementException: key not found: 0
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:58)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:64)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:106)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

This came on the heels of a lot of lost executors with error messages like:
{code}
26 Nov 2014 06:38:20,330 ERROR [spark-akka.actor.default-dispatcher-15] TaskSchedulerImpl - Lost executor 31 on xxx: remote Akka client disassociated
{code}",,apachespark,silvermast,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 28 06:48:11 UTC 2014,,,,,,,,,,"0|i22tf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 17:19;silvermast;It looks like a race where the KillTask message comes in shortly after a RemoveExecutor message. If the executor has already been removed KillTask should no-op:
  executorActor.get(executorId).foreach(_ ! KillTask(taskId, executorId, interruptThread));;;","26/Nov/14 17:38;silvermast;https://github.com/apache/spark/pull/3483;;;","26/Nov/14 17:38;apachespark;User 'roxchkplusony' has created a pull request for this issue:
https://github.com/apache/spark/pull/3483;;;","28/Nov/14 06:47;apachespark;User 'roxchkplusony' has created a pull request for this issue:
https://github.com/apache/spark/pull/3502;;;","28/Nov/14 06:48;apachespark;User 'roxchkplusony' has created a pull request for this issue:
https://github.com/apache/spark/pull/3503;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the some error infomation if using spark-sql in yarn-cluster mode,SPARK-4623,12757962,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,carlmartin,carlmartin,26/Nov/14 12:45,01/Dec/14 00:21,14/Jul/23 06:26,01/Dec/14 00:21,,,,,,,,1.3.0,,,,,,Deploy,,,,0,,,,,,"If using spark-sql in yarn-cluster mode, print an error infomation just as the spark shell in yarn-cluster mode.",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4622,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 12:49:39 UTC 2014,,,,,,,,,,"0|i22t3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 12:49;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3479;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Double ""ms"" in ShuffleBlockFetcherIterator log",SPARK-4619,12757922,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maji2014,maji2014,maji2014,26/Nov/14 08:16,28/Nov/14 08:36,14/Jul/23 06:26,28/Nov/14 08:36,1.1.2,,,,,,,1.2.0,,,,,,,,,28/Nov/14 00:00,0,,,,,,"log as followings: 
ShuffleBlockFetcherIterator: Got local blocks in  8 ms ms

reason:
logInfo(""Got local blocks in "" + Utils.getUsedTimeMs(startTime) + "" ms"")




",,apachespark,maji2014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 08:36:59 UTC 2014,,,,,,,,,,"0|i22sv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 08:36;apachespark;User 'maji2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/3475;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot disconnect from spark-shell,SPARK-4615,12757887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,eggsby,eggsby,26/Nov/14 03:17,09/Dec/14 18:16,14/Jul/23 06:26,09/Dec/14 18:16,1.2.0,,,,,,,1.2.0,,,,,,Spark Shell,,,,0,,,,,,"When running spark-shell using the `v1.2.0-snapshot1` tag, using the instructions at: http://spark.apache.org/docs/latest/building-with-maven.html

When attemping to disconnect from a spark shell (C-c) the terminal locks and does not interrupt the process.

The spark-shell was built with:

./make-distribution.sh --tgz -Pyarn -Phive -Phadoop-2.3 -Dhadoop.version=2.3.0-cdh5.1.0 -DskipTests

Using oracle jdk6 & maven 3.2",Ubuntu 12/14,eggsby,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 18:15:47 UTC 2014,,,,,,,,,,"0|i22snb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 11:58;srowen;I can't reproduce this, although I'm using Java 7, Ubuntu, and HEAD. Same on OS X, Java 8.
Maybe you can ""kill -QUIT"" the Java process to see what threads are waiting on what?
Is any of your code starting a thread or adding a shutdown hook?;;;","09/Dec/14 18:15;eggsby;I can verify that this is fixed for me using the 1.2 rc1 git tag and the same build setup.

Marking as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmitDriverBootstrapper does not propagate EOF to child JVM,SPARK-4606,12757791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,25/Nov/14 20:08,24/Dec/14 03:10,14/Jul/23 06:26,24/Dec/14 00:09,1.1.1,1.2.0,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,,,,0,,,,,,"Run this with ""spark.driver.extraJavaOptions"" set in your spark-defaults.conf:

{code}
  echo """" | spark-shell --master local -Xnojline
{code}

You'll end up with a child process that cannot read from stdin (you can CTRL-C out of it though). That's because when the bootstrapper's stdin reaches EOF, that is not propagated to the child JVM that's actually doing the reading.",,apachespark,joshrosen,stephen,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 24 03:10:01 UTC 2014,,,,,,,,,,"0|i22s2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/14 20:13;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3460;;;","24/Dec/14 00:09;joshrosen;Issue resolved by pull request 3460
[https://github.com/apache/spark/pull/3460];;;","24/Dec/14 02:43;stephen;[~vanzin] since you're poking around in this section of the code, can you look at SPARK-4704 as well?;;;","24/Dec/14 03:07;vanzin;@stephen hmm, I'm working on some other changes that might make that bug obsolete. I'd rather concentrate on those since I'm almost finished with a working prototype.;;;","24/Dec/14 03:10;stephen;Cool, that sounds great; thanks, Marcelo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saveAsNewAPIHadoopFiles by default does not use SparkContext's hadoop configuration ,SPARK-4602,12757715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,25/Nov/14 15:02,25/Nov/14 22:44,14/Jul/23 06:26,25/Nov/14 22:44,1.0.2,1.1.1,,,,,,1.1.2,1.2.0,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 15:09:34 UTC 2014,,,,,,,,,,"0|i22rmf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Nov/14 15:09;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3457;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Call site of jobs generated by streaming incorrect in Spark UI,SPARK-4601,12757684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,25/Nov/14 12:55,25/Nov/14 14:55,14/Jul/23 06:26,25/Nov/14 14:55,,,,,,,,1.2.0,1.3.0,,,,,DStreams,,,,0,,,,,,"When running the NetworkWordCount, the description of the word count jobs are set as ""getCallsite at DStream:xxx"" . This should be set to the line number of the streaming application that has the output operation that led to the job being created. This is because the callsite is incorrectly set in the thread launching the jobs. ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 13:20:40 UTC 2014,,,,,,,,,,"0|i22rfz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Nov/14 13:20;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3455;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use proper exception and reset variable in Utils.createTempDir() method,SPARK-4597,12757642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,25/Nov/14 08:21,29/Nov/14 02:07,14/Jul/23 06:26,29/Nov/14 02:07,1.0.2,1.1.1,1.2.0,,,,,1.0.3,1.1.2,1.2.1,,,,Spark Core,,,,0,,,,,,"In Utils.scala, File.exists() and File.mkdirs() only throw SecurityException instead of IOException. Then, when an exception is thrown, the variable ""dir"" should be reset too.",,apachespark,joshrosen,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 29 02:07:44 UTC 2014,,,,,,,,,,"0|i22r6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/14 08:22;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/3449;;;","29/Nov/14 02:07;joshrosen;Resolved by https://github.com/apache/spark/pull/3449;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark MetricsServlet is not worked because of initialization ordering,SPARK-4595,12757628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jerryshao,jerryshao,jerryshao,25/Nov/14 07:03,17/Dec/14 20:05,14/Jul/23 06:26,17/Dec/14 19:55,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"Web UI is initialized before MetricsSystem is started, at that time MetricsSerlvet is not yet created, which will make MetricsServlet fail to register  into web UI. 

Instead MetricsServlet handler should be added to the web UI after MetricsSystem is started.",,apachespark,jerryshao,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:05:02 UTC 2014,,,,,,,,,,"0|i22r3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/14 07:30;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/3444;;;","25/Nov/14 07:46;joshrosen;Marking this as blocker until we triage tomorrow since it seems like this might be a 1.2.0 regression.  [~jerryshao], do you know if this bug is new in 1.2.0?;;;","25/Nov/14 08:06;jerryshao;Hi Josh, I think this bug may probably  be introduced by {{6aa08c3 [SPARK-1386] Web UI for Spark Streaming}}, this is not a 1.2.0 new bug.;;;","17/Dec/14 19:55;joshrosen;It looks like this actually _was_ a 1.2.0 regression, since I was unable to reproduce this in {{spark-shell}} using the 1.1.1 release.  It's been fixed for 1.2.1, though.;;;","17/Dec/14 20:05;joshrosen;It looks like this bug was introduced in https://github.com/apache/spark/pull/2432, which moved the {{MetricsSystem.start()}} call from SparkEnv to SparkContext, which allowed it to take place after the web UI had started.  That patch addressed SPARK-3377 and the fix was only applied to 1.2.0, so this looks like a 1.2.0 regression.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sum(1/0) would produce a very large number,SPARK-4593,12757624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,adrian-wang,adrian-wang,25/Nov/14 06:53,03/Dec/14 10:09,14/Jul/23 06:26,02/Dec/14 22:24,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,SELECT max(1/0) FROM src would get a very large number.,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4720,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 22:24:28 UTC 2014,,,,,,,,,,"0|i22r2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/14 06:58;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3443;;;","02/Dec/14 22:24;marmbrus;Issue resolved by pull request 3443
[https://github.com/apache/spark/pull/3443];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Worker registration failed: Duplicate worker ID"" error during Master failover",SPARK-4592,12757621,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,joshrosen,joshrosen,25/Nov/14 06:13,25/Nov/14 23:47,14/Jul/23 06:26,25/Nov/14 23:47,1.2.0,,,,,,,1.2.0,,,,,,Deploy,Spark Core,,,0,,,,,,"When running Spark Standalone in high-availability mode, we sometimes see ""Worker registration failed: Duplicate worker ID"" errors which prevent workers from reconnecting to the new active master.  I've attached full logs from a reproduction in my integration tests suite (which runs something similar to Spark's FaultToleranceTest).  Here's the relevant excerpt from a worker log during a failed run of the ""rolling outage"" test, which creates a multi-master cluster then repeatedly kills the active master, waits for workers to reconnect to a new active master, then kills that master, and so on.

{code}
14/11/23 02:23:02 INFO WorkerWebUI: Started WorkerWebUI at http://172.17.0.90:8081
14/11/23 02:23:02 INFO Worker: Connecting to master spark://172.17.0.86:7077...
14/11/23 02:23:02 INFO Worker: Connecting to master spark://172.17.0.87:7077...
14/11/23 02:23:02 INFO Worker: Connecting to master spark://172.17.0.88:7077...
14/11/23 02:23:02 INFO Worker: Successfully registered with master spark://172.17.0.86:7077
14/11/23 02:23:03 INFO Worker: Asked to launch executor app-20141123022303-0000/1 for spark-integration-tests
14/11/23 02:23:03 INFO ExecutorRunner: Launch command: ""java"" ""-cp"" ""::/opt/sparkconf:/opt/spark/assembly/target/scala-2.10/spark-assembly-1.2.1-SNAPSHOT-hadoop1.0.4.jar"" ""-XX:MaxPermSize=128m"" ""-Dspark.driver.port=51271"" ""-Xms512M"" ""-Xmx512M"" ""org.apache.spark.executor.CoarseGrainedExecutorBackend"" ""akka.tcp://sparkDriver@joshs-mbp.att.net:51271/user/CoarseGrainedScheduler"" ""1"" ""172.17.0.90"" ""8"" ""app-20141123022303-0000"" ""akka.tcp://sparkWorker@172.17.0.90:8888/user/Worker""
14/11/23 02:23:14 INFO Worker: Disassociated [akka.tcp://sparkWorker@172.17.0.90:8888] -> [akka.tcp://sparkMaster@172.17.0.86:7077] Disassociated !
14/11/23 02:23:14 ERROR Worker: Connection to master failed! Waiting for master to reconnect...
14/11/23 02:23:14 INFO Worker: Connecting to master spark://172.17.0.86:7077...
14/11/23 02:23:14 INFO Worker: Connecting to master spark://172.17.0.87:7077...
14/11/23 02:23:14 INFO Worker: Connecting to master spark://172.17.0.88:7077...
14/11/23 02:23:14 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkMaster@172.17.0.86:7077] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
14/11/23 02:23:14 INFO Worker: Disassociated [akka.tcp://sparkWorker@172.17.0.90:8888] -> [akka.tcp://sparkMaster@172.17.0.86:7077] Disassociated !
14/11/23 02:23:14 ERROR Worker: Connection to master failed! Waiting for master to reconnect...
14/11/23 02:23:14 INFO RemoteActorRefProvider$RemoteDeadLetterActorRef: Message [org.apache.spark.deploy.DeployMessages$RegisterWorker] from Actor[akka://sparkWorker/user/Worker#-1246122173] to Actor[akka://sparkWorker/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/11/23 02:23:14 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already.
14/11/23 02:23:14 INFO LocalActorRef: Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%40172.17.0.86%3A7077-2#343365613] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/11/23 02:23:25 INFO Worker: Retrying connection to master (attempt # 1)
14/11/23 02:23:25 INFO Worker: Connecting to master spark://172.17.0.86:7077...
14/11/23 02:23:25 INFO Worker: Connecting to master spark://172.17.0.87:7077...
14/11/23 02:23:25 INFO Worker: Connecting to master spark://172.17.0.88:7077...
14/11/23 02:23:36 INFO Worker: Retrying connection to master (attempt # 2)
14/11/23 02:23:36 INFO Worker: Connecting to master spark://172.17.0.86:7077...
14/11/23 02:23:36 INFO Worker: Connecting to master spark://172.17.0.87:7077...
14/11/23 02:23:36 INFO Worker: Connecting to master spark://172.17.0.88:7077...
14/11/23 02:23:42 INFO Worker: Master has changed, new master is at spark://172.17.0.87:7077
14/11/23 02:23:47 INFO Worker: Retrying connection to master (attempt # 3)
14/11/23 02:23:47 INFO Worker: Connecting to master spark://172.17.0.86:7077...
14/11/23 02:23:47 INFO Worker: Connecting to master spark://172.17.0.87:7077...
14/11/23 02:23:47 INFO Worker: Connecting to master spark://172.17.0.88:7077...
14/11/23 02:23:47 ERROR Worker: Worker registration failed: Duplicate worker ID
14/11/23 02:23:47 INFO ExecutorRunner: Killing process!
{code}",,andrewor14,apachespark,asukhenko,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-981,,,,,,,,,,,,,,,,,,,,,,"25/Nov/14 06:15;joshrosen;log.txt;https://issues.apache.org/jira/secure/attachment/12683491/log.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 08:15:27 UTC 2014,,,,,,,,,,"0|i22r27:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Nov/14 06:15;joshrosen;It looks like this is a bug that was introduced in [the patch|https://github.com/apache/spark/pull/2828] for SPARK-3736.  Prior to that patch, a worker that became disassociated from a master would wait for a master to initiate a reconnection.  This behavior caused problems when a master failed by stopping and restarting, since the restarted master would never know to initiate a reconnection.  That patch addressed this issue by having workers attempt to reconnect to the master if they think they've become disconnected.  When reviewing that patch, I wrote a [long comment|https://github.com/apache/spark/pull/2828#issuecomment-59602394] that explains it in much more detail; that's a good reference for understanding its motivation.

This change introduced a problem, though: there's now a race-condition during multi-master failover.  In the old multi-master code, a worker never initiates a reconnection attempt to a master; instead, it reconnects after the new active / primary master tells the worker that it's the new master.  With the addition of worker-initiated reconnect, there's a race-condition where the worker detects that it's become disconnected, goes down the list of known masters and tries to connect to each of them, successfully connects to the new primary master, then receives a {{ChangedMaster}} event and attempts to connect to the new primary master _even though it's already connected_, causing a duplicate worker registration.

There are a number of ways that we might fix this, but we have to be careful because it seems likely that the worker-initiated reconnect could have introduced other problems:

- What happens if a worker sends a reconnection attempt to a live master which is not the new primary?  Will that non-primary master reject or redirect those registrations, or will it register the workers and cause a split-brain scenario to occur?
- The Worker is implemented as an actor and thus does not have synchronization of its internal state since it assumes message-at-a-time processing, but the asynchronous re-registration timer thread may violate this assumption because it directly calls internal worker methods instead of sending messages to the worker's own mailbox.

One simple fix might be to have the worker never initiate reconnection attempts when running in a multi-master environment.  I still need to think through whether this will cause new problems similar to SPARK-3736.  I don't think it will be a problem because that patch was motivated by cases where the master forgot who the worker was and couldn't initiate a reconnect.  If the list of registered workers is stored durably in ZooKeeper such that a worker is never told that it has registered until a record of its registration has become durable, then I think this is fine: if a live master thinks that a worker has disconnected, then it will initiate a reconnection; when a new master fails over, it will reconnect all workers based on the list from ZooKeeper. ;;;","25/Nov/14 08:10;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3447;;;","25/Nov/14 08:15;andrewor14;I submitted a fix at https://github.com/apache/spark/pull/3447. For more detail, please read the message there. It should be noted that even this fix is still subject to more obscure race conditions. For instance:

1. Master A dies, worker tries to reconnect
2. Master B comes up and notifies worker
3. Master A comes back up, and worker successfully re-registers with Master A
4. Worker additionally receives the notification from Master B, and now it listens to both masters

As noted in the PR description, these race conditions are much less likely than the one this issue is trying to fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2x Performance regression for Spark-on-YARN,SPARK-4584,12757591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,nravi,nravi,25/Nov/14 02:01,28/Nov/14 20:17,14/Jul/23 06:26,28/Nov/14 20:17,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,Significant performance regression observed for Spark-on-YARN (upto 2x) after 1.2 rebase. The offending commit is: 70e824f750aa8ed446eec104ba158b0503ba58a9  from Oct 7th. Problem can be reproduced with JavaWordCount against a large enough input dataset in YARN cluster mode.,,andrewor14,apachespark,diederik,nravi,sandyr,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 18:50:07 UTC 2014,,,,,,,,,,"0|i22qvr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Nov/14 08:10;sandyr;I took a look at the jobs Nishkam ran before and after that commit.  The second stage in the ""before"" job takes 69 seconds and the second stage in the ""after"" job takes 158 seconds.  This seems to be caused by the individual tasks taking longer.

Totally confused about how that commit could have caused the regression.;;;","25/Nov/14 22:24;nravi;Looked into this issue a bit more. Source of the problem is setupSystemSecurityManager(). If I comment out the invocation, we get the performance back. ;;;","25/Nov/14 22:33;nravi;Looks like java.lang.SecurityManager is a hog. I'm looking to see if we use it elsewhere in Spark. If so, we might want to reconsider removing it everywhere and replacing it by something more efficient.;;;","26/Nov/14 00:25;nravi;I don't see SecurityManager anywhere else in Spark, which is great. 

Marcelo and I tried a lightweight version of SecurityManager with all methods except for checkExit() stubbed out and continue to see the performance issue. 

Maybe something like Runtime.addShutdownHook could be used instead?;;;","26/Nov/14 00:28;nravi;Use of SecurityManager may be suppressing a JIT compiler optimization.;;;","26/Nov/14 01:28;andrewor14;Hey [~nravi] how much data are you shuffling? Can you check the ""Shuffle Write"" field on the Spark UI?;;;","26/Nov/14 01:35;nravi;[~andrewor14] Just curious, why is that relevant here? ;;;","26/Nov/14 01:38;nravi;Around 2GB in the map stage and 1.5GB in the collect phase.;;;","26/Nov/14 02:00;andrewor14;It may not be, but I just wanted to get a sense of how large your dataset is. I wasn't able to reproduce the performance discrepancy between the two commits on my end. I'm doing a simple groupBy on spark-perf that shuffles about 16GB with 2000 partitions, and I haven't observed a significant performance difference (~2%, likely just noise). These results are based on the median over 10 runs.

The other thing is that this only seems to affect the application master, which becomes the driver in cluster mode but otherwise shouldn't affect the application performance at all. What mode were you running in, client or cluster mode? How many partitions did you have?;;;","26/Nov/14 02:17;nravi;I would recommend working with JavaWordCount. We don't see this regression for most of our other workloads either, including those from spark-perf. Don't think you would need a large input dataset to reproduce the problem. To see the perf diff, commenting out invocation to setupSystemSecurityManager() would suffice. I'm running YARN cluster mode and have 450 partitions. ;;;","26/Nov/14 08:27;srowen;{{SecurityManager}} is something that loads of the JVM code consults, if it exists:

{code}
SecurityManager sm = SecurityManager.getSystemSecurityManager();
if (sm != null) {
  ...
}
{code}

Setting any {{SecurityManager}} is like turning on a whole lot of not-cheap permission checks throughout the JDK. I think setting one is pretty undesirable from a performance perspective. It also precludes the possibility of enabling a real SecurityManager for contexts that want to although I find it unlikely that would ever really work with Spark.

How about just documenting and telling users ""don't System.exit in your code"", which is widely accepted as a no-no in Java/Scala anyway?;;;","26/Nov/14 17:35;vanzin;[~sowen] that was going to be my suggestion. For 1.2, I'll just remote the security manager and declare ""use System.exit() at your own risk"" - the behavior should then be the same as 1.1. Post 1.2, we could add some new exception (e.g. {{SparkAppException}}) that users can throw if they want the runtime to exit with a specific error code. But even that I don't think is strictly necessary - just a nice to have.

BTW, we've tried to extend the security manager so that all operations except for {{checkExit}} are no-ops, but even that doesn't help.;;;","26/Nov/14 18:50;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3484;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduling Delay appears negative,SPARK-4579,12757538,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,arahuja,arahuja,24/Nov/14 22:27,26/Mar/15 05:26,14/Jul/23 06:26,27/Feb/15 01:36,1.2.0,,,,,,,1.2.2,1.3.0,,,,,Web UI,,,,0,,,,,,"!https://cloud.githubusercontent.com/assets/455755/5174438/23d08604-73ff-11e4-9a76-97233b610544.png!
",,andrewor14,apachespark,arahuja,kayousterhout,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5877,,,SPARK-6543,,,,,SPARK-4571,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 23:36:52 UTC 2015,,,,,,,,,,"0|i22qk7:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,"18/Feb/15 00:34;kayousterhout;This is only for running tasks, I'm guessing because we compute the scheduler delay as something like finish time - start time - some stuff, and the finish time is 0 while a task is running.;;;","18/Feb/15 09:11;pwendell;[~andrewor14] Can you take a look at this one?;;;","18/Feb/15 09:18;srowen;See also https://issues.apache.org/jira/browse/SPARK-4571 which is a similar UI issue and might be worth fixing at the same time.;;;","26/Feb/15 23:36;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row.asDict() should keep the type of values,SPARK-4578,12757531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,24/Nov/14 22:09,25/Nov/14 00:42,14/Jul/23 06:26,25/Nov/14 00:42,1.2.0,,,,,,,1.2.0,,,,,,PySpark,SQL,,,1,,,,,,"Current, the nested Row will be returned as tuple, it should be Row.",,apachespark,davies,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 00:42:15 UTC 2014,,,,,,,,,,"0|i22qin:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Nov/14 23:17;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3434;;;","25/Nov/14 00:42;pwendell;Thanks davies I've resolved this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server shows negative time,SPARK-4571,12757349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsudukim,andrewor14,andrewor14,24/Nov/14 08:56,26/Feb/15 23:33,14/Jul/23 06:26,26/Feb/15 23:33,1.1.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,See attachment,,andrewor14,boyork,csun,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4579,,,,,,,,,,,,,,,,"24/Nov/14 08:57;andrewor14;Screen Shot 2014-11-21 at 2.49.25 PM.png;https://issues.apache.org/jira/secure/attachment/12683295/Screen+Shot+2014-11-21+at+2.49.25+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 23:33:45 UTC 2015,,,,,,,,,,"0|i22pev:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"26/Feb/15 23:33;srowen;I'm quite sure this was solved as part of SPARK-2458, and this change: https://github.com/apache/spark/commit/6e74edeca31acd7dc84a34402e430e017591d858#diff-a19a4359f1a7f63bc020acf145664af4R132;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Rename ""externalSorting"" in Aggregator",SPARK-4569,12757338,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,ilganeli,sandyr,sandyr,24/Nov/14 06:53,17/May/20 18:30,14/Jul/23 06:26,21/Jan/15 18:51,1.2.0,,,,,,,1.1.2,1.2.1,1.3.0,,,,Shuffle,Spark Core,,,0,,,,,,"While technically all spilling in Spark does result in sorting, calling this variable externalSorting makes it seem like ExternalSorter will be used, when in fact it just means whether spilling is enabled.",,apachespark,joshrosen,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 19:25:54 UTC 2015,,,,,,,,,,"0|i22pcn:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,"10/Dec/14 17:55;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/3666;;;","21/Jan/15 19:25;joshrosen;It looks like all backports have been completed, so I'm removing the {{backport-needed}} label.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding support for ucase and lcase,SPARK-4559,12757260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,scwf,scwf,23/Nov/14 01:28,16/Sep/15 08:05,14/Jul/23 06:26,15/Sep/15 21:17,1.1.0,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,Adding support for ucase and lcase in spark sql,,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4867,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:17:48 UTC 2015,,,,,,,,,,"0|i22ovb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/14 01:28;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3418;;;","15/Sep/15 21:17;marmbrus;we have {{upper}} and {{lower}} now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query for parquet table with string fields in spark sql hive get binary result,SPARK-4553,12757153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,scwf,scwf,22/Nov/14 09:19,16/Feb/15 09:39,14/Jul/23 06:26,16/Feb/15 09:39,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"run 
create table test_parquet(key int, value string) stored as parquet;
insert into table test_parquet select * from src;
select * from test_parquet;
get result as follow

...
282 [B@38fda3b
138 [B@1407a24
238 [B@12de6fb
419 [B@6c97695
15 [B@4885067
118 [B@156a8d3
72 [B@65d20dd
90 [B@4c18906
307 [B@60b24cc
19 [B@59cf51b
435 [B@39fdf37
10 [B@4f799d7
277 [B@3950951
273 [B@596bf4b
306 [B@3e91557
224 [B@3781d61
309 [B@2d0d128",,apachespark,lian cheng,scwf,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 09:39:18 UTC 2015,,,,,,,,,,"0|i22ojr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"22/Nov/14 09:26;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3414;;;","11/Feb/15 18:08;yhuai;[~lian cheng] Should we close it (since setting spark.sql.parquet.binaryAsString to true can be used to get string results)? Or, we want to remove spark.sql.parquet.binaryAsString?;;;","11/Feb/15 18:14;lian cheng;We still need {{spark.sql.parquet.binaryAsString}} to read Parquet files written by old versions of Parquet files which are not managed by Hive. I'll close this later when the new Parquet data source PR gets merged.;;;","12/Feb/15 10:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4563;;;","16/Feb/15 09:39;lian cheng;Issue resolved by pull request 4563
[https://github.com/apache/spark/pull/4563];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query for empty parquet table in spark sql hive get IllegalArgumentException,SPARK-4552,12757151,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,scwf,scwf,22/Nov/14 09:15,03/Dec/14 22:18,14/Jul/23 06:26,03/Dec/14 22:18,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"run
create table test_parquet(key int, value string) stored as parquet;
select * from test_parquet;
get error as follow

java.lang.IllegalArgumentException: Could not find Parquet metadata at path file:/user/hive/warehouse/test_parquet
at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$4.apply(ParquetTypes.scala:459)
at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$4.apply(ParquetTypes.scala:459)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.sql.parquet.ParquetTypesConverter$.readMetaData(ParquetTypes.sc",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4702,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 22:18:21 UTC 2014,,,,,,,,,,"0|i22ojb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Nov/14 09:21;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3413;;;","03/Dec/14 18:41;marmbrus;It turns out this manifests also when writing a query that doesn't match any partitions, which seems much more common.  I'm going to do a quick surgical fix given the proximity to the release.;;;","03/Dec/14 19:25;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3586;;;","03/Dec/14 22:18;marmbrus;Issue resolved by pull request 3586
[https://github.com/apache/spark/pull/3586];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python broadcast perf regression from Spark 1.1,SPARK-4548,12757118,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,21/Nov/14 23:53,25/Nov/14 01:18,14/Jul/23 06:26,25/Nov/14 01:18,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"Python broadcast in 1.2 is much slower than 1.1: 

In spark-perf tests:
      name                        1.1     1.2          speedup
python-broadcast-w-set	3.63	16.68	-78.23%",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 01:18:37 UTC 2014,,,,,,,,,,"0|i22oc7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Nov/14 20:05;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3417;;;","25/Nov/14 01:18;joshrosen;Issue resolved by pull request 3417
[https://github.com/apache/spark/pull/3417];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM when making bins in BinaryClassificationMetrics,SPARK-4547,12757111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,21/Nov/14 23:17,31/Dec/14 21:37,14/Jul/23 06:26,31/Dec/14 21:37,1.1.0,,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"Also following up on http://mail-archives.apache.org/mod_mbox/spark-dev/201411.mbox/%3CCAMAsSdK4s4TNkf3_ecLC6yD-pLpys_PpT3WB7Tp6=yoXUxFpMA@mail.gmail.com%3E -- this one I intend to make a PR for a bit later. The conversation was basically:

{quote}
Recently I was using BinaryClassificationMetrics to build an AUC curve for a classifier over a reasonably large number of points (~12M). The scores were all probabilities, so tended to be almost entirely unique.

The computation does some operations by key, and this ran out of memory. It's something you can solve with more than the default amount of memory, but in this case, it seemed unuseful to create an AUC curve with such fine-grained resolution.

I ended up just binning the scores so there were ~1000 unique values
and then it was fine.
{quote}

and:

{quote}
Yes, if there are many distinct values, we need binning to compute the AUC curve. Usually, the scores are not evenly distribution, we cannot simply truncate the digits. Estimating the quantiles for binning is necessary, similar to RangePartitioner:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L104

Limiting the number of bins is definitely useful.
{quote}
",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 21:37:15 UTC 2014,,,,,,,,,,"0|i22oan:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"15/Dec/14 16:19;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3702;;;","31/Dec/14 21:37;mengxr;Issue resolved by pull request 3702
[https://github.com/apache/spark/pull/3702];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve HistoryServer first time user experience,SPARK-4546,12757108,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,21/Nov/14 23:14,05/Dec/14 03:57,14/Jul/23 06:26,05/Dec/14 03:57,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"The first thing a user would try to do is run the command as documented:
{code}
sbin/start-history-server.sh
{code}
This will throw an exception, however, complaining that the logging directory is not set. We need two things: (1) a default logging directory that corresponds to what EventLoggingListener uses, and (2) a better landing page when there are no event logs found.",,andrewor14,apachespark,ckadner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/14 23:23;andrewor14;after.png;https://issues.apache.org/jira/secure/attachment/12682985/after.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 23:24:44 UTC 2014,,,,,,,,,,"0|i22o9z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Nov/14 23:24;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3411;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sqrt and abs to Spark SQL DSL,SPARK-4536,12756931,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,21/Nov/14 09:54,02/Dec/14 20:11,14/Jul/23 06:26,02/Dec/14 20:11,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,Spark SQL havs embeded sqrt and abs but DSL doesn't support those functions.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 09:56:48 UTC 2014,,,,,,,,,,"0|i22n7z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Nov/14 09:56;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the error in comments,SPARK-4535,12756923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,waterman,waterman,21/Nov/14 09:07,25/Nov/14 12:07,14/Jul/23 06:26,25/Nov/14 12:07,1.1.0,,,,,,,1.2.0,1.3.0,,,,,DStreams,,,,0,,,,,,"change ""NetworkInputDStream"" to ""ReceiverInputDStream""
change ""ReceiverInputTracker"" to ""ReceiverTracker""",,apachespark,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 09:10:28 UTC 2014,,,,,,,,,,"0|i22n67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/14 09:10;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3400;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make-distribution in Spark 1.2 does not correctly detect whether Hive is enabled,SPARK-4532,12756912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,21/Nov/14 06:52,21/Nov/14 20:13,14/Jul/23 06:26,21/Nov/14 20:13,,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"I noticed that Hive support isn't working in the recent release artifacts. Because the hive profile is no longer defined in the root pom, the following doesn't work:

{code}
SPARK_HIVE=$(mvn help:evaluate -Dexpression=project.activeProfiles $@ 2>/dev/null\
    | grep -v ""INFO""\
    | fgrep --count ""<id>hive</id>"";\
    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\
    # because we use ""set -o pipefail""
    echo -n)
{code}

We need to simply change it to this

{code}
SPARK_HIVE=$(mvn help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2>/dev/null\
    | grep -v ""INFO""\
    | fgrep --count ""<id>hive</id>"";\
    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\
    # because we use ""set -o pipefail""
    echo -n)
{code}",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 06:56:28 UTC 2014,,,,,,,,,,"0|i22n3r:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Nov/14 06:56;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/3398;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"GradientDescent get a wrong gradient value according to the gradient formula, which is caused by the miniBatchSize parameter.",SPARK-4530,12756909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,21/Nov/14 06:29,12/Dec/14 12:58,14/Jul/23 06:26,12/Dec/14 12:58,1.0.0,1.1.0,1.2.0,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"This bug is caused by {{RDD.sample}}
The number of  {{RDD.sample}}  returns is not fixed.",,apachespark,gq,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 12:58:32 UTC 2014,,,,,,,,,,"0|i22n33:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Nov/14 23:31;srowen;See comments on the PR. I don't think these things rise to the level of 'blocker';;;","25/Nov/14 10:03;mengxr;PR: https://github.com/apache/spark/pull/3399;;;","25/Nov/14 10:46;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3399;;;","12/Dec/14 12:58;srowen;This was evidently merged for master and 1.2: https://github.com/apache/spark/commit/f515f9432b05f7e090b651c5536aa706d1cde487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MesosSchedulerBackend.resourceOffers cannot decline unused offers from acceptedOffers,SPARK-4525,12756853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jongyoul,jongyoul,jongyoul,20/Nov/14 23:51,03/Dec/14 00:48,14/Jul/23 06:26,25/Nov/14 03:15,1.2.0,1.3.0,,,,,,1.2.0,,,,,,Mesos,,,,0,,,,,,"After resourceOffers function is refactored - SPARK-2269 -, that function doesn't decline unused offers from accepted offers. That's because when driver.launchTasks is called, if that's tasks is empty, driver.launchTask calls the declineOffer(offer.id). 
{quote}
Invoking this function with an empty collection of tasks declines offers in their entirety (see SchedulerDriver.declineOffer(OfferID, Filters)).
- http://mesos.apache.org/api/latest/java/org/apache/mesos/MesosSchedulerDriver.html#launchTasks(OfferID,%20java.util.Collection,%20Filters)
{quote}

In branch-1.1, resourcesOffers calls a launchTask function for all offered offers, so driver declines unused resources, however, in current master, at first offers are divided accepted and declined offers by their resources, and delinedOffers are declined explicitly, and offers with task from acceptedOffers are launched by driver.launchTasks, but, offers without from acceptedOfers are not launched with empty task or declined explicitly. Thus, mesos master judges thats offers used by TaskScheduler and there are no resources remaing.",,apachespark,jongyoul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 00:37:36 UTC 2014,,,,,,,,,,"0|i22mqn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Nov/14 00:08;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/3393;;;","25/Nov/14 00:37;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/3436;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to read parquet schema with missing metadata.,SPARK-4522,12756849,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,20/Nov/14 23:20,24/Nov/14 21:01,14/Jul/23 06:26,24/Nov/14 21:01,,,,,,,,,,,,,,SQL,,,,0,,,,,,"Our JSON parsing code is very strict and will fail, even if optional fields like metadata are missing.  This means that people who are reading parquet files written with code from between 1.1 and 1.2 might loose type information for no reason.",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4523,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 23:30:16 UTC 2014,,,,,,,,,,"0|i22mpr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Nov/14 23:30;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3392;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL exception when reading certain columns from a parquet file,SPARK-4520,12756808,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sadhan,sadhan,sadhan,20/Nov/14 20:16,03/Jun/15 15:59,14/Jul/23 06:26,24/Apr/15 17:59,1.2.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"I am seeing this issue with spark sql throwing an exception when trying to read selective columns from a thrift parquet file and also when caching them.
On some further digging, I was able to narrow it down to at-least one particular column type: map<string, set<string>> to be causing this issue. To reproduce this I created a test thrift file with a very basic schema and stored some sample data in a parquet file:

Test.thrift
===========
{code}
typedef binary SomeId

enum SomeExclusionCause {
  WHITELIST = 1,
  HAS_PURCHASE = 2,
}

struct SampleThriftObject {
  10: string col_a;
  20: string col_b;
  30: string col_c;
  40: optional map<SomeExclusionCause, set<SomeId>> col_d;
}
{code}
=============

And loading the data in spark through schemaRDD:
{code}
import org.apache.spark.sql.SchemaRDD
val sqlContext = new org.apache.spark.sql.SQLContext(sc);
val parquetFile = ""/path/to/generated/parquet/file""
val parquetFileRDD = sqlContext.parquetFile(parquetFile)
parquetFileRDD.printSchema
root
 |-- col_a: string (nullable = true)
 |-- col_b: string (nullable = true)
 |-- col_c: string (nullable = true)
 |-- col_d: map (nullable = true)
 |    |-- key: string
 |    |-- value: array (valueContainsNull = true)
 |    |    |-- element: string (containsNull = false)

parquetFileRDD.registerTempTable(""test"")
sqlContext.cacheTable(""test"")
sqlContext.sql(""select col_a from test"").collect() <-- see the exception stack here 
{code}
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/xyz/part-r-00000.parquet
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)
	at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:1223)
	at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:1223)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.ArrayList.elementData(ArrayList.java:418)
	at java.util.ArrayList.get(ArrayList.java:431)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.PrimitiveColumnIO.getLast(PrimitiveColumnIO.java:80)
	at parquet.io.PrimitiveColumnIO.isLast(PrimitiveColumnIO.java:74)
	at parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:282)
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)
	at parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)
	at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:126)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)
	... 27 more
{code}

If you take out the col_d from the thrift file, the problem goes away. The problem also shows up when trying to read the particular column without caching the table first. The same file can be dumped/read using parquet-tools just fine. Here is the file dump using parquet-tools:
{code}
row group 0 
--------------------------------------------------------------------------------
col_a:           BINARY UNCOMPRESSED DO:0 FPO:4 SZ:89/89/1.00 VC:9 ENC [more]...
col_b:           BINARY UNCOMPRESSED DO:0 FPO:93 SZ:89/89/1.00 VC:9 EN [more]...
col_c:           BINARY UNCOMPRESSED DO:0 FPO:182 SZ:89/89/1.00 VC:9 E [more]...
col_d:          
.map:           
..key:           BINARY UNCOMPRESSED DO:0 FPO:271 SZ:29/29/1.00 VC:9 E [more]...
..value:        
...value_tuple:  BINARY UNCOMPRESSED DO:0 FPO:300 SZ:29/29/1.00 VC:9 E [more]...

    col_a TV=9 RL=0 DL=1
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:60 VC:9

    col_b TV=9 RL=0 DL=1
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:60 VC:9

    col_c TV=9 RL=0 DL=1
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:60 VC:9

    col_d.map.key TV=9 RL=1 DL=2
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:RLE VLE:PLAIN SZ:12 VC:9

    col_d.map.value.value_tuple TV=9 RL=2 DL=4
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:RLE VLE:PLAIN SZ:12 VC:9

BINARY col_a 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:1 D:1 V:a1
value 2: R:1 D:1 V:a2
value 3: R:1 D:1 V:a3
value 4: R:1 D:1 V:a4
value 5: R:1 D:1 V:a5
value 6: R:1 D:1 V:a6
value 7: R:1 D:1 V:a7
value 8: R:1 D:1 V:a8
value 9: R:1 D:1 V:a9

BINARY col_b 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:1 D:1 V:b1
value 2: R:1 D:1 V:b2
value 3: R:1 D:1 V:b3
value 4: R:1 D:1 V:b4
value 5: R:1 D:1 V:b5
value 6: R:1 D:1 V:b6
value 7: R:1 D:1 V:b7
value 8: R:1 D:1 V:b8
value 9: R:1 D:1 V:b9

BINARY col_c 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:1 D:1 V:c1
value 2: R:1 D:1 V:c2
value 3: R:1 D:1 V:c3
value 4: R:1 D:1 V:c4
value 5: R:1 D:1 V:c5
value 6: R:1 D:1 V:c6
value 7: R:1 D:1 V:c7
value 8: R:1 D:1 V:c8
value 9: R:1 D:1 V:c9

BINARY col_d.map.key 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:0 D:0 V:<null>
value 2: R:0 D:0 V:<null>
value 3: R:0 D:0 V:<null>
value 4: R:0 D:0 V:<null>
value 5: R:0 D:0 V:<null>
value 6: R:0 D:0 V:<null>
value 7: R:0 D:0 V:<null>
value 8: R:0 D:0 V:<null>
value 9: R:0 D:0 V:<null>

BINARY col_d.map.value.value_tuple 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:0 D:0 V:<null>
value 2: R:0 D:0 V:<null>
value 3: R:0 D:0 V:<null>
value 4: R:0 D:0 V:<null>
value 5: R:0 D:0 V:<null>
value 6: R:0 D:0 V:<null>
value 7: R:0 D:0 V:<null>
value 8: R:0 D:0 V:<null>
value 9: R:0 D:0 V:<null>
{code}",,alexlevenson,apachespark,lian cheng,sadhan,trrichard,yanboliang,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/14 20:17;sadhan;part-r-00000.parquet;https://issues.apache.org/jira/secure/attachment/12682731/part-r-00000.parquet",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 15:59:52 UTC 2015,,,,,,,,,,"0|i22mgv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"20/Nov/14 20:17;sadhan;Attaching a sample parquet file which can be used to reproduce the issue with the given schema in the description.;;;","19/Dec/14 22:04;sadhan;[~marmbrus], can you assign this to me - we fixed this issue internally and would be happy to submit a fix for it.;;;","08/Jan/15 19:23;alexlevenson;Awesome! What did the issue turn out to be? Is it a parquet bug?;;;","13/Jan/15 23:34;trrichard;I'm also interested in the solution to this. I'm having a similar problem with reading parquet data fields with repeating elements. Sadhan thanks for your research into this.

parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/parquetSample/committed_2015-01-13_15-03-40.915-08-00/part-m-00014.gz.parquet
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.ArrayList.elementData(ArrayList.java:371)
	at java.util.ArrayList.get(ArrayList.java:384)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.PrimitiveColumnIO.getLast(PrimitiveColumnIO.java:80)
	at parquet.io.PrimitiveColumnIO.isLast(PrimitiveColumnIO.java:74)
	at parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:290)
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)
	at parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)
	at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:126)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)
	... 26 more
;;;","13/Jan/15 23:36;trrichard;Schema:
requestedSchema: message root {
  required group key {
    required int64 marketplaceId;
    required binary iaId (UTF8);
  }
  required group value {
    required group time {
      required int32 grain;
      required int64 milliseconds;
    }
    required group series {
      required group values (LIST) {
        repeated double array;
      }
    }
  }
}


Just like Sadhan I can read the data with parquet-tools but not with spark.;;;","16/Jan/15 19:23;sadhan;Tyler, Alex - the problem is not with parquet but how we are reading the parquet columns.  Just wanted to make sure that you are seeing this problem with thrift generated parquet files as well? I am going to submit my fix this weekend now that I have some availability, my apologies for the delay.;;;","16/Jan/15 19:29;trrichard;No rush. Just interested. I figured my problem was something along the lines of :
schema !=  my custom serializer != the spark deserializer

But it looks like the problems may lie with the spark deserializer more than my own serialization.;;;","21/Jan/15 23:51;apachespark;User 'sadhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4148;;;","24/Apr/15 17:59;yhuai;[~lian cheng] I am resolving this one.;;;","03/Jun/15 15:59;lian cheng;This issue is related to Parquet backwards compatibility. The column name with {{_tuple}} postfix is a historical issue of parquet-thrift. Related logic [still exists|https://github.com/apache/parquet-mr/blob/apache-parquet-1.7.0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConvertVisitor.java#L114-L145] in the most recent Parquet 1.7.0 release.

The most recent Parquet format spec (not released yet up until writing) handles this situation via [LIST backwards compatibility rules|https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#backward-compatibility-rules]. IIRC, at least these rules have been implemented properly in parquet-avro, not quite sure about situations of other Parquet submodules.

SPARK-6774 aims to fix these stuff for Spark SQL Parquet support.

Was reviewing Parquet backwards compatibility related issues. Just leave a comment here for future reference.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filestream does not use hadoop configuration set within sparkContext.hadoopConfiguration,SPARK-4519,12756804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,20/Nov/14 20:10,24/Nov/14 21:52,14/Jul/23 06:26,24/Nov/14 21:52,1.0.2,1.1.1,,,,,,1.2.0,,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 23 01:30:54 UTC 2014,,,,,,,,,,"0|i22mfz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Nov/14 20:19;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3389;;;","23/Nov/14 01:30;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3419;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filestream sometimes processes files twice,SPARK-4518,12756801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,20/Nov/14 20:09,24/Nov/14 21:53,14/Jul/23 06:26,24/Nov/14 21:53,1.0.2,1.1.1,,,,,,1.2.0,,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 23 01:30:46 UTC 2014,,,,,,,,,,"0|i22mfb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Nov/14 20:19;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3389;;;","23/Nov/14 01:30;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3419;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty off-heap memory use causes executors to be killed by OS,SPARK-4516,12756790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,adav,hector.yee,hector.yee,20/Nov/14 19:49,17/May/20 18:30,14/Jul/23 06:26,26/Nov/14 04:58,1.2.0,,,,,,,1.2.0,,,,,,Shuffle,Spark Core,,,0,netty,shuffle,,,,"The netty block transfer manager has a race condition where it closes an active connection resulting in the error below. Switching to nio seems to alleviate the problem.

{code}
14/11/20 18:53:43 INFO TransportClientFactory: Found inactive connection to i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773, closing it.
14/11/20 18:53:43 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks 
java.io.IOException: Failed to connect to i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773
at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:141)
at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:87)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:148)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:288)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at com.airbnb.common.ml.training.LinearRankerTrainer$$anonfun$7.apply(LinearRankerTrainer.scala:246)
at com.airbnb.common.ml.training.LinearRankerTrainer$$anonfun$7.apply(LinearRankerTrainer.scala:235)
at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:56)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
{code}","Linux, Mesos",andrewor14,apachespark,hector.yee,ilikerps,kotime42@gmail.com,pwendell,yobibytes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 04:58:22 UTC 2014,,,,,,,,,,"0|i22mcv:",9223372036854775807,,,,,rxin,,,,,,,,,1.2.0,,,,,,,,,,,"20/Nov/14 21:18;andrewor14;Hey [~hector.yee] so what is the race condition?;;;","20/Nov/14 21:24;hector.yee;The channel is marked as inactive while it is being used I believe. I didn't dig into the code so I have no idea but according
to the logs that is what seems to be happening.;;;","20/Nov/14 22:11;ilikerps;The code in question, despite the log message, does not actually close the socket, but just forgets about it. It seems at least somewhat likely that the client is in fact inactive because the port on the other side went down.

Note that the second error is from us trying to create a new connection, not from reusing an old connection that was closed. This suggests that the server is in fact in a bad state. If you ever get the chance to reproduce this and look at the server's logs with netty on, that would be much appreciated.;;;","20/Nov/14 22:17;ilikerps;Also ideally there would be some other exception earlier on the client that indicates when/why the connection became inactive, like a ClosedChannelException. If not, we may be silently ignoring an exception that should be logged.;;;","20/Nov/14 22:45;hector.yee;Digging deeper it looks like you are right, the first machine fails silently with no reason in the log.
My guess is that it ran out of memory, when this kind of thing happens. The last time this happened was when the native snappy
library used too much ram. I upped the mesos overhead to 1G to fix those snappy errors: --conf spark.mesos.executor.memoryOverhead=1024 
Is it possible that netty uses something off java heap and is allocating too much?
Or maybe a silent failure somewhere that is not logged?

Diagnostics follow:

1st machine fails (f20aaa19) with nothing in the log. The last thing it says is starting 3 remote fetchers

14/11/20 22:35:18 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
14/11/20 22:35:18 INFO MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@i-62305989.inst.aws.airbnb.com:46605/user/MapOutputTracker#-1862215473]
14/11/20 22:35:18 INFO MapOutputTrackerWorker: Got the output locations
14/11/20 22:35:18 INFO ShuffleBlockFetcherIterator: Getting 498 non-empty blocks out of 724 blocks
14/11/20 22:35:18 INFO ShuffleBlockFetcherIterator: Started 3 remote fetches in 67 ms

On the master it says
14/11/20 22:36:25 ERROR TaskSchedulerImpl: Lost executor 20141023-174642-3852091146-5050-41161-1224 on i-f20aaa19.inst.aws.airbnb.com: remote Akka client disassociated
14/11/20 22:36:25 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@i-f20aaa19.inst.aws.airbnb.com:54417] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
14/11/20 22:36:25 INFO TaskSetManager: Re-queueing tasks for 20141023-174642-3852091146-5050-41161-1224 from TaskSet 0.0

14/11/20 22:36:25 WARN TaskSetManager: Lost task 7.0 in stage 1.0 (TID 898, i-f20aaa19.inst.aws.airbnb.com): ExecutorLostFailure (executor 20141023-174642-3852091146-5050-41161-1224 lost)
14/11/20 22:36:25 ERROR CoarseMesosSchedulerBackend: Asked to remove non-existent executor 20141023-174642-3852091146-5050-41161-1224

2nd machine fails saying it could not connect

14/11/20 22:36:36 INFO TransportClientFactory: Found inactive connection to i-f20aaa19.inst.aws.airbnb.com/10.225.139.181:51003, closing it.
14/11/20 22:36:36 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks 
java.io.IOException: Failed to connect to i-f20aaa19.inst.aws.airbnb.com/10.225.139.181:51003
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:141)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:87);;;","21/Nov/14 01:05;pwendell;[~hector.yee] Yes - the netty transfer service will use off heap buffers whereas the NIO service doesn't. Is their any way you can verify that your executors are getting killed by the container manager (i.e. dmesg). That would likely narrow it down to this issue with 100% certainty.

In terms of how to fix it - [~adav] will know better than I. But one thing is I think you can ask netty not to use off heap buffers by setting ""spark.shuffle.io.preferDirectBufs"" to ""false"".

https://github.com/apache/spark/blob/master/network/common/src/main/java/org/apache/spark/network/util/TransportConf.java

I thought we had other settings to limit the memory usage, but [~ilikerps] will know more.;;;","21/Nov/14 01:12;pwendell;[~hector.yee] I updated the title, let me know if you decide this is not related to being killed by the OS but it seems like that is the case.;;;","21/Nov/14 01:29;hector.yee;I checked the mesos log on the slave and it was an OOM kill

I1120 22:36:20.193279 95373 slave.cpp:3321] Current usage 33.69%. Max allowed age: 1.126220739407303days
I1120 22:36:23.329488 95371 mem.cpp:532] OOM notifier is triggered for container def5caa2-c0f3-4175-9f5c-210735e6e009
I1120 22:36:23.329684 95371 mem.cpp:551] OOM detected for container def5caa2-c0f3-4175-9f5c-210735e6e009
I1120 22:36:23.330762 95371 mem.cpp:605] Memory limit exceeded: Requested: 26328MB Maximum Used: 26328MB

MEMORY STATISTICS: 
cache 126976
rss 27606781952
rss_huge 0
mapped_file 16384
writeback 0
swap 0
pgpgin 14435895
pgpgout 7695927
pgfault 63682623
pgmajfault 824
inactive_anon 0
active_anon 27606781952
inactive_file 126976
active_file 0
unevictable 0
hierarchical_memory_limit 27606908928
hierarchical_memsw_limit 18446744073709551615
total_cache 126976
total_rss 27606781952
total_rss_huge 0
total_mapped_file 16384
total_writeback 0
total_swap 0
total_pgpgin 14435895
total_pgpgout 7695927
total_pgfault 63682623
total_pgmajfault 824
total_inactive_anon 0
total_active_anon 27606781952
total_inactive_file 126976
total_active_file 0
total_unevictable 0
I1120 22:36:23.330862 95371 containerizer.cpp:1133] Container def5caa2-c0f3-4175-9f5c-210735e6e009 has reached its limit for resource mem(*):26328 and will be terminated
I1120 22:36:23.330899 95371 containerizer.cpp:946] Destroying container 'def5caa2-c0f3-4175-9f5c-210735e6e009'
I1120 22:36:23.332049 95367 cgroups.cpp:2207] Freezing cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009
I1120 22:36:23.434741 95371 cgroups.cpp:1374] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009 after 102.648064ms
I1120 22:36:23.436122 95391 cgroups.cpp:2224] Thawing cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009
I1120 22:36:23.437611 95391 cgroups.cpp:1403] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009 after 1.39904ms
I1120 22:36:23.439303 95394 containerizer.cpp:1117] Executor for container 'def5caa2-c0f3-4175-9f5c-210735e6e009' has exited
I1120 22:36:25.953094 95368 slave.cpp:2898] Executor '33' of framework 20141119-235105-1873488138-31272-108823-0041 terminated with signal Killed
I1120 22:36:25.953872 95368 slave.cpp:2215] Handling status update TASK_FAILED (UUID: 986cf483-d400-4edc-8423-dd0e51dfeeb8) for task 33 of framework 20141119-235105-1873488138-31272-108823-0041 from @0.0.0.0:0
I1120 22:36:25.953943 95368 slave.cpp:4305] Terminating task 33
;;;","21/Nov/14 01:43;hector.yee;Also the log was from  tmp  mesos  slaves  20141023-174642-3852091146-5050-41161-1224  frameworks  20141119-235105-1873488138-31272-108823-0041  executors  33  runs  def5caa2-c0f3-4175-9f5c-210735e6e009 so just to confirm the executor and container IDs match up;;;","21/Nov/14 22:36;pwendell;Okay sounds good. Does changing the netty config help?
;;;","22/Nov/14 02:26;hector.yee;Yes turning off direct buffers worked with Netty;;;","22/Nov/14 20:45;pwendell;Okay then I think this is just a documentation issue. We should add the documentation about direct buffers to the main configuration page and also mention it in the doc about network options.;;;","26/Nov/14 02:49;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3465;;;","26/Nov/14 04:13;ilikerps;It turns out there was a real bug which caused us to allocate memory proportional to both number of cores and number of _executors_ in the cluster. PR [#3465|https://github.com/apache/spark/pull/3465] should remove the latter factor, which should greatly decrease the amount of off-heap memory allocated.

Do note that even with this patch, one key feature of the Netty transport service is that we do allocate and reuse significant off-heap buffer space rather than on-heap, which helps reduce GC pauses. So it's possible that certain environments which previously heavily constrained off-heap memory (by giving almost all of the container/cgroup's memory to the Spark heap) may have to be modified to ensure that at least 32 * (number of cores) MB is available to be allocated off-JVM heap.

If this is not possible, you can either disable direct byte buffer usage via ""spark.shuffle.io.preferDirectBufs"" or set ""spark.shuffle.io.serverThreads"" and ""spark.shuffle.io.clientThreads"" to something smaller than the number of executor cores. Typically we find that 10GB/s network cannot saturate more than, say, 8 cores on a machine (in practice I've never seen even that many required), so we would expect no performance degradation if you set these parameters such on beefier machines, and it should cap off-heap allocation to order of 256 MB.;;;","26/Nov/14 04:42;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3469;;;","26/Nov/14 04:58;ilikerps;About my last point, [~rxin], [~pwendell], and I decided it may be better if we just cap the number of threads we use by default to 8, to try to avoid issues for people who use executors with very large number of cores and were on the edge of their off-heap limits already. #3469 implements this, which may cause a performance regression if we're wrong about the magic number 8 being an upper bound on the useful number of cores. It can be overridden via the serverThreads/clientThreads properties, but if anyone sees this as an issue, please let me know.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext localProperties does not inherit property updates across thread reuse,SPARK-4514,12756776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,reggert1980,eje,eje,20/Nov/14 19:27,16/Dec/15 03:49,14/Jul/23 06:26,16/Dec/15 03:49,1.1.0,1.1.1,1.2.0,,,,,2.0.0,,,,,,Spark Core,,,,1,,,,,,"The current job group id of a Spark context is stored in the {{localProperties}} member value.   This data structure is designed to be thread local, and its settings are not preserved when {{ComplexFutureAction}} instantiates a new {{Future}}.  

One consequence of this is that {{takeAsync()}} does not behave in the same way as other async actions, e.g. {{countAsync()}}.  For example, this test (if copied into StatusTrackerSuite.scala), will fail, because {{""my-job-group2""}} is not propagated to the Future which actually instantiates the job:

{code:java}
  test(""getJobIdsForGroup() with takeAsync()"") {
    sc = new SparkContext(""local"", ""test"", new SparkConf(false))
    sc.setJobGroup(""my-job-group2"", ""description"")
    sc.statusTracker.getJobIdsForGroup(""my-job-group2"") should be (Seq.empty)
    val firstJobFuture = sc.parallelize(1 to 1000, 1).takeAsync(1)
    val firstJobId = eventually(timeout(10 seconds)) {
      firstJobFuture.jobIds.head
    }
    eventually(timeout(10 seconds)) {
      sc.statusTracker.getJobIdsForGroup(""my-job-group2"") should be (Seq(firstJobId))
    }
  }
{code}

It also impacts current PR for SPARK-1021, which involves additional uses of {{ComplexFutureAction}}.",,aash,apachespark,eje,glenn.strycker@gmail.com,joshrosen,reggert1980,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1021,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 22 21:24:05 UTC 2015,,,,,,,,,,"0|i22m9r:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,,,,,,,,,,"21/Nov/14 01:24;joshrosen;I plan to investigate / fix this soon, but I'm a bit swamped with other work; [~eje], any chance that you have spare cycles to hack on this?;;;","21/Nov/14 03:05;eje;Yes, I can cook up a PR.   I've been thinking about possible solution policies:

* optional parameter to pass just the group id down into the ComplexFutureAction?
* allow passing entire localProperty context?
* are the above exposed only through the async actions, or accessible to anybody instantiating a ComplexFutureAction?
* should ComplexFutureAction want to inherit local properties by default, or not inherit by default?
;;;","22/Nov/14 04:50;joshrosen;Haven't had a chance to dig into this much, but if possible it would be nice to err in favor of a simpler approach that minimizes further use of thread locals since they're hard to reason about.  I don't recall if ComplexFutureAction itself is intended to be public / directly instantiated by users.

Probably the most intuitive semantics is for all jobs run in the ComplexFutureAction to be run using the job group used when the future action was launched.  This might be hard to support, though, due to all of the thread locals and implicit state.;;;","12/Dec/14 18:44;eje;The root problem appears to be the behavior of {{InheritableThreadLocal}} and {{childValue}} -- in particular, {{childValue}} only gets called once.  When a thread is reused, its value is not updated.  So multiple calls to methods such as {{setJobGroup}} are effectively ignored in the case of threads being reused.   Changing implementations of asynchronous actions to use {{ComplexFutureAction}} causes more threads to be invoked, and reused, and this causes the problem to show up in the unit testing.

In general, {{InheritableThreadLocal}} seems like a difficult structure to use properly.   It only works intuitively if inherited values will not change after inheritance.
http://docs.oracle.com/javase/8/docs/api/java/lang/InheritableThreadLocal.html

Discussion of this topic in forums suggests that passing data down via constructor parameters is the more reliable and intuitive approach in the use cases where inheritance with updates is required.

IMO, this adds weight to solutions involving parameters to {{ComplexFutureAction}}.
;;;","31/Mar/15 06:12;joshrosen;I don't know that there's a good way to fix this for all arbitrary ways in which users might create or re-use threads.  This inheritance behavior is slightly more understandable in cases where users explicitly create child threads.  Although our documentation doesn't seem to explicitly promise that properties will be inherited, I think that users might have come to rely on this behavior so I don't think that we can remove it at this point.  We can certainly fix it for the AsyncRDDActions case, though, because we can manually thread the properties in the constructor.

This pain could have probably been avoided if the original design used something like Scala's {{DynamicVariable}} where you're forced to explicitly consider the scope / lifecycle of the thread-local property.
 
I'm going to try to fix this for the AsyncRDDActions case and will try to improve the documentation to warn about this pitfall for the more general cases involving arbitrary user code.  Let me know if you can spot another solution which won't break existing user code that relies on property inheritance in the non-thread-reuse cases.;;;","31/Mar/15 08:27;joshrosen;I've filed SPARK-6629 to fix a related issue where inherited job groups did not play nicely with cancellation.;;;","24/Apr/15 18:54;ilganeli;[~joshrosen] - given your work on SPARK-6629, is this still relevant? I saw that there was a comment there stating that issue may not be a problem. I can knock this one out if it's still necessary.;;;","22/Nov/15 20:49;reggert1980;The unit test attached to this issue fails in master, but passes in https://github.com/apache/spark/pull/9264 , which is intended to fix SPARK-9026.;;;","22/Nov/15 21:07;reggert1980;This test, however, still fails:

{code}
 test(""getJobIdsForGroup() with takeAsync() across multiple partitions"") {
    sc = new SparkContext(""local"", ""test"", new SparkConf(false))
    sc.setJobGroup(""my-job-group2"", ""description"")
    sc.statusTracker.getJobIdsForGroup(""my-job-group2"") shouldBe empty
    val firstJobFuture = sc.parallelize(1 to 1000, 2).takeAsync(999)
    val firstJobId = eventually(timeout(10 seconds)) {
      firstJobFuture.jobIds.head
    }
    eventually(timeout(10 seconds)) {
      sc.statusTracker.getJobIdsForGroup(""my-job-group2"") should have size 2
    }
  }
{code};;;","22/Nov/15 21:17;reggert1980;I implemented a two-line fix that causes this test to now pass in that PR.;;;","22/Nov/15 21:24;apachespark;User 'reggert' has created a pull request for this issue:
https://github.com/apache/spark/pull/9264;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support relational operator '<=>' in Spark SQL,SPARK-4513,12756769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ravi.pesala,ravi.pesala,20/Nov/14 19:14,20/Nov/14 23:34,14/Jul/23 06:26,20/Nov/14 23:34,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,The relational operator '<=>'  is not working in Spark SQL. Same works in Spark HiveQL,,apachespark,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 23:34:43 UTC 2014,,,,,,,,,,"0|i22m87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/14 19:18;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/3387;;;","20/Nov/14 23:34;marmbrus;Issue resolved by pull request 3387
[https://github.com/apache/spark/pull/3387];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unresolved Attribute Exception for sort by,SPARK-4512,12756706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,20/Nov/14 14:49,30/Dec/14 20:15,14/Jul/23 06:26,30/Dec/14 20:14,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"It will cause exception while do query like:

SELECT key+key FROM src sort by value;",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4487,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 14:59:46 UTC 2014,,,,,,,,,,"0|i22luf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"20/Nov/14 14:59;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3386;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert EC2 tag-based cluster membership patch in branch-1.2,SPARK-4509,12756644,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,joshrosen,joshrosen,20/Nov/14 07:22,10/Mar/15 03:14,14/Jul/23 06:26,26/Nov/14 00:08,1.2.0,,,,,,,1.2.0,,,,,,EC2,,,,0,,,,,,"Since we haven't fixed SPARK-3332 (the issue where tags aren't a safe strategy for determining spark-ec2 cluster membership), let's revert SPARK-2333 in {{branch-1.2}}.

I'm filing a blocker JIRA for this so that we don't forget to do this before cutting a 1.2.0 release candidate.  Unfortunately, the patch doesn't cleanly revert, so this may take a bit of work to do correctly.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3332,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 09:53:25 UTC 2014,,,,,,,,,,"0|i22lgn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Nov/14 09:53;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3453;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PR merge script should support closing multiple JIRA tickets,SPARK-4507,12756641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hase1031,joshrosen,joshrosen,20/Nov/14 07:15,30/Nov/14 04:18,14/Jul/23 06:26,30/Nov/14 04:18,,,,,,,,1.3.0,,,,,,Project Infra,,,,0,starter,,,,,"For pull requests that reference multiple JIRAs in their titles, it would be helpful if the PR merge script offered to close all of them.",,apachespark,hase1031,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 24 11:25:09 UTC 2014,,,,,,,,,,"0|i22lfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/14 11:24;apachespark;User 'hase1031' has created a pull request for this issue:
https://github.com/apache/spark/pull/3428;;;","24/Nov/14 11:25;hase1031;This is my first pull-request for spark, so please let me know if you want something done differently.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation to clarify whether standalone-cluster mode is now officially supported,SPARK-4506,12756638,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,joshrosen,joshrosen,20/Nov/14 06:58,25/Jan/15 23:25,14/Jul/23 06:26,05/Dec/14 21:39,1.1.0,1.1.1,1.2.0,,,,,1.1.1,1.2.0,,,,,Documentation,,,,0,,,,,,"The ""Launching Compiled Spark Applications"" section of the Spark Standalone docs claims that standalone mode only supports {{client}} deploy mode:

{quote}
The spark-submit script provides the most straightforward way to submit a compiled Spark application to the cluster. For standalone clusters, Spark currently only supports deploying the driver inside the client process that is submitting the application (client deploy mode).
{quote}

It looks like {{standalone-cluster}} mode actually works (I've used it and have heard from users that are successfully using it, too).

The current line was added in SPARK-2259 when {{standalone-cluster}} mode wasn't officially supported.  It looks like SPARK-2260 fixed a number of bugs in {{standalone-cluster}} mode, so we should update the documentation if we're now ready to officially support it.",,andrewor14,apachespark,asimjalis,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2754,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 25 23:25:00 UTC 2015,,,,,,,,,,"0|i22lfb:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"20/Nov/14 16:41;andrewor14;Thanks for filing this Josh. It is actually supported and the docs are just out-dated. I'll get to fixing them shortly.;;;","05/Dec/14 21:39;andrewor14;This was actually fixed by https://github.com/apache/spark/pull/2461;;;","21/Jan/15 22:47;asimjalis;This bug should be reopened. The doc needs some more changes.
Doc: https://github.com/apache/spark/blob/master/docs/submitting-applications.md
Current text: Note that cluster mode is currently not supported for standalone clusters, Mesos clusters, or Python applications.
Proposed text: Note that cluster mode is currently not supported for Mesos clusters, or Python applications.;;;","22/Jan/15 10:28;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4160;;;","25/Jan/15 23:25;andrewor14;Thanks for pointing that out. I have merged the patch that fixes it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run-example fails if multiple example jars present in target folder,SPARK-4504,12756623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gvramana,gvramana,gvramana,20/Nov/14 04:44,19/Jan/15 21:46,14/Jul/23 06:26,19/Jan/15 21:46,1.1.0,1.2.0,,,,,,1.1.2,1.2.1,1.3.0,,,,Examples,,,,0,,,,,,"Giving following error:

bin/run-example: line 39: [: /mnt/d/spark/spark/examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar: binary operator expected
Failed to find Spark examples assembly in /mnt/d/spark/spark/lib or /mnt/d/spark/spark/examples/target
You need to build Spark before running this program",,apachespark,gvramana,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 19 21:46:04 UTC 2015,,,,,,,,,,"0|i22lbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/14 04:52;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3069;;;","20/Nov/14 04:54;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/3377;;;","19/Jan/15 21:46;joshrosen;Issue resolved by pull request 3377
[https://github.com/apache/spark/pull/3377];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone Master can fail to recognize completed/failed applications,SPARK-4498,12756532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,markhamstra,airhorns,airhorns,19/Nov/14 20:48,03/Dec/14 23:11,14/Jul/23 06:26,03/Dec/14 23:08,1.1.1,1.2.0,,,,,,1.1.2,1.2.0,,,,,Deploy,Spark Core,,,1,,,,,,"We observe the spark standalone master not detecting that a driver application has completed after the driver process has shut down indefinitely, leaving that driver's resources consumed indefinitely. The master reports applications as Running, but the driver process has long since terminated. The master continually spawns one executor for the application. It boots, times out trying to connect to the driver application, and then dies with the exception below. The master then spawns another executor on a different worker, which does the same thing. The application lives until the master (and workers) are restarted. 

This happens to many jobs at once, all right around the same time, two or three times a day, where they all get suck. Before and after this ""blip"" applications start, get resources, finish, and are marked as finished properly. The ""blip"" is mostly conjecture on my part, I have no hard evidence that it exists other than my identification of the pattern in the Running Applications table. See http://cl.ly/image/2L383s0e2b3t/Screen%20Shot%202014-11-19%20at%203.43.09%20PM.png : the applications started before the blip at 1.9 hours ago still have active drivers. All the applications started 1.9 hours ago do not, and the applications started less than 1.9 hours ago (at the top of the table) do in fact have active drivers.


Deploy mode:
 - PySpark drivers running on one node outside the cluster, scheduled by a cron-like application, not master supervised
 

Other factoids:
 - In most places, we call sc.stop() explicitly before shutting down our driver process
 - Here's the sum total of spark configuration options we don't set to the default:
{code}
    ""spark.cores.max"": 30
    ""spark.eventLog.dir"": ""hdfs://nn.shopify.com:8020/var/spark/event-logs""
    ""spark.eventLog.enabled"": true
    ""spark.executor.memory"": ""7g""
    ""spark.hadoop.fs.defaultFS"": ""hdfs://nn.shopify.com:8020/""
    ""spark.io.compression.codec"": ""lzf""
    ""spark.ui.killEnabled"": true
{code}
 - The exception the executors die with is this:
{code}
14/11/19 19:42:37 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
14/11/19 19:42:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/11/19 19:42:37 INFO SecurityManager: Changing view acls to: spark,azkaban
14/11/19 19:42:37 INFO SecurityManager: Changing modify acls to: spark,azkaban
14/11/19 19:42:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark, azkaban); users with modify permissions: Set(spark, azkaban)
14/11/19 19:42:37 INFO Slf4jLogger: Slf4jLogger started
14/11/19 19:42:37 INFO Remoting: Starting remoting
14/11/19 19:42:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@dn13.chi.shopify.com:37682]
14/11/19 19:42:38 INFO Utils: Successfully started service 'driverPropsFetcher' on port 37682.
14/11/19 19:42:38 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: spark-etl1.chi.shopify.com/172.16.126.88:58849
14/11/19 19:43:08 ERROR UserGroupInformation: PriviledgedActionException as:azkaban (auth:SIMPLE) cause:java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException: Unknown exception in doAs
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1421)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:59)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:115)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:163)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
Caused by: java.security.PrivilegedActionException: java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	... 4 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:127)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:60)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:59)
	... 7 more
{code}


Cluster history:
 - We run spark versions built from apache/spark#master snapshots. We did not observe this behaviour on {{7eb9cbc273d758522e787fcb2ef68ef65911475f}} (sorry its so old), but now observe it on {{c6e0c2ab1c29c184a9302d23ad75e4ccd8060242}}. We can try new versions to assist debugging.
"," - Linux dn11.chi.shopify.com 3.2.0-57-generic #87-Ubuntu SMP 3 x86_64 x86_64 x86_64 GNU/Linux
 - Standalone Spark built from apache/spark#c6e0c2ab1c29c184a9302d23ad75e4ccd8060242
 - Python 2.7.3
java version ""1.7.0_71""
Java(TM) SE Runtime Environment (build 1.7.0_71-b14)
Java HotSpot(TM) 64-Bit Server VM (build 24.71-b01, mixed mode)
 - 1 Spark master, 40 Spark workers with 32 cores a piece and 60-90 GB of memory a piece
 - All client code is PySpark",airhorns,alexangelini,andrewor14,apachespark,dklassen,joshrosen,markhamstra,pwendell,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/14 20:49;airhorns;all-master-logs-around-blip.txt;https://issues.apache.org/jira/secure/attachment/12682484/all-master-logs-around-blip.txt","19/Nov/14 20:49;airhorns;one-applications-master-logs.txt;https://issues.apache.org/jira/secure/attachment/12682483/one-applications-master-logs.txt",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 23:08:48 UTC 2014,,,,,,,,,,"0|i22kun:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.0,,,,,,,,,,"19/Nov/14 20:49;airhorns;These are the logs from the standalone master for one of the applications started when a recent blip happened;;;","19/Nov/14 20:49;airhorns;These are all the master logs around a recent blip;;;","19/Nov/14 20:56;airhorns;For the simple canary spark application (who's master logs are attached), the first executor does exactly what it is supposed to, and then the driver shuts down and it disassociates. Then, a second executor is started, when it never should have been. The first executors stderr:

{code}
14/11/19 18:48:16 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
14/11/19 18:48:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/11/19 18:48:16 INFO SecurityManager: Changing view acls to: spark,azkaban
14/11/19 18:48:16 INFO SecurityManager: Changing modify acls to: spark,azkaban
14/11/19 18:48:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark, azkaban); users with modify permissions: Set(spark, azkaban)
14/11/19 18:48:17 INFO Slf4jLogger: Slf4jLogger started
14/11/19 18:48:17 INFO Remoting: Starting remoting
14/11/19 18:48:17 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@dn42.chi.shopify.com:36365]
14/11/19 18:48:17 INFO Utils: Successfully started service 'driverPropsFetcher' on port 36365.
14/11/19 18:48:18 INFO SecurityManager: Changing view acls to: spark,azkaban
14/11/19 18:48:18 INFO SecurityManager: Changing modify acls to: spark,azkaban
14/11/19 18:48:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark, azkaban); users with modify permissions: Set(spark, azkaban)
14/11/19 18:48:18 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/11/19 18:48:18 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/11/19 18:48:18 INFO Slf4jLogger: Slf4jLogger started
14/11/19 18:48:18 INFO Remoting: Starting remoting
14/11/19 18:48:18 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@dn42.chi.shopify.com:39974]
14/11/19 18:48:18 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
14/11/19 18:48:18 INFO Utils: Successfully started service 'sparkExecutor' on port 39974.
14/11/19 18:48:18 INFO CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849/user/CoarseGrainedScheduler
14/11/19 18:48:18 INFO WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@dn42.chi.shopify.com:41095/user/Worker
14/11/19 18:48:18 INFO WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@dn42.chi.shopify.com:41095/user/Worker
14/11/19 18:48:18 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
14/11/19 18:48:18 INFO SecurityManager: Changing view acls to: spark,azkaban
14/11/19 18:48:18 INFO SecurityManager: Changing modify acls to: spark,azkaban
14/11/19 18:48:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark, azkaban); users with modify permissions: Set(spark, azkaban)
14/11/19 18:48:18 INFO AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849/user/MapOutputTracker
14/11/19 18:48:18 INFO AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849/user/BlockManagerMaster
14/11/19 18:48:18 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20141119184818-e5ae
14/11/19 18:48:18 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
14/11/19 18:48:18 INFO LogReporter: Creating metrics output file: /tmp/spark-metrics
14/11/19 18:48:18 INFO NettyBlockTransferService: Server created on 44406
14/11/19 18:48:18 INFO BlockManagerMaster: Trying to register BlockManager
14/11/19 18:48:18 INFO BlockManagerMaster: Registered BlockManager
14/11/19 18:48:18 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849/user/HeartbeatReceiver
14/11/19 18:48:18 INFO CoarseGrainedExecutorBackend: Got assigned task 0
14/11/19 18:48:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
14/11/19 18:48:18 INFO Executor: Fetching hdfs://nn01.chi.shopify.com:8020/tmp/starscream_pyfiles_cache/packages-68badf293ff9e4d13929073572892d7f3d0a3546.egg with timestamp 1416422896241
14/11/19 18:48:19 INFO TorrentBroadcast: Started reading broadcast variable 1
14/11/19 18:48:19 INFO MemoryStore: ensureFreeSpace(3391) called with curMem=0, maxMem=278302556
14/11/19 18:48:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 265.4 MB)
14/11/19 18:48:19 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
14/11/19 18:48:19 INFO BlockManager: Got told to re-register updating block broadcast_1_piece0
14/11/19 18:48:19 INFO BlockManager: BlockManager re-registering with master
14/11/19 18:48:19 INFO BlockManagerMaster: Trying to register BlockManager
14/11/19 18:48:19 INFO TorrentBroadcast: Reading broadcast variable 1 took 218 ms
14/11/19 18:48:19 INFO BlockManagerMaster: Registered BlockManager
14/11/19 18:48:19 INFO BlockManager: Reporting 1 blocks to the master.
14/11/19 18:48:19 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
14/11/19 18:48:20 INFO MemoryStore: ensureFreeSpace(5288) called with curMem=3391, maxMem=278302556
14/11/19 18:48:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.2 KB, free 265.4 MB)
14/11/19 18:48:20 INFO HadoopRDD: Input split: hdfs://nn01.chi.shopify.com:8020/data/static/canary/part-00000:0+6
14/11/19 18:48:20 INFO TorrentBroadcast: Started reading broadcast variable 0
14/11/19 18:48:20 INFO MemoryStore: ensureFreeSpace(10705) called with curMem=8679, maxMem=278302556
14/11/19 18:48:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.5 KB, free 265.4 MB)
14/11/19 18:48:20 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
14/11/19 18:48:20 INFO TorrentBroadcast: Reading broadcast variable 0 took 17 ms
14/11/19 18:48:20 WARN Configuration: fs.default.name is deprecated. Instead, use fs.defaultFS
14/11/19 18:48:20 INFO MemoryStore: ensureFreeSpace(186596) called with curMem=19384, maxMem=278302556
14/11/19 18:48:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 182.2 KB, free 265.2 MB)
14/11/19 18:48:20 INFO PythonRDD: Times: total = 531, boot = 229, init = 302, finish = 0
14/11/19 18:48:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1847 bytes result sent to driver
14/11/19 18:48:22 ERROR CoarseGrainedExecutorBackend: Driver Disassociated [akka.tcp://sparkExecutor@dn42.chi.shopify.com:39974] -> [akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849] disassociated! Shutting down.
{code}

and the second executor's stderr:

{code}
14/11/19 18:48:24 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
14/11/19 18:48:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/11/19 18:48:24 INFO SecurityManager: Changing view acls to: spark,azkaban
14/11/19 18:48:24 INFO SecurityManager: Changing modify acls to: spark,azkaban
14/11/19 18:48:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark, azkaban); users with modify permissions: Set(spark, azkaban)
14/11/19 18:48:25 INFO Slf4jLogger: Slf4jLogger started
14/11/19 18:48:25 INFO Remoting: Starting remoting
14/11/19 18:48:25 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@dn42.chi.shopify.com:56495]
14/11/19 18:48:25 INFO Utils: Successfully started service 'driverPropsFetcher' on port 56495.
14/11/19 18:48:25 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:58849]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: spark-etl1.chi.shopify.com/172.16.126.88:58849
14/11/19 18:48:55 ERROR UserGroupInformation: PriviledgedActionException as:azkaban (auth:SIMPLE) cause:java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException: Unknown exception in doAs
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1421)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:59)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:115)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:163)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
Caused by: java.security.PrivilegedActionException: java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	... 4 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:127)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:60)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:59)
	... 7 more
{code};;;","29/Nov/14 20:55;joshrosen;Hi [~airhorns],

I finally got a chance to look into this and, based on reading the code, I have a theory about what might be happening.  If you look at the [current Master.scala file|https://github.com/apache/spark/blob/317e114e11669899618c7c06bbc0091b36618f36/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L668], you'll notice that there are only two situations where the standalone Master removes applications:

- The master receives a DisassociatedEvent due to the application actor shutting down and calls {{finishApplication}}.
- An executor exited with a non-zero exit status and the maximum number of executor failures has been succeeded.

Now, imagine that for some reason the standalone Master does not receive a DisassociatedEvent.  When executors eventually start to die, the standalone master will discover this via ExecutorStateChanged.  If it hasn't hit the maximum number of executor failures, [it will attempt to re-schedule the application|https://github.com/apache/spark/blob/317e114e11669899618c7c06bbc0091b36618f36/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L325] and obtain new resources.  If a new executor is granted, this will [cause the ""maximum failed executors count"" to reset to zero|https://github.com/apache/spark/blob/317e114e11669899618c7c06bbc0091b36618f36/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L313], leading to a sort of livelock behavior where executors die because they can't contact the application but keep being launched because executors keep entering the ExecutorState.RUNNING state ([it looks like|https://github.com/apache/spark/blob/317e114e11669899618c7c06bbc0091b36618f36/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala#L148] executors transition to this state when they launch, not once they've registered with the driver).

It looks like the line

{code}
          if (state == ExecutorState.RUNNING) { appInfo.resetRetryCount() }
{code}

was introduced in SPARK-2425.  It looks like this was introduced after the earliest commit that you mentioned, so it seems like this is be a regression in 1.2.0.  I don't think that we should revert SPARK-2425, since that fixes another fairly important bug.  Instead, I'd like to try to figure out how an application could fail without a DisassociatedEvent causing it to be removed.

Could this be due to our use of non-standard Akka timeout / failure detector settings?  I would think that we'd still get a DisassociatedEvent when a network connection was closed or something.  Maybe we could switch to relying on our own explicit heartbeats for failure detection, like we do elsewhere in Spark.

[~markhamstra], do you have any ideas here?;;;","29/Nov/14 21:03;joshrosen;Adding 1.1.1 as an affected version, too, since SPARK-2425 was backported to that release, too.;;;","29/Nov/14 21:33;markhamstra;On a quick look-through, your analysis looks likely to be correct, [~joshrosen].

Making sure that failed applications are always accompanied by a DisassociatedEvent would be a good thing.  The belt-and-suspenders fix would be to also change the executor state-change semantics so that either RUNNING means not just that the executor process is running, but also that it has successfully connected to the application, or else introduce an additional executor state (perhaps REGISTERED) along with state transitions and finer-grained state logic controlling executor restart and application removal.;;;","29/Nov/14 22:13;joshrosen;In addition to exploring the ""missing DisassociatedEvent"" theory, it might also be worthwhile to brainstorm whether problems at other steps in the cleanup process could cause an application to fail to be removed.  I'm not sure that a single missing DisassociatedEvent could explain the ""blip"" behavior observed here, where an entire group of applications fail to be marked as completed / failed.

In the DisassociatedEvent handler, we index into {{addressToApp}} to determine which app corresponded to the DisassociatedEvent:

{code}
    case DisassociatedEvent(_, address, _) => {
      // The disconnected client could've been either a worker or an app; remove whichever it was
      logInfo(s""$address got disassociated, removing it."")
      addressToWorker.get(address).foreach(removeWorker)
      addressToApp.get(address).foreach(finishApplication)
      if (state == RecoveryState.RECOVERING && canCompleteRecovery) { completeRecovery() }
    }
{code}

If the {{addressToApp}} entry was empty / wrong, then we wouldn't properly clean up the app.  However, I don't think that there should be any problems here because each application actor system should have its own distinct address and Akka's {{Address}} class properly implements hashCode / equals.  Even if drivers run on the same host, their actor systems should have different port numbers.

Continuing along:

{code}
  def removeApplication(app: ApplicationInfo, state: ApplicationState.Value) {
    if (apps.contains(app)) {
      logInfo(""Removing app "" + app.id)
{code}

Is there any way that the {{apps}} HashSet could fail to contain {{app}}?  I don't think so: {{ApplicationInfo}} doesn't override equals/hashCode, but I don't think that's a problem since we only create one ApplicationInfo per app, so the default object identity comparison should be fine.  We should probably log an error if we call {{removeApplication}} on an application that has already been removed, though.  (Also, why do we need the {{apps}} HashSet when we could just use {{idToApp.values}}?);;;","29/Nov/14 22:50;joshrosen;Here's an interesting pattern to grep for in all-master-logs-around-blip.txt: {{sparkDriver@spark-etl1.chi.shopify.com:52047}}.  Note that this log is in reverse-chronological order.

The earliest occurrence is in a DisassociatedEvent log message:

{code}
14-11-19_18:48:31.34508 14/11/19 18:48:31 ERROR EndpointWriter: AssociationError [akka.tcp://sparkMaster@dn05.chi.shopify.com:7077] <- [akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:52047]: Error [Shut down address: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:52047] [
2014-11-19_18:48:31.34510 akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:52047
2014-11-19_18:48:31.34511 Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
2014-11-19_18:48:31.34512 ]
2014-11-19_18:48:31.34521 14/11/19 18:48:31 INFO LocalActorRef: Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%40172.16.126.88%3A48040-1355#-59270061] was not delivered. [2859] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
2014-11-19_18:48:31.34603 14/11/19 18:48:31 INFO Master: akka.tcp://sparkDriver@spark-etl1.chi.shopify.com:52047 got disassociated, removing it.
2014-11-19_18:48:31.20255 14/11/19 18:48:31 INFO Master: Removing executor app-20141119184815-1316/7 because it is EXITED
{code}

Even though INFO-level logging is enabled, there's no ""INFO: master: Removing app ...."" message near this event.  The entire log contains many repetitions of this same DisassociatedEvent log.

The same log also contains many executors that launch and immediately fail:

{code}
2014-11-19_18:52:51.84000 14/11/19 18:52:51 INFO Master: Launching executor app-20141119184815-1313/75 on worker worker-20141118172622-dn19.chi.shopify.com-38498
2014-11-19_18:52:51.83981 14/11/19 18:52:51 INFO Master: Removing executor app-20141119184815-1313/67 because it is EXITED
{code}

I couldn't find a {{removing app app-20141119184815-1313}} event.

Another interesting thing: even though it looks like this log contains information for 39 drivers, there are 100 disassociated events:

{code}
[joshrosen ~]$ cat /Users/joshrosen/Desktop/all-master-logs-around-blip.txt | grep -e ""\d\d\d\d\d got disassociated"" -o | cut -d ' ' -f 1 | sort | uniq | wc -l
      39

[joshrosen ~]$ cat /Users/joshrosen/Desktop/all-master-logs-around-blip.txt | grep -e ""\d\d\d\d\d got disassociated"" -o | cut -d ' ' -f 1 | sort | wc -l
     100
{code};;;","02/Dec/14 01:31;joshrosen;[~andrewor14] and I just had a long discussion about this.  To recap things, I think that there are two distinct bugs:

1. Driver disconnection is not properly detected by the Master.
2. The application failure detection logic SPARK-2425 is broken.

The second bug can mask the first one.  If something is wrong with the logic for detecting a disconnected application / driver, then the application still might eventually be removed if the ""fail an application when its executors fail"" logic worked correctly, since every executor assigned to the exited driver will fail.

To simulate bugs in the driver disconnected logic, we can simply comment out the relevant line in the Master's {{DisassociatedEvent}} handler.  After doing this, I was able to reproduce behavior similar to the issue reported in this ticket.  With a local standalone cluster with one master, one worker, and two executor slots, I was able to start {{spark-shell}}, kill it with {{kill -9}}, then browse to the Master web UI and see that executors were continually launching and failing even though the driver had exited (this continued for 30+ minutes; it's still happening).

I think that the logic for SPARK-2425 is completely broken in the sense that there are some cluster configurations for which it will _never_ kill an application even though every executor that it launches fails, and that there are very few scenarios where it will ever fail an application.  To help explain this, I've created a commit that factors out the failure detection logic into its own class: https://github.com/JoshRosen/spark/commit/87d7960d660b218a9a965fd7d344e2aae0250128.  That commit also includes unit tests of the failure detection logic in isolation, which helps to illustrate the current bugs.

If you look at the current (broken) failure detection logic, two conditions must be met for an application to be marked as failed:

1. No executor can be running: {{!execs.exists(_.state == ExecutorState.RUNNING)}}
2. More than {{MAX_NUM_RETRY}} consecutive executor failure events must have been received: {{!appInfo.incrementRetryCount() < ApplicationState.MAX_NUM_RETRY}}

With the current logic, though, these conditions can almost never be met.  The current (hardcoded) value of {{MAX_NUM_RETRY}} is 10.  Imagine that I have a cluster that only has room for 2 concurrent executors and imagine that all executors fail immediately after launching.  In order for the retry count to be incremented past the threshold, we must have at least 10 executor failures.  In order for this to happen, though, executor launches must occur in between some of those failures, since we can only have 2 executors running at the same time.  When an executor fails and a new one is relaunched, the new executor enters the RUNNING state and sends a message back to the Master, which causes the master to reset the retry count back to 0.  Therefore, it's impossible for the failure detector to report failure in a cluster with fewer than 10 executor slots, even if every executor crashes.

Even for larger clusters, it's still nearly impossible for the current logic to declare failure.  Because of the ""no executor can be running"" condition, all executors must be dead in order for it to declare failure.  The Master immediately calls {{schedule()}} after an executor failure, though, so in most cases a new executor will launch before we can see 10 consecutive failures (the test suite in my commit includes a contrived execution where it does declare failure, though).

So, what do we do, considering that we need to fix this before 1.2.0 and as a bugfix to be included in 1.1.2?  I don't think that we should just revert SPARK-2425, since it's not okay to introduce one regression to fix another.  A correct application failure detector needs to strike this balance between protecting long-running applications from failures and quickly detecting buggy applications that will never work.  Long running applications imply that we can't have counters / state that monotonically progresses towards failure, since we can assume that over an infinite execution there will be an infinite number of executor failures.  Intuitively, I think we want something that's window-based: if a large fraction of ""recently"" launched executors have failed, then declare that the application has failed.

Unfortunately, the master does not receive any signals about non-faulty executors (until they exit cleanly).  One solution would be to add a driver -> master RPC that acknowledges that launched executors are in a good state (e.g. after a task has been sent to that executor).  This would allow us to properly implement a ""fail the application if the last _n_ launched executors failed"" condition.  However, this is still prone to false-positives if a single large, buggy host crashes every executor launched on it.  Therefore, we might want to also incorporate some notion of worker-blacklisting in to the master.  This would still have to be application-specific, since executors could exit uncleanly due to application bugs, so we don't want one buggy application to impact other applications' blacklists.

This proposal is getting fairly complex, though, so I'd like to see if we can come up with a narrower fix to replace the current logic.

Separately, I think that we should implement an explicit Driver -> Master heartbeat as an extra layer of defense against driver disconnection detection errors, as well as adding lots of additional debug logging in those code paths.;;;","02/Dec/14 03:03;pwendell;Hey Josh,

The proposal you gave here seems to be complex. Why not just revert SPARK-2425? It's only a ""regression"" from something that was released in the last few days and which we may need to revert anyways for a quick Spark 1.1.2 release due to its severity. It seems like it might be worth it to just revert SPARK-2425 and put out a quick 1.1.2 release without the feature, then revert it in the 1.2 branch also.;;;","02/Dec/14 03:15;markhamstra;I'd argue against reverting 2425 on the grounds that a long-running application being killed when it is still able to make progress is a worse bug than Executors repeatedly trying to run an application that no longer exists.

Either way, it seems to me that the existing logic may not be too far from being right.  The flaw simply seems to be that an Executor process starting and successfully connecting to stderr and stdout are necessary but not sufficient conditions for that Executor to be transitioned to RUNNING.  If the Executor doesn't become RUNNING until it succeeds in connecting to its Application, then I think the problem is almost entirely and perhaps completely solved.  (Although I think SPARK-2424 should also be implemented.) ;;;","02/Dec/14 03:44;andrewor14;Ok, [~joshrosen] and I talked about this more offline.

In summary, there are two problems:
(1) If enough executors fail over time, the application dies unconditionally. This is fixed by SPARK-2425.
(2) If the application exited and the Master doesn't know that, then the Master might keep on launching executors for that application indefinitely. This is caused by SPARK-2425.

The goal of this issue is to fix both. This means we can't just revert SPARK-2425 because that does not address (1) (and this patch was released in 1.1.1 so it will be a regression if we reverted it). We came up with the following solution that addresses both problems:

Driver periodically sends a heartbeat to Master to indicate whether it is healthy and ready to schedule tasks (i.e. whether at least one executor has registered). Meanwhile, as in the existing code, the Master keeps a counter of how many executors have failed. This counter is reset whenever the Master receives a ""healthy"" message from the driver. When the counter reaches a configurable threshold, meaning the driver has not reported ""healthy"" in a while, then the Master fails the application. Additionally, when the Master hasn't received a heartbeat from the driver after some time, it fails the application.

In a nutshell, this prevents the following from happening:
- BEFORE SPARK-2425, we never reset the counter
- AFTER SPARK-2425, we always reset the counter
- Instead, we reset it periodically whenever we get a heartbeat from the driver
- The heartbeat also provides a mechanism for the Master to detect whether an application has exited in addition to relying on the akka DisassociatedEvent;;;","02/Dec/14 03:45;andrewor14;Yes [~markhamstra] SPARK-2424 will actually be included in this proposal.;;;","02/Dec/14 04:04;pwendell;Going back to the original comment by Josh. Just wondering - what is the basis you have for assuming that driver disassociation is not working in master? Is there a known reason for this or is it just speculation based on observed logs.;;;","02/Dec/14 04:14;joshrosen;{quote}
What is the basis you have for assuming that driver disassociation is not working in master? Is there a known reason for this or is it just speculation based on observed logs.
{quote}

I suppose it's just speculation, but I think it's a good idea to include a defensive / preemptive fix for this.  We don't know for sure whether driver disassociation was broken / buggy in earlier releases, since the application would still eventually get killed when running with the old failure detection code; it's possible that the executor failure handling bug might have uncovered a long-standing bug that was masked by correct failure handling code.

Based on chatting with Andrew, I think that we can ship a pretty small patch that adds a heartbeat mechanism and solves both issues.  I'm working on a prototype now and hope to have a really rough version out soon for an initial round of reviews.;;;","02/Dec/14 05:00;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3548;;;","02/Dec/14 05:02;joshrosen;I've submitted a really rough-draft WIP PR for the heartbeat mechanism that we've discussed.  I'll be back later tonight to address any comments and finish writing tests.  If you have any spare review time, please take a look.;;;","02/Dec/14 07:42;apachespark;User 'markhamstra' has created a pull request for this issue:
https://github.com/apache/spark/pull/3550;;;","03/Dec/14 23:08;joshrosen;Issue resolved by pull request 3550
[https://github.com/apache/spark/pull/3550];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in JobProgressListener due to `spark.ui.retainedJobs` not being used,SPARK-4495,12756458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,19/Nov/14 16:54,20/Nov/14 00:51,14/Jul/23 06:26,20/Nov/14 00:51,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In the JobProgressListener, we never actually remove jobs from the jobIdToData map. In fact, there's a somewhat embarrassing typo: _retailed_ jobs:

{code}
  val retailedJobs = conf.getInt(""spark.ui.retainedJobs"", DEFAULT_RETAINED_JOBS)
{code}

This was introduced by my patch for [SPARK-2321]: I should have added a test that set {{spark.ui.retainedJobs}} to some low threshold and checked that we only reported information for that many jobs; there was already a corresponding ""test LRU eviction of stages"" test that I should have just copied and modified.

This only affects the unreleased 1.2.0 version.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 00:51:04 UTC 2014,,,,,,,,,,"0|i22kef:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Nov/14 22:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3372;;;","20/Nov/14 00:51;joshrosen;Issue resolved by pull request 3372
[https://github.com/apache/spark/pull/3372];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't pushdown Eq, NotEq, Lt, LtEq, Gt and GtEq predicates with nulls for Parquet",SPARK-4493,12756434,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,19/Nov/14 16:00,20/Dec/14 03:11,14/Jul/23 06:26,17/Dec/14 20:48,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Predicates like {{a = NULL}} and {{a < NULL}} can't be pushed down since Parquet {{Lt}}, {{LtEq}}, {{Gt}}, {{GtEq}} doesn't accept null value. Not that {{Eq}} and {{NotEq}} can only be used with {{null}} to represent predicates like {{a IS NULL}} and {{a IS NOT NULL}}.

However, normally this issue doesn't cause NPE because any value compared to {{NULL}} results {{NULL}}, and Spark SQL automatically optimizes out {{NULL}} predicate in the {{SimplifyFilters}} rule. Only testing code that intentionally disables the optimizer may trigger this issue. (That's why this issue is not marked as blocker).",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 20 03:11:50 UTC 2014,,,,,,,,,,"0|i22k93:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Nov/14 16:17;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3367;;;","17/Dec/14 20:48;marmbrus;Issue resolved by pull request 3367
[https://github.com/apache/spark/pull/3367];;;","20/Dec/14 03:11;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3748;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix attribute reference resolution error when using ORDER BY.,SPARK-4487,12756356,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,sarutak,sarutak,19/Nov/14 10:07,24/Nov/14 20:55,14/Jul/23 06:26,24/Nov/14 20:55,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"When we use ORDER BY clause, at first, attributes referenced by projection are resolved (1).
And then, attributes referenced at ORDER BY clause are resolved (2).
 But when resolving attributes referenced at ORDER BY clause, the resolution result generated in (1) is discarded so for example, following query fails.

{code}
SELECT c1 + c2 FROM mytable ORDER BY c1;
{code}
The query above fails because when resolving the attribute reference 'c1', the resolution result of 'c2' is discarded.",,apachespark,marmbrus,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4512,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 24 20:55:29 UTC 2014,,,,,,,,,,"0|i22jrz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Nov/14 10:44;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3363;;;","24/Nov/14 20:55;marmbrus;Issue resolved by pull request 3363
[https://github.com/apache/spark/pull/3363];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[CORE] Treat maxResultSize as unlimited when set to 0; improve error message",SPARK-4484,12756335,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,nravi,nravi,nravi,19/Nov/14 08:00,20/Nov/14 01:24,14/Jul/23 06:26,20/Nov/14 01:24,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"The check for maxResultSize > 0 is missing, results in failures when set to 0. Also, error message needs to be improved so the developers know that there is a new parameter to be configured",,apachespark,nravi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 09:41:07 UTC 2014,,,,,,,,,,"0|i22jnb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Nov/14 08:00;nravi;PR: https://github.com/apache/spark/pull/3360/;;;","19/Nov/14 08:02;apachespark;User 'nishkamravi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/3360;;;","19/Nov/14 08:05;srowen;This is causing a number of benchmarks to fail, no? it sounded more serious than maybe it comes across here, since there's no way to say ""no limit"".;;;","19/Nov/14 09:38;nravi;Yeah, most workloads break with a large enough input dataset.;;;","19/Nov/14 09:41;srowen;I'll be so bold as to raise the priority as I see this actually stopped a significant number of jobs from running in some internal tests. It's easy to address too. (You might put [CORE] in the PR title to make sure it shows up on the radar.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReceivedBlockTracker's write ahead log is enabled by default,SPARK-4482,12756297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,19/Nov/14 03:20,19/Nov/14 21:16,14/Jul/23 06:26,19/Nov/14 21:16,,,,,,,,1.2.0,,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 03:23:25 UTC 2014,,,,,,,,,,"0|i22jev:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Nov/14 03:23;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3358;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid many small spills in external data structures,SPARK-4480,12756282,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor14,19/Nov/14 01:52,20/Nov/14 02:08,14/Jul/23 06:26,20/Nov/14 02:07,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"The following output is provided by [~shenhong] in SPARK-4380.

{code}
14/11/13 19:20:43 INFO collection.ExternalSorter: Thread 60 spilling in-memory batch of 4792 B to disk (292769 spills so far)
14/11/13 19:20:43 INFO collection.ExternalSorter: Thread 60 spilling in-memory batch of 4760 B to disk (292770 spills so far)
14/11/13 19:20:43 INFO collection.ExternalSorter: Thread 60 spilling in-memory batch of 4520 B to disk (292771 spills so far)
14/11/13 19:20:43 INFO collection.ExternalSorter: Thread 60 spilling in-memory batch of 4560 B to disk (292772 spills so far)
14/11/13 19:20:43 INFO collection.ExternalSorter: Thread 60 spilling in-memory batch of 4792 B to disk (292773 spills so far)
14/11/13 19:20:43 INFO collection.ExternalSorter: Thread 60 spilling in-memory batch of 4784 B to disk (292774 spills so far)
{code}

Spilling many small files has two implications. First, it can cause ""too many open files"" exceptions, as we observed in SPARK-3633. Second, it causes degradation in performance. We have seen slight performance regressions from 1.0.2 to 1.1.0, and this is likely the cause.

Note that this is spun-off from SPARK-4452, the fixing of which involves a bigger change in the way we track shuffle memory. This issue is smaller in scope in that it only makes sure we don't constantly spill, regardless of the policy we use for tracking shuffle memory.",,andrewor14,apachespark,hammer,rafal.kwasny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 02:37:03 UTC 2014,,,,,,,,,,"0|i22jbj:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"19/Nov/14 02:08;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3353;;;","19/Nov/14 02:37;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3354;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid unnecessary defensive copies when Sort based shuffle is on,SPARK-4479,12756280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,19/Nov/14 01:39,24/Nov/14 20:44,14/Jul/23 06:26,24/Nov/14 20:44,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 24 20:44:25 UTC 2014,,,,,,,,,,"0|i22jb3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Nov/14 23:34;pwendell;I did some research into this. In the long term what we probably want is an ability for a shuffle dependency to include information about whether objects from the source RDD must be copied if they are buffered. This would require having some Copyable interface (or maybe doing something that is not type safe and assuming if someone selects this there is a copy() method).

In the short term the best way to do this is to have the exchange operator anticipate whether the rows will be buffered. The code where the sort based shuffle will bypass buffering is here:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala#L134

The Exchange operator can be modified to only copy if the number of partitions is > 200 or there is an ordering (during a sort). We need to comment clearly in the Exchange operator and the External sorter that this logical dependency exists.

This at least means in the out-of-the-box case there will be no regression.;;;","23/Nov/14 18:07;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3422;;;","24/Nov/14 20:44;marmbrus;Issue resolved by pull request 3422
[https://github.com/apache/spark/pull/3422];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
totalRegisteredExecutors not updated properly,SPARK-4478,12756274,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aaranya,aaranya,aaranya,19/Nov/14 01:00,20/Nov/14 01:22,14/Jul/23 06:26,20/Nov/14 01:22,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"I'm trying to write a new scheduler backend that relies on org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.  Specifically, I want to use the field totalRegisteredExecutors, but this field is only incremented; not reduced when an executor is lost.",,aaranya,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 22:52:26 UTC 2014,,,,,,,,,,"0|i22j9r:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Nov/14 17:35;apachespark;User 'coolfrood' has created a pull request for this issue:
https://github.com/apache/spark/pull/3368;;;","19/Nov/14 22:52;apachespark;User 'coolfrood' has created a pull request for this issue:
https://github.com/apache/spark/pull/3373;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 blockManagerIdFromJson function throws exception while BlockManagerId be null in MetadataFetchFailedException,SPARK-4471,12756060,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,SuYan,SuYan,SuYan,18/Nov/14 09:21,25/Nov/14 23:51,14/Jul/23 06:26,25/Nov/14 23:51,1.1.0,1.1.1,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"1. when throws MetadataFetchFailedException, it will create a FetchFailedException, which BlockManagerId is null.
  FetchFailedException(null, shuffleId, -1, reduceId) 

2. JsonProtocol.TaskEndReasonToJson, mark BlockManagerId as JNothing

3. while use JsonProtocol.TaskEndReasonFromJson, it didn't take  JNothing into account, so BlockManagerIdFromJson will throw exception

error log belows:

org.json4s.package$MappingException: Did not find value which can be converted into java.lang.String
        at org.json4s.reflect.package$.fail(package.scala:96)
        at org.json4s.Extraction$.convert(Extraction.scala:554)
        at org.json4s.Extraction$.extract(Extraction.scala:331)
        at org.json4s.Extraction$.extract(Extraction.scala:42)
        at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21)
        at org.apache.spark.util.JsonProtocol$.blockManagerIdFromJson(JsonProtocol.scala:662)
        at org.apache.spark.util.JsonProtocol$.taskEndReasonFromJson(JsonProtocol.scala:643)
        at org.apache.spark.util.JsonProtocol$.taskEndFromJson(JsonProtocol.scala:450)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:410)
        at org.apache.spark.scheduler.ReplayListenerBus$$anonfun$replay$2$$anonfun$apply$1.apply(ReplayListenerBus.scala:71)
        at org.apache.spark.scheduler.ReplayListenerBus$$anonfun$replay$2$$anonfun$apply$1.apply(ReplayListenerBus.scala:69)
",,apachespark,SuYan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 09:25:22 UTC 2014,,,,,,,,,,"0|i22hzr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"18/Nov/14 09:25;apachespark;User 'suyanNone' has created a pull request for this issue:
https://github.com/apache/spark/pull/3340;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Parquet filters are created for all inequality predicates with literals on the left hand side,SPARK-4468,12756002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,lian cheng,lian cheng,18/Nov/14 02:51,26/Nov/14 02:59,14/Jul/23 06:26,19/Nov/14 01:40,1.0.0,1.1.0,1.1.1,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,"For expressions like {{10 < someVar}}, we should create a {{Operators.Gt}} filter, but right now a {{Operators.Lt}} is created. This issue affects all inequality predicates with literals on the left hand side.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 01:40:53 UTC 2014,,,,,,,,,,"0|i22hn3:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"18/Nov/14 02:59;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3334;;;","18/Nov/14 06:54;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3338;;;","19/Nov/14 01:40;marmbrus;Issue resolved by pull request 3338
[https://github.com/apache/spark/pull/3338];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of elements read is never reset in ExternalSorter,SPARK-4467,12755995,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tianshuo,andrewor14,andrewor14,18/Nov/14 02:06,24/Nov/14 20:50,14/Jul/23 06:26,19/Nov/14 18:03,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Shuffle,Spark Core,,,0,,,,,,"This is originally reported by [~tianshuo]. This is a spin-off of the original JIRA SPARK-4452, which described a bigger problem.",,andrewor14,apachespark,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4515,,,,SPARK-4452,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 18:38:48 UTC 2014,,,,,,,,,,"0|i22hlj:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"18/Nov/14 02:09;andrewor14;For master and branch-1.2: https://github.com/apache/spark/pull/3302
For branch-1.1: https://github.com/apache/spark/pull/3330;;;","18/Nov/14 02:09;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3330;;;","18/Nov/14 18:38;apachespark;User 'tsdeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3302;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
runAsSparkUser doesn't affect TaskRunner in Mesos environment at all.,SPARK-4465,12755989,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jongyoul,jongyoul,jongyoul,18/Nov/14 01:29,05/Jan/15 20:06,14/Jul/23 06:26,05/Jan/15 20:05,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Deploy,Input/Output,Mesos,,0,,,,,,"runAsSparkUser enable classes using hadoop library to change an active user to spark User, however in Mesos environment, because the function calls before running within JNI, runAsSparkUser doesn't affect anything, and meaningless code. fix the Appropriate scope of function and remove meaningless code. That's a bug because of running program incorrectly. That's related to SPARK-3223 but setting framework user is not perfect solution in my tests.",,apachespark,jongyoul,joshrosen,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 05 20:05:59 UTC 2015,,,,,,,,,,"0|i22hk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/14 06:44;jongyoul;I've dug into mesos code and jni and java.security which is used by UserGroupInformation in runAsSparkUser. Finally I've found an exact wrong point of MesosExecutorBackend, these are about java.security scope and how mesos c++ executor works. Unlike to standalone and yarn, in a task under mesos cluster, jni runs each task in a native method via callback. There are several problems about running java code under jni environment, especially it's not covered by java.security. JDK spec warn the following
{quote}
Some important points about being privileged: Firstly, this concept only exists within a single thread. As soon as the privileged code completes, the privilege is guaranteed to be erased or revoked.
-- http://docs.oracle.com/javase/7/docs/technotes/guides/security/spec/security-spec.doc4.html#20573
{quote}

{code:title=MesosExecutorBackend.scala}
    SparkHadoopUtil.get.runAsSparkUser { () =>
        MesosNativeLibrary.load()
        // Create a new Executor and start it running
        val runner = new MesosExecutorBackend()
        new MesosExecutorDriver(runner).run()
    }
{code}

Show that code. Only run() method is only covered by runAsSparkUser, however, the actual code running tasks is not covered by runAsSparkUser. As a result, MesosExecutorBackend.launchTask doesn't affect by runAsSparkUser.

I'll fix that bug by adding runAsSparkUser into MesosExecutorBackend.launchTask rather and adding them into Executor.launchTask, because Executor.launchTask is already covered in case of standalone and yarn. Thus I judged that code is exact point of fix that bug.;;;","19/Dec/14 07:09;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/3741;;;","05/Jan/15 20:05;joshrosen;Issue resolved by pull request 3741
[https://github.com/apache/spark/pull/3741];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Description about configuration options need to be modified in docs.,SPARK-4464,12755978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tsudukim,tsudukim,tsudukim,18/Nov/14 00:22,05/Dec/14 03:33,14/Jul/23 06:26,05/Dec/14 03:33,1.1.0,,,,,,,1.2.1,,,,,,Documentation,,,,0,,,,,,"There are 2 points need to be modified in Spark Standalone Mode page of docs.

1.
Configuration options {{--ip}}, {{-i}} are now deprecated,
and {{--host}} and {{-h}} are recommended instead.
But in Spark Standalone Mode page of docs, there are only former options.

2.
Option {{--properties-file}} is added by [SPARK-2098].
We need to add a description about this option in the docs.",,apachespark,joshrosen,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 03:33:45 UTC 2014,,,,,,,,,,"0|i22hhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/14 00:33;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/3329;;;","05/Dec/14 03:33;joshrosen;Issue resolved by pull request 3329
[https://github.com/apache/spark/pull/3329];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass java options to yarn master to handle system properties correctly.,SPARK-4461,12755935,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhazhan,zzhan,zzhan,17/Nov/14 22:28,18/Dec/14 16:02,14/Jul/23 06:26,18/Dec/14 16:02,,,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"Currently spark read mapred-site.xml to get the class path. From hadoop 2.6, the library is shipped to cluster with distributed cache at run-time, and may not be available at every node manager. 

Instead of relying on mapred-site.xml, spark should handle this by its own, for example, through ADD_JARs, SPARK_CLASSPATH, etc",,apachespark,erwaman,h_o,sandyr,vanzin,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4181,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 22:21:35 UTC 2014,,,,,,,,,,"0|i22h8f:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"21/Nov/14 00:50;vanzin;The fix for SPARK-2669 should handle this issue to (although the current PR seems to have been abandoned).;;;","21/Nov/14 21:51;zzhan;I changed the title so that to reflect the issue fixed in the PR.;;;","21/Nov/14 22:03;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3409;;;","21/Nov/14 22:21;zzhan;Thanks for the information Marcelo. I changed the title to reflect the change. It handles a different issue. But the PR you referred should also be fixed.

Currently, there is no way to pass yarn am specific java options. It cause some potential issues when reading classpath from hadoop configuration file. Hadoop configuration actually replace variables in its property with the system property passed in java options. How to specify the value depends on different hadoop distribution.

The new options are SPARK_YARN_JAVA_OPTS or spark.yarn.extraJavaOptions. I make it as spark global level, because typically we don't want user to specify this in their command line each time submitting spark job after it is setup in spark-defaults.conf.

In addition, with this new extra options enabled to be passed to AM, it provides more flexibility. How to specify the value

For example int the following valid mapred-site.xml file, we have the class path which specify values using system property. Hadoop can correctly handle it because it has java options passed in.


mapreduce.application.classpath
/etc/hadoop/${hadoop.version}/mapreduce/*
In the meantime, we cannot relies on mapreduce.admin.map.child.java.opts in mapred-site.xml, because it has its own extra java options specified, which does not apply to Spark.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JavaRDDLike.groupBy[K](f: JFunction[T, K]) may fail with typechecking errors",SPARK-4459,12755921,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,alokito,alokito,alokito,17/Nov/14 21:19,04/Dec/14 23:02,14/Jul/23 06:26,04/Dec/14 22:59,1.0.2,1.1.0,,,,,,1.1.2,1.2.1,,,,,Java API,,,,0,,,,,,"I believe this issue is essentially the same as SPARK-668.
Original error: 
{code}
[ERROR] /Users/saldaal1/workspace/JavaSparkSimpleApp/src/main/java/SimpleApp.java:[29,105] no suitable method found for groupBy(org.apache.spark.api.java.function.Function<scala.Tuple2<java.lang.String,java.lang.Long>,java.lang.Long>)
[ERROR] method org.apache.spark.api.java.JavaPairRDD.<K>groupBy(org.apache.spark.api.java.function.Function<scala.Tuple2<K,java.lang.Long>,K>) is not applicable
[ERROR] (inferred type does not conform to equality constraint(s)
{code}
from core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala 
{code}
211  /**
212    * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements
213    * mapping to that key.
214    */
215   def groupBy[K](f: JFunction[T, K]): JavaPairRDD[K, JIterable[T]] = {
216     implicit val ctagK: ClassTag[K] = fakeClassTag
217     implicit val ctagV: ClassTag[JList[T]] = fakeClassTag
218     JavaPairRDD.fromRDD(groupByResultToJava(rdd.groupBy(f)(fakeClassTag)))
219   }
{code}
Then in core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:
{code}
  45 class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
  46                        (implicit val kClassTag: ClassTag[K], implicit val vClassTag: ClassTag[V])
  47   extends JavaRDDLike[(K, V), JavaPairRDD[K, V]] {
{code}
The problem is that the type parameter T in JavaRDDLike is Tuple2[K,V], which means the combined signature for groupBy in the JavaPairRDD is 

{code}
groupBy[K](f: JFunction[Tuple2[K,V], K])
{code}
which imposes an unfortunate correlation between the Tuple2 and the return type of the grouping function, namely that the return type of the grouping function must be the same as the first type of the JavaPairRDD.


If we compare the method signature to flatMap:

{code}
105   /**
106    *  Return a new RDD by first applying a function to all elements of this
107    *  RDD, and then flattening the results.
108    */
109   def flatMap[U](f: FlatMapFunction[T, U]): JavaRDD[U] = {
110     import scala.collection.JavaConverters._
111     def fn = (x: T) => f.call(x).asScala
112     JavaRDD.fromRDD(rdd.flatMap(fn)(fakeClassTag[U]))(fakeClassTag[U])
113   }
{code}

we see there should be an easy fix by changing the type parameter of the groupBy function from K to U.",,alokito,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 04 22:59:31 UTC 2014,,,,,,,,,,"0|i22h5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/14 21:20;alokito;I created a standalone gist to demonstrate the problem, please see https://gist.github.com/alokito/40878fc25af21984463f;;;","17/Nov/14 22:24;apachespark;User 'alokito' has created a pull request for this issue:
https://github.com/apache/spark/pull/3327;;;","04/Dec/14 22:59;joshrosen;Issue resolved by pull request 3327
[https://github.com/apache/spark/pull/3327];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude dependency on hbase-annotations module,SPARK-4455,12755882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tedyu@apache.org,yuzhihong@gmail.com,yuzhihong@gmail.com,17/Nov/14 19:05,11/Dec/14 20:01,14/Jul/23 06:26,19/Nov/14 08:56,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"As Patrick mentioned in the thread 'Has anyone else observed this build break?' :

The error I've seen is this when building the examples project:
{code}
spark-examples_2.10: Could not resolve dependencies for project
org.apache.spark:spark-examples_2.10:jar:1.2.0-SNAPSHOT: Could not
find artifact jdk.tools:jdk.tools:jar:1.7 at specified path
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/../lib/tools.jar
{code}
The reason for this error is that hbase-annotations is using a
""system"" scoped dependency in their hbase-annotations pom, and this
doesn't work with certain JDK layouts such as that provided on Mac OS:

http://central.maven.org/maven2/org/apache/hbase/hbase-annotations/0.98.7-hadoop2/hbase-annotations-0.98.7-hadoop2.pom

hbase-annotations module is transitively brought in through other HBase modules, we should exclude it from related modules.",,apachespark,rmetzger,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 11 20:01:14 UTC 2014,,,,,,,,,,"0|i22gxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/14 19:26;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/3286;;;","11/Dec/14 20:01;srowen;FWIW this change also unfortunately causes compiler warnings, so hopefully it can be undone some day. But it's the right thing for now.

{code}
[warn] Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.
[warn] Class org.apache.hadoop.hbase.classification.InterfaceStability not found - continuing with a stub.
[warn] Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.
[warn] Class org.apache.hadoop.hbase.classification.InterfaceStability not found - continuing with a stub.
[warn] Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.
...
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in DAGScheduler,SPARK-4454,12755877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,rafal.kwasny,rafal.kwasny,17/Nov/14 18:45,17/May/20 17:48,14/Jul/23 06:26,02/Aug/15 08:29,1.1.0,,,,,,,1.3.0,,,,,,Scheduler,Spark Core,,,2,,,,,,"It seems to be a race condition in DAGScheduler that manifests on jobs with high concurrency:

{noformat}
 Exception in thread ""main"" java.util.NoSuchElementException: key not found: 35
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:58)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:64)
        at org.apache.spark.scheduler.DAGScheduler.getCacheLocs(DAGScheduler.scala:201)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1292)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply$mcVI$sp(DAGScheduler.scala:1307)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1306)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1306)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1306)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1304)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1304)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply$mcVI$sp(DAGScheduler.scala:1307)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1306)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1306)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1306)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1304)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1304)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply$mcVI$sp(DAGScheduler.scala:1307)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1306)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1306)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1306)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1304)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1304)
        at org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.scala:1275)
        at org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:937)
        at org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.scala:175)
        at org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:192)
        at org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:191)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
        at org.apache.spark.rdd.PartitionCoalescer$LocationIterator.next(CoalescedRDD.scala:203)
        at org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.scala:257)
        at org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:338)
        at org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:84)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1150)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:995)
        at me.wwsga.driveclub.EnhancedRDD.saveAsPartitioned(Enhanced.scala:53)
        at Import$$anonfun$22$$anonfun$apply$9$$anonfun$apply$10.apply(Import.scala:186)
        at Import$$anonfun$22$$anonfun$apply$9$$anonfun$apply$10.apply(Import.scala:181)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Code:
{noformat}
  private def getCacheLocs(rdd: RDD[_]): Array[Seq[TaskLocation]] = {
    if (!cacheLocs.contains(rdd.id)) {
      val blockIds = rdd.partitions.indices.map(index => RDDBlockId(rdd.id, index)).toArray[BlockId]
      val locs = BlockManager.blockIdsToBlockManagers(blockIds, env, blockManagerMaster)
      cacheLocs(rdd.id) = blockIds.map { id =>
        locs.getOrElse(id, Nil).map(bm => TaskLocation(bm.host, bm.executorId))
      }
    }
    cacheLocs(rdd.id)
  }
{noformat}
Probably getOrElseUpdate pattern would be better for this code.

",,apachespark,asloane,dougb,gu chi,jkleckner,joshrosen,mcchang,pwendell,pwolfe,rafal.kwasny,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2002,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 02 08:29:43 UTC 2015,,,,,,,,,,"0|i22gw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/14 11:29;apachespark;User 'mag-' has created a pull request for this issue:
https://github.com/apache/spark/pull/3345;;;","17/Feb/15 19:30;srowen;Per PR discussion;;;","17/Feb/15 19:34;joshrosen;I'm re-opening this issue.  [~srowen], we shouldn't resolve this as ""Won't Fix""; that PR was closed because it might not be the proper fix for this bug, but we should still work on a fix.  I'm investigating this now and will submit a PR shortly.;;;","17/Feb/15 19:36;pwendell;[~srowen] yeah I meant the particular PR was bad, not that the issue does not exist.;;;","17/Feb/15 19:37;srowen;My fault, I misunderstood the resolution. yes please keep it open.;;;","17/Feb/15 20:45;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4660;;;","18/Feb/15 01:42;pwendell;We can't be 100% sure this is fixed because it was not a reproducible issue. However, Josh has committed a patch that I think should make it hard to have race conditions around the cache location data structure.;;;","18/Feb/15 01:44;pwendell;Actually, re-opening this since we need to back port it.;;;","18/Feb/15 09:40;rafal.kwasny;Thanks for looking into this
The problem manifested when I had a job with multiple stages ( like 100 ) running in parallel ( through Scala Futures ) on the same RDD ( that had ~1000 partitions )
I will try to verify that this patch fixes the problem.
;;;","02/Aug/15 08:29;srowen;Given the unlikelihood of a further 1.2.x release, I'm closing this as no longer needing a back port;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle data structures can starve others on the same thread for memory ,SPARK-4452,12755858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,tianshuo,tianshuo,17/Nov/14 17:51,20/Jun/16 17:01,14/Jul/23 06:26,21/Apr/16 17:02,1.1.0,,,,,,,2.0.0,,,,,,Spark Core,,,,8,,,,,,"When an Aggregator is used with ExternalSorter in a task, spark will create many small files and could cause too many files open error during merging.

Currently, ShuffleMemoryManager does not work well when there are 2 spillable objects in a thread, which are ExternalSorter and ExternalAppendOnlyMap(used by Aggregator) in this case. Here is an example: Due to the usage of mapside aggregation, ExternalAppendOnlyMap is created first to read the RDD. It may ask as much memory as it can, which is totalMem/numberOfThreads. Then later on when ExternalSorter is created in the same thread, the ShuffleMemoryManager could refuse to allocate more memory to it, since the memory is already given to the previous requested object(ExternalAppendOnlyMap). That causes the ExternalSorter keeps spilling small files(due to the lack of memory)

I'm currently working on a PR to address these two issues. It will include following changes:

1. The ShuffleMemoryManager should not only track the memory usage for each thread, but also the object who holds the memory
2. The ShuffleMemoryManager should be able to trigger the spilling of a spillable object. In this way, if a new object in a thread is requesting memory, the old occupant could be evicted/spilled. Previously the spillable objects trigger spilling by themselves. So one may not trigger spilling even if another object in the same thread needs more memory. After this change The ShuffleMemoryManager could trigger the spilling of an object if it needs to.
3. Make the iterator of ExternalAppendOnlyMap spillable. Previously ExternalAppendOnlyMap returns an destructive iterator and can not be spilled after the iterator is returned. This should be changed so that even after the iterator is returned, the ShuffleMemoryManager can still spill it.

Currently, I have a working branch in progress: https://github.com/tsdeng/spark/tree/enhance_memory_manager. Already made change 3 and have a prototype of change 1 and 2 to evict spillable from memory manager, still in progress. I will send a PR when it's done.

Any feedback or thoughts on this change is highly appreciated !
",,amcelwee,andrewor14,apachespark,copris,davies,diederik,DjvuLee,dougb,dparekh,emlyn,glenn.strycker@gmail.com,hammer,hanxue,jerryshao,joshrosen,kiszk,lianhuiwang,liqusha,liyezhang556520,matei,rafal.kwasny,rdub,romi-totango,sandyr,sb58,tianshuo,xhao1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14560,,,,,SPARK-4467,SPARK-3633,,,,,,,SPARK-2711,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-9697,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 10 14:13:04 UTC 2016,,,,,,,,,,"0|i22grz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/14 17:52;apachespark;User 'tsdeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3302;;;","17/Nov/14 18:32;tianshuo;Originally, we found this problem by seeing Too Many Files Open exception when running SVD job on a big dataset. And it's caused by ExternalSorter dumping too many small files.

With the fixes mentioned above(with the prototype), the problem is resolved. I'm still working on finalizing the prototype.
;;;","17/Nov/14 19:21;sandyr;I haven't thought the implications out fully, but it worries me that data structures wouldn't be in charge of their own spilling.  It seems like for different data structures, different spilling patterns and sizes could be more efficient, and placing control in the hands of the shuffle memory manager negates this.

Another fix (simpler but maybe less flexible) worth considering would be to define a static fraction of memory that goes to each of the aggregator and the external sorter, right?  One way that could be implemented is that each time the aggregator gets memory, it allocates some for the sorter.

Also, for SPARK-2926, we are working on a tiered merger that would avoid the ""Too many open files"" problem by only merging a limited number of files at once.;;;","17/Nov/14 19:33;sandyr;A third possible fix would be to have the shuffle memory manager consider fairness in terms of spillable data structures instead of threads.;;;","17/Nov/14 21:39;tianshuo;Hi, [~sandyr]:
Your concern about data structures wouldn' be in charge of their spilling is legit. That's why I'm trying to make a incremental change:
1. The data structure still asks the ShuffleMemoryManager and decides if to spill itself.
2. But ShuffleMemoryManager can also trigger the spill of an object if the memory quota of a thread is used up.

2 happens as a last resort when memory is not enough for the requesting object.
Also as you mentioned in the third solution, if the shuffle manager consider fairness among objs，it has to have a way to trigger the spilling of an object in a situation where current allocation is ""not fair"". The memory manager has more of a global knowledge about memory allocation, so giving spilling ability to the manager could lead to more optimal memory allocation. If the spilling can only be triggered from the object itself, like currently, one obj may not be aware of the memory usage of other objs and keep holding the memory.

My point is the data structure should be able to trigger spilling by itself, but it should also be able to handle when shuffleManager asks it to spill. I'm also considering the obj can reject to spill itself do address the concern you mentioned.


 ;;;","17/Nov/14 21:45;tianshuo;Currently, the two instances of Spillable, ExternalSorter and ExternalAppendOnlyMap does not seem to have complex logic to decide when to spill. They all ask as much memory as it can. I also consider this general strategy of asking as much memory as it can would apply to most cases. So I guess it would be better if the memory manager could trigger the spilling. Also make the spillable to reject a spilling request could possibly address your concern?;;;","17/Nov/14 22:05;sandyr;Updated the title to reflect the specific problem.;;;","17/Nov/14 23:27;andrewor14;Hey [~tianshuo] do you see this issue only for sort-based shuffle? Have you been able to reproduce it on hash-based shuffle?;;;","17/Nov/14 23:33;tianshuo;Hi, [~andrewor14]:
Actually hash-based shuffle does not go as bad as sort-based shuffle on this particular problem. We were able to bypass this problem by using hash-based shuffle. This problem was so bad for me also because the elementsRead bug, so that could be also another reason why hash-based shuffle didn't break as badly.;;;","17/Nov/14 23:36;tianshuo;Hi, [~andrewor14]:
The elementsRead bug that makes the situation so bad and went to ""too many files open"" is fixed here: https://github.com/apache/spark/pull/3302.
I will send another PR for the memory starving problem mentioned in this ticket soon.;;;","17/Nov/14 23:44;matei;How much of this gets fixed if you fix the elementsRead bug in ExternalSorter?

With forcing data structures to spill, the problem is that it will introduce complexity in every spillable data structure. I wonder if we can make it just give out memory in smaller increments, so that threads check whether they should spill more often. In addition, we can set a better minimum or maximum on each thread (e.g. always let it ramp up to, say, 5 MB, or some fraction of the memory space).

I do like the idea of making the ShuffleMemoryManager track limits per object. I actually considered this when I wrote that and didn't do it, possibly because it would've created more complexity in figuring out when an object is done. But it seems like it should be straightforward to add in, as long as you also track which objects come from which thread so that you can still releaseMemoryForThisThread() to clean up.;;;","17/Nov/14 23:46;andrewor14;I see, in other words, there are two separate issues affecting sort-based shuffle:

1. The `elementsRead` variable is not updated
2. External data structures starve each other if they're in the same thread

where (2) is also common in hash-based shuffle. Your PR https://github.com/apache/spark/pull/3302 fixes (1), but we still need to address (2) at some point. However, fixing (1) is important enough because we previously just unconditionally spilled every 32 records after a while.;;;","17/Nov/14 23:51;tianshuo;Hi, [~andrewor14],
Yeah exactly. Actually this ticket is more for addressing the (2) problem, I have a branch in progress for that: https://github.com/tsdeng/spark/tree/enhance_memory_manager

It's still a prototype, but greatly alleviate the problem for us. Just trying to finalize that.;;;","17/Nov/14 23:54;sandyr;[~andrewor14], IIUC, (2) shouldn't happen in hash-based shuffle at all, because hash-based shuffle doesn't use multiple spillable data structures in each task.;;;","17/Nov/14 23:59;andrewor14;[~sandyr] hash-based shuffle can still use two ExternalAppendOnlyMaps in 1 task if you have back-to-back shuffles where the second shuffle does a map-side combine.;;;","18/Nov/14 00:01;sandyr;Ah, true.;;;","18/Nov/14 00:01;tianshuo;[~matei]:
You are right, it does add more complexity if we force the data structure to spill. But in my prototype branch I already made changes to ExternalSorter and ExternalAppendOnlyMap to make it support that. And it's not too hard and doable. 
In terms of coding, it does add complexity, but the property we get from it is pretty nice: able to spill the object as we want to.
Also ExternalSorter and ExternalAppendOnlyMap are the only two that need to be changed.

For your question, after fixing the elementsRead bug, we do not see the exception, but could still see tons of small files due to the memory starvation.
;;;","18/Nov/14 00:35;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3330;;;","18/Nov/14 01:20;matei;Got it. It would be fine to do this if you found it to help, I was just wondering whether simpler fixes would get us far enough. For the forced spilling change, I'd suggest writing a short design doc, or making sure that the comments in the code about it are very detailed (essentially having a design doc at the top of the class). This can have a lot of tricky cases due to concurrency so it's important to document the design.;;;","18/Nov/14 01:21;matei;BTW we may also want to create a separate JIRA for the short-term fix for 1.1 and 1.2.;;;","18/Nov/14 02:08;andrewor14;I have created SPARK-4467 for the `elementsRead` bug since the bigger issue here is distinct.;;;","18/Nov/14 19:57;matei;BTW I've thought about this more and here's what I'd suggest: try a version where each object is allowed to ramp up to a certain size (say 5 MB) before being subject to the limit, and if that doesn't work, then maybe go for the forced-spilling one. The reason is that as soon as N objects are active, the ShuffleMemoryManager will not let any object ramp up to more than 1/N, so it just has to fill up its current quota and stop. This means that scenarios with very little free memory might only happen at the beginning (when tasks start up). If we can make this work, then we avoid a lot of concurrency problems that would happen with forced spilling. 

Another improvement would be to make the Spillables request less than 2x their current memory when they ramp up, e.g. 1.5x. They'd then make more requests but it would lead to slower ramp-up and more of a chance for other threads to grab memory. But I think this will have less impact than simply increasing that free minimum amount.;;;","18/Nov/14 22:29;sandyr;One issue with a limits-by-object approach is that it could result in extra wasted memory over the current approach for tasks that produce less shuffle data than they read.  E.g. consider a rdd.reduceByKey(...).map(...).reduceByKey(...)...

The object aggregating inputs used to have access to the full memory allotted to the task, but now it only gets half the memory.  In situations where the object aggregating outputs doesn't need as much memory (because there is less output data), some of the memory that previously would have been used is unused.

A forced spilling approach seems like it could give some of the advantages that preemption provides in cluster scheduling - better utilization through enabling objects to use more than their ""fair"" amount until it turns out other objects need those resources.
;;;","19/Nov/14 01:54;andrewor14;I have opened a JIRA that targets on fixing this on a smaller scope: SPARK-4480. I intend to pull this smaller fix into 1.1.1, and maybe it's sufficient for 1.2.0. This particular JIRA (SPARK-4452) likely involves a much bigger change that is too ambitious for either release at the moment.;;;","19/Nov/14 02:09;andrewor14;[~matei] I have implemented your first suggestion here: https://github.com/apache/spark/pull/3353. In my particular workload, I've noticed at least an order of magnitude reduction in the number of shuffle files written. More details provided in the PR.;;;","19/Nov/14 03:12;matei;Forced spilling is orthogonal to how you set the limits actually. For example, if there are N objects, one way to set limits is to reserve at least 1/N of memory for each one. But another way would be to group them by thread, and use a different algorithm for allocation within a thread (e.g. set each object's cap to more if other objects in their thread are using less). Whether you force spilling or not, you'll have to decide what the right limit for each thing is.;;;","19/Nov/14 03:27;sandyr;[~matei] my point is not that forced spilling allows us to avoid setting limits, but that it allows those limits to be soft: if an entity (thread or object) is not requesting the 1/N memory reserved for it, that memory can be given to other entities that need it.  Then, if the entity later requests the memory reserved to it, the other entities above their fair allocation can be forced to spill.

(I don't necessarily mean to argue that this advantage is worth the added complexity.);;;","19/Nov/14 18:35;tianshuo;Hi,
While I'm working on this ticket, I have an related question:
I noticed an extra constraint in the usage of ExternalAppendOnlyMap. 
Even in the current implementation(master), If an ExternalAppendOnlyMap exported a iterator(spilled), you can not get the iterator again, since the memory iterator is destructive.
But in our unit tests, the constraint seems to be ignored... many tests are calling iterator multiple times. It works because the data is small and does not trigger the spilling in unit test.

But I just want to confirm, if it's ok I explicitly adding this constraint to the code and unit test: Iterator of an ExternalAppendOnlyMap can only be exported once;;;","19/Nov/14 19:05;andrewor14;[~tianshuo] That is a correct assumption for ExternalAppendOnlyMap: once it has spilled and we called `iterator`, which destroyed the underlying map, we should not be able to call `iterator` again or insert any items into the map. We should really document that clearly, but your understanding is correct.;;;","20/Nov/14 18:57;tianshuo;Hi, [~matei]:

My way of implementing it is more like the 2nd way you suggested. I will put up a design doc. But I would like to give a preview of my implementation first

 I already implemented following and seems work for me

1. Memory Allocation and spilling is divided into two levels. SpillableTaskMemoryManager for memory allocation and spilling of current thread/task. ShuffleMemoryManager coordinates memory allocation among threads/tasks

2. SpillableTaskMemoryManager: objects are grouped by threads, each STMM maps to one thread/task. If an object requires more memory, it asks STMM for it. STMM will ask ShuffleMemoryManager for more memory for current thread. if the returned memory does not satisfy the request, it will tries to spill objs in current thread to give up memory. Notice the objects it may spill are thread-local, so there is no contention

3. ShuffleMemoryManager: The algorithm in thread memory allocation is basically unchanged. Only thing is that spillables do not ask SMM directly for more memory, instead STMM asks for memory for the thread.

By making this change, spilling is triggered from STMM. This design has following properties in mind:

- Incremental change, thread memory allocation algorithm is unchanged. This way each task/thread get a fair share of memory.
- Spiling is thread local and is triggered from STMM to avoid unnecessary locking and contention. 
- Two levels of memory allocation makes a distinction between allocating memory for tasks and allocating memory/spilling objs in the current task. This distinction makes contention management more clear and easier;;;","20/Nov/14 21:48;tianshuo;Here is a link of the diff:
https://github.com/tsdeng/spark/compare/fix_memory_starvation?expand=1
It's still in progress and needs lots of cleanups, but could show the idea of the design.
Hope can get some early feedback on this route
;;;","24/Nov/14 01:33;sandyr;[~tianshuo], I took a look at the patch, and the general approach looks reasonable to me.

A couple additional thoughts that apply both to the current approach and Tianshuo's patch:
* When we chain an ExternalAppendOnlyMap to an ExternalSorter for processing combined map outputs in sort based shuffle, we end up double counting, no? Both data structures will be holding references to the same objects and estimating their size based on these objects.
* We could make the in-memory iterators destructive as well right?  I.e. if the data structures can release references to objects as they yield them, then we can give memory back to the shuffle memory manager and make it available to other data structures in the same thread.

If we can avoid double and holding on to unneeded objects, it would obviate some of the need for intra-thread limits / forced spilling.;;;","25/Nov/14 18:55;tianshuo;[~sandyr]:
Thanks for the feedback!

For double counting, yes, the external data structure may results to double counting. But it only applies to the in-memory portion of the data. In my PR, in ExternalOnlyMap, once the in-memory portion is spilled, the memory is recycled(by giving an empty iterator and empty map).

 So there are two approaches I can do
1. Minor change based on my current change: also recycle the memory when memory iterator is drained
2. A little bigger change: Make the memory iterator destructive by nulling out the underlying element in the array when the element is returned, this also requires spillable data structure to report back the memory occupied when the iterator is being consumed, while currently it only reports the memory usage when new data is being inserted.

So change 1 seems adding less constraints to the spillable data structure, what do you think?

;;;","26/Nov/14 23:04;sandyr;Thinking about the current change a little more, an issue is that it will spill all the in-memory data to disk in situations where this is probably overkill.  E.g. consider the typical situation of shuffle data slightly exceeding memory.  We end up spilling the entire data structure if a downstream data structure needs even a small amount of memory.

I think that your proposed change 2 is probably worthwhile.;;;","24/Jan/15 23:27;srowen;Can this JIRA be resolved now that its children are resolved, or is the more to this one?;;;","24/Jan/15 23:41;sandyr;I think there's more to this one, the subtasks solved the most egregious issues, but shuffle data structures can still hog memory in detrimental ways described in some of the comments above.;;;","30/Jun/15 16:30;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7130;;;","30/Jun/15 21:55;joshrosen;I've linked this to the Project Tungsten JIRA epic, since the increased uses of spillable collections in the Tunsgten code will magnify this issue.;;;","28/Nov/15 10:14;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/10024;;;","21/Apr/16 17:02;davies;Issue resolved by pull request 10024
[https://github.com/apache/spark/pull/10024];;;","24/Apr/16 17:34;romi-totango;Hi, what's the reason this will only be available in Spark 2.0.0, and not 1.6.4 or 1.7.0?;;;","25/Apr/16 17:09;davies;We only backport critical bug fix into released branch.

There is no 1.7.0, 2.0 will released around June 2016.;;;","09/May/16 08:35;xhao1;Since this is an old issue which impact Spark since 1.1.0, can the patch be merged to Spark 1.6.X ? This will be very helpful for Spark 1.6.X users. Thanks.;;;","10/May/16 04:16;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/13020;;;","10/May/16 14:13;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/13027;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support ConstantObjectInspector for unwrapping data,SPARK-4448,12755755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,chenghao,chenghao,17/Nov/14 08:51,18/Nov/14 00:36,14/Jul/23 06:26,18/Nov/14 00:36,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"ClassCastException when retrieving primitive object from a ConstantObjectInspector by specifying parameters. This is probably a bug for Hive, we just provide a work around in ""HiveInspectors"".",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 00:36:17 UTC 2014,,,,,,,,,,"0|i22g5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/14 08:52;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3308;;;","18/Nov/14 00:36;marmbrus;Issue resolved by pull request 3308
[https://github.com/apache/spark/pull/3308];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetadataCleaner schedule task with a wrong param for delay time .,SPARK-4446,12755750,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Liu Hao,Liu Hao,Liu Hao,17/Nov/14 08:00,20/Nov/14 02:20,14/Jul/23 06:26,20/Nov/14 02:20,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,MetadataCleaner schedule task with a wrong param for delay time .,,apachespark,Liu Hao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 08:02:32 UTC 2014,,,,,,,,,,"0|i22g4f:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Nov/14 08:02;apachespark;User 'Leolh' has created a pull request for this issue:
https://github.com/apache/spark/pull/3306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't display storage level in toDebugString unless RDD is persisted,SPARK-4445,12755749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,pwendell,pwendell,17/Nov/14 07:48,17/Nov/14 18:41,14/Jul/23 06:26,17/Nov/14 18:41,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"The current approach lists the storage level all the time, even if the RDD is not persisted. The storage level should only be listed if the RDD is persisted. We just need to guard it with a check.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 09:18:08 UTC 2014,,,,,,,,,,"0|i22g47:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Nov/14 09:18;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/3310;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statistics bug for external table in spark sql hive,SPARK-4443,12755740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,scwf,scwf,17/Nov/14 06:55,18/Nov/14 00:34,14/Jul/23 06:26,18/Nov/14 00:34,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"When table is external, the `totalSize` is always zero, which will influence join strategy(always use broadcast join for external table)",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 00:34:11 UTC 2014,,,,,,,,,,"0|i22g27:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Nov/14 06:59;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3304;;;","18/Nov/14 00:34;marmbrus;Issue resolved by pull request 3304
[https://github.com/apache/spark/pull/3304];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close Tachyon client when TachyonBlockManager is shut down,SPARK-4441,12755723,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shimingfei,shimingfei,shimingfei,17/Nov/14 05:13,17/May/20 18:21,14/Jul/23 06:26,19/Nov/14 06:18,1.1.0,,,,,,,1.2.0,,,,,,Block Manager,Spark Core,,,0,,,,,,Currently Tachyon client is not shut down when TachyonBlockManager is shut down. which causes some resources in Tachyon not reclaimed.,,apachespark,shimingfei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 05:16:33 UTC 2014,,,,,,,,,,"0|i22fyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/14 05:16;apachespark;User 'shimingfei' has created a pull request for this issue:
https://github.com/apache/spark/pull/3299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-submit cluster deploy mode JAR URLs are broken in 1.1.1,SPARK-4434,12755686,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,joshrosen,joshrosen,16/Nov/14 22:34,18/Nov/14 20:20,14/Jul/23 06:26,18/Nov/14 20:20,1.1.1,1.2.0,,,,,,1.1.1,1.2.0,,,,,Deploy,Spark Core,,,0,,,,,,"When submitting a driver using {{spark-submit}} in cluster mode, Spark 1.1.0 allowed you to omit the {{file://}} or {{hdfs://}} prefix from the application JAR URL, e.g.

{code}
./bin/spark-submit --deploy-mode cluster --master spark://joshs-mbp.att.net:7077 --class org.apache.spark.examples.SparkPi /Users/joshrosen/Documents/old-spark-releases/spark-1.1.0-bin-hadoop1/lib/spark-examples-1.1.0-hadoop1.0.4.jar
{code}

In Spark 1.1.1 and 1.2.0, this same command now fails with an error:

{code}
./bin/spark-submit --deploy-mode cluster --master spark://joshs-mbp.att.net:7077 --class org.apache.spark.examples.SparkPi /Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar
Jar url 'file:/Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar' is not in valid format.
Must be a jar file path in URL format (e.g. hdfs://XX.jar, file://XX.jar)

Usage: DriverClient [options] launch <active-master> <jar-url> <main-class> [driver options]
Usage: DriverClient kill <active-master> <driver-id>
{code}

I tried changing my URL to conform to the new format, but this either resulted in an error or a job that failed:

{code}
./bin/spark-submit --deploy-mode cluster --master spark://joshs-mbp.att.net:7077 --class org.apache.spark.examples.SparkPi file:///Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar
Jar url 'file:///Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar' is not in valid format.
Must be a jar file path in URL format (e.g. hdfs://XX.jar, file://XX.jar)
{code}

If I omit the extra slash:

{code}
./bin/spark-submit --deploy-mode cluster --master spark://joshs-mbp.att.net:7077 --class org.apache.spark.examples.SparkPi file://Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar
Sending launch command to spark://joshs-mbp.att.net:7077
Driver successfully submitted as driver-20141116143235-0002
... waiting before polling master for driver state
... polling master for driver state
State of driver-20141116143235-0002 is ERROR
Exception from cluster was: java.lang.IllegalArgumentException: Wrong FS: file://Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar, expected: file:///
java.lang.IllegalArgumentException: Wrong FS: file://Users/joshrosen/Documents/Spark/examples/target/scala-2.10/spark-examples_2.10-1.1.2-SNAPSHOT.jar, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:381)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:55)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:393)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:329)
	at org.apache.spark.deploy.worker.DriverRunner.org$apache$spark$deploy$worker$DriverRunner$$downloadUserJar(DriverRunner.scala:157)
	at org.apache.spark.deploy.worker.DriverRunner$$anon$1.run(DriverRunner.scala:74)
{code}

This bug effectively prevents users from using {{spark-submit}} in cluster mode to run drivers whose JARs are stored on shared cluster filesystems.",,andrewor14,apachespark,ilikerps,joshrosen,matei,sarutak,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 20:20:19 UTC 2014,,,,,,,,,,"0|i22fqf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"16/Nov/14 22:48;joshrosen;I think that there are only a small number of patches in branch-1.1 that are related to this, so I'm going to see if I can narrow it down to a specific commit.  https://github.com/apache/spark/pull/2925 is one potential culprit, but there may be others.

I'm not sure whether this affects HDFS URLs; I haven't tried it yet since I don't have a Docker-ized HDFS set up in my integration tests project.;;;","17/Nov/14 01:02;joshrosen;It looks like this was caused by https://github.com/apache/spark/pull/2925 (SPARK-4075), since reverting that fixes this issue.  I'll work on committing my test code to our internal tests repository and open a PR to investigate / revert that commit.;;;","17/Nov/14 01:14;matei;[~joshrosen] make sure to revert this on 1.2 and master as well.;;;","17/Nov/14 01:26;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3295;;;","17/Nov/14 01:28;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3296;;;","17/Nov/14 01:28;ilikerps;Side note: the error message about ""file://"", which was not introduced in the patch you reverted, is incorrect. A ""file://XX.jar"" URI is never valid. One or three slashes must be used; two slashes indicates that a hostname follows.;;;","17/Nov/14 02:06;joshrosen;Fellow Databricks folks: I've added a regression test for this in https://github.com/databricks/spark-integration-tests/commit/f121f45aecbeafcec21d3bb670737fc9f7d6da0b (I'm sharing this link here so that it's easy to find this test once we open-source that repository).  The test is essentially a scripted / automated version of the commands that I've listed in this JIRA.  These tests confirm that reverting that earlier PR fixes this issue.

[~adav], do you want to open a separate JIRA to fix the ""file://"" error message?;;;","17/Nov/14 02:15;joshrosen;As a regression test, we should probably add a triple-slash test case to ClientSuite.;;;","17/Nov/14 19:23;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3320;;;","17/Nov/14 19:46;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3320;;;","17/Nov/14 22:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3326;;;","18/Nov/14 20:20;andrewor14;Reverted in 1.1.1, and fixed in master and 1.2 through https://github.com/apache/spark/pull/3326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Racing condition in zipWithIndex,SPARK-4433,12755645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,16/Nov/14 04:34,19/Nov/14 00:27,14/Jul/23 06:26,19/Nov/14 00:27,1.0.2,1.1.1,1.2.0,,,,,1.0.3,1.1.1,1.2.0,,,,Spark Core,,,,0,,,,,,"Spark hangs with the following code:

{code}
sc.parallelize(1 to 10).zipWithIndex.repartition(10).count()
{code}

This is because ZippedWithIndexRDD triggers a job in getPartitions and it cause a deadlock in DAGScheduler.getPreferredLocs.",,apachespark,glenn.strycker@gmail.com,huasanyelao,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 00:27:31 UTC 2014,,,,,,,,,,"0|i22fhr:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.1,1.2.0,,,,,,,,,"16/Nov/14 04:44;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3291;;;","19/Nov/14 00:27;mengxr;Issue resolved by pull request 3291
[https://github.com/apache/spark/pull/3291];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource(InStream) is not closed in TachyonStore,SPARK-4432,12755642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shimingfei,shimingfei,shimingfei,16/Nov/14 03:27,17/May/20 18:21,14/Jul/23 06:26,19/Nov/14 06:19,1.1.0,,,,,,,1.2.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"In TachyonStore, InStream is not closed after data is read  from Tachyon. which makes the blocks in Tachyon locked after accessed.",,apachespark,shimingfei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 16 04:06:02 UTC 2014,,,,,,,,,,"0|i22fh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/14 04:06;apachespark;User 'shimingfei' has created a pull request for this issue:
https://github.com/apache/spark/pull/3290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache RAT Checks fail spuriously on test files,SPARK-4430,12755634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,rdub,rdub,15/Nov/14 23:56,26/Jan/15 16:03,14/Jul/23 06:26,26/Jan/15 16:03,1.1.0,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"Several of my recent runs of {{./dev/run-tests}} have failed quickly due to Apache RAT checks, e.g.:

{code}
$ ./dev/run-tests

=========================================================================
Running Apache RAT checks
=========================================================================
Could not find Apache license headers in the following files:
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b732c105-4fd3-4330-ba6d-a366b340c303/test/28
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b732c105-4fd3-4330-ba6d-a366b340c303/test/29
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b732c105-4fd3-4330-ba6d-a366b340c303/test/30
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/10
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/11
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/12
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/13
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/14
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/15
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/16
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/17
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/18
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/19
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/20
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/21
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/22
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/23
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/24
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/25
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/26
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/27
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/28
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/29
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/30
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/7
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/8
 !????? /Users/ryan/c/spark/streaming/FailureSuite/b98beebe-98b0-472a-b4a5-060bcd91e401/test/9
[error] Got a return code of 1 on line 114 of the run-tests script.
{code}

I think it's fair to say that these are not useful errors for {{run-tests}} to crash on. Ideally we could tell the linter which files we care about having it lint and which we don't.

",,apachespark,hammer,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 24 18:27:06 UTC 2015,,,,,,,,,,"0|i22ffb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/14 00:03;rdub;I did find [this RAT JIRA|https://issues.apache.org/jira/browse/RAT-161] that seems somewhat related, but if there's anything we could do to work around this in Spark in the shorter term that would be great too.;;;","16/Nov/14 07:01;srowen;I imagine the real issue is that the test should clean up these files if it does not already. Those aren't in the source tree. It is not really a RAT config issue. If the files were left because tests crashed or were killed then just delete them. ;;;","24/Jan/15 18:27;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4189;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build for Scala 2.11 using sbt fails.,SPARK-4429,12755605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,15/Nov/14 13:43,09/Oct/15 21:14,14/Jul/23 06:26,19/Nov/14 22:41,,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"I tried to build for Scala 2.11 using sbt with the following command:

{quote}
$ sbt/sbt -Dscala-2.11 assembly
{quote}

but it ends with the following error messages:

{quote}
\[error\] (streaming-kafka/*:update) sbt.ResolveException: unresolved dependency: org.apache.kafka#kafka_2.11;0.8.0: not found
\[error\] (catalyst/*:update) sbt.ResolveException: unresolved dependency: org.scalamacros#quasiquotes_2.11;2.0.1: not found
{quote}
",,apachespark,hoangelos,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 21:14:41 UTC 2015,,,,,,,,,,"0|i22f8v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"18/Nov/14 10:26;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3342;;;","09/Oct/15 21:14;hoangelos;I'm wondering where this is at?  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The symbol of BitwiseOr is wrong, should not be '&'",SPARK-4426,12755596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,15/Nov/14 09:11,18/Nov/14 11:28,14/Jul/23 06:26,16/Nov/14 06:25,1.3.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The symbol of BitwiseOr is defined as '&' but I think it's wrong. It should be '|'.",,apachespark,prasad.ch,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 15 12:25:30 UTC 2014,,,,,,,,,,"0|i22f6v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Nov/14 09:13;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3284;;;","15/Nov/14 12:25;prasad.ch;please see my issue ...................... i hope all of you!

SPARK-4427
thanks......!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle NaN or Infinity cast to Timestamp correctly,SPARK-4425,12755593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,15/Nov/14 08:00,18/Nov/14 00:29,14/Jul/23 06:26,18/Nov/14 00:29,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,{{Cast}} from {{NaN}} or {{Infinity}} of {{Double}} or {{Float}} to {{TimestampType}} throws {{NumberFormatException}}.,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 00:29:02 UTC 2014,,,,,,,,,,"0|i22f67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/14 08:05;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3283;;;","18/Nov/14 00:29;marmbrus;Issue resolved by pull request 3283
[https://github.com/apache/spark/pull/3283];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In some cases, Vectors.fromBreeze get wrong results.",SPARK-4422,12755590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,15/Nov/14 06:20,31/Dec/14 19:37,14/Jul/23 06:26,31/Dec/14 19:36,1.2.0,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"{noformat}
import breeze.linalg.{DenseVector => BDV, DenseMatrix => BDM, sum => brzSum} 
var x = BDM.zeros[Double](10, 10)
val v = Vectors.fromBreeze(x(::, 0))
assert(v.size == x.rows)
{noformat}",,apachespark,gq,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 19:37:06 UTC 2014,,,,,,,,,,"0|i22f5j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Nov/14 06:29;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3281;;;","17/Nov/14 05:32;mengxr;Issue resolved by pull request 3281
[https://github.com/apache/spark/pull/3281];;;","17/Nov/14 05:34;mengxr;Reopened for branch-1.0 and branch-1.1. Changed the priority to minor because `fromBreeze` is private and we don't use breeze matrix slicing in MLlib.;;;","12/Dec/14 13:02;srowen;I think this should be marked for backport?;;;","19/Dec/14 21:48;mengxr;fromBreeze is private and we don't use breeze vectors in a way that can trigger this bug. Actually it is not really necessary to backport this patch.;;;","27/Dec/14 08:10;pwendell;[~mengxr] so should we just close this and decide not to backport it then?;;;","31/Dec/14 19:37;mengxr;Marked as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong link in spark-standalone.html,SPARK-4421,12755583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tsudukim,tsudukim,tsudukim,15/Nov/14 05:34,05/Dec/14 02:15,14/Jul/23 06:26,05/Dec/14 02:15,,,,,,,,1.1.2,1.2.1,,,,,Documentation,,,,0,,,,,,"The link about building spark in the document page ""Spark Standalone Mode"" (spark-standalone.html) is wrong.
That link is pointed at {{index.html#building}}, but it is only available until 0.9. The building guide was moved to another page ({{building-with-maven.html}} in 1.0 and 1.1, or {{building-spark.html}} in 1.2).",,apachespark,joshrosen,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 02:15:18 UTC 2014,,,,,,,,,,"0|i22f3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/14 05:38;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/3279;;;","15/Nov/14 05:58;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/3280;;;","15/Nov/14 06:02;tsudukim;I sent 2 pull requests.
#3279 is for Spark 1.2, and #3280 is for Spark 1.1.;;;","05/Dec/14 02:15;joshrosen;Issue resolved by pull request 3279
[https://github.com/apache/spark/pull/3279];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change nullability of Cast from DoubleType/FloatType to DecimalType.,SPARK-4420,12755579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,15/Nov/14 04:48,18/Nov/14 00:27,14/Jul/23 06:26,18/Nov/14 00:27,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,This is follow-up of SPARK-4390.,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 00:27:16 UTC 2014,,,,,,,,,,"0|i22f33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/14 04:55;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3278;;;","18/Nov/14 00:27;marmbrus;Issue resolved by pull request 3278
[https://github.com/apache/spark/pull/3278];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Driver did not exit after python driver had exited.,SPARK-4415,12755510,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,14/Nov/14 22:41,15/Nov/14 04:15,14/Jul/23 06:26,15/Nov/14 04:15,1.1.0,1.2.0,,,,,,1.2.0,,,,,,PySpark,Spark Core,,,0,,,,,,"If we have spark_driver_memory in spark-default.cfg, and start the spark job by 
{code}
$ python xxx.py
{code}

Then the spark driver will not exit after the python process had exited.",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 23:37:11 UTC 2014,,,,,,,,,,"0|i22enr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"14/Nov/14 23:37;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3274;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift server for 0.13.1 doesn't deserialize complex types properly,SPARK-4407,12755422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,lian cheng,lian cheng,14/Nov/14 17:32,17/Nov/14 02:31,14/Jul/23 06:26,16/Nov/14 22:27,1.1.2,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The following snippet can reproduce this issue:
{code}
CREATE TABLE t0(m MAP<INT, STRING>);
INSERT OVERWRITE TABLE t0 SELECT MAP(key, value) FROM src LIMIT 10;
SELECT * FROM t0;
{code}
Exception throw:
{code}
java.lang.RuntimeException: java.lang.ClassCastException: scala.collection.immutable.Map$Map1 cannot be cast to java.lang.String
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:84)
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
        at com.sun.proxy.$Proxy21.fetchResults(Unknown Source)
        at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:405)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:530)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: scala.collection.immutable.Map$Map1 cannot be cast to java.lang.String
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.addNonNullColumnValue(Shim13.scala:142)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getNextRowSet(Shim13.scala:165)
        at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:192)
        at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:471)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
        ... 19 more
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 02:31:30 UTC 2014,,,,,,,,,,"0|i22e4n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"14/Nov/14 17:53;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3178;;;","16/Nov/14 23:04;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3292;;;","17/Nov/14 02:31;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3298;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmitDriverBootstrapper should stop after its SparkSubmit sub-process ends ,SPARK-4404,12755397,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,14/Nov/14 15:51,19/Nov/14 01:04,14/Jul/23 06:26,19/Nov/14 01:04,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"When we have spark.driver.extra* or spark.driver.memory in SPARK_SUBMIT_PROPERTIES_FILE, spark-class will use SparkSubmitDriverBootstrapper to launch driver.
If we get process id of SparkSubmitDriverBootstrapper and wanna kill it during its running, we expect its SparkSubmit sub-process stop also.",,apachespark,davies,vanzin,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 16 05:10:38 UTC 2014,,,,,,,,,,"0|i22dz3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"14/Nov/14 16:00;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3266;;;","16/Nov/14 01:26;davies;After this patch, the SparkSubmitDriverBootstrapper will not exit if SparkSubmit die first.;;;","16/Nov/14 01:38;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3289;;;","16/Nov/14 03:01;davies;Also pyspark failed to start, if spark.driver.memory is set, I have not investigate the details yet.;;;","16/Nov/14 03:23;vanzin;Hmmm, my reading of the bug title and the bug description don't match.

Title: ""SparkSubmitDriverBootstrapper should stop after SparkSubmit ends""
Descriptiont: ""killing SparkSubmitDriverBootstrapper should also kill SparkSubmit""

Pardon if I misunderstood something, but could you clarify what's not working as expected?;;;","16/Nov/14 05:10;davies;This JIRA is re-opened, the new bug is introduced by this JIRA.

Should I create a new JIRA for it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet Filter pushdown flag should be set with SQLConf,SPARK-4391,12755199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,13/Nov/14 23:27,14/Nov/14 22:59,14/Jul/23 06:26,14/Nov/14 22:59,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 22:59:57 UTC 2014,,,,,,,,,,"0|i22cs7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"14/Nov/14 00:02;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3258;;;","14/Nov/14 22:59;marmbrus;Issue resolved by pull request 3258
[https://github.com/apache/spark/pull/3258];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad casts to decimal throw instead of returning null,SPARK-4390,12755191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,13/Nov/14 23:15,14/Nov/14 22:57,14/Jul/23 06:26,14/Nov/14 22:57,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 22:57:18 UTC 2014,,,,,,,,,,"0|i22cqf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Nov/14 23:16;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3256;;;","14/Nov/14 22:57;marmbrus;Issue resolved by pull request 3256
[https://github.com/apache/spark/pull/3256];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many open files during sort in pyspark,SPARK-4384,12755145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,13/Nov/14 20:04,19/Nov/14 23:47,14/Jul/23 06:26,19/Nov/14 23:47,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"Reported in maillist:

On Thu, Nov 13, 2014 at 11:28 AM, santon <steven.m.anton@gmail.com> wrote:
> Thanks for the thoughts. I've been testing on Spark 1.1 and haven't seen the
> IndexError yet. I've run into some other errors (""too many open files""), but
> these issues seem to have been discussed already. The dataset, by the way,
> was about 40 Gb and 188 million lines; I'm running a sort on 3 worker nodes
> with a total of about 80 cores.

",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 23:47:08 UTC 2014,,,,,,,,,,"0|i22cgv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Nov/14 20:10;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3252;;;","19/Nov/14 23:47;joshrosen;Issue resolved by pull request 3252
[https://github.com/apache/spark/pull/3252];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delay scheduling doesn't work right when jobs have tasks with different locality levels,SPARK-4383,12755138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kayousterhout,kayousterhout,13/Nov/14 19:14,17/May/20 17:47,14/Jul/23 06:26,08/Feb/15 02:18,1.0.2,1.1.0,,,,,,1.3.0,,,,,,Scheduler,Spark Core,,,0,,,,,,"Copied from mailing list discussion:

Now our application will load data from hdfs in the same spark cluster, it will get NODE_LOCAL and RACK_LOCAL level tasks during loading stage, if the tasks in loading stage have same locality level, ether NODE_LOCAL or RACK_LOCAL it works fine.
But if the tasks in loading stage get mixed locality level, such as 3 NODE_LOCAL tasks, and 2 RACK_LOCAL tasks, then the TaskSetManager of loading stage will submit the 3 NODE_LOCAL tasks as soon as resources were offered, then wait for spark.locality.wait.node, which was set to 30 minutes, the 2 RACK_LOCAL tasks will wait 30 minutes even though resources are available.

Fixing this is quite tricky -- do we need to track the locality level individually for each task?",,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4939,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 08 02:17:54 UTC 2015,,,,,,,,,,"0|i22cfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/15 02:17;kayousterhout;This issue was mostly fixed by the fix for 4939.  The specific issue discussed in the description has been fixed.  If a task set has, say, node-local and rack-local tasks, it's still the case that none of the rack-local tasks will be scheduled until all of the node-local ones have been scheduled, but with Davies' fix for 4939, one all of the node-local tasks have been scheduled, we'll immediately start scheduling the rack-local ones rather than waiting for a timer to expire.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperPersistenceEngine: java.lang.IllegalStateException: Trying to deserialize a serialized ActorRef without an ActorSystem in scope.,SPARK-4377,12754981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,joshrosen,joshrosen,13/Nov/14 05:46,07/Feb/20 17:23,14/Jul/23 06:26,23/Nov/14 02:40,1.3.0,,,,,,,1.3.0,,,,,,Deploy,Spark Core,,,0,,,,,,"It looks like ZooKeeperPersistenceEngine is broken in the current Spark master (23f5bdf06a388e08ea5a69e848f0ecd5165aa481).  Here's a log excerpt from a secondary master when it takes over from a failed primary master:

{code}
14/11/13 04:37:12 WARN ConnectionStateManager: There are no ConnectionStateListeners registered.
14/11/13 04:37:19 INFO Master: Registering worker 172.17.0.223:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:20 INFO Master: Registering worker 172.17.0.224:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:35 INFO Master: Registering worker 172.17.0.223:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:35 INFO Master: Registering worker 172.17.0.224:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:43 INFO Master: Registering worker 172.17.0.224:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:47 INFO Master: Registering worker 172.17.0.223:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:51 INFO Master: Registering worker 172.17.0.224:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:59 INFO Master: Registering worker 172.17.0.223:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:37:59 INFO Master: Registering worker 172.17.0.224:8888 with 8 cores, 984.0 MB RAM
14/11/13 04:38:06 INFO ZooKeeperLeaderElectionAgent: We have gained leadership
14/11/13 04:38:06 WARN ZooKeeperPersistenceEngine: Exception while reading persisted file, deleting
java.io.IOException: java.lang.IllegalStateException: Trying to deserialize a serialized ActorRef without an ActorSystem in scope. Use 'akka.serialization.Serialization.currentSystem.withValue(system) { ... }'
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:988)
	at org.apache.spark.deploy.master.ApplicationInfo.readObject(ApplicationInfo.scala:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:81)
	at org.apache.spark.deploy.master.ZooKeeperPersistenceEngine.deserializeFromFile(ZooKeeperPersistenceEngine.scala:69)
	at org.apache.spark.deploy.master.ZooKeeperPersistenceEngine$$anonfun$read$1.apply(ZooKeeperPersistenceEngine.scala:54)
	at org.apache.spark.deploy.master.ZooKeeperPersistenceEngine$$anonfun$read$1.apply(ZooKeeperPersistenceEngine.scala:54)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.deploy.master.ZooKeeperPersistenceEngine.read(ZooKeeperPersistenceEngine.scala:54)
	at org.apache.spark.deploy.master.ZooKeeperPersistenceEngine.read(ZooKeeperPersistenceEngine.scala:32)
	at org.apache.spark.deploy.master.PersistenceEngine$class.readPersistedData(PersistenceEngine.scala:84)
	at org.apache.spark.deploy.master.ZooKeeperPersistenceEngine.readPersistedData(ZooKeeperPersistenceEngine.scala:32)
	at org.apache.spark.deploy.master.Master$$anonfun$receiveWithLogging$1.applyOrElse(Master.scala:181)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:53)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
	at org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.deploy.master.Master.aroundReceive(Master.scala:48)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.IllegalStateException: Trying to deserialize a serialized ActorRef without an ActorSystem in scope. Use 'akka.serialization.Serialization.currentSystem.withValue(system) { ... }'
	at akka.actor.SerializedActorRef.readResolve(ActorRef.scala:407)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1807)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
	at org.apache.spark.deploy.master.ApplicationInfo$$anonfun$readObject$1.apply$mcV$sp(ApplicationInfo.scala:52)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:985)
	... 44 more
{code}

I found this by running my custom Spark integration tests framework, which has a test that roughly corresponds to Apache Spark's {{FaultToleranceTest}}, specifically the ""single-master-halt"" test.",,apachespark,joshrosen,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 09:59:49 UTC 2014,,,,,,,,,,"0|i22bgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/14 06:14;joshrosen;I re-ran my suite on v1.1.0 and it passed, so this isn't a bug in my tests; looks like a legitimate regression, so this is a 1.2 blocker.;;;","19/Nov/14 00:22;joshrosen;[~prashant], could you take a look at this?  I think that you modified the PersistenceEngine stuff recently.;;;","19/Nov/14 11:16;prashant;Looking at it now.;;;","21/Nov/14 09:59;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/3402;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assembly built with Maven is missing most of repl classes,SPARK-4375,12754946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,sandyr,sandyr,13/Nov/14 01:56,04/Dec/14 04:08,14/Jul/23 06:26,14/Nov/14 22:22,1.2.0,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"In particular, the ones in the split scala-2.10/scala-2.11 directories aren't being added",,apachespark,prashant,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 04 04:08:57 UTC 2014,,,,,,,,,,"0|i22b8v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Nov/14 03:49;sandyr;The issue here is that the activeByDefault Maven option doesn't work as expected: it turns off all default profiles if any profile is given on the command line.  Circumventing this by removing the scala-2.10 profile and setting properties that get overridden when the scala-2.11 profile is turned on is pretty straightforward.  Except in a couple cases:

* When using Scala 2.11, we don't want to build the external/kafka module
* When using Scala 2.11, we don't want the examples module to include the kafka module as a dependency
* When using Scala 2.11, we don't want the examples module to include algebird as a dependency

As far as I can tell, there aren't Maven doesn't let us do these things in any ways that aren't deeply hacky.

For dealing with the first issue, I had talked with Patrick about adding profiles for all of the external modules (including kafka).  An issue with this is that the examples depend on all these modules, so we would need to create separate modules or directories for the examples tied to each of these modules. 

Another thing we could do is add a ""scala-2.10-only"" profile that turns on building the kafka module and turns on the examples and examples dependencies that only work with scala-2.10.
;;;","13/Nov/14 05:36;pwendell;Hey Sandy,

What about the following solution:

1. For the repl case, we make the change you are suggesting and simply drop the need for -Pscala-2.10 to be there explicilty.
2. We no longer include the examples module or the external project modules by default in the build. There is a profile for each of the external projects and a profile for the examples.
2. When building the examples, you need to specify, somewhat pedantically, all of the necessary external sub projects and also -Pscala-2.10 or -Pscala-2.11. We can just give people the exact commands to run for 2.10 and 2.11 examples in the maven docs.

The main benefits I see is that there is no regression for someone doing a package for Scala 2.10 which is the common case. If someone wants to build the examples, they need to go and do a bit of extra work to look up the new command, but it's mostly straightforward. Of course, all of our packages will still have the examples pre-built.;;;","13/Nov/14 05:42;pwendell;One thing we could add onto that to make the user have to write less code is something like this:

{code}
mvn package -Pexamples -Pscala-2.10 -Dexamples-2.10
{code}

The internally we have profiles that are activated by the ""examples-2.10"" property and add the relevant modules that are required by the examples build in 2.10.;;;","13/Nov/14 06:50;sandyr;This all makes sense to me.  Will put up a patch.;;;","13/Nov/14 06:51;prashant;I was inclined towards not removing that option for one reason, that it shows to our new and seasoned users that there is something like scala version that ""NOW"" that we have crossbuilding for scala versions, it needs to passed as a command line option. I am saying this realizing that kafka's exclusion is temporary and so on. About producing a helpful message we can probably break the maven build at validation step saying one of the profile is mandatory (If maven allows this, I think I can try !). Thoughts ?;;;","13/Nov/14 07:25;pwendell;So my favorite option now is just moving the Kafka example to the kafka directory (it will still be on github, in the source code, etc). Then doing Sandy's trick for the repl project. And for the kafka project make it enabled with a flag only.;;;","13/Nov/14 07:31;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3239;;;","04/Dec/14 04:08;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3595;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLlib unit tests failed maven test,SPARK-4373,12754909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,12/Nov/14 23:36,13/Nov/14 02:15,14/Jul/23 06:26,13/Nov/14 02:15,1.2.0,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,We should make sure there is at most one SparkContext running at any time inside the same JVM. Maven initializes all test classes first and then runs tests. So we cannot initialize sc as a member.,,apachespark,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 02:15:53 UTC 2014,,,,,,,,,,"0|i22b1z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Nov/14 23:41;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3235;;;","13/Nov/14 02:15;joshrosen;Issue resolved by pull request 3235
[https://github.com/apache/spark/pull/3235];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make LR and SVM's default parameters consistent in Scala and Python ,SPARK-4372,12754869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,12/Nov/14 19:57,13/Nov/14 21:55,14/Jul/23 06:26,13/Nov/14 21:54,1.2.0,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,0,,,,,,"The current default regParam is 1.0 and regType is claimed to be none in Python (but actually it is l2), while regParam = 0.0 and regType is L2 in Scala. We should make the default values consistent.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 21:54:53 UTC 2014,,,,,,,,,,"0|i22atj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Nov/14 20:08;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3232;;;","13/Nov/14 21:54;mengxr;Issue resolved by pull request 3232
[https://github.com/apache/spark/pull/3232];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit cores used by Netty transfer service based on executor size,SPARK-4370,12754827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilikerps,ilikerps,ilikerps,12/Nov/14 18:13,13/Nov/14 02:58,14/Jul/23 06:26,13/Nov/14 02:58,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Right now, the NettyBlockTransferService uses the total number of cores on the system as the number of threads and buffer arenas to create. The latter is more troubling -- this can lead to significant allocation of extra heap and direct memory in situations where executors are relatively small compared to the whole machine. For instance, on a machine with 32 cores, we will allocate (32 cores * 16MB per arena = 512MB) * 2 for client and server = 1GB direct and heap memory. This can be a huge overhead if you're only using, say, 8 of those cores.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 18:21:57 UTC 2014,,,,,,,,,,"0|i22akf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Nov/14 18:21;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TreeModel.predict does not work with RDD,SPARK-4369,12754820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,12/Nov/14 17:58,12/Nov/14 21:57,14/Jul/23 06:26,12/Nov/14 21:57,1.2.0,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,0,,,,,,"{code}
Stack Trace
-------------
Traceback (most recent call last):
  File ""/home/rprabhu/Coding/github/SDNDDoS/classification/DecisionTree.py"",
line 49, in <module>
    predictions = model.predict(parsedData.map(lambda x: x.features))
  File ""/home/rprabhu/Software/spark/python/pyspark/mllib/tree.py"", line 42,
in predict
    return self.call(""predict"", x.map(_convert_to_vector))
  File ""/home/rprabhu/Software/spark/python/pyspark/mllib/common.py"", line
140, in call
    return callJavaFunc(self._sc, getattr(self._java_model, name), *a)
  File ""/home/rprabhu/Software/spark/python/pyspark/mllib/common.py"", line
117, in callJavaFunc
    return _java2py(sc, func(*args))
  File
""/home/rprabhu/Software/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"",
line 538, in __call__
  File
""/home/rprabhu/Software/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"",
line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o39.predict. Trace:
py4j.Py4JException: Method predict([class
org.apache.spark.api.java.JavaRDD]) does not exist
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
        at py4j.Gateway.invoke(Gateway.java:252)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:207)
        at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 21:57:01 UTC 2014,,,,,,,,,,"0|i22aiv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Nov/14 18:15;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3230;;;","12/Nov/14 21:57;mengxr;Issue resolved by pull request 3230
[https://github.com/apache/spark/pull/3230];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OnlineSummarizer doesn't merge mean correctly,SPARK-4355,12754672,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,12/Nov/14 05:05,09/Mar/15 18:45,14/Jul/23 06:26,09/Mar/15 18:45,1.0.2,1.1.1,1.2.0,,,,,1.0.3,1.1.1,1.2.0,,,,MLlib,,,,0,backport-needed,,,,,It happens when the mean on one side is zero. I will send an PR with some code clean-up.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 23:01:32 UTC 2014,,,,,,,,,,"0|i229m7:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.1,1.2.0,,,,,,,,,"12/Nov/14 06:21;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3220;;;","12/Nov/14 09:50;mengxr;Issue resolved by pull request 3220
[https://github.com/apache/spark/pull/3220];;;","12/Nov/14 09:53;mengxr;Reopen this issue because we haven't fixed branch-1.1 and branch-1.0 yet.;;;","13/Nov/14 19:08;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3251;;;","13/Nov/14 23:36;mengxr;Issue resolved by pull request 3251
[https://github.com/apache/spark/pull/3251];;;","13/Nov/14 23:36;mengxr;branch-1.0 pending;;;","30/Dec/14 23:01;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3850;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.mllib.random conflicts with random module,SPARK-4348,12754594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,11/Nov/14 22:06,13/Jan/15 00:17,14/Jul/23 06:26,13/Nov/14 18:25,1.1.0,1.2.0,,,,,,1.1.0,1.2.0,,,,,MLlib,PySpark,,,0,,,,,,"There are conflict in two cases:

1. random module is used by pyspark.mllib.feature, if the first part of sys.path is not '', then the hack in pyspark/__init__.py will fail to fix the conflict.

2. Run tests in mllib/xxx.py, the '' should be popped out before import anything, or it will fail.

The first one is not fully fixed for user, it will introduce problems in some cases, such as:

{code}
>>> import sys
>>> import sys.insert(0, PATH_OF_MODULE)
>>> import pyspark
>>> # use Word2Vec will fail
{code}

I'd like to rename mllib/random.py as random/_random.py, then in mllib/__init.py

{code}
import pyspark.mllib._random as random
{code}


cc [~mengxr] [~dorx]",,apachespark,davies,dorx,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3662,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 00:17:49 UTC 2015,,,,,,,,,,"0|i2295z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"11/Nov/14 22:25;dorx;I fully support this. It took a lot of hacking just to override the default random module in Python, and it wasn't clear if the override was the ideal solution.;;;","12/Nov/14 01:49;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3216;;;","12/Nov/14 02:02;davies;After some experiments, I found it's more harder than expected, it still need some hack to make it work (see the PR), but I think this hack is safer than before:

1. the rand.py module will not overwrite default random module, so it's safe to run the mllib/xxx.py without hacking, also we do not need hack to use random in mllib package.

2. the RandomModuleHook only installed when user try to import 'pyspark.mllib', it also only works for 'pyspark.mllib.random'.

Note: In order to use default random module, we need 'from __future__ import absolute_import' in the caller module, this also need as more. Without this, 'import random' can be translated as 'from pyspark.mllib import random'.  So, there is a bug in master (Word2Vec);;;","13/Nov/14 18:25;mengxr;Issue resolved by pull request 3216
[https://github.com/apache/spark/pull/3216];;;","14/Nov/14 03:06;mengxr;Note that after this fix, it is very likely that the bytecode file `random.pyc` still sits under `pyspark/mllib`. We need to remove it manually to prevent ""import random"" taking that file. ;;;","12/Jan/15 22:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4011;;;","13/Jan/15 00:17;joshrosen;I've also fixed this in 1.1.2 by backporting the 1.2 patch:

https://github.com/apache/spark/pull/4011;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly log number of iterations in RuleExecutor,SPARK-4333,12754342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,DoingDone9,DoingDone9,11/Nov/14 02:35,14/Nov/14 22:28,14/Jul/23 06:26,14/Nov/14 22:28,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"RuleExecutor breaks, num of iteration should be $(iteration -1) not $(iteration) .
Log looks like ""Fixed point reached for batch $(batch.name) after 3 iterations."", but it did 2 iterations really!",,apachespark,DoingDone9,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4401,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 22:28:31 UTC 2014,,,,,,,,,,"0|i227p3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/14 02:42;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/3180;;;","14/Nov/14 22:28;marmbrus;Issue resolved by pull request 3180
[https://github.com/apache/spark/pull/3180];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unidoc is broken on master,SPARK-4326,12754257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,10/Nov/14 21:45,13/Nov/14 21:18,14/Jul/23 06:26,13/Nov/14 21:18,1.3.0,,,,,,,1.2.0,,,,,,Build,Documentation,,,0,,,,,,"On master, `jekyll build` throws the following error:

{code}
[error] /Users/meng/src/spark/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala:205: value hashInt is not a member of com.google.common.hash.HashFunction
[error]   private def rehash(h: Int): Int = Hashing.murmur3_32().hashInt(h).asInt()
[error]                                                          ^
[error] /Users/meng/src/spark/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala:426: value limit is not a member of object com.google.common.io.ByteStreams
[error]         val bufferedStream = new BufferedInputStream(ByteStreams.limit(fileStream, end - start))
[error]                                                                  ^
[error] /Users/meng/src/spark/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala:558: value limit is not a member of object com.google.common.io.ByteStreams
[error]         val bufferedStream = new BufferedInputStream(ByteStreams.limit(fileStream, end - start))
[error]                                                                  ^
[error] /Users/meng/src/spark/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala:261: value hashInt is not a member of com.google.common.hash.HashFunction
[error]   private def hashcode(h: Int): Int = Hashing.murmur3_32().hashInt(h).asInt()
[error]                                                            ^
[error] /Users/meng/src/spark/core/src/main/scala/org/apache/spark/util/collection/Utils.scala:37: type mismatch;
[error]  found   : java.util.Iterator[T]
[error]  required: Iterable[?]
[error]     collectionAsScalaIterable(ordering.leastOf(asJavaIterator(input), num)).iterator
[error]                                                              ^
[error] /Users/meng/src/spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetTableOperations.scala:421: value putAll is not a member of com.google.common.cache.Cache[org.apache.hadoop.fs.FileStatus,parquet.hadoop.Footer]
[error]           footerCache.putAll(newFooters)
[error]                       ^
[warn] /Users/meng/src/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala:34: @deprecated now takes two arguments; see the scaladoc.
[warn] @deprecated(""No code should depend on FakeParquetHiveSerDe as it is only intended as a "" +
[warn]  ^
[info] No documentation generated with unsucessful compiler run
[warn] two warnings found
[error] 6 errors found
[error] (spark/scalaunidoc:doc) Scaladoc generation failed
[error] Total time: 48 s, completed Nov 10, 2014 1:31:01 PM
{code}

It doesn't happen on branch-1.2.",,apachespark,donnchadh,ilikerps,mengxr,nchammas,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 20:22:12 UTC 2014,,,,,,,,,,"0|i2278n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"11/Nov/14 10:50;srowen;Hm. {{hashInt}} isn't in Guava 11, but is in 12. This leads me to believe that unidoc is picking up Guava 11 from Hadoop, and not Guava 14 from Spark since it's shaded. I would like to phone a friend: [~vanzin];;;","11/Nov/14 21:58;vanzin;Funny that it doesn't happen on 1.2 since the dependency mess should be the same in both. I'll try this out when I'm done with some other tests, to see if I can figure it out.;;;","11/Nov/14 22:11;nchammas;Side question: Should we be (or are we already) regularly building the docs to catch these problems at PR/review time?;;;","12/Nov/14 07:30;mengxr;I was wondering why it worked on 1.2, and then I found unidoc on branch-1.2 was also broken (same error). Maybe I didn't update my local branch-1.2 when I created this JIRA.

I think we are adding unidoc to nightly Jenkins builds [~joshrosen].;;;","13/Nov/14 00:46;vanzin;So, this is really weird. Unidoc is run by the sbt build, where none of the shading shenanigans from the maven build should apply. The root pom.xml adds guava as a dependency for everybody with compile scope when the sbt profile is enabled.

That being said, if you look at the output of {{show allDependencies}} from within an sbt shell, it will show some components with a ""guava 11.0.2 provided"" dependency. So the profile isn't taking?

Another fun fact is that the dependencies for the core project, where the errors above come from, are correct in the output of {{show allDependencies}}; it shows ""guava 14.0.1 compile"" as it should.

I was able to workaround this by adding guava explicitly in SparkBuild.scala, in the {{sharedSettings}} variable:

{code}
    libraryDependencies += ""com.google.guava"" % ""guava"" % ""14.0.1""
{code}

That got rid of the above errors, but it didn't fix the overall build. Anyone more familiar with sbt/unidoc knows what's going on here?

Here are the errors with that hack applied:

{noformat}
[error] /work/apache/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/UploadBlock.java:55: not found: type Type
[error]   protected Type type() { return Type.UPLOAD_BLOCK; }
[error]             ^
[error] /work/apache/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/RegisterExecutor.java:44: not found: type Type
[error]   protected Type type() { return Type.REGISTER_EXECUTOR; }
[error]             ^
[error] /work/apache/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/OpenBlocks.java:40: not found: type Type
[error]   protected Type type() { return Type.OPEN_BLOCKS; }
[error]             ^
[error] /work/apache/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/StreamHandle.java:39: not found: type Type
[error]   protected Type type() { return Type.STREAM_HANDLE; }
[error]             ^
{noformat}
;;;","13/Nov/14 01:25;mengxr;[~vanzin] Thanks for looking into this issue! This is the commit that caused the problem:

SPARK-3796: https://github.com/apache/spark/commit/f55218aeb1e9d638df6229b36a59a15ce5363482

It adds Guava 11.0.1 in the pom, which is perhaps not the correct way to specify Guava version. [~adav] Could you explain which Guava version you need per Hadoop profile?;;;","13/Nov/14 03:11;vanzin;Hmm, but core/pom.xml defines an explicit dependency on guava 14, so it should override the 11.0.2 dependency from the shuffle module (which is correct, btw). And maven's / sbt's dependency resolution seems to indicate that's happening, although unidoc doesn't. That's the weird part. Maybe some bug in the unidoc plugin?;;;","13/Nov/14 04:23;ilikerps;[~mengxr] perhaps you pointed to the wrong commit, here I added guava 11.0.2 specifically:
https://github.com/apache/spark/commit/4c42986cc070d9c5c55c7bf8a2a67585967b1082#diff-e1e0dc857976f8b64c5f3a99435ff947R53

I can't explain why this would be causing this issue, as the dependency should be overridden as Marcelo said. However, this dependency is actually not necessary, it's only to avoid people writing code that won't compile in YARN or other Hadoop versions. We could just remove it and find some other way to deal with people possibly writing invalid code.

The second issue regarding compile errors in the shuffle protocol is even weirder, as that shouldn't require any dependencies and is simply a java compiler thing. Maybe it doesn't compile on a sufficiently old JDK? Perhaps an incorrect version of SBT?;;;","13/Nov/14 20:21;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3253;;;","13/Nov/14 20:22;mengxr;Sorry, I pointed to the wrong commit ... I just sent a PR to fix this. Please help review and see whether it causes other problems. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Struct fields can't be used as sub-expression of grouping fields,SPARK-4322,12754227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,10/Nov/14 19:09,19/Nov/14 19:04,14/Jul/23 06:26,14/Nov/14 23:11,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Some examples:
{code}
sqlContext.jsonRDD(sc.parallelize(""""""{""a"": {""b"": [{""c"": 1}]}}"""""" :: Nil)).registerTempTable(""data1"")
sqlContext.sql(""SELECT a.b[0].c FROM data1 GROUP BY a.b[0].c"").collect()

sqlContext.jsonRDD(sc.parallelize(""""""{""a"": {""b"": 1}}"""""" :: Nil)).registerTempTable(""data2"")
sqlContext.sql(""SELECT a.b + 1 FROM data2 GROUP BY a.b + 1"").collect()
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4296,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 23:11:28 UTC 2014,,,,,,,,,,"0|i22727:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Nov/14 17:59;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3248;;;","14/Nov/14 23:11;marmbrus;Issue resolved by pull request 3248
[https://github.com/apache/spark/pull/3248];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix empty sum distinct.,SPARK-4318,12754158,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,10/Nov/14 15:07,20/Nov/14 23:43,14/Jul/23 06:26,20/Nov/14 23:43,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,Executing sum distinct for empty table throws {{java.lang.UnsupportedOperationException: empty.reduceLeft}}.,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 23:43:43 UTC 2014,,,,,,,,,,"0|i226nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/14 15:11;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3184;;;","20/Nov/14 23:43;marmbrus;Issue resolved by pull request 3184
[https://github.com/apache/spark/pull/3184];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark pickling of pyspark.sql.Row objects is extremely inefficient,SPARK-4315,12754110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,adam_d,adam_d,10/Nov/14 10:43,09/Jul/15 00:04,14/Jul/23 06:26,09/Jul/15 00:04,1.1.0,,,,,,,1.3.2,1.4.0,,,,,PySpark,,,,0,,,,,,"Working with an RDD of pyspark.sql.Row objects, created by reading a file with SQLContext in a local PySpark context.

Operations on the RDD, such as: data.groupBy(lambda x: x.field_name) are extremely slow (more than 10x slower than an equivalent Scala/Spark implementation). Obviously I expected it to be somewhat slower, but I did a bit of digging given the difference was so huge.

Luckily it's fairly easy to add profiling to the Python workers. I see that the vast majority of time is spent in:

spark-1.1.0-bin-cdh4/python/pyspark/sql.py:757(_restore_object)

It seems that this line attempts to accelerate pickling of Rows with the use of a cache. Some debugging reveals that this cache becomes quite big (100s of entries). Disabling the cache by adding:

return _create_cls(dataType)(obj)

as the first line of _restore_object made my query run 5x faster. Implying that the caching is not providing the desired acceleration...","Ubuntu, Python 2.7, Spark 1.1.0",adam_d,davies,lvsoft,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 00:01:46 UTC 2015,,,,,,,,,,"0|i226cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/14 08:14;lvsoft;I'm interested in this issue, but I can't reproduce your problem. 
I constructed a very simple workload according to your description, like this:

from pyspark.sql import SQLContext
from pyspark import SparkContext

sc = SparkContext(appName=""test"")
sqlContext = SQLContext(sc)

lines = sc.parallelize(range(1000000), 12)
people = lines.map(lambda x:{""name"": str(x % 1000), ""age"":x})

schemaPeople = sqlContext.inferSchema(people)
schemaPeople.registerAsTable(""people"")

grouped = schemaPeople.groupBy(lambda x:x.name)
grouped.collect()

And tested over spark-1.1(2f9b2bd) and spark-master(0fe54cff).
It finished in 3-4 seconds on both spark versions.

After disabled _restore_object's cache ( adding ""return _create_cls(dataType)(obj)"" ), it becomes obviously slow(waited minutes, no need to wait more).

Could you please give me more detailed information?;;;","27/Nov/14 09:23;adam_d;Sure, will try to say what I can. Unfortunately I don't think I can easily give you a sample of the data. If we can't figure it out I can try to produce a fake sample that still exhibits the problem. But first let me try to come up with a few possibly salient differences:

1. My data is very wide, about 80 columns.
2. This size of the resulting groups in the groupBy is very ragged, whereas yours here is very even. Probably exponentially distributed in my case or more extreme. I wonder if this is generating many different Row ""types"" somehow.
3. My Row objects are constructed via the parquet functions of SQLContext

In my debugging I noticed that the cache size was reaching hundreds of entries or more from printing the number of items in the dict.

I'll also include part of the code I was using:

conf = pyspark.SparkConf().setMaster(""local[24]"").setAppName(""test"")
sc = pyspark.SparkContext(conf = conf)
sqlc = pyspark.sql.SQLContext(sc)
data = sqlc.parquetFile(""/home/adam/parquet_test"")

def getnow():
    return int(round(time.time() * 1000))

def applyfunc2(data):
    <some work which returns a list object>

print ""CHECKPOINT 1: %i"" % (getnow())
data.cache()
junk = data.map(lambda x: 0).collect() # This part introduced to separate the timing of disk load and computation
print ""CHECKPOINT 2: %i"" % (getnow())
grouped = data.groupBy(lambda x: x.unique_user_identifier)
print ""CHECKPOINT 3: %i"" % (getnow())
calced = grouped.flatMap(applyfunc2)
print ""CHECKPOINT 4: %i"" % (getnow())
counts = calced.collect()
print ""CHECKPOINT 5: %i"" % (getnow());;;","09/Jul/15 00:01;davies;This is fixed by https://github.com/apache/spark/pull/5445;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Thread Dump"" link is broken in yarn-cluster mode",SPARK-4313,12754076,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,10/Nov/14 07:43,14/Nov/14 21:36,14/Jul/23 06:26,14/Nov/14 21:36,1.2.0,,,,,,,1.2.0,,,,,,Web UI,YARN,,,0,,,,,,"In yarn-cluster mode, the Web UI is running behind a yarn proxy server. Some features(or bugs?) of yarn proxy server will break the links for thread dump.

1. Yarn proxy server will do http redirect internally, so if opening ""http://example.com:8088/cluster/app/application_1415344371838_0012/executors"", it will fetch ""http://example.com:8088/cluster/app/application_1415344371838_0012/executors/"" and return the content but won't change the link in the browser. Then when a user clicks ""Thread Dump"", it will jump to ""http://example.com:8088/proxy/application_1415344371838_0012/threadDump/?executorId=2"". This is a wrong link. The correct link should be ""http://example.com:8088/proxy/application_1415344371838_0012/executors/threadDump/?executorId=2"".

2. Yarn proxy server has a bug about the URL encode/decode. When a user accesses ""http://example.com:8088/proxy/application_1415344371838_0006/executors/threadDump/?executorId=%3Cdriver%3E"", the yarn proxy server will require ""http://example.com:36429/executors/threadDump/?executorId=%25253Cdriver%25253E"". But Spark web server expects ""http://example.com:36429/executors/threadDump/?executorId=%3Cdriver%3E"". I will report this issue to Hadoop community later.",,apachespark,hammer,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,YARN-2844,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 07:48:45 UTC 2014,,,,,,,,,,"0|i22653:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"10/Nov/14 07:48;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bash can't `die`,SPARK-4312,12754062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,10/Nov/14 06:05,10/Nov/14 20:39,14/Jul/23 06:26,10/Nov/14 20:39,1.2.0,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"sbt-launch-lib.bash includes 'die' command but it's not valid command for Linux, MacOS X or Windows.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 06:23:57 UTC 2014,,,,,,,,,,"0|i2261z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"10/Nov/14 06:23;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3182;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Submitted"" column in Stage page doesn't sort by time",SPARK-4310,12754039,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,10/Nov/14 02:29,13/Nov/14 22:37,14/Jul/23 06:26,13/Nov/14 22:37,1.0.0,1.1.0,,,,,,1.2.0,,,,,,Web UI,,,,0,easyfix,,,,,It's sorted in alphabetical order now.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 02:31:31 UTC 2014,,,,,,,,,,"0|i225x3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"10/Nov/14 02:31;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3179;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Date type support missing in HiveThriftServer2,SPARK-4309,12754003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,lian cheng,lian cheng,09/Nov/14 15:24,17/Nov/14 02:31,14/Jul/23 06:26,16/Nov/14 22:27,1.1.1,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,Date type is not supported while retrieving result set in HiveThriftServer2.,,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 02:31:29 UTC 2014,,,,,,,,,,"0|i225pb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"09/Nov/14 16:42;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3178;;;","16/Nov/14 22:27;marmbrus;Issue resolved by pull request 3178
[https://github.com/apache/spark/pull/3178];;;","16/Nov/14 23:04;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3292;;;","17/Nov/14 02:31;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3298;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL operation state is not properly set when exception is thrown,SPARK-4308,12754002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,09/Nov/14 15:11,26/Nov/14 04:04,14/Jul/23 06:26,11/Nov/14 00:57,1.1.1,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,"In {{HiveThriftServer2}}, when an exception is thrown during a SQL execution, the SQL operation state should be set to {{ERROR}}, but now it remains {{RUNNING}}. This affects the result of the GetOperationStatus Thrift API.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 00:57:06 UTC 2014,,,,,,,,,,"0|i225p3:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"09/Nov/14 15:12;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3175;;;","09/Nov/14 15:20;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3176;;;","11/Nov/14 00:57;marmbrus;Issue resolved by pull request 3175
[https://github.com/apache/spark/pull/3175];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn-alpha profile won't build due to network/yarn module,SPARK-4305,12753903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,08/Nov/14 09:24,11/Nov/14 18:31,14/Jul/23 06:26,11/Nov/14 18:31,1.2.0,,,,,,,1.2.0,1.3.0,,,,,,,,,0,,,,,,"SPARK-3797 introduced the {{network/yarn}} module, but its YARN code depends on YARN APIs not present in older versions covered by the {{yarn-alpha}}. As a result builds like {{mvn -Pyarn-alpha -Phadoop-0.23 -Dhadoop.version=0.23.7 -DskipTests clean package}} fail.

The solution is just to not build {{network/yarn}} with profile {{yarn-alpha}}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 08 09:26:04 UTC 2014,,,,,,,,,,"0|i2253j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/14 09:26;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3167;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sortByKey() will fail on empty RDD,SPARK-4304,12753873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,08/Nov/14 01:09,08/Nov/14 04:58,14/Jul/23 06:26,08/Nov/14 04:58,1.0.2,1.1.0,1.2.0,,,,,1.0.3,1.1.1,1.2.0,,,,PySpark,,,,0,,,,,,"{code}
>>> sc.parallelize(zip(range(4), range(0)), 5).sortByKey().count()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 532, in sortByKey
    for i in range(0, numPartitions - 1)]
IndexError: list index out of range
>>>
{code}",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 08 04:58:34 UTC 2014,,,,,,,,,,"0|i224x3:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.1,1.2.0,,,,,,,,,"08/Nov/14 01:21;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3162;;;","08/Nov/14 01:30;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3163;;;","08/Nov/14 04:58;joshrosen;Issue resolved by pull request 3163
[https://github.com/apache/spark/pull/3163];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingContext should not allow start() to be called after calling stop(),SPARK-4301,12753766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,07/Nov/14 18:51,11/Nov/14 13:14,14/Jul/23 06:26,09/Nov/14 02:12,1.0.0,1.0.1,1.0.2,1.1.0,,,,1.0.3,1.1.1,1.2.0,,,,DStreams,,,,0,,,,,,"In Spark 1.0.0+, calling {{stop()}} on a StreamingContext that has not been started is a no-op which has no side-effects.  This allows users to call {{stop()}} on a fresh StreamingContext followed by {{start()}}.  I believe that this almost always indicates an error and is not behavior that we should support.  Since we don't allow {{start() stop() start()}} then I don't think it makes sense to allow {{stop() start()}}.

The current behavior can lead to resource leaks when StreamingContext constructs its own SparkContext: if I call {{stop(stopSparkContext=True)}}, then I expect StreamingContext's underlying SparkContext to be stopped irrespective of whether the StreamingContext has been started.  This is useful when writing unit test fixtures.

Prior discussions:

- https://github.com/apache/spark/pull/3053#discussion-diff-19710333R490
- https://github.com/apache/spark/pull/3121#issuecomment-61927353",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 13:14:01 UTC 2014,,,,,,,,,,"0|i2249r:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"07/Nov/14 19:22;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3160;;;","11/Nov/14 13:14;srowen;Sorry, a bit late, but I noticed this is pretty related to https://issues.apache.org/jira/browse/SPARK-2645 which discusses calling stop() twice.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition during SparkWorker shutdown,SPARK-4300,12753741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,alexliu68,alexliu68,07/Nov/14 17:18,01/Aug/18 10:35,14/Jul/23 06:26,13/Mar/15 17:55,1.1.0,,,,,,,1.2.2,1.3.1,1.4.0,,,,Spark Shell,,,,0,,,,,,"When a shark job is done. there are some error message as following show in the log

{code}
INFO 22:10:41,635 SparkMaster: akka.tcp://sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
 INFO 22:10:41,640 SparkMaster: Removing app app-20141106221014-0000
 INFO 22:10:41,687 SparkMaster: Removing application Shark::ip-172-31-11-204.us-west-1.compute.internal
 INFO 22:10:41,710 SparkWorker: Asked to kill executor app-20141106221014-0000/0
 INFO 22:10:41,712 SparkWorker: Runner thread for executor app-20141106221014-0000/0 interrupted
 INFO 22:10:41,714 SparkWorker: Killing process!
ERROR 22:10:41,738 SparkWorker: Error writing stream to file /var/lib/spark/work/app-20141106221014-0000/0/stdout
ERROR 22:10:41,739 SparkWorker: java.io.IOException: Stream closed
ERROR 22:10:41,739 SparkWorker: 	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
ERROR 22:10:41,740 SparkWorker: 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
ERROR 22:10:41,740 SparkWorker: 	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
ERROR 22:10:41,740 SparkWorker: 	at java.io.FilterInputStream.read(FilterInputStream.java:107)
ERROR 22:10:41,741 SparkWorker: 	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
ERROR 22:10:41,741 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
ERROR 22:10:41,741 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
ERROR 22:10:41,742 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
ERROR 22:10:41,742 SparkWorker: 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
ERROR 22:10:41,742 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
 INFO 22:10:41,838 SparkMaster: Connected to Cassandra cluster: 4299
 INFO 22:10:41,839 SparkMaster: Adding host 172.31.11.204 (Analytics)
 INFO 22:10:41,840 SparkMaster: New Cassandra host /172.31.11.204:9042 added
 INFO 22:10:41,841 SparkMaster: Adding host 172.31.11.204 (Analytics)
 INFO 22:10:41,842 SparkMaster: Adding host 172.31.11.204 (Analytics)
 INFO 22:10:41,852 SparkMaster: akka.tcp://sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
 INFO 22:10:41,853 SparkMaster: akka.tcp://sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
 INFO 22:10:41,853 SparkMaster: akka.tcp://sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
 INFO 22:10:41,857 SparkMaster: akka.tcp://sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
 INFO 22:10:41,862 SparkMaster: Adding host 172.31.11.204 (Analytics)
 WARN 22:10:42,200 SparkMaster: Got status update for unknown executor app-20141106221014-0000/0
 INFO 22:10:42,211 SparkWorker: Executor app-20141106221014-0000/0 finished with state KILLED exitStatus 143
{code}

/var/lib/spark/work/app-20141106221014-0000/0/stdout is on the disk. It is trying to write to a close IO stream. 

Spark worker shuts down by {code}
 private def killProcess(message: Option[String]) {
    var exitCode: Option[Int] = None
    logInfo(""Killing process!"")
    process.destroy()
    process.waitFor()
    if (stdoutAppender != null) {
      stdoutAppender.stop()
    }
    if (stderrAppender != null) {
      stderrAppender.stop()
    }
    if (process != null) {
    exitCode = Some(process.waitFor())
    }
    worker ! ExecutorStateChanged(appId, execId, state, message, exitCode)
 
{code}

But stdoutAppender concurrently writes to output log file, which creates race condition. ",,alexliu68,apachespark,lior.c@taboola.com,liqingan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9844,,SPARK-9844,,,,,,,,,,,,,,"01/Aug/18 10:34;liqingan;B{~TP2PW~}1TYA2AG{CA41H.png;https://issues.apache.org/jira/secure/attachment/12933898/B%7B%7ETP2PW%7E%7D1TYA2AG%7BCA41H.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 01 10:26:42 UTC 2018,,,,,,,,,,"0|i2244f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/15 10:49;srowen;Is this specific to shark? I don't imagine so. It seems like the appender has to stop before the process is destroyed, since the appender can't keep reading its stdout/stderr after it is destroyed. I'll try a PR.;;;","26/Feb/15 10:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4787;;;","29/Jul/15 01:23;alexliu68;We find this issue still exists in 1.3.1

{code}
ERROR [Thread-6] 2015-07-28 22:49:57,653 SparkWorker-0 ExternalLogger.java:96 - Error writing stream to file /var/lib/spark/worker/worker-0/app-20150728224954-0003/0/stderr
ERROR [Thread-6] 2015-07-28 22:49:57,653 SparkWorker-0 ExternalLogger.java:96 - java.io.IOException: Stream closed
ERROR [Thread-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170) ~[na:1.8.0_40]
ERROR [Thread-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283) ~[na:1.8.0_40]
ERROR [Thread-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_40]
ERROR [Thread-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.FilterInputStream.read(FilterInputStream.java:107) ~[na:1.8.0_40]
ERROR [Thread-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70) ~[spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [Thread-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [Thread-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [Thread-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [Thread-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [Thread-6] 2015-07-28 22:49:57,656 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
{code}

at  https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala#L159

The process auto shuts down, but the log appenders are still running, which causes the error log messages.;;;","10/Aug/15 14:06;lior.c@taboola.com;Also exists in spark 1.4:

{panel}
12:31:10.821 [File appending thread for /var/lib/spark/data/disk1/app-20150809122638-0000/13/stdout] ERROR org.apache.spark.util.logging.FileAppender - Error writing stream to file /var/lib/spark/data/disk1/app-2015080
9122638-0000/13/stdout
java.io.IOException: Stream closed
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:145) ~[?:1.6.0_41]
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:255) ~[?:1.6.0_41]
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317) ~[?:1.6.0_41]
        at java.io.FilterInputStream.read(FilterInputStream.java:90) ~[?:1.6.0_41]
        at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
{panel}

And later on I see:
{panel}
12:22:30.861 [sparkWorker-akka.actor.default-dispatcher-2] ERROR akka.actor.ActorSystemImpl - Uncaught fatal error from thread [sparkWorker-akka.remote.default-remote-dispatcher-5] shutting down ActorSystem [sparkWorker]
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at org.spark_project.protobuf.ByteString.copyFrom(ByteString.java:192) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at org.spark_project.protobuf.ByteString.copyFrom(ByteString.java:204) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.serialization.MessageContainerSerializer.serializeSelection(MessageContainerSerializer.scala:36) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.serialization.MessageContainerSerializer.toBinary(MessageContainerSerializer.scala:25) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:844) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.EndpointWriter.writeSend(Endpoint.scala:747) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:722) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) ~[spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.actor.ActorCell.invoke(ActorCell.scala:487) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.dispatch.Mailbox.run(Mailbox.scala:220) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0]
Exception in thread ""qtp1853216600-31"" java.lang.OutOfMemoryError: GC overhead limit exceeded
        at org.spark-project.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:708)
        at org.spark-project.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
        at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:662)Exception in thread ""qtp1853216600-37"" java.lang.OutOfMemoryError: GC overhead limit exceeded
{panel}

In application log I see:
{panel}
2015-08-10 12:41:33,761 WARN  [task-result-getter-0] TaskSetManager - Lost task 165.2 in stage 207.7 (TID 141815, 10.10.0.83): java.io.IOException: Failed to connect to /10.10.0.67:42846
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
        at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
{panel};;;","11/Aug/15 20:47;alexliu68;I created SPARK-9844 for the unsolved issue.;;;","01/Aug/18 10:26;liqingan;i feel upset for this issue ! (n)
|Uncaught fatal error from thread [sparkWorker-akka.actor.default-dispatcher-56] shutting down ActorSystem [sparkWorker]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.Arrays.copyOfRange(Arrays.java:2694)
	at java.lang.String.<init>(String.java:203)
	at java.lang.StringBuilder.toString(StringBuilder.java:405)
	at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3068)
	at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:2864)
	at java.io.ObjectInputStream.readString(ObjectInputStream.java:1638)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1341)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)
	at akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
	at scala.util.Try$.apply(Try.scala:161)
	at akka.serialization.Serialization.deserialize(Serialization.scala:98)
	at akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
	at akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:55)
	at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:55)
	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:73)
	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:764)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)|
|早上2点12:12.751|ERROR|org.apache.spark.util.logging.FileAppender|Error writing stream to file /hadoop/var/run/spark/work/app-20180727141925-0019/38075/stderr
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1468)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)|
|早上2点12:12.752|ERROR|org.apache.spark.util.logging.FileAppender|Error writing stream to file /hadoop/var/run/spark/work/app-20180727142159-0032/30823/stderr
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1468)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)|;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The spark-submit cannot read Main-Class from Manifest.,SPARK-4298,12753701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,boyork,straka,straka,07/Nov/14 14:11,13/Mar/20 09:10,14/Jul/23 06:26,31/Dec/14 20:02,1.1.0,,,,,,,1.0.3,1.1.2,1.2.1,1.3.0,,,Spark Core,,,,0,,,,,,"Consider trivial {{test.scala}}:
{code:title=test.scala|borderStyle=solid}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

object Main {
  def main(args: Array[String]) {
    val sc = new SparkContext()
    sc.stop()
  }
}
{code}

When built with {{sbt}} and executed using {{spark-submit target/scala-2.10/test_2.10-1.0.jar}}, I get the following error:
{code}
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Error: Cannot load main class from JAR: file:/ha/home/straka/s/target/scala-2.10/test_2.10-1.0.jar
Run with --help for usage help or --verbose for debug output
{code}

When executed using {{spark-submit --class Main target/scala-2.10/test_2.10-1.0.jar}}, it works.

The jar file has correct MANIFEST.MF:
{code:title=MANIFEST.MF|borderStyle=solid}
Manifest-Version: 1.0
Implementation-Vendor: test
Implementation-Title: test
Implementation-Version: 1.0
Implementation-Vendor-Id: test
Specification-Vendor: test
Specification-Title: test
Specification-Version: 1.0
Main-Class: Main
{code}

The problem is that in {{org.apache.spark.deploy.SparkSubmitArguments}}, line 127:
{code}
  val jar = new JarFile(primaryResource)
{code}
the primaryResource has String value {{""file:/ha/home/straka/s/target/scala-2.10/test_2.10-1.0.jar""}}, which is URI, but JarFile can use only Path. One way to fix this would be using
{code}
  val uri = new URI(primaryResource)
  val jar = new JarFile(uri.getPath)
{code}","Linux
spark-1.1.0-bin-hadoop2.4.tgz
java version ""1.7.0_72""
Java(TM) SE Runtime Environment (build 1.7.0_72-b14)
Java HotSpot(TM) 64-Bit Server VM (build 24.72-b04, mixed mode)",apachespark,boyork,Hannahchz,jimit@softpath.net,joshrosen,straka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/20 09:08;Hannahchz;Hannah 6598199927.png;https://issues.apache.org/jira/secure/attachment/12996629/Hannah+6598199927.png","13/Mar/20 09:07;Hannahchz;Screenshot 2020-03-13 at 4.24.02 PM.png;https://issues.apache.org/jira/secure/attachment/12996628/Screenshot+2020-03-13+at+4.24.02+PM.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 09:10:57 UTC 2020,,,,,,,,,,"0|i223vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/14 20:10;boyork;I'll take this one. Can someone assign to me? Thanks!;;;","02/Dec/14 19:49;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/3561;;;","04/Dec/14 16:49;boyork;[~pwendell] could you take a look at this? This is an annoying issue our developers continue to run into and would like to see this pushed into the next release. Thanks!;;;","31/Dec/14 20:02;joshrosen;This was fixed by [~boyork]'s PR, which I've merged into all of the maintenance branches.;;;","24/Apr/16 00:25;jimit@softpath.net;Brennon / Friends,

Any chance this issue is open in Spark 1.5.2 (for Hadoop 2.4)?

I still had to launch my program as follows:
./spark-submit --class problem1 /a/b/c/def.jar

My MANIFEST is a simple 2-liner:

Manifest-Version: 1.0
Main-Class: problem1;;;","13/Mar/20 09:10;Hannahchz;Please assist my issue writing spark output to cassandra on hortonworks sandbox 2.6.5 (img and phone number attached);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Throw ""Expression not in GROUP BY"" when using same expression in group by clause and  select clause",SPARK-4296,12753672,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,zsxwing,zsxwing,07/Nov/14 11:19,21/Mar/17 22:01,14/Jul/23 06:26,20/Jan/15 22:01,1.1.0,1.1.1,1.2.0,,,,,1.2.1,1.3.0,,,,,SQL,,,,1,,,,,,"When the input data has a complex structure, using same expression in group by clause and  select clause will throw ""Expression not in GROUP BY"".

{code:java}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.createSchemaRDD
case class Birthday(date: String)
case class Person(name: String, birthday: Birthday)
val people = sc.parallelize(List(Person(""John"", Birthday(""1990-01-22"")), Person(""Jim"", Birthday(""1980-02-28""))))
people.registerTempTable(""people"")
val year = sqlContext.sql(""select count(*), upper(birthday.date) from people group by upper(birthday.date)"")
year.collect
{code}

Here is the plan of year:
{code:java}
SchemaRDD[3] at RDD at SchemaRDD.scala:105
== Query Plan ==
== Physical Plan ==
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Expression not in GROUP BY: Upper(birthday#1.date AS date#9) AS c1#3, tree:
Aggregate [Upper(birthday#1.date)], [COUNT(1) AS c0#2L,Upper(birthday#1.date AS date#9) AS c1#3]
 Subquery people
  LogicalRDD [name#0,birthday#1], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:36
{code}

The bug is the equality test for `Upper(birthday#1.date)` and `Upper(birthday#1.date AS date#9)`.

Maybe Spark SQL needs a mechanism to compare Alias expression and non-Alias expression.",,apachespark,dyross,gvramana,irinatruong,lian cheng,marmbrus,pwendell,tridib,yhuai,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4322,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 21:58:20 UTC 2017,,,,,,,,,,"0|i223pj:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"07/Nov/14 11:20;zsxwing;Stack trace:
{code:java}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Expression not in GROUP BY: Upper(birthday#11.date AS date#17) AS c1#13, tree:
Aggregate [Upper(birthday#11.date)], [COUNT(1) AS c0#12L,Upper(birthday#11.date AS date#17) AS c1#13]
 Subquery people
  LogicalRDD [name#10,birthday#11], MapPartitionsRDD[5] at mapPartitions at ExistingRDD.scala:36

	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3$$anonfun$applyOrElse$7.apply(Analyzer.scala:133)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3$$anonfun$applyOrElse$7.apply(Analyzer.scala:130)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3.applyOrElse(Analyzer.scala:130)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3.applyOrElse(Analyzer.scala:115)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$.apply(Analyzer.scala:115)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$.apply(Analyzer.scala:113)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:34)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)

{code};;;","07/Nov/14 11:25;zsxwing;Original reported by Tridib Samanta at http://apache-spark-user-list.1001560.n3.nabble.com/sql-group-by-on-UDF-not-working-td18339.html;;;","07/Nov/14 17:36;tridib;I wish we can use alias of calculated column in group by clause, which will avoid specifying long calculated fields to be repeated.;;;","10/Nov/14 02:50;gvramana;Alias are being added implicitly to structure fields in group by for aggregate expressions, so aggregate expression comparison to group by expression is failing.
So Upper(birthday#11.date AS date#17) is compared against Upper(birthday#11.date);;;","19/Nov/14 19:04;marmbrus;I think this was fixed by the linked issue.  Please reopen if I am wrong.;;;","17/Dec/14 21:39;dyross;I can still reproduce this issue. The test case above does appear to be fixed, but if you use other types of agg functions, it can fail. For example:

{code}
CREATE TABLE test_spark_4296(s STRING);
SELECT UPPER(s) FROM test_spark_4296 GROUP BY UPPER(s);
{code}

That works. But this query doesn't:

{code}
SELECT REGEXP_EXTRACT(s, "".*"", 1) FROM test_spark_4296 GROUP BY REGEXP_EXTRACT(s, "".*"", 1);
{code}

The error is similar to the one above:

{code}
14/12/17 21:39:22 INFO thriftserver.SparkExecuteStatementOperation: Running query 'SELECT REGEXP_EXTRACT(s, "".*"", 1) FROM test_spark_4296 GROUP BY REGEXP_EXTRACT(s, "".*"", 1)'
14/12/17 21:39:22 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on slave0:50816 in memory (size: 5.2 KB, free: 534.4 MB)
14/12/17 21:39:22 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on slave1:45411 in memory (size: 5.2 KB, free: 534.4 MB)
14/12/17 21:39:22 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on slave2:59650 in memory (size: 5.2 KB, free: 534.4 MB)
14/12/17 21:39:22 INFO storage.BlockManager: Removing broadcast 7
14/12/17 21:39:22 INFO storage.BlockManager: Removing block broadcast_7_piece0
14/12/17 21:39:22 INFO storage.MemoryStore: Block broadcast_7_piece0 of size 5308 dropped from memory (free 276233416)
14/12/17 21:39:22 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on master:34621 in memory (size: 5.2 KB, free: 265.0 MB)
14/12/17 21:39:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
14/12/17 21:39:22 INFO storage.BlockManager: Removing block broadcast_7
14/12/17 21:39:22 INFO storage.MemoryStore: Block broadcast_7 of size 9344 dropped from memory (free 276242760)
14/12/17 21:39:22 INFO spark.ContextCleaner: Cleaned broadcast 7
14/12/17 21:39:22 INFO parse.ParseDriver: Parsing command: SELECT REGEXP_EXTRACT(s, "".*"", 1) FROM test_spark_4296 GROUP BY REGEXP_EXTRACT(s, "".*"", 1)
14/12/17 21:39:22 INFO parse.ParseDriver: Parse Completed
14/12/17 21:39:22 INFO spark.ContextCleaner: Cleaned shuffle 1
14/12/17 21:39:22 INFO storage.BlockManager: Removing broadcast 6
14/12/17 21:39:22 INFO storage.BlockManager: Removing block broadcast_6_piece0
14/12/17 21:39:22 INFO storage.MemoryStore: Block broadcast_6_piece0 of size 47235 dropped from memory (free 276289995)
14/12/17 21:39:22 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on master:34621 in memory (size: 46.1 KB, free: 265.0 MB)
14/12/17 21:39:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
14/12/17 21:39:22 INFO storage.BlockManager: Removing block broadcast_6
14/12/17 21:39:22 INFO storage.MemoryStore: Block broadcast_6 of size 523775 dropped from memory (free 276813770)
14/12/17 21:39:22 INFO spark.ContextCleaner: Cleaned broadcast 6
14/12/17 21:39:22 INFO storage.BlockManager: Removing broadcast 5
14/12/17 21:39:22 INFO storage.BlockManager: Removing block broadcast_5_piece0
14/12/17 21:39:22 INFO storage.MemoryStore: Block broadcast_5_piece0 of size 7179 dropped from memory (free 276820949)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on master:34621 in memory (size: 7.0 KB, free: 265.0 MB)
14/12/17 21:39:23 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
14/12/17 21:39:23 INFO storage.BlockManager: Removing block broadcast_5
14/12/17 21:39:23 INFO storage.MemoryStore: Block broadcast_5 of size 12784 dropped from memory (free 276833733)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on slave0:50816 in memory (size: 7.0 KB, free: 534.4 MB)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on slave1:45411 in memory (size: 7.0 KB, free: 534.4 MB)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on slave2:59650 in memory (size: 7.0 KB, free: 534.4 MB)
14/12/17 21:39:23 INFO spark.ContextCleaner: Cleaned broadcast 5
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on slave1:45411 in memory (size: 7.9 KB, free: 534.4 MB)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on slave2:59650 in memory (size: 7.9 KB, free: 534.4 MB)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on slave0:50816 in memory (size: 7.9 KB, free: 534.4 MB)
14/12/17 21:39:23 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query:
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Expression not in GROUP BY: HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFRegExpExtract(s#609,.*,1) AS _c0#608, tree:
Aggregate [HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFRegExpExtract(s#609,.*,1)], [HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFRegExpExtract(s#609,.*,1) AS _c0#608]
 MetastoreRelation as_adventure, test_spark_4296, None

	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3$$anonfun$applyOrElse$7.apply(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3$$anonfun$applyOrElse$7.apply(Analyzer.scala:126)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3.applyOrElse(Analyzer.scala:126)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$$anonfun$apply$3.applyOrElse(Analyzer.scala:109)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$.apply(Analyzer.scala:109)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckAggregation$.apply(Analyzer.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:34)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:412)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:412)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:413)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:413)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:418)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:416)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:422)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:422)
	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:444)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:181)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
14/12/17 21:39:23 WARN thrift.ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Expression not in GROUP BY: HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFRegExpExtract(s#609,.*,1) AS _c0#608, tree:
Aggregate [HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFRegExpExtract(s#609,.*,1)], [HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFRegExpExtract(s#609,.*,1) AS _c0#608]
 MetastoreRelation as_adventure, test_spark_4296, None

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:192)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
14/12/17 21:39:23 INFO storage.BlockManager: Removing broadcast 4
14/12/17 21:39:23 INFO storage.BlockManager: Removing block broadcast_4_piece0
14/12/17 21:39:23 INFO storage.MemoryStore: Block broadcast_4_piece0 of size 8076 dropped from memory (free 276841809)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on master:34621 in memory (size: 7.9 KB, free: 265.0 MB)
14/12/17 21:39:23 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
14/12/17 21:39:23 INFO storage.BlockManager: Removing block broadcast_4
14/12/17 21:39:23 INFO storage.MemoryStore: Block broadcast_4 of size 14488 dropped from memory (free 276856297)
14/12/17 21:39:23 INFO spark.ContextCleaner: Cleaned broadcast 4
14/12/17 21:39:23 INFO spark.ContextCleaner: Cleaned shuffle 0
14/12/17 21:39:23 INFO storage.BlockManager: Removing broadcast 3
14/12/17 21:39:23 INFO storage.BlockManager: Removing block broadcast_3_piece0
14/12/17 21:39:23 INFO storage.MemoryStore: Block broadcast_3_piece0 of size 47609 dropped from memory (free 276903906)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on master:34621 in memory (size: 46.5 KB, free: 265.1 MB)
14/12/17 21:39:23 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
14/12/17 21:39:23 INFO storage.BlockManager: Removing block broadcast_3
14/12/17 21:39:23 INFO storage.MemoryStore: Block broadcast_3 of size 524287 dropped from memory (free 277428193)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on slave0:50816 in memory (size: 46.5 KB, free: 534.5 MB)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on slave2:59650 in memory (size: 46.5 KB, free: 534.5 MB)
14/12/17 21:39:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on slave1:45411 in memory (size: 46.5 KB, free: 534.5 MB)
14/12/17 21:39:23 INFO spark.ContextCleaner: Cleaned broadcast 3
{code};;;","17/Dec/14 22:05;marmbrus;David, is that using Spark 1.2?;;;","17/Dec/14 22:18;dyross;Hi Michael, We are trunk: {{1.3.0-SNAPSHOT}}, as of https://github.com/apache/spark/commit/3d0c37b8118f6057a663f959321a79b8061132b6;;;","05/Jan/15 18:36;lian cheng;This issue is a duplicate of SPARK-4322, which has already been fixed in 1.2.0.;;;","05/Jan/15 18:39;lian cheng;Sorry, missed David's comment below.;;;","06/Jan/15 19:15;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3910;;;","12/Jan/15 03:38;yhuai;[~lian cheng] Seems this issues is similar with [this one|https://issues.apache.org/jira/browse/SPARK-2063?focusedCommentId=14055193&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14055193]. The main problem is that we use the last part of a reference of a field in a struct as the alias. Is it possible that we can fix that one as well?;;;","12/Jan/15 03:44;yhuai;I was wondering if we can also find this issue at other places. Maybe we can resolve this issue thoroughly.;;;","12/Jan/15 21:49;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4010;;;","14/Jan/15 00:39;lian cheng;Yeah, I think whenever we use expressions that are not {{NamedExpression}} in GROUP BY, this issue may be triggered, because an intermediate alias is introduced during analysis phase. That's why I tried to fix all similar aliases in PR #3910 (but failed).;;;","21/Jan/15 06:50;pwendell;Note this was fixed in https://github.com/apache/spark/pull/3987 in the 1.2 branch (per discussion with [~lian cheng]).;;;","21/Mar/17 21:58;irinatruong;I have the same exception with pyspark when my expression uses a compiled and registered Scala UDF. This is how it's registered:


{noformat}
    sqlContext.registerJavaFunction(""round_date"", 'my.package.RoundDate')
{noformat}


And this is how it's called:

{noformat}
ipdb> sqlContext.sql(""SELECT round_date(t.ts, '1day') from (select timestamp('2017-02-02T10:11:12') as ts union select timestamp('2017-02-02T10:19:00') as ts) as t group by round_date(t.ts, '1day')"").show()
*** AnalysisException: u""expression 't.`ts`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\nAggregate [UDF(ts#80, 1day)], [UDF(ts#80, 1day) AS UDF(ts, 1day)#82]\n+- SubqueryAlias t\n   +- Distinct\n      +- Union\n         :- Project [cast(2017-02-02T10:11:12 as timestamp) AS ts#80]\n         :  +- OneRowRelation$\n         +- Project [cast(2017-02-02T10:19:00 as timestamp) AS ts#81]\n            +- OneRowRelation$\n""
{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[External]Exception throws in SparkSinkSuite although all test cases pass,SPARK-4295,12753653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,maji2014,maji2014,07/Nov/14 08:35,11/Nov/14 10:20,14/Jul/23 06:26,11/Nov/14 10:20,1.1.0,,,,,,,1.1.1,1.2.0,,,,,DStreams,,,09/Nov/14 00:00,0,,,,,,"[reproduce]
Run test suite normally, after the first test case, all other test cases throw ""javax.management.InstanceAlreadyExistsException: org.apache.flume.channel:type=null"" 

[exception stack]
exception as followings:

14/11/07 00:24:51 ERROR MonitoredCounterGroup: Failed to register monitored counter group for type: CHANNEL, name: null
javax.management.InstanceAlreadyExistsException: org.apache.flume.channel:type=null
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:108)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:88)
	at org.apache.flume.channel.MemoryChannel.start(MemoryChannel.java:345)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite$$anonfun$2.apply$mcV$sp(SparkSinkSuite.scala:63)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite$$anonfun$2.apply(SparkSinkSuite.scala:61)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite$$anonfun$2.apply(SparkSinkSuite.scala:61)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.scalatest.FunSuite.run(FunSuite.scala:1555)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:144)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:35)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)

[reason]
InstanceAlreadyExistsException, all channel name is null

[solution]
Set different channel name in each test case and then no exception throws
",,apachespark,maji2014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 09 16:02:42 UTC 2014,,,,,,,,,,"0|i223lb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/14 16:02;apachespark;User 'maji2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/3177;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect result set in JDBC/ODBC,SPARK-4292,12753622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,07/Nov/14 05:32,07/Nov/14 20:55,14/Jul/23 06:26,07/Nov/14 20:55,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"select * from src, get result as follows:
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |
| 97   | val_97   |",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 20:55:55 UTC 2014,,,,,,,,,,"0|i223ef:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"07/Nov/14 05:34;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3149;;;","07/Nov/14 20:55;marmbrus;Issue resolved by pull request 3149
[https://github.com/apache/spark/pull/3149];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Drop ""Code"" from network module names",SPARK-4291,12753615,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,07/Nov/14 04:21,08/Nov/14 07:17,14/Jul/23 06:26,08/Nov/14 07:17,1.2.0,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"In maven, the network modules have the suffix ""Code"", which is inconsistent with other modules.
{code}
[INFO] Reactor Build Order:
[INFO] 
[INFO] Spark Project Parent POM
[INFO] Spark Project Common Network Code
[INFO] Spark Project Shuffle Streaming Service Code
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project Streaming
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project ML Library
[INFO] Spark Project Tools
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Parent POM
[INFO] Spark Project YARN Stable API
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Kafka
[INFO] Spark Project External Flume Sink
[INFO] Spark Project External Flume
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External MQTT
[INFO] Spark Project Examples
[INFO] Spark Project Yarn Shuffle Service Code
{code}

My proposal is to drop it especially before they make it into an official release.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 05:20:37 UTC 2014,,,,,,,,,,"0|i223cv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"07/Nov/14 05:20;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3148;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stopping flag in YarnClientSchedulerBackend should be volatile,SPARK-4282,12753531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,06/Nov/14 22:01,11/Nov/14 18:34,14/Jul/23 06:26,11/Nov/14 18:34,1.2.0,,,,,,,1.2.0,1.3.0,,,,,YARN,,,,0,,,,,,"In YarnClientSchedulerBackend, a variable ""stopping"" is used as a flag and it's accessed by some threads so it should be volatile.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 22:02:25 UTC 2014,,,,,,,,,,"0|i222vb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Nov/14 22:02;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3143;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn shuffle service jars need to include dependencies,SPARK-4281,12753511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor14,06/Nov/14 21:12,19/Nov/14 00:41,14/Jul/23 06:26,19/Nov/14 00:41,1.2.0,,,,,,,1.2.0,,,,,,Build,YARN,,,0,,,,,,"When we package we only get the jars with the classes. We need to make an assembly jar for the ""network-yarn"" module that includes all of its dependencies.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 05:12:28 UTC 2014,,,,,,,,,,"0|i222rr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"07/Nov/14 05:12;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3147;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL job failing with java.lang.ClassCastException,SPARK-4278,12753474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,pdeyhim,pdeyhim,06/Nov/14 18:56,15/Sep/15 21:26,14/Jul/23 06:26,15/Sep/15 21:26,,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The following job fails with the java.lang.ClassCastException error. Ideally SparkSQL should have the ability to ignore records that don't conform with the inferred schema. 

The steps that gets me to this error: 

1) infer schema from a small subset of data
2) apply the schema to a larger dataset
3) do a simple join of two datasets

sample code:
{code}
val sampleJson = sqlContext.jsonRDD(sc.textFile("".../dt=2014-10-10/file.snappy""))
val mydata = sqlContext.jsonRDD(larger_dataset,sampleJson.schema)
mydata.registerTempTable(""mytable1"")

other dataset:
val x = sc.textFile(""....."")
case class Dataset(a:String,state:String, b:String, z:String, c:String, d:String)
val xSchemaRDD = x.map(_.split(""\t"")).map(f=>Dataset(f(0),f(1),f(2),f(3),f(4),f(5)))
xSchemaRDD.registerTempTable(""mytable2"")

{code}


java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
        scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:389)
        org.apache.spark.sql.json.JsonRDD$$anonfun$enforceCorrectType$1.apply(JsonRDD.scala:397)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        scala.collection.AbstractTraversable.map(Traversable.scala:105)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:397)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1$$anonfun$apply$4.apply(JsonRDD.scala:410)
        scala.Option.map(Option.scala:145)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:409)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:407)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        org.apache.spark.sql.json.JsonRDD$.org$apache$spark$sql$json$JsonRDD$$asRow(JsonRDD.scala:407)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:398)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1$$anonfun$apply$4.apply(JsonRDD.scala:410)
        scala.Option.map(Option.scala:145)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:409)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:407)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        org.apache.spark.sql.json.JsonRDD$.org$apache$spark$sql$json$JsonRDD$$asRow(JsonRDD.scala:407)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:398)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1$$anonfun$apply$4.apply(JsonRDD.scala:410)
        scala.Option.map(Option.scala:145)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:409)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:407)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        org.apache.spark.sql.json.JsonRDD$.org$apache$spark$sql$json$JsonRDD$$asRow(JsonRDD.scala:407)
        org.apache.spark.sql.json.JsonRDD$$anonfun$jsonStringToRow$1.apply(JsonRDD.scala:41)
        org.apache.spark.sql.json.JsonRDD$$anonfun$jsonStringToRow$1.apply(JsonRDD.scala:41)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:209)
        org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:65)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:724)",,glenn.strycker@gmail.com,ishepherd,pdeyhim,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:26:40 UTC 2015,,,,,,,,,,"0|i222k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/15 02:38;ishepherd;I certainly wouldn't want Spark SQL to ignore bad records, as Parviz suggested.

However: It would aid troubleshooting, if this exception was wrapped in another that gave additional context.
eg: the ToString of the bad value; the incorrect column definition.;;;","15/Sep/15 21:26;yhuai;In Spark 1.5, if the type of data does not match the schema, we will throw a run time exception saying that we cannot parse a value as the type defined in the schema and the error message also contains the token we got from Jackson parser. I think it is a better error than a ClassCastException. So, I am resolving this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support external shuffle service on Worker,SPARK-4277,12753466,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,06/Nov/14 18:42,07/Nov/14 01:21,14/Jul/23 06:26,07/Nov/14 01:21,1.2.0,,,,,,,1.2.0,,,,,,Deploy,Spark Core,,,0,,,,,,"It's less useful to have an external shuffle service on the Spark Standalone Worker than on YARN or Mesos (as executor allocations tend to be more static), but it would be good to help test the code path. It would also make Spark more resilient to particular executor failures.

Cool side-feature: When SPARK-4236 is fixed and integrated, the Worker will take care of cleaning up executor directories, which will mean executors terminate more quickly and that we don't leak data if the executor dies forcefully.",,apachespark,ilikerps,junping_du,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 18:47:57 UTC 2014,,,,,,,,,,"0|i222if:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Nov/14 18:47;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3142;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in printing the details of query plan,SPARK-4274,12753408,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,chenghao,chenghao,06/Nov/14 14:30,11/Nov/14 01:55,14/Jul/23 06:26,11/Nov/14 01:55,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"NPE in printing the details of query plan, if the query is not valid. This will be great helpful in Hive comparison test, which could provide more information.",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 01:55:00 UTC 2014,,,,,,,,,,"0|i2225j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 14:34;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3139;;;","11/Nov/14 01:55;marmbrus;Issue resolved by pull request 3139
[https://github.com/apache/spark/pull/3139];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Cast from DateType to DecimalType.,SPARK-4270,12753362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,06/Nov/14 10:08,07/Nov/14 20:31,14/Jul/23 06:26,07/Nov/14 20:31,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,{{Cast}} from {{DateType}} to {{DecimalType}} throws {{NullPointerException}}.,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 20:31:17 UTC 2014,,,,,,,,,,"0|i221vj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 10:10;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3134;;;","07/Nov/14 20:31;marmbrus;Issue resolved by pull request 3134
[https://github.com/apache/spark/pull/3134];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make wait time in BroadcastHashJoin configurable,SPARK-4269,12753340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jackylk,jackylk,06/Nov/14 09:28,16/Dec/14 23:35,14/Jul/23 06:26,16/Dec/14 23:35,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"In BroadcastHashJoin, currently it is using a hard coded value (5 minutes) to wait for the execution and broadcast of the small table. 

In my opinion, it should be a configurable value since broadcast may exceed 5 minutes in some case, like in a busy/congested network environment.",,apachespark,jackylk,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 16 23:35:12 UTC 2014,,,,,,,,,,"0|i221qn:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"06/Nov/14 09:37;apachespark;User 'jackylk' has created a pull request for this issue:
https://github.com/apache/spark/pull/3133;;;","16/Dec/14 23:35;marmbrus;Issue resolved by pull request 3133
[https://github.com/apache/spark/pull/3133];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing to launch jobs on Spark on YARN with Hadoop 2.5.0 or later,SPARK-4267,12753331,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,ozawa,ozawa,06/Nov/14 08:37,13/Feb/15 14:26,14/Jul/23 06:26,09/Feb/15 18:34,,,,,,,,1.2.2,1.3.0,,,,,YARN,,,,0,,,,,,"Currently we're trying Spark on YARN included in Hadoop 2.5.1. Hadoop 2.5 uses protobuf 2.5.0 so I compiled with protobuf 2.5.1 like this:

{code}
 ./make-distribution.sh --name spark-1.1.1 --tgz -Pyarn -Dhadoop.version=2.5.1 -Dprotobuf.version=2.5.0
{code}

Then Spark on YARN fails to launch jobs with NPE.

{code}
$ bin/spark-shell --master yarn-client
scala>     sc.textFile(""hdfs:///user/ozawa/wordcountInput20G"").flatMap(line => line.split("" "")).map(word => (word, 1)).persist().reduceByKey((a, b) => a + b, 16).saveAsTextFile(""hdfs:///user/ozawa/sparkWordcountOutNew2"");
java.lang.NullPointerException                                                                                                                                                                                                                                
        at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:1284)
        at org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:1291)                                                                                                                                                                        
        at org.apache.spark.SparkContext.textFile$default$2(SparkContext.scala:480)
        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:13)                                                                                                                                                                                                           
        at $iwC$$iwC$$iwC.<init>(<console>:18)
        at $iwC$$iwC.<init>(<console>:20)                                                                                                                                                                                                                     
        at $iwC.<init>(<console>:22)
        at <init>(<console>:24)                                                                                                                                                                                                                               
        at .<init>(<console>:28)
        at .<clinit>(<console>)                                                                                                                                                                                                                               
        at .<init>(<console>:7)
        at .<clinit>(<console>)                                                                                                                                                                                                                               
        at $print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                                                                                                                        
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)                                                                                                                                                              
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)                                                                                                                                                                          
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)                                                                                                                                                                             
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)                                                                                                                                                                                   
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:823)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:868)                                                                                                                                                                       
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:780)
        at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:625)                                                                                                                                                                               
        at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:633)
        at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:638)                                                                                                                                                                                        
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:963)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:911)                                                                                                                                                                    
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:911)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)                                                                                                                                                             
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:911)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1006)                                                                                                                                                                                    
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)                                                                                                                                                                                                        
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)                                                                                                                                                                      
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)                                                                                                                                                                                                   
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:329)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)                                                                                                                                                                                    
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
",,apachespark,mdaniel,ozawa,sandyr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 21:34:51 UTC 2015,,,,,,,,,,"0|i221on:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/14 11:45;ozawa;[~sandyr] [~pwendell] do you have any workarounds to deal with this problem?;;;","07/Nov/14 17:32;sandyr;Strange.  Checked in the code and it seems like this must mean the taskScheduler is null.  Did you see any errors farther up in the shell before this happened?  Does it work in local mode?;;;","12/Nov/14 20:07;sarutak;Hi [~ozawa], On my YARN 2.5.1(JDK 1.7.0_60) cluster, Spark Shell works well.

I built with following command.
{code}
sbt/sbt -Dhadoop.version=2.5.1 -Pyarn  assembly
{code}

And launched Spark Shell with following command.
{code}
bin/spark-shell --master yarn --deploy-mode client --executor-cores 1 --driver-memory 512M --executor-memory 512M --num-executors 1
{code}

And then, I ran job with following script.
{code}
sc.textFile(""hdfs:///user/kou/LICENSE.txt"").flatMap(line => line.split("" "")).map(word => (word, 1)).persist().reduceByKey((a, b) => a + b, 16).saveAsTextFile(""hdfs:///user/kou/LICENSE.txt.count"")
{code}

So I think the problem is not caused by the version of Hadoop.
One possible case is that SparkContext#stop is called between instantiating SparkContext and running job accidentally.
Did you see any ERROR log on the shell?;;;","13/Nov/14 03:24;mdaniel;Apologies, I don't know if we want log verbiage inline or as an attachment.

I experienced this NPE on an EMR cluster, AMI 3.3.0 which is Amazon Hadoop 2.4.0 against a {{make-distribution.sh}} version with {{-Pyarn}} and {{-Phadoop-2.2}} with {{-Dhadoop.version=2.2.0}}. I built it against 2.2 because some of our jobs run on 2.2, and I thought 2.4 would be backwards compatible.

I will try building as you said, using {{sbt assembly}}, but I wanted to reply to your comment that yes, I do see an {{ERROR}} line but it isn't helpful to me, so I hope it's meaningful to others.

{noformat}
14/11/13 02:58:23 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
	 appMasterRpcPort: -1
	 appStartTime: 1415847498993
	 yarnAppState: ACCEPTED

14/11/13 02:58:23 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, PROXY_HOST=10.166.39.198,PROXY_URI_BASE=http://10.166.39.198:9046/proxy/application_1415840940647_0001, /proxy/application_1415840940647_0001
14/11/13 02:58:23 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
14/11/13 02:58:24 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
	 appMasterRpcPort: 0
	 appStartTime: 1415847498993
	 yarnAppState: RUNNING

14/11/13 02:58:29 ERROR cluster.YarnClientSchedulerBackend: Yarn application already ended: FINISHED
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
14/11/13 02:58:29 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
14/11/13 02:58:29 INFO ui.SparkUI: Stopped Spark web UI at http://ip-10-166-39-198.ec2.internal:4040
14/11/13 02:58:29 INFO scheduler.DAGScheduler: Stopping DAGScheduler
14/11/13 02:58:29 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
14/11/13 02:58:29 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
14/11/13 02:58:29 INFO cluster.YarnClientSchedulerBackend: Stopped
14/11/13 02:58:30 INFO spark.MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
14/11/13 02:58:30 INFO network.ConnectionManager: Selector thread was interrupted!
14/11/13 02:58:30 INFO network.ConnectionManager: ConnectionManager stopped
14/11/13 02:58:30 INFO storage.MemoryStore: MemoryStore cleared
14/11/13 02:58:30 INFO storage.BlockManager: BlockManager stopped
14/11/13 02:58:30 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
14/11/13 02:58:30 INFO spark.SparkContext: Successfully stopped SparkContext
14/11/13 02:58:30 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/11/13 02:58:30 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/11/13 02:58:30 INFO Remoting: Remoting shut down
14/11/13 02:58:30 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
14/11/13 02:58:47 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
{noformat};;;","13/Nov/14 04:05;mdaniel;I rebuilt the assembly using {{./sbt/sbt -Dhadoop.version=2.4.0 -Pyarn assembly}} and moved the old jars out of the {{lib}} in the unpacked distribution directory and moved the new {{assembly/target/scala-2.10/spark-assembly-1.1.0-hadoop2.4.0.jar}} into their place. Running {{bin/spark-shell}} yields the same output, and (as one might expect) the same NPE.

I've tried to trim the log output to be contextual without being verbose, so please let me know if there are more details that would help with the diagnosis.

{noformat}
14/11/13 03:57:13 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, PROXY_HOST=10.166.39.198,PROXY_URI_BASE=http://10.166.39.198:9046/proxy/application_1415840940647_0002, /proxy/application_1415840940647_0002
14/11/13 03:57:13 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
14/11/13 03:57:13 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
	 appMasterRpcPort: 0
	 appStartTime: 1415851029508
	 yarnAppState: RUNNING

14/11/13 03:57:18 ERROR cluster.YarnClientSchedulerBackend: Yarn application already ended: FINISHED
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
14/11/13 03:57:18 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
14/11/13 03:57:18 INFO ui.SparkUI: Stopped Spark web UI at http://ip-10-166-39-198.ec2.internal:4040
14/11/13 03:57:18 INFO scheduler.DAGScheduler: Stopping DAGScheduler
14/11/13 03:57:18 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
14/11/13 03:57:18 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
14/11/13 03:57:18 INFO cluster.YarnClientSchedulerBackend: Stopped
14/11/13 03:57:19 INFO spark.MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
14/11/13 03:57:19 INFO network.ConnectionManager: Selector thread was interrupted!
14/11/13 03:57:19 INFO network.ConnectionManager: ConnectionManager stopped
14/11/13 03:57:19 INFO storage.MemoryStore: MemoryStore cleared
14/11/13 03:57:19 INFO storage.BlockManager: BlockManager stopped
14/11/13 03:57:19 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
14/11/13 03:57:19 INFO spark.SparkContext: Successfully stopped SparkContext
14/11/13 03:57:19 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/11/13 03:57:19 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/11/13 03:57:19 INFO Remoting: Remoting shut down
14/11/13 03:57:19 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
14/11/13 03:57:37 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
14/11/13 03:57:37 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
{noformat};;;","13/Nov/14 04:17;sarutak;Hi [~bugzilla@mdaniel.scdi.com].
The NPE is caused by SparkContext stopped because Application finished accidentally.
I don't know why your application finished before running job for now.
Can you see some ERROR message on the logs of ApplicationMaster or ResourceManager?
;;;","13/Nov/14 05:52;mdaniel;My searches for {{ERROR}} didn't yield anything, but I found the text at the bottom of this comment in a file {{yarn-hadoop-nodemanager-ip-10-171-57-176.ec2.internal.log.2014-11-13-03}} on one of the yarn slaves which sheds light on the situation.

I reverted {{spark-defaults.conf}} to just the bare bones:
{noformat}
spark.master yarn
spark.driver.memory 1G
spark.executor.memory 5G
{noformat}
and then the {{SparkContext}} was initialized as expected. To be honest, I perhaps should not have uncommented the {{# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=""one two three""}} but I wanted to see what it did. Now I know what it does: bring down yarn containers. :-)

It's too bad such a grave *error* is reported at _warn_ level, and I hope in the master branch that NPE has been cleaned up because those exceptions are not helpful at all.

Nevertheless, I hope this helps the original submitter track down their problem, too.

{noformat}
2014-11-13 03:57:14,085 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor (ContainersLauncher #3): Exception from container-launch with container ID: container_1415840940647_0002_01_000002 and exit code: 1
org.apache.hadoop.util.Shell$ExitCodeException: Usage: java [-options] class [args...]
           (to execute a class)
   or  java [-options] -jar jarfile [args...]
           (to execute a jar file)
where options include:
{noformat};;;","24/Jan/15 18:09;srowen;The warning is from YARN, I believe, rather than Spark. Yeah maybe should be an error. 

Your info however points to the problem; I'm sure it's {{-Dnumbers=""one two three""}}. {{Utils.splitCommandString}} strips quotes as it parses them, so will turn it into {{-Dnumbers=one two three}} so the command is becoming {{java -Dnumbers=one two three ...}} and this isn't valid.

I suggest that {{Utils.splitCommandString}} not strip the quotes that it parses, so that the reconstructed command line is exactly like the original. It's just splitting, not interpreting the command. This also seems less surprising. PR coming to demonstrate.;;;","24/Jan/15 18:11;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4188;;;","07/Feb/15 15:01;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4452;;;","12/Feb/15 21:34;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4575;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid expensive JavaScript for StagePages with huge numbers of tasks,SPARK-4266,12753328,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kayousterhout,kayousterhout,kayousterhout,06/Nov/14 08:08,25/Nov/14 02:11,14/Jul/23 06:26,25/Nov/14 02:11,1.2.0,,,,,,,1.2.0,,,,,,Web UI,,,,0,,,,,,"Some of the new javascript added to handle hiding metrics significantly slows the page load for stages with a lot of tasks (e.g., for a job with 10K tasks, it took over a minute for the page to finish loading in Chrome on my laptop).  There are at least two issues here:

(1) The new table striping java script is much slower than the old CSS.  The fancier javascript is only needed for the stage summary table, so we should change the task table back to using CSS so that it doesn't slow the page load for jobs with lots of tasks.

(2) The javascript associated with hiding metrics is expensive when jobs have lots of tasks, I think because the jQuery selectors have to traverse a much larger DOM.   The ID selectors are much more efficient, so we should consider switching to these, and/or avoiding this code in additional-metrics.js:

    $(""input:checkbox:not(:checked)"").each(function() {
        var column = ""table ."" + $(this).attr(""name"");
        $(column).hide();
    });

by initially hiding the data when we generate the page in the render function instead, which should be easy to do.",,apachespark,kayousterhout,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 02:11:23 UTC 2014,,,,,,,,,,"0|i221nz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Nov/14 23:26;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/3328;;;","25/Nov/14 02:11;pwendell;[~kayousterhout] I'm resolving this because I saw you merged it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL HashJoin induces ""refCnt = 0"" error in ShuffleBlockFetcherIterator",SPARK-4264,12753285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ilikerps,ilikerps,ilikerps,06/Nov/14 02:39,24/Nov/14 20:38,14/Jul/23 06:26,06/Nov/14 18:46,1.2.0,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"This is because it calls hasNext twice, which invokes the completion iterator twice, unintuitively.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 02:49:36 UTC 2014,,,,,,,,,,"0|i221ev:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Nov/14 02:49;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3128;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Httpbroadcast should set connection timeout.,SPARK-4260,12753235,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,06/Nov/14 00:14,12/Dec/14 12:55,14/Jul/23 06:26,12/Dec/14 12:55,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,Httpbroadcast sets read timeout but doesn't set connection timeout.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 12:55:48 UTC 2014,,,,,,,,,,"0|i2213r:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Nov/14 00:15;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3122;;;","12/Dec/14 12:55;srowen;https://github.com/apache/spark/commit/60969b0336930449a826821a48f83f65337e8856  This was merged and appears in master and 1.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLLib BinaryClassificationMetricComputers try to divide by zero,SPARK-4256,12753182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,abull,abull,abull,05/Nov/14 21:19,13/Nov/14 06:16,14/Jul/23 06:26,13/Nov/14 06:16,,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,MLLib BinaryClassificationMetricComputers try to divide by zero in cases where there are no positive or negative examples. Resulting in NaNs in metrics.,,abull,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 06:16:08 UTC 2014,,,,,,,,,,"0|i220s7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 21:19;apachespark;User 'abull' has created a pull request for this issue:
https://github.com/apache/spark/pull/3118;;;","13/Nov/14 06:16;mengxr;Issue resolved by pull request 3118
[https://github.com/apache/spark/pull/3118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table striping is incorrect on page load,SPARK-4255,12753174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,05/Nov/14 20:43,06/Nov/14 08:10,14/Jul/23 06:26,06/Nov/14 08:10,,,,,,,,1.2.0,,,,,,Web UI,,,,0,,,,,,"Currently, table striping (where every other row is grey, to aid readability) happens before table rows get hidden (because some metrics are hidden by default).  If an odd number of contiguous table rows are hidden on page load, this means adjacent rows can both end up the same color. We should fix the table striping to happen after row hiding.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 08:10:02 UTC 2014,,,,,,,,,,"0|i220qf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Nov/14 20:45;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/3117;;;","06/Nov/14 08:10;kayousterhout;Fixed by https://github.com/apache/spark/commit/2c84178b8283269512b1c968b9995a7bdedd7aa5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MovieLensALS example fails from including Params in closure,SPARK-4254,12753146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,josephkb,josephkb,josephkb,05/Nov/14 19:34,06/Nov/14 03:51,14/Jul/23 06:26,06/Nov/14 03:51,,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"MovieLensALS example fails when run.
The problem is that the parameter are included in the closure, but they are not Serializable.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 03:51:56 UTC 2014,,,,,,,,,,"0|i220kf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Nov/14 19:45;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/3116;;;","06/Nov/14 03:51;mengxr;Issue resolved by pull request 3116
[https://github.com/apache/spark/pull/3116];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore spark.driver.host in yarn-cluster and standalone-cluster mode,SPARK-4253,12753063,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,05/Nov/14 15:10,04/Dec/14 20:09,14/Jul/23 06:26,04/Dec/14 20:09,,,,,,,,1.1.2,1.2.0,,,,,YARN,,,,0,,,,,,"We actually don't know where driver will be before it is launched in yarn-cluster mode. If we set spark.driver.host property, Spark will create Actor on the hostname or ip as setted, which will leads an error.
So we should ignore this config item in yarn-cluster mode.

As [~joshrosen]] pointed, we also ignore it in standalone cluster mode.",,apachespark,joshrosen,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 02:27;WangTaoTheTonic;Cannot assign requested address.txt;https://issues.apache.org/jira/secure/attachment/12679763/Cannot+assign+requested+address.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 04 20:09:34 UTC 2014,,,,,,,,,,"0|i2202n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 15:25;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3112;;;","06/Nov/14 02:27;WangTaoTheTonic;Attach the exception content.;;;","04/Dec/14 20:09;joshrosen;Issue resolved by pull request 3112
[https://github.com/apache/spark/pull/3112];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create constant null value for Hive Inspectors,SPARK-4250,12753057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chenghao,chenghao,05/Nov/14 14:37,11/Nov/14 01:24,14/Jul/23 06:26,11/Nov/14 01:24,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,Constant null value is not accepted while creating the Hive Inspectors.,,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 01:24:03 UTC 2014,,,,,,,,,,"0|i2201b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 16:35;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3114;;;","11/Nov/14 01:24;marmbrus;Issue resolved by pull request 3114
[https://github.com/apache/spark/pull/3114];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A problem of EdgePartitionBuilder in Graphx,SPARK-4249,12753031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,Cookies,Cookies,05/Nov/14 13:00,26/Nov/14 04:20,14/Jul/23 06:26,06/Nov/14 18:50,1.1.0,,,,,,,1.0.3,1.1.1,1.2.0,,,,GraphX,,,,0,,,,,,"https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartitionBuilder.scala#L48
in function toEdgePartition, code snippet     "" index.update(srcIds(0), 0)""
the elements in array srcIds are all not initialized and are all 0. The effect is that if vertex ids don't contain 0, indexSize is equal to (realIndexSize + 1) and 0 is added to the `index`
It seems that all versions have the problem",,ankurd,apachespark,Cookies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4173,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 18:50:58 UTC 2014,,,,,,,,,,"0|i21zvj:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.1,1.2.0,,,,,,,,,"06/Nov/14 14:08;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3138;;;","06/Nov/14 18:50;ankurd;Issue resolved by pull request 3138
[https://github.com/apache/spark/pull/3138];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] spark sql not support add jar ,SPARK-4248,12753009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,qiaohaijun,qiaohaijun,05/Nov/14 10:14,19/Dec/14 21:21,14/Jul/23 06:26,19/Dec/14 21:21,1.1.1,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"add jar not support

the udf jar need use --jars upload","java:1.7
hadoop:2.3.0-cdh5.0.0
spark:1.1.1
thriftserver-with-hive:0.12

hive metaserver:0.13.1",qiaohaijun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-11-05 10:14:30.0,,,,,,,,,,"0|i21zqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix containsNull of the result ArrayType of CreateArray expression.,SPARK-4245,12752998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,05/Nov/14 08:58,08/Dec/14 21:08,14/Jul/23 06:26,14/Nov/14 22:21,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,The {{containsNull}} of the result {{ArrayType}} of {{CreateArray}} should be true only if the children is empty or there exists nullable child.,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4293,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 22:21:43 UTC 2014,,,,,,,,,,"0|i21zo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 09:04;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3110;;;","14/Nov/14 22:21;marmbrus;Issue resolved by pull request 3110
[https://github.com/apache/spark/pull/3110];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConstantFolding has to be done before initialize the Generic UDF,SPARK-4244,12752995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,chenghao,chenghao,05/Nov/14 08:44,21/Nov/14 00:51,14/Jul/23 06:26,21/Nov/14 00:51,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The query is 
""SELECT named_struct(lower(""AA""), ""12"", lower(""Bb""), ""13"") FROM src LIMIT 1:

It throws exception
{panel}
Even arguments to NAMED_STRUCT must be a constant STRING.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector@798a6e99
org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: Even arguments to NAMED_STRUCT must be a constant STRING.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector@798a6e99
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.initialize(GenericUDFNamedStruct.java:55)
	at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector$lzycompute(hiveUdfs.scala:157)
	at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector(hiveUdfs.scala:157)
	at org.apache.spark.sql.hive.HiveGenericUdf.dataType$lzycompute(hiveUdfs.scala:173)
	at org.apache.spark.sql.hive.HiveGenericUdf.dataType(hiveUdfs.scala:173)
	at org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$output$1.apply(basicOperators.scala:25)
	at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$output$1.apply(basicOperators.scala:25)
{panel}
",,apachespark,chenghao,marmbrus,nemccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 21 00:51:46 UTC 2014,,,,,,,,,,"0|i21znj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 08:50;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3109;;;","21/Nov/14 00:51;marmbrus;Issue resolved by pull request 3109
[https://github.com/apache/spark/pull/3109];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SASL to external shuffle service,SPARK-4242,12752985,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,05/Nov/14 07:47,07/Nov/14 04:24,14/Jul/23 06:26,07/Nov/14 04:24,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"It's already added to NettyBlockTransferService, let's just add it to ExternalShuffleClient as well.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 07:55:52 UTC 2014,,,,,,,,,,"0|i21zlj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Nov/14 07:55;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3108;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
External shuffle service must cleanup its shuffle files,SPARK-4236,12752962,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilikerps,ilikerps,ilikerps,05/Nov/14 02:41,07/Nov/14 04:23,14/Jul/23 06:26,07/Nov/14 04:23,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"When external shuffle service is enabled, executors no longer cleanup their own shuffle files, as the external server can continue to serve them. The external serer must clean these files up once it knows that they will never be used again (i.e., when the application terminates or when Spark's ContextCleaner requests that they be deleted).",,apachespark,ilikerps,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 01:17:41 UTC 2014,,,,,,,,,,"0|i21zgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 01:17;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3126;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doc for spark.default.parallelism is incorrect,SPARK-4230,12752912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,05/Nov/14 00:42,10/Nov/14 20:41,14/Jul/23 06:26,10/Nov/14 20:41,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"The default default parallelism for shuffle transformations is actually the maximum number of partitions in dependent RDDs.

Should also probably be clear about what SparkContext.defaultParallelism is used for",,apachespark,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 07:35:43 UTC 2014,,,,,,,,,,"0|i21z5j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Nov/14 07:35;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3107;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jdbc/odbc error when using maven build spark,SPARK-4225,12752821,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,scwf,scwf,04/Nov/14 19:44,07/Nov/14 19:46,14/Jul/23 06:26,07/Nov/14 19:46,1.1.0,,,,,,,1.2.0,,,,,,Build,SQL,,,0,,,,,,"use command as follows to build spark
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.1 -Phive -DskipTests clean package

then use beeline to connect to thrift server ,get this error:
 
14/11/04 11:30:31 INFO ObjectStore: Initialized ObjectStore
14/11/04 11:30:31 INFO AbstractService: Service:ThriftBinaryCLIService is started.
14/11/04 11:30:31 INFO AbstractService: Service:HiveServer2 is started.
14/11/04 11:30:31 INFO HiveThriftServer2: HiveThriftServer2 started
14/11/04 11:30:31 INFO ThriftCLIService: ThriftBinaryCLIService listening on 0.0.0.0/0.0.0.0:10000
14/11/04 11:33:26 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
14/11/04 11:33:26 INFO HiveMetaStore: No user is added in admin role, since config is empty
14/11/04 11:33:26 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/11/04 11:33:26 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
14/11/04 11:33:26 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Cannot write a TUnion with no set value!
	at org.apache.thrift.TUnion$TUnionStandardScheme.write(TUnion.java:240)
	at org.apache.thrift.TUnion$TUnionStandardScheme.write(TUnion.java:213)
	at org.apache.thrift.TUnion.write(TUnion.java:152)
	at org.apache.hive.service.cli.thrift.TGetInfoResp$TGetInfoRespStandardScheme.write(TGetInfoResp.java:456)
	at org.apache.hive.service.cli.thrift.TGetInfoResp$TGetInfoRespStandardScheme.write(TGetInfoResp.java:406)
	at org.apache.hive.service.cli.thrift.TGetInfoResp.write(TGetInfoResp.java:341)
	at org.apache.hive.service.cli.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:3754)
	at org.apache.hive.service.cli.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:3718)
	at org.apache.hive.service.cli.thrift.TCLIService$GetInfo_result.write(TCLIService.java:3669)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 19:46:40 UTC 2014,,,,,,,,,,"0|i21ykn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"04/Nov/14 19:46;scwf;it seems there is some difference between using sbt and maven to build spark.;;;","05/Nov/14 03:58;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3103;;;","05/Nov/14 05:54;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3105;;;","07/Nov/14 19:46;marmbrus;Issue resolved by pull request 3105
[https://github.com/apache/spark/pull/3105];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FixedLengthBinaryRecordReader should readFully,SPARK-4222,12752794,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jswisher,jswisher,jswisher,04/Nov/14 18:35,05/Nov/14 23:39,14/Jul/23 06:26,05/Nov/14 23:39,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"The new FixedLengthBinaryRecordReader currently uses a read() call to read from the FSDataInputStream, without checking the number of bytes actually returned. The currentPosition variable is updated assuming that the full number of requested bytes are returned, which could lead to data corruption or other problems if fewer bytes come back than requested.

",,apachespark,jswisher,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 12:27:46 UTC 2014,,,,,,,,,,"0|i21yev:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Nov/14 12:27;apachespark;User 'industrial-sloth' has created a pull request for this issue:
https://github.com/apache/spark/pull/3093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow requesting executors only on Yarn (for now),SPARK-4215,12752595,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,04/Nov/14 00:45,21/Jan/15 18:52,14/Jul/23 06:26,21/Jan/15 18:52,1.2.0,,,,,,,1.3.0,,,,,,Spark Core,YARN,,,0,,,,,,"Currently if the user attempts to call `sc.requestExecutors` or enables dynamic allocation on, say, standalone mode, it just fails silently. We must at the very least log a warning to say it's only available for Yarn currently, or maybe even throw an exception.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 04:01:49 UTC 2014,,,,,,,,,,"0|i21x7r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"05/Dec/14 04:01;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkSQL - ParquetFilters - No support for LT, LTE, GT, GTE operators",SPARK-4213,12752541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,terry.siu,terry.siu,03/Nov/14 22:03,18/Nov/14 02:06,14/Jul/23 06:26,07/Nov/14 19:57,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"When I issue a hql query against a HiveContext where my predicate uses a column of string type with one of LT, LTE, GT, or GTE operator, I get the following error:

scala.MatchError: StringType (of class org.apache.spark.sql.catalyst.types.StringType$)

Looking at the code in org.apache.spark.sql.parquet.ParquetFilters, StringType is absent from the corresponding functions for creating these filters.

To reproduce, in a Hive 0.13.1 shell, I created the following table (at a specified DB):

create table sparkbug (
  id int,
  event string
) stored as parquet;

Insert some sample data:

insert into table sparkbug select 1, '2011-06-18' from <some table> limit 1;
insert into table sparkbug select 2, '2012-01-01' from <some table> limit 1;

Launch a spark shell and create a HiveContext to the metastore where the table above is located.

import org.apache.spark.sql._
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.hive.HiveContext
val hc = new HiveContext(sc)
hc.setConf(""spark.sql.shuffle.partitions"", ""10"")
hc.setConf(""spark.sql.hive.convertMetastoreParquet"", ""true"")
hc.setConf(""spark.sql.parquet.compression.codec"", ""snappy"")
import hc._
hc.hql(""select * from <db>.sparkbug where event >= '2011-12-01'"")

A scala.MatchError will appear in the output.","CDH5.2, Hive 0.13.1, Spark 1.2 snapshot (commit hash 76386e1a23c)",apachespark,marmbrus,sarutak,terry.siu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 02:06:53 UTC 2014,,,,,,,,,,"0|i21ww7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"04/Nov/14 00:53;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3083;;;","07/Nov/14 19:57;marmbrus;Issue resolved by pull request 3083
[https://github.com/apache/spark/pull/3083];;;","07/Nov/14 20:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3161;;;","17/Nov/14 18:26;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3317;;;","18/Nov/14 02:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3333;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query which has syntax like 'not like' is not working in Spark SQL,SPARK-4207,12752440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,ravi.pesala,ravi.pesala,03/Nov/14 15:27,03/Nov/14 21:08,14/Jul/23 06:26,03/Nov/14 21:08,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Queries which has 'not like' is not working in Spark SQL. Same works in Spark HiveQL.

{code}
sql(""SELECT * FROM records where value not like 'val%'"")
{code}

The above query fails with below exception

{code}
Exception in thread ""main"" java.lang.RuntimeException: [1.39] failure: ``IN'' expected but `like' found

SELECT * FROM records where value not like 'val%'
                                      ^
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:33)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:75)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:75)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:186)

{code}",,apachespark,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 21:08:16 UTC 2014,,,,,,,,,,"0|i21wav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/14 16:46;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/3075;;;","03/Nov/14 21:08;marmbrus;Issue resolved by pull request 3075
[https://github.com/apache/spark/pull/3075];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.exceptionString only return the information for the outermost exception,SPARK-4204,12752418,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,03/Nov/14 12:32,07/Nov/14 05:53,14/Jul/23 06:26,07/Nov/14 05:53,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"An exception may contain some inner exceptions, but Utils.exceptionString only return the information for the outermost exception.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 12:44:29 UTC 2014,,,,,,,,,,"0|i21w5z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Nov/14 12:44;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3073;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition directories in random order when inserting into hive table,SPARK-4203,12752416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tbfenet,tbfenet,03/Nov/14 12:12,07/Nov/14 20:54,14/Jul/23 06:26,07/Nov/14 20:54,1.1.0,1.2.0,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"When doing an insert into hive table with partitions the folders written to the file system are in a random order instead of the order defined in table creation. Seems that the loadPartition method in Hive.java has a Map<String,String> parameter but expects to be called with a map that has a defined ordering such as  LinkedHashMap. Have a patch which I will do a PR for.     ",,apachespark,marmbrus,tbfenet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 20:54:02 UTC 2014,,,,,,,,,,"0|i21w5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/14 18:27;apachespark;User 'tbfenet' has created a pull request for this issue:
https://github.com/apache/spark/pull/3076;;;","07/Nov/14 20:54;marmbrus;Issue resolved by pull request 3076
[https://github.com/apache/spark/pull/3076];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't use concat() on partition column in where condition (Hive compatibility problem),SPARK-4201,12752377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,dongxu,dongxu,03/Nov/14 07:47,19/Dec/14 21:19,14/Jul/23 06:26,19/Dec/14 21:19,1.0.0,1.1.0,,,,,,1.2.0,,,,,,SQL,,,,0,com,,,,,"The team used hive to query,we try to  move it to spark-sql.
when I search sentences like that. 
select count(1) from  gulfstream_day_driver_base_2 where  concat(year,month,day) = '20140929';
It can't work ,but it work well in hive.
I have to rewrite the sql to  ""select count(1) from  gulfstream_day_driver_base_2 where  year = 2014 and  month = 09 day= 29.
There are some error log.
14/11/03 15:05:03 ERROR SparkSQLDriver: Failed in [select count(1) from  gulfstream_day_driver_base_2 where  concat(year,month,day) = '20140929']
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Aggregate false, [], [SUM(PartialCount#1390L) AS c_0#1337L]
 Exchange SinglePartition
  Aggregate true, [], [COUNT(1) AS PartialCount#1390L]
   HiveTableScan [], (MetastoreRelation default, gulfstream_day_driver_base_2, None), Some((HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat(year#1339,month#1340,day#1341) = 20140929))

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.execution.Aggregate.execute(Aggregate.scala:126)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd$lzycompute(HiveContext.scala:360)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd(HiveContext.scala:360)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:415)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:59)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:291)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange SinglePartition
 Aggregate true, [], [COUNT(1) AS PartialCount#1390L]
  HiveTableScan [], (MetastoreRelation default, gulfstream_day_driver_base_2, None), Some((HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat(year#1339,month#1340,day#1341) = 20140929))

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:44)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1.apply(Aggregate.scala:128)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1.apply(Aggregate.scala:127)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
	... 16 more
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Aggregate true, [], [COUNT(1) AS PartialCount#1390L]
 HiveTableScan [], (MetastoreRelation default, gulfstream_day_driver_base_2, None), Some((HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat(year#1339,month#1340,day#1341) = 20140929))

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.execution.Aggregate.execute(Aggregate.scala:126)
	at org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:86)
	at org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:45)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
	... 20 more
Caused by: org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1242)
	at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:597)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1.apply(Aggregate.scala:128)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1.apply(Aggregate.scala:127)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
	... 24 more
Caused by: java.io.NotSerializableException: org.apache.spark.sql.hive.HiveGenericUdf$DeferredObjectAdapter
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1164)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1346)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1154)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:330)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:164)
	... 30 more
",Hive 0.12+hadoop 2.4/hadoop 2.2 +spark 1.1,dongxu,gvramana,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 21:19:16 UTC 2014,,,,,,,,,,"0|i21vwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/14 18:28;gvramana;I found the same is working on latest master, please confirm.;;;","19/Dec/14 21:19;marmbrus;Since this was reported working in master I'm going to close.  Please reopen if you are still having problems.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming + checkpointing + saveAsNewAPIHadoopFiles = NotSerializableException for Hadoop Configuration,SPARK-4196,12752311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,srowen,srowen,02/Nov/14 17:50,26/Nov/14 13:52,14/Jul/23 06:26,25/Nov/14 22:44,1.1.0,,,,,,,1.1.2,1.2.0,,,,,DStreams,,,,0,,,,,,"I am reasonably sure there is some issue here in Streaming and that I'm not missing something basic, but not 100%. I went ahead and posted it as a JIRA to track, since it's come up a few times before without resolution, and right now I can't get checkpointing to work at all.

When Spark Streaming checkpointing is enabled, I see a NotSerializableException thrown for a Hadoop Configuration object, and it seems like it is not one from my user code.

Before I post my particular instance see http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408135046777-12202.post@n3.nabble.com%3E for another occurrence.

I was also on customer site last week debugging an identical issue with checkpointing in a Scala-based program and they also could not enable checkpointing without hitting exactly this error.

The essence of my code is:

{code}
    final JavaSparkContext sparkContext = new JavaSparkContext(sparkConf);

    JavaStreamingContextFactory streamingContextFactory = new
JavaStreamingContextFactory() {
      @Override
      public JavaStreamingContext create() {
        return new JavaStreamingContext(sparkContext, new
Duration(batchDurationMS));
      }
    };

      streamingContext = JavaStreamingContext.getOrCreate(
          checkpointDirString, sparkContext.hadoopConfiguration(),
streamingContextFactory, false);
      streamingContext.checkpoint(checkpointDirString);
{code}

It yields:

{code}
2014-10-31 14:29:00,211 ERROR OneForOneStrategy:66
org.apache.hadoop.conf.Configuration
- field (class ""org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9"",
name: ""conf$2"", type: ""class org.apache.hadoop.conf.Configuration"")
- object (class
""org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9"",
<function2>)
- field (class ""org.apache.spark.streaming.dstream.ForEachDStream"",
name: ""org$apache$spark$streaming$dstream$ForEachDStream$$foreachFunc"",
type: ""interface scala.Function2"")
- object (class ""org.apache.spark.streaming.dstream.ForEachDStream"",
org.apache.spark.streaming.dstream.ForEachDStream@cb8016a)
...
{code}


This looks like it's due to PairRDDFunctions, as this saveFunc seems
to be  org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9
:

{code}
def saveAsNewAPIHadoopFiles(
    prefix: String,
    suffix: String,
    keyClass: Class[_],
    valueClass: Class[_],
    outputFormatClass: Class[_ <: NewOutputFormat[_, _]],
    conf: Configuration = new Configuration
  ) {
  val saveFunc = (rdd: RDD[(K, V)], time: Time) => {
    val file = rddToFileName(prefix, suffix, time)
    rdd.saveAsNewAPIHadoopFile(file, keyClass, valueClass,
outputFormatClass, conf)
  }
  self.foreachRDD(saveFunc)
}
{code}

Is that not a problem? but then I don't know how it would ever work in Spark. But then again I don't see why this is an issue and only when checkpointing is enabled.

Long-shot, but I wonder if it is related to closure issues like https://issues.apache.org/jira/browse/SPARK-1866",,apachespark,bcwalrus,koeninger,sandyr,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 13:52:52 UTC 2014,,,,,,,,,,"0|i21vin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/14 19:22;koeninger;Have you tried replacing

JavaStreamingContext.getOrCreate(checkpointDirString, sparkContext.hadoopConfiguration(),

with

JavaStreamingContext.getOrCreate(checkpointDirString, org.apache.spark.deploy.SparkHadoopUtil.get.conf,

;;;","04/Nov/14 20:33;srowen;Same problem I'm afraid. The serialization error suggests it's the fact that the configuration -- whatever its source in the caller -- is serialized in a call to foreachRDD in saveAsNewAPIHadoopFiles.;;;","11/Nov/14 12:54;srowen;More info. The problem is that {{CheckpointWriter}} serializes the {{DStreamGraph}} when checkpointing is enabled. In the case of, for example, {{saveAsNewAPIHadoopFiles}}, this includes a {{ForEachDStream}} with a reference to a Hadoop {{Configuration}}.

This isn't a problem without checkpointing because Spark is not going to need to serialize this {{ForEachDStream}} closure to execute it in general. But it does to checkpoint it.

Does that make sense? I'm not sure what to do but this is presenting a significant problem to me as I can't see a sly workaround to make streaming, with saving Hadoop files, with checkpointing, to work.


Here's a cobbled-together test that shows the problem:

{code}
  test(""recovery with save to HDFS stream"") {
    // Set up the streaming context and input streams
    val testDir = Utils.createTempDir()
    val outDir = Utils.createTempDir()
    var ssc = new StreamingContext(master, framework, Seconds(1))
    ssc.checkpoint(checkpointDir)
    val fileStream = ssc.textFileStream(testDir.toString)
    for (i <- Seq(1, 2, 3)) {
      Files.write(i + ""\n"", new File(testDir, i.toString), Charset.forName(""UTF-8""))
      // wait to make sure that the file is written such that it gets shown in the file listings
    }

    val reducedStream = fileStream.map(x => (x, x)).saveAsNewAPIHadoopFiles(
      outDir.toURI.toString,
      ""saveAsNewAPIHadoopFilesTest"",
      classOf[Text],
      classOf[Text],
      classOf[TextOutputFormat[Text,Text]],
      ssc.sparkContext.hadoopConfiguration)

    ssc.start()
    ssc.awaitTermination(5000)
    ssc.stop()

    val checkpointDirFile = new File(checkpointDir)
    assert(outDir.listFiles().length > 0)
    assert(checkpointDirFile.listFiles().length == 1)
    assert(checkpointDirFile.listFiles()(0).listFiles().length > 0)

    Utils.deleteRecursively(testDir)
    Utils.deleteRecursively(outDir)
  }
{code}

You'll see the {{NotSerializableException}} clearly if you hack {{Checkpoint.write()}}:

{code}
  def write(checkpoint: Checkpoint) {
    val bos = new ByteArrayOutputStream()
    val zos = compressionCodec.compressedOutputStream(bos)
    val oos = new ObjectOutputStream(zos)
    try {
      oos.writeObject(checkpoint)
    } catch {
      case e: Exception =>
        e.printStackTrace()
        throw e
    }
    ...
{code};;;","25/Nov/14 14:17;tdas;Let me try to take a pass on this. ;;;","25/Nov/14 14:56;tdas;As for now there is a workaround. You could directly use 
{code}
dstream.foreachRDD { rdd => 
    
    rdd.saveAsNewAPIHadoopFile(...) 
}
{code}

And in that you will have more control over the configuration object. You can wrap the Writable configuration object using SerializableWritable (https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SerializableWritable.scala). ;;;","25/Nov/14 15:07;srowen;That didn't work for me, IIRC. The problem is that the checkpointing process still serializes the whole graph, and this results in the same graph. saveAsNewAPIHadoopFile creates an anonymous function that always has a reference to a HadoopConfiguration.

I think I can dig deeper and copy-and-paste the saveAsNewAPIHadoopFile implementation and eventually make it serialize the Configuration with SerializableWritable, sure. If that's the answer, cool, but it should go back upstream into Spark too I suppose?;;;","25/Nov/14 15:09;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3457;;;","26/Nov/14 13:52;srowen;Looks good. At least, this commit got me past this particular issue. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions thrown during SparkContext or SparkEnv construction might lead to resource leaks or corrupted global state,SPARK-4194,12752277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,joshrosen,joshrosen,02/Nov/14 06:28,16/Apr/15 09:49,14/Jul/23 06:26,16/Apr/15 09:48,1.2.0,1.3.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"The SparkContext and SparkEnv constructors instantiate a bunch of objects that may need to be cleaned up after they're no longer needed.  If an exception is thrown during SparkContext or SparkEnv construction (e.g. due to a bad configuration setting), then objects created earlier in the constructor may not be properly cleaned up.

This is unlikely to cause problems for batch jobs submitted through {{spark-submit}}, since failure to construct SparkContext will probably cause the JVM to exit, but it is a potentially serious issue in interactive environments where a user might attempt to create SparkContext with some configuration, fail due to an error, and re-attempt the creation with new settings.  In this case, resources from the previous creation attempt might not have been cleaned up and could lead to confusing errors (especially if the old, leaked resources share global state with the new SparkContext).",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 09:48:42 UTC 2015,,,,,,,,,,"0|i21vb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/14 06:34;joshrosen;I've marked this a blocker of SPARK-4180, an issue that tries to add exceptions when users try to create multiple active SparkContexts in the same JVM.  PySpark already guards against this, but earlier versions of its error-checking code ran into issues where users would fail their initial attempt to create a SparkContext and then be unable to create new ones because we didn't clear the {{activeSparkContext}} variable after the constructor threw an exception.

To fix this, we need to wrap the constructor code in a {{try}} block. ;;;","02/Apr/15 19:49;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5335;;;","16/Apr/15 09:48;srowen;Issue resolved by pull request 5335
[https://github.com/apache/spark/pull/5335];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable doclint in Java 8 to prevent from build error.,SPARK-4193,12752270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,02/Nov/14 02:21,30/Nov/14 04:20,14/Jul/23 06:26,28/Nov/14 18:01,,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,,,apachespark,pwendell,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4543,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 28 18:01:00 UTC 2014,,,,,,,,,,"0|i21v9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/14 02:27;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3058;;;","28/Nov/14 18:01;pwendell;https://github.com/apache/spark/pull/3058;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON schema inference failed when dealing with type conflicts in arrays,SPARK-4185,12752216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,01/Nov/14 18:40,02/Nov/14 23:47,14/Jul/23 06:26,02/Nov/14 23:47,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)
val diverging = sparkContext.parallelize(List(""""""{""branches"": [""foo""]}"""""", """"""{""branches"": [{""foo"":42}]}""""""))
sqlContext.jsonRDD(diverging)  // throws a MatchError
{code}
The case is from http://chapeau.freevariable.com/2014/10/fedmsg-and-spark.html",,apachespark,guoxu1231,marmbrus,willbenton,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 02 23:47:21 UTC 2014,,,,,,,,,,"0|i21uxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/14 21:26;willbenton;I'm actually not sure this is a bug!  My main concern in this case is that inferring any typing for this collection of objects makes it very difficult to write meaningful queries.  In the fedmsg case, the problem was that the source data overloaded the meaning of a field name, so I was able to preprocess the fields to do the renaming.  I was thinking that maybe a good solution might be to have Spark SQL automatically rename fields with conflicting types in different records (e.g. to “branches_1” and “branches_2” in this case).;;;","02/Nov/14 00:28;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3056;;;","02/Nov/14 00:32;yhuai;[~willbenton] yeah, agreed. We need to have a better story on handling this issue. Let me think about it more. 

How about we resolve this match error first and then have another JIRA for the improvement? So, this match error will not prevent users from querying other fields.;;;","02/Nov/14 23:47;marmbrus;Issue resolved by pull request 3056
[https://github.com/apache/spark/pull/3056];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable Netty-based BlockTransferService by default,SPARK-4183,12752198,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,01/Nov/14 16:42,03/Nov/14 00:27,14/Jul/23 06:26,03/Nov/14 00:27,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"Spark's NettyBlockTransferService relies on Netty to achieve high performance and zero-copy IO, which is a big simplification over the manual connection management that's done in today's NioBlockTransferService. Additionally, the core functionality of the NettyBlockTransferService has been extracted out of spark core into the ""network"" package, with the intention of reusing this code for SPARK-3796 ([PR #3001|https://github.com/apache/spark/pull/3001/files#diff-54]).

We should turn NettyBlockTransferService on by default in order to improve debuggability and stability of the network layer (which has historically been more challenging with the current BlockTransferService).",,apachespark,ilikerps,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 00:27:31 UTC 2014,,,,,,,,,,"0|i21utr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"01/Nov/14 16:46;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3049;;;","01/Nov/14 22:19;pwendell;Reverted this due to a test issue - seems like some state is not getting cleaned up.;;;","01/Nov/14 22:38;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/3053;;;","03/Nov/14 00:27;pwendell;Resolved a second time via: https://github.com/apache/spark/pull/3053;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Caching tables containing boolean, binary, array, struct and/or map columns doesn't work",SPARK-4182,12752187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,01/Nov/14 15:34,02/Nov/14 23:14,14/Jul/23 06:26,02/Nov/14 23:14,1.1.1,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"If a table contains a column whose type is binary, array, struct, map, and for some reason, boolean, in-memory columnar caching doesn't work because a {{NoopColumnStats}} is used to collect column statistics. {{NoopColumnStats}} returns an empty statistics row, and thus breaks {{InMemoryRelation}} statistics calculation.

Execute the following snippet to reproduce this bug in {{spark-shell}}:
{code}
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.catalyst.types._

val sparkContext = sc
import sparkContext._

val sqlContext = new SQLContext(sparkContext)
import sqlContext._

case class BoolField(flag: Boolean)

val schemaRDD = parallelize(true :: false :: Nil).map(BoolField(_)).toSchemaRDD
schemaRDD.cache().count()
schemaRDD.count()
{code}
Exception thrown:
{code}
java.lang.ArrayIndexOutOfBoundsException: 4
        at org.apache.spark.sql.catalyst.expressions.GenericRow.apply(Row.scala:142)
        at org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:37)
        at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$computeSizeInBytes$1.apply(InMemoryColumnarTableScan.scala:66)
        at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$computeSizeInBytes$1.apply(InMemoryColumnarTableScan.scala:66)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.columnar.InMemoryRelation.computeSizeInBytes(InMemoryColumnarTableScan.scala:66)
        at org.apache.spark.sql.columnar.InMemoryRelation.statistics(InMemoryColumnarTableScan.scala:87)
        at org.apache.spark.sql.columnar.InMemoryRelation.statisticsToBePropagated(InMemoryColumnarTableScan.scala:73)
        at org.apache.spark.sql.columnar.InMemoryRelation.withOutput(InMemoryColumnarTableScan.scala:147)
        at org.apache.spark.sql.CacheManager$$anonfun$useCachedData$1$$anonfun$applyOrElse$1.apply(CacheManager.scala:122)
        at org.apache.spark.sql.CacheManager$$anonfun$useCachedData$1$$anonfun$applyOrElse$1.apply(CacheManager.scala:122)
        ...
{code}",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 02 23:14:58 UTC 2014,,,,,,,,,,"0|i21urb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"02/Nov/14 03:43;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3059;;;","02/Nov/14 15:20;lian cheng;This bug is introduced by [PR #2860|https://github.com/apache/spark/pull/2860].;;;","02/Nov/14 23:14;marmbrus;Issue resolved by pull request 3059
[https://github.com/apache/spark/pull/3059];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext constructor should throw exception if another SparkContext is already running,SPARK-4180,12752165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,01/Nov/14 07:46,01/Mar/15 11:02,14/Jul/23 06:26,12/Feb/15 18:47,,,,,,,,1.2.0,1.3.0,,,,,Spark Core,,,,1,,,,,,"Spark does not currently support multiple concurrently-running SparkContexts in the same JVM (see SPARK-2243).  Therefore, SparkContext's constructor should throw an exception if there is an active SparkContext that has not been shut down via {{stop()}}.

PySpark already does this, but the Scala SparkContext should do the same thing.  The current behavior with multiple active contexts is unspecified / not understood and it may be the source of confusing errors (see the user error report in SPARK-4080, for example).

This should be pretty easy to add: just add a {{activeSparkContext}} field to the SparkContext companion object and {{synchronize}} on it in the constructor and {{stop()}} methods; see PySpark's {{context.py}} file for an example of this approach.",,apachespark,joshrosen,pwendell,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4194,,,,,,SPARK-3558,,,,,,SPARK-4424,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 18:47:12 UTC 2015,,,,,,,,,,"0|i21umn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"02/Nov/14 07:27;pwendell;Yeah [~adav] just ran into an issue where a lot of our tests weren't properly cleaning SparkContext's and it was a huge pain. We could also log the callsite where a SparkContext is created, so if you try to create another one it actually tells you where the previous one was created and throws a helpful exception.;;;","05/Nov/14 23:43;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3121;;;","12/Feb/15 18:47;joshrosen;I'm going to resolve this as fixed since it was included in 1.2.0.  Now that we're about to release 1.3, I don't think that we need to backport this into branch-1.0, so I'm going to remove the {{backport-needed}} label.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop input metrics ignore bytes read in RecordReader instantiation,SPARK-4178,12752115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,31/Oct/14 23:44,03/Nov/14 23:19,14/Jul/23 06:26,03/Nov/14 23:19,1.2.0,,,,,,,,,,,,,Spark Core,,,,0,,,,,,,,apachespark,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 23:53:48 UTC 2014,,,,,,,,,,"0|i21ubj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"31/Oct/14 23:53;sandyr;Thanks [~kostas] for noticing this.;;;","31/Oct/14 23:53;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3045;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update build doc for already supporting hive 13 in jdbc/cli,SPARK-4177,12752103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,31/Oct/14 23:14,03/Nov/14 06:12,14/Jul/23 06:26,03/Nov/14 06:12,1.1.0,,,,,,,1.2.0,,,,,,Documentation,,,,0,,,,,,fix build doc since already support hive 13 in jdbc/cli,,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 23:15:50 UTC 2014,,,,,,,,,,"0|i21u93:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"31/Oct/14 23:15;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/3042;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception on stage page,SPARK-4175,12752087,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,sandyr,sandyr,31/Oct/14 22:00,01/Nov/14 00:23,14/Jul/23 06:26,01/Nov/14 00:23,1.2.0,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"{code}
14/10/31 14:52:58 WARN servlet.ServletHandler: /stages/stage/
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:313)
	at scala.None$.get(Option.scala:311)
	at org.apache.spark.ui.jobs.StagePage.taskRow(StagePage.scala:331)
	at org.apache.spark.ui.jobs.StagePage$$anonfun$8.apply(StagePage.scala:173)
	at org.apache.spark.ui.jobs.StagePage$$anonfun$8.apply(StagePage.scala:173)
	at org.apache.spark.ui.UIUtils$$anonfun$listingTable$2.apply(UIUtils.scala:282)
	at org.apache.spark.ui.UIUtils$$anonfun$listingTable$2.apply(UIUtils.scala:282)
	at scala.collection.immutable.Stream.map(Stream.scala:376)
	at org.apache.spark.ui.UIUtils$.listingTable(UIUtils.scala:282)
	at org.apache.spark.ui.jobs.StagePage.render(StagePage.scala:171)
	at org.apache.spark.ui.WebUI$$anonfun$attachPage$1.apply(WebUI.scala:68)
	at org.apache.spark.ui.WebUI$$anonfun$attachPage$1.apply(WebUI.scala:68)
	at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:68)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:164)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:722)
{code}

I'm guessing this was caused by SPARK-4016?",,apachespark,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 22:15:38 UTC 2014,,,,,,,,,,"0|i21u5j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"31/Oct/14 22:15;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3043;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EdgePartitionBuilder uses wrong value for first clustered index,SPARK-4173,12752031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,31/Oct/14 18:38,06/Nov/14 18:56,14/Jul/23 06:26,06/Nov/14 18:56,1.0.2,1.1.0,1.2.0,,,,,1.0.3,1.1.1,1.2.0,,,,GraphX,,,,0,,,,,,"Lines 48 and 49 in EdgePartitionBuilder reference {{srcIds}} before it has been initialized, causing an incorrect value to be stored for the first cluster.

https://github.com/apache/spark/blob/23468e7e96bf047ba53806352558b9d661567b23/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartitionBuilder.scala#L48-49",,ankurd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4249,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 18:56:44 UTC 2014,,,,,,,,,,"0|i21ttb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 18:56;ankurd;Issue resolved by pull request 3138
[https://github.com/apache/spark/pull/3138];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Closure problems when running Scala app that ""extends App""",SPARK-4170,12751928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,31/Oct/14 09:55,23/Feb/19 03:19,14/Jul/23 06:26,27/Nov/14 17:08,1.1.0,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"Michael Albert noted this problem on the mailing list (http://apache-spark-user-list.1001560.n3.nabble.com/BUG-when-running-as-quot-extends-App-quot-closures-don-t-capture-variables-td17675.html):

{code}
object DemoBug extends App {
    val conf = new SparkConf()
    val sc = new SparkContext(conf)

    val rdd = sc.parallelize(List(""A"",""B"",""C"",""D""))
    val str1 = ""A""

    val rslt1 = rdd.filter(x => { x != ""A"" }).count
    val rslt2 = rdd.filter(x => { str1 != null && x != ""A"" }).count
    
    println(""DemoBug: rslt1 = "" + rslt1 + "" rslt2 = "" + rslt2)
}
{code}

This produces the output:

{code}
DemoBug: rslt1 = 3 rslt2 = 0
{code}

If instead there is a proper ""main()"", it works as expected.


I also this week noticed that in a program which ""extends App"", some values were inexplicably null in a closure. When changing to use main(), it was fine.

I assume there is a problem with variables not being added to the closure when main() doesn't appear in the standard way.",,apachespark,boyork,inred,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2175,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 27 09:29:31 UTC 2014,,,,,,,,,,"0|i21t7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/14 06:48;boyork;[~srowen] this bug seems to be an issue with the way Scala has been defined and the differences between what happens at runtime versus compile time with respect to the way App leverages the [delayedInit function|http://www.scala-lang.org/api/2.11.1/index.html#scala.App].

I tried to replicate the issue on my local machine under both compile time and runtime with only the latter producing the issue (as expected through the Scala documentation). The former was tested by creating a simple application, compiled with sbt, and executed while the latter was setup within the spark-shell REPL. I'm wondering if we can't close this issue and just provide a bit of documentation somewhere to reference that, when building even simple Spark apps, extending the App interface will result in delayed initialization and, likely, set null values within that closure. Thoughts?;;;","27/Nov/14 08:25;srowen;Thanks [~boyork], I will propose a PR that resolves this with a bit of documentation somewhere.;;;","27/Nov/14 09:29;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3497;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Core] Locale dependent code,SPARK-4169,12751927,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,numbnut,numbnut,numbnut,31/Oct/14 09:49,11/Nov/14 19:10,14/Jul/23 06:26,10/Nov/14 19:38,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,patch,test,,,,"With a non english locale the method isBindCollision in

core/src/main/scala/org/apache/spark/util/Utils.scala

doesn't work because it checks the exception message, which is locale dependent.

The test suite 
core/src/test/scala/org/apache/spark/util/UtilsSuite.scala
also contains a locale dependent test ""string formatting of time durations"" which uses a DecimalSeperator which is locale dependent.

I created a pull request on github to solve this issue.","Debian, Locale: de_DE",apachespark,numbnut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,900,900,,0%,900,900,,,,,,,,,SPARK-4316,SPARK-4271,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 11:21:51 UTC 2014,,,,,,,,,,"0|i21t73:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"31/Oct/14 11:21;apachespark;User 'numbnut' has created a pull request for this issue:
https://github.com/apache/spark/pull/3036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark shell class path is not correctly set if ""spark.driver.extraClassPath"" is set in defaults.conf",SPARK-4161,12751856,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,shay,shay,31/Oct/14 02:26,09/Feb/15 21:31,14/Jul/23 06:26,09/Feb/15 21:31,1.1.0,1.2.0,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Shell,,,,0,,,,,,"(1) I want to launch a spark-shell + with jars that are only required by the driver (ie. not shipped to slaves)
 
(2) I added ""spark.driver.extraClassPath  /mypath/to.jar"" to my spark-defaults.conf
I launched spark-shell with:  ./spark-shell

Here I see on the WebUI that spark.driver.extraClassPath has been set, but I am NOT able to access any methods in the jar.

(3) I removed ""spark.driver.extraClassPath"" from my spark-default.conf
I launched spark-shell with  ./spark-shell --driver.class.path /mypath/to.jar

Again I see that the WebUI spark.driver.extraClassPath has been set. 
But this time I am able to access the methods in the jar. 


Looks like when the driver class path is loaded from spark-default.conf, the REPL's classpath is not correctly appended.","Mac, Ubuntu",andrewor14,apachespark,rdub,shay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 21:30:58 UTC 2015,,,,,,,,,,"0|i21srb:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,"01/Nov/14 17:33;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3050;;;","01/Nov/14 17:34;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3051;;;","09/Feb/15 21:06;srowen;[~andrewor14] It looks like this made it into the 1.3.0 branch but barely missed 1.2.1, and is not in the 1.2 branch? https://github.com/apache/spark/commit/742e7093eca8865225c29bacf4344f2e89bfea41  It was separately back-ported to 1.1. I think this is complete then if the commit can be cherry-picked into the 1.2 branch or am I missing a step?;;;","09/Feb/15 21:30;andrewor14;It was actually back ported to all three branches. I did a `git log --oneline | grep SPARK-4161` to confirm. I'm going to close this issue as such.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven build doesn't run JUnit test suites,SPARK-4159,12751815,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,pwendell,pwendell,30/Oct/14 22:38,30/Apr/15 12:56,14/Jul/23 06:26,30/Apr/15 12:56,,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"It turns out our Maven build isn't running any Java test suites, and likely hasn't ever.

After some fishing I believe the following is the issue. We use scalatest [1] in our maven build which, by default can't automatically detect JUnit tests. Scalatest will allow you to enumerate a list of suites via ""JUnitClasses"", but I cant' find a way for it to auto-detect all JUnit tests. It turns out this works in SBT because of our use of the junit-interface[2] which does this for you. 

An okay fix for this might be to simply enable the normal (surefire) maven tests in addition to our scalatest in the maven build. The only thing to watch out for is that they don't overlap in some way. We'd also have to copy over environment variables, memory settings, etc to that plugin.

[1] http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin
[2] https://github.com/sbt/junit-interface",,apachespark,nchammas,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4814,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 12:56:17 UTC 2015,,,,,,,,,,"0|i21sin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 23:29;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3651;;;","08/Jan/15 01:32;sandyr;[~pwendell] [~srowen] After this change it looks like the wiki's advice on how to run a single test (https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools) no longer holds.
{code}
mvn test -DwildcardSuites=org.apache.spark.io.CompressionCodecSuite
{code}
will fun that single Scala test and all Java tests.

I was able to work around this with:
{code}
mvn test -DwildcardSuites=org.apache.spark.io.CompressionCodecSuite -Dtest=nothing -DfailIfNoTests=false
{code}
is there anything we can do in the build to fix this or should I just change the wiki?;;;","08/Jan/15 10:24;srowen;[~sandyr] Ah right. Now you have two sets of properties controlling which Java test, or Scala test, to run. I would suggest setting {{failIfNoTests=true}} by default on both plugins. Then update the wiki. The current instructions aren't broken, at least, just means people will run all Java tests every time, of which there aren't many.;;;","11/Jan/15 21:18;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3993;;;","30/Apr/15 12:56;srowen;I suggest we leave it at this for now. Making tests run in 1.2 or before might only reveal that some fail, but with 1.2.2 already out and 1.4 coming shortly, may be not much value in bothering for the 1.2.x branch anyhow. Certainly even less so 1.1.x, 1.0.x.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark throws exception when Mesos resources are missing,SPARK-4158,12751786,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brenden,brenden,brenden,30/Oct/14 20:34,06/Nov/14 00:03,14/Jul/23 06:26,06/Nov/14 00:03,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Mesos,,,,0,,,,,,"Spark throws an exception when trying to check resources which haven't been offered by Mesos.  This is an error in Spark, and should be corrected as such.  Here's a sample:

{code}
val data Exception in thread ""Thread-41"" java.lang.IllegalArgumentException: No resource called cpus in [name: ""mem""
type: SCALAR
scalar {
  value: 2067.0
}
role: ""*""
, name: ""disk""
type: SCALAR
scalar {
  value: 900.0
}
role: ""*""
, name: ""ports""
type: RANGES
ranges {
  range {
    begin: 31000
    end: 32000
  }
}
role: ""*""
]
        at org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend.org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$getResource(CoarseMesosSchedulerBackend.scala:236)
        at org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1.apply(CoarseMesosSchedulerBackend.scala:200)
        at org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1.apply(CoarseMesosSchedulerBackend.scala:197)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend.resourceOffers(CoarseMesosSchedulerBackend.scala:197)
{code}",,brenden,rnowling,tnachen,tstclair,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 20:36:40 UTC 2014,,,,,,,,,,"0|i21sc7:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"31/Oct/14 20:36;rnowling;I verified that the associated patch fixes this issue on our local cluster running Spark 1.1.0 and Mesos 0.21.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consolidate usages of <driver>,SPARK-4155,12751756,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,andrewor14,andrewor14,30/Oct/14 18:41,30/Oct/14 22:33,14/Jul/23 06:26,30/Oct/14 22:33,1.0.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"We have ""<driver>"" everywhere to represent the driver. Let's consolidate the usage of this into 1 variable.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 18:50:34 UTC 2014,,,,,,,,,,"0|i21s5r:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"30/Oct/14 18:50;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Query does not work if it has ""not between "" in Spark SQL and HQL",SPARK-4154,12751686,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,ravi.pesala,ravi.pesala,30/Oct/14 14:14,31/Oct/14 18:33,14/Jul/23 06:26,31/Oct/14 18:33,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"if the query contains ""not between"" does not work.
{code}
SELECT * FROM src where key not between 10 and 20
{code}

It gives the following error

{code}
Exception in thread ""main"" java.lang.RuntimeException: 
Unsupported language features in query: SELECT * FROM src where key not between 10 and 20
TOK_QUERY
  TOK_FROM
    TOK_TABREF
      TOK_TABNAME
        src
  TOK_INSERT
    TOK_DESTINATION
      TOK_DIR
        TOK_TMP_FILE
    TOK_SELECT
      TOK_SELEXPR
        TOK_ALLCOLREF
    TOK_WHERE
      TOK_FUNCTION
        between
        KW_TRUE
        TOK_TABLE_OR_COL
          key
        10
        20

scala.NotImplementedError: No parse rules for ASTNode type: 256, text: KW_TRUE :
KW_TRUE
"" +
         
org.apache.spark.sql.hive.HiveQl$.nodeToExpr(HiveQl.scala:1088)
        
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:251)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:50)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:49)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
{code}",,apachespark,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 18:33:52 UTC 2014,,,,,,,,,,"0|i21rrj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/14 17:46;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/3017;;;","31/Oct/14 18:33;marmbrus;Issue resolved by pull request 3017
[https://github.com/apache/spark/pull/3017];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All columns of the application table in the history page are sorted in alphabetical order,SPARK-4153,12751639,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,zsxwing,zsxwing,zsxwing,30/Oct/14 09:02,30/Oct/14 22:34,14/Jul/23 06:26,30/Oct/14 22:34,1.0.2,1.1.0,,,,,,1.2.0,,,,,,Web UI,,,,0,,,,,,"""Started"", ""Completed"", ""Duration"" and ""Last Updated"" columns should be sorted by time.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 09:04:47 UTC 2014,,,,,,,,,,"0|i21rh3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"30/Oct/14 09:04;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3014;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid data change in CTAS while table already existed,SPARK-4152,12751633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,30/Oct/14 08:19,03/Nov/14 22:00,14/Jul/23 06:26,03/Nov/14 22:00,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"CREATE TABLE t1 (a String);
CREATE TABLE t1 AS SELECT key FROM src; -- throw exception
CREATE TABLE if not exists t1 AS SELECT key FROM src; -- expect do nothing, actually will overwrite the t1.",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 22:00:17 UTC 2014,,,,,,,,,,"0|i21rfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/14 08:26;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/3013;;;","03/Nov/14 22:00;marmbrus;Issue resolved by pull request 3013
[https://github.com/apache/spark/pull/3013];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark's sample uses the same seed for all partitions,SPARK-4148,12751588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,30/Oct/14 02:31,19/Dec/14 03:58,14/Jul/23 06:26,19/Dec/14 03:58,1.0.2,1.1.0,,,,,,1.0.3,1.1.1,1.2.0,,,,PySpark,,,,0,,,,,,"The current way of seed distribution makes the random sequences from partition i and i+1 offset by 1.

{code}
In [14]: import random

In [15]: r1 = random.Random(10)

In [16]: r1.randint(0, 1)
Out[16]: 1

In [17]: r1.random()
Out[17]: 0.4288890546751146

In [18]: r1.random()
Out[18]: 0.5780913011344704

In [19]: r2 = random.Random(10)

In [20]: r2.randint(0, 1)
Out[20]: 1

In [21]: r2.randint(0, 1)
Out[21]: 0

In [22]: r2.random()
Out[22]: 0.5780913011344704
{code}

So the second value from partition 1 is the same as the first value from partition 2.",,apachespark,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 03:58:54 UTC 2014,,,,,,,,,,"0|i21r5z:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.1,1.2.0,,,,,,,,,"30/Oct/14 03:36;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3010;;;","05/Nov/14 05:50;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3104;;;","05/Nov/14 05:54;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3106;;;","05/Nov/14 18:30;mengxr;Issue resolved by pull request 3104
[https://github.com/apache/spark/pull/3104];;;","06/Nov/14 21:57;mengxr;reopen this issue because branch-1.0 is not fixed;;;","19/Dec/14 03:58;joshrosen;I've merged the backport into {{branch-1.0}} (for 1.0.3), so I think that completes the backports.  Therefore, I'm going to resolve this as 'Fixed'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[GraphX] Modify option name according to example doc in SynthBenchmark ,SPARK-4146,12751584,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,grace.huang,grace.huang,grace.huang,30/Oct/14 01:35,31/Oct/14 00:31,14/Jul/23 06:26,31/Oct/14 00:31,1.1.0,1.1.1,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,"Now graphx.SynthBenchmark example has an option of iteration number named as ""niter"". However, in its document, it is named as ""niters"". The mismatch between the implementation and document causes certain IllegalArgumentException while trying that example.",,grace.huang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-10-30 01:35:44.0,,,,,,,,,,"0|i21r53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad Default for GraphLoader Edge Partitions,SPARK-4142,12751562,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jegonzal,jegonzal,jegonzal,29/Oct/14 23:35,01/Nov/14 08:18,14/Jul/23 06:26,01/Nov/14 08:18,,,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,The default number of edge partitions for the GraphLoader is set to 1 rather than the default parallelism.,,apachespark,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 00:08:11 UTC 2014,,,,,,,,,,"0|i21r07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/14 00:08;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/3006;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Guard against incompatible settings on the number of executors,SPARK-4138,12751492,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,29/Oct/14 20:33,30/Oct/14 22:31,14/Jul/23 06:26,30/Oct/14 22:31,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"After SPARK-3822 and SPARK-3795, we now set a lower bound and an upper bound for the number of executors. These settings are incompatible if the user sets the number of executors explicitly, however. We need to add a guard against this.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 29 20:42:18 UTC 2014,,,,,,,,,,"0|i21qkf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"29/Oct/14 20:42;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Relative paths don't get handled correctly by spark-ec2,SPARK-4137,12751485,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,nchammas,nchammas,29/Oct/14 20:22,06/Nov/14 04:47,14/Jul/23 06:26,06/Nov/14 04:47,1.1.0,,,,,,,1.2.0,,,,,,EC2,,,,0,,,,,,,,nchammas,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 04:46:50 UTC 2014,,,,,,,,,,"0|i21qiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 04:46;shivaram;Resolved by https://github.com/apache/spark/pull/2988;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation: tone down scary executor lost messages when killing on purpose,SPARK-4134,12751415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,andrewor14,andrewor14,29/Oct/14 16:08,23/Nov/15 17:34,14/Jul/23 06:26,23/Nov/15 17:34,1.2.0,,,,,,,1.6.0,,,,,,Spark Core,,,,3,,,,,,"After SPARK-3822 goes in, we are now able to dynamically kill executors after an application has started. However, when we do that we get a ton of scary error messages telling us that we've done wrong somehow. It would be good to detect when this is the case and prevent these messages from surfacing.

This maybe difficult, however, because the connection manager tends to be quite verbose in unconditionally logging disconnection messages. This is a very nice-to-have for 1.2 but certainly not a blocker.",,andrewor14,apachespark,ashwinshankar77,cheolsoo,mkman84,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11789,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 21:48:04 UTC 2015,,,,,,,,,,"0|i21q2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 03:58;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6310;;;","19/Nov/15 21:48;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/9780;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PARSING_ERROR(2) when upgrading issues from 1.0.2 to 1.1.0,SPARK-4133,12751361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ajnavarro,ajnavarro,29/Oct/14 11:53,05/Feb/15 18:56,14/Jul/23 06:26,05/Feb/15 18:56,1.1.0,,,,,,,,,,,,,DStreams,,,,3,,,,,,"Snappy related problems found when trying to upgrade existing Spark Streaming App from 1.0.2 to 1.1.0.

We can not run an existing 1.0.2 spark app if upgraded to 1.1.0

> IOException is thrown by snappy (parsing_error(2))

{code}
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG storage.BlockManager - Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
Executor task launch worker-0 DEBUG storage.BlockManager - Getting block broadcast_0 from memory
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG executor.Executor - Task 0's epoch is 0
Executor task launch worker-0 DEBUG storage.BlockManager - Block broadcast_0 not registered locally
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Started reading broadcast variable 0
sparkDriver-akka.actor.default-dispatcher-4 INFO  receiver.ReceiverSupervisorImpl - Registered receiver 0
Executor task launch worker-0 INFO  util.RecurringTimer - Started timer for BlockGenerator at time 1414656492400
Executor task launch worker-0 INFO  receiver.BlockGenerator - Started BlockGenerator
Thread-87 INFO  receiver.BlockGenerator - Started block pushing thread
Executor task launch worker-0 INFO  receiver.ReceiverSupervisorImpl - Starting receiver
sparkDriver-akka.actor.default-dispatcher-5 INFO  scheduler.ReceiverTracker - Registered receiver for stream 0 from akka://sparkDriver
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Starting Kafka Consumer Stream with group: stratioStreaming
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Connecting to Zookeeper: node.stratio.com:2181
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-6 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] handled message (8.442354 ms) StatusUpdate(0,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] handled message (8.412421 ms) StatusUpdate(0,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-6 DEBUG local.LocalActor - [actor] handled message (8.385471 ms) StatusUpdate(0,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
Executor task launch worker-0 INFO  utils.VerifiableProperties - Verifying properties
Executor task launch worker-0 INFO  utils.VerifiableProperties - Property group.id is overridden to stratioStreaming
Executor task launch worker-0 INFO  utils.VerifiableProperties - Property zookeeper.connect is overridden to node.stratio.com:2181
Executor task launch worker-0 INFO  utils.VerifiableProperties - Property zookeeper.connection.timeout.ms is overridden to 10000
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Reading broadcast variable 0 took 0.033998997 s
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414656492293-8ecb3e3a], Connecting to zookeeper instance at node.stratio.com:2181
Executor task launch worker-0 DEBUG zkclient.ZkConnection - Creating new ZookKeeper instance to connect to node.stratio.com:2181.
ZkClient-EventThread-169-node.stratio.com:2181 INFO  zkclient.ZkEventThread - Starting ZkClient event thread.
Executor task launch worker-0 INFO  zookeeper.ZooKeeper - Initiating client connection, connectString=node.stratio.com:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5b4bdc81
Executor task launch worker-0 DEBUG zkclient.ZkClient - Awaiting connection to Zookeeper server
Executor task launch worker-0 DEBUG zkclient.ZkClient - Waiting for keeper state SyncConnected
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Opening socket connection to server node.stratio.com/172.19.0.96:2181. Will not attempt to authenticate using SASL (unknown error)
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Socket connection established to node.stratio.com/172.19.0.96:2181, initiating session
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Session establishment request sent on node.stratio.com/172.19.0.96:2181
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Session establishment complete on server node.stratio.com/172.19.0.96:2181, sessionid = 0x1496007e6710002, negotiated timeout = 6000
Executor task launch worker-0-EventThread DEBUG zkclient.ZkClient - Received event: WatchedEvent state:SyncConnected type:None path:null
Executor task launch worker-0-EventThread INFO  zkclient.ZkClient - zookeeper state changed (SyncConnected)
Executor task launch worker-0-EventThread DEBUG zkclient.ZkClient - Leaving process event
Executor task launch worker-0 DEBUG zkclient.ZkClient - State is SyncConnected
RecurringTimer - BlockGenerator DEBUG util.RecurringTimer - Callback for BlockGenerator called at time 1414656492400
Executor task launch worker-0 DEBUG utils.KafkaScheduler - Initializing task scheduler.
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414656492293-8ecb3e3a], starting auto committer every 60000 ms
Executor task launch worker-0 DEBUG utils.KafkaScheduler - Scheduling task kafka-consumer-autocommit with initial delay 60000 ms and period 60000 ms.
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Connected to node.stratio.com:2181
Executor task launch worker-0 DEBUG consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414656492293-8ecb3e3a], entering consume 
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414656492293-8ecb3e3a], begin registering consumer stratioStreaming_ajn-stratio-1414656492293-8ecb3e3a in ZK
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG storage.BlockManager - Block broadcast_0 not registered locally
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Started reading broadcast variable 0
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Reading broadcast variable 0 took 5.5676E-5 s
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.IOException: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
	at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:545)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:125)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:232)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.IOException: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
	at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:545)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:125)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:232)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2144 cap=2144]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2144 cap=2144]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] handled message (1.213476 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2144 cap=2144]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG local.LocalActor - [actor] handled message (1.543991 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2144 cap=2144]) from Actor[akka://sparkDriver/deadLetters]
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: PARSING_ERROR(2)
        org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
        org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
        org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:545)
        org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:125)
        org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
        org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
        org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
        org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:232)
        org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: PARSING_ERROR(2)
        org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
        org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
        org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:545)
        org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:125)
        org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
        org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
        org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
        org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:232)
        org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Result resolver thread-0 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Result resolver thread-0 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Result resolver thread-0 INFO  scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result resolver thread-0 INFO  scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.TaskSchedulerImpl - Cancelling stage 0
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.TaskSchedulerImpl - Cancelling stage 0
Thread-84 INFO  scheduler.DAGScheduler - Failed to run runJob at ReceiverTracker.scala:275
Thread-85 INFO  scheduler.DAGScheduler - Failed to run runJob at ReceiverTracker.scala:275
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - Removing running stage 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - Removing running stage 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710002, packet:: clientPath:null serverPath:null finished:false header:: 1,1  replyHeader:: 1,25,-101  request:: '/consumers/stratioStreaming/ids/stratioStreaming_ajn-stratio-1414656492293-8ecb3e3a,#7b2276657273696f6e223a312c22737562736372697074696f6e223a7b227374726174696f5f73747265616d696e675f616374696f6e223a317d2c227061747465726e223a22737461746963222c2274696d657374616d70223a2231343134363536343932343737227d,v{s{31,s{'world,'anyone}}},1  response::  
{code}
> Only spark version changed

As far as we have checked, snappy will throw this error when dealing with zero bytes length arrays.

We have tried:

> Changing from snappy to LZF
{code}
Executor task launch worker-0 DEBUG zkclient.ZkConnection - Creating new ZookKeeper instance to connect to node.stratio.com:2181.
ZkClient-EventThread-166-node.stratio.com:2181 INFO  zkclient.ZkEventThread - Starting ZkClient event thread.
Executor task launch worker-0 INFO  zookeeper.ZooKeeper - Initiating client connection, connectString=node.stratio.com:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5a4f889
Executor task launch worker-0 DEBUG zkclient.ZkClient - Awaiting connection to Zookeeper server
Executor task launch worker-0 DEBUG zkclient.ZkClient - Waiting for keeper state SyncConnected
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Opening socket connection to server node.stratio.com/172.19.0.96:2181. Will not attempt to authenticate using SASL (unknown error)
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Socket connection established to node.stratio.com/172.19.0.96:2181, initiating session
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Session establishment request sent on node.stratio.com/172.19.0.96:2181
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Session establishment complete on server node.stratio.com/172.19.0.96:2181, sessionid = 0x1496007e6710009, negotiated timeout = 6000
Executor task launch worker-0-EventThread DEBUG zkclient.ZkClient - Received event: WatchedEvent state:SyncConnected type:None path:null
Executor task launch worker-0-EventThread INFO  zkclient.ZkClient - zookeeper state changed (SyncConnected)
Executor task launch worker-0-EventThread DEBUG zkclient.ZkClient - Leaving process event
Executor task launch worker-0 DEBUG zkclient.ZkClient - State is SyncConnected
ProducerSendThread- DEBUG async.ProducerSendThread - 5000 ms elapsed. Queue time reached. Sending..
ProducerSendThread- DEBUG async.ProducerSendThread - Handling 0 events
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG storage.BlockManager - Block broadcast_0 not registered locally
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Started reading broadcast variable 0
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Reading broadcast variable 0 took 1.002E-4 s
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Executor task launch worker-0 DEBUG utils.KafkaScheduler - Initializing task scheduler.
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414658065894-94786a0e], starting auto committer every 60000 ms
Executor task launch worker-0 DEBUG utils.KafkaScheduler - Scheduling task kafka-consumer-autocommit with initial delay 60000 ms and period 60000 ms.
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Connected to node.stratio.com:2181
Executor task launch worker-0 DEBUG consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414658065894-94786a0e], entering consume 
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] handled message (1.674221 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=179 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-5 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-5 DEBUG local.LocalActor - [actor] handled message (0.994221 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.EOFException: 
        java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
        java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
        java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
        java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
        org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
        org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.JobGenerator - Got event GenerateJobs(1414658066000 ms)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG streaming.DStreamGraph - Generating jobs for time 1414658066000 ms
RecurringTimer - BlockGenerator DEBUG util.RecurringTimer - Callback for BlockGenerator called at time 1414658066000
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.EOFException: 
        java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
        java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
        java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
        java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
        org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
        org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
RecurringTimer - JobGenerator DEBUG util.RecurringTimer - Callback for JobGenerator called at time 1414658066000
Result resolver thread-0 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Result resolver thread-0 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
RecurringTimer - JobGenerator DEBUG util.RecurringTimer - Callback for JobGenerator called at time 1414658066000
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.JobGenerator - Got event GenerateJobs(1414658066000 ms)
RecurringTimer - JobGenerator DEBUG util.RecurringTimer - Callback for JobGenerator called at time 1414658066000
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.JobGenerator - Got event GenerateJobs(1414658066000 ms)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG streaming.DStreamGraph - Generating jobs for time 1414658066000 ms
sparkDriver-akka.actor.default-dispatcher-3 DEBUG streaming.DStreamGraph - Generating jobs for time 1414658066000 ms
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.FilteredDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG kafka.KafkaInputDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 INFO  scheduler.ReceiverTracker - Stream 0 received 0 blocks
Result resolver thread-0 INFO  scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result resolver thread-0 INFO  scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
sparkDriver-akka.actor.default-dispatcher-3 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.FilteredDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 DEBUG dstream.FilteredDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MapValuedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.ShuffledDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MapPartitionedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 DEBUG kafka.KafkaInputDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 INFO  scheduler.ReceiverTracker - Stream 0 received 0 blocks
sparkDriver-akka.actor.default-dispatcher-5 DEBUG kafka.KafkaInputDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 INFO  scheduler.ReceiverTracker - Stream 0 received 0 blocks
sparkDriver-akka.actor.default-dispatcher-5 INFO  scheduler.TaskSchedulerImpl - Cancelling stage 0
sparkDriver-akka.actor.default-dispatcher-3 INFO  scheduler.TaskSchedulerImpl - Cancelling stage 0
Thread-85 INFO  scheduler.DAGScheduler - Failed to run runJob at ReceiverTracker.scala:275
sparkDriver-akka.actor.default-dispatcher-3 INFO  kafka.KafkaInputDStream - Persisting RDD 1 for time 1414658066000 ms to StorageLevel(false, true, false, false, 1) at time 1414658066000 ms
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414658065894-94786a0e], begin registering consumer stratioStreaming_ajn-stratio-1414658065894-94786a0e in ZK
Thread-84 INFO  scheduler.DAGScheduler - Failed to run runJob at ReceiverTracker.scala:275
sparkDriver-akka.actor.default-dispatcher-5 INFO  kafka.KafkaInputDStream - Persisting RDD 1 for time 1414658066000 ms to StorageLevel(false, true, false, false, 1) at time 1414658066000 ms
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.DAGScheduler - Removing running stage 0
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.DAGScheduler - Removing running stage 0
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.FilteredDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 DEBUG streaming.DStreamGraph - Generated 1 jobs for time 1414658066000 ms
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.FilteredDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 INFO  scheduler.JobScheduler - Added jobs for time 1414658066000 ms
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.JobGenerator - Got event DoCheckpoint(1414658066000 ms)
sparkDriver-akka.actor.default-dispatcher-5 INFO  scheduler.JobScheduler - Starting job streaming job 1414658066000 ms.0 from job set of time 1414658066000 ms
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.FilteredDStream - Time 1414658066000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG dstream.MappedDStream - Time 1414658066000 ms is valid
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710009, packet:: clientPath:null serverPath:null finished:false header:: 1,1  replyHeader:: 1,50,0  request:: '/consumers/stratioStreaming/ids/stratioStreaming_ajn-stratio-1414658065894-94786a0e,#7b2276657273696f6e223a312c22737562736372697074696f6e223a7b227374726174696f5f73747265616d696e675f616374696f6e223a317d2c227061747465726e223a22737461746963222c2274696d657374616d70223a2231343134363538303636303136227d,v{s{31,s{'world,'anyone}}},1  response:: '/consumers/stratioStreaming/ids/stratioStreaming_ajn-stratio-1414658065894-94786a0e 
{code}
> Changing spark.broadcast.compress false
{code}
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Reading broadcast variable 0 took 0.240869283 s
sparkDriver-akka.actor.default-dispatcher-4 DEBUG kafka.KafkaInputDStream - Cleared 1 RDDs that were older than 1414657342000 ms: 1414657342000 ms
sparkDriver-akka.actor.default-dispatcher-4 DEBUG kafka.KafkaInputDStream - Cleared 1 RDDs that were older than 1414657342000 ms: 1414657342000 ms
sparkDriver-akka.actor.default-dispatcher-4 DEBUG streaming.DStreamGraph - Cleared old metadata for time 1414657344000 ms
sparkDriver-akka.actor.default-dispatcher-13 DEBUG storage.BlockManagerSlaveActor - removing RDD 3
Executor task launch worker-1 DEBUG storage.BlockManager - Getting local block broadcast_1
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.MappedDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-13 INFO  storage.BlockManager - Removing RDD 3
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - [actor] handled message (134.08408 ms) RemoveRdd(3) from Actor[akka://sparkDriver/temp/$f]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - [actor] received message RemoveRdd(2) from Actor[akka://sparkDriver/temp/$i]
sparkDriver-akka.actor.default-dispatcher-4 DEBUG storage.BlockManagerSlaveActor - removing RDD 2
sparkDriver-akka.actor.default-dispatcher-4 INFO  storage.BlockManager - Removing RDD 2
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - [actor] handled message (0.050955 ms) RemoveRdd(2) from Actor[akka://sparkDriver/temp/$i]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - [actor] received message RemoveRdd(1) from Actor[akka://sparkDriver/temp/$j]
sparkDriver-akka.actor.default-dispatcher-5 DEBUG storage.BlockManagerSlaveActor - removing RDD 1
sparkDriver-akka.actor.default-dispatcher-5 INFO  storage.BlockManager - Removing RDD 1
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - [actor] handled message (0.037738 ms) RemoveRdd(1) from Actor[akka://sparkDriver/temp/$j]
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG storage.BlockManagerSlaveActor - Done removing RDD 1, response is 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - Done removing RDD 2, response is 0
sparkDriver-akka.actor.default-dispatcher-3 DEBUG storage.BlockManagerSlaveActor - Done removing RDD 3, response is 0
Executor task launch worker-1 DEBUG storage.BlockManager - Level for block broadcast_1 is StorageLevel(true, true, false, true, 1)
sparkDriver-akka.actor.default-dispatcher-3 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.MappedDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-3 DEBUG streaming.DStreamGraph - Generated 4 jobs for time 1414657344000 ms
sparkDriver-akka.actor.default-dispatcher-3 INFO  scheduler.JobScheduler - Added jobs for time 1414657344000 ms
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.JobGenerator - Got event DoCheckpoint(1414657344000 ms)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG storage.BlockManagerSlaveActor - Sent response: 0 to Actor[akka://sparkDriver/temp/$j]
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManagerSlaveActor - Sent response: 0 to Actor[akka://sparkDriver/temp/$i]
Executor task launch worker-1 DEBUG storage.BlockManager - Getting block broadcast_1 from memory
sparkDriver-akka.actor.default-dispatcher-3 DEBUG storage.BlockManagerSlaveActor - Sent response: 0 to Actor[akka://sparkDriver/temp/$f]
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG storage.BlockManager - Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
Executor task launch worker-0 DEBUG storage.BlockManager - Getting block broadcast_0 from memory
Executor task launch worker-0 DEBUG executor.Executor - Task 0's epoch is 0
Executor task launch worker-1 DEBUG executor.Executor - Task 1's epoch is 0
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG storage.BlockManager - Block broadcast_0 not registered locally
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Started reading broadcast variable 0
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.MappedDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
Executor task launch worker-0 INFO  broadcast.TorrentBroadcast - Reading broadcast variable 0 took 7.0321E-5 s
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
sparkDriver-akka.actor.default-dispatcher-5 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-5 DEBUG local.LocalActor - [actor] handled message (1.681797 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] handled message (0.688875 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=2066 cap=2066]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.MappedDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-5 DEBUG spark.HeartbeatReceiver - [actor] received message Heartbeat(localhost,[Lscala.Tuple2;@1a94802,BlockManagerId(<driver>, ajn-stratio.local, 53377, 0)) from Actor[akka://sparkDriver/temp/$c]
sparkDriver-akka.actor.default-dispatcher-3 INFO  receiver.ReceiverSupervisorImpl - Registered receiver 0
Executor task launch worker-0 INFO  util.RecurringTimer - Started timer for BlockGenerator at time 1414657344800
Executor task launch worker-0 INFO  receiver.BlockGenerator - Started BlockGenerator
Executor task launch worker-0 INFO  receiver.ReceiverSupervisorImpl - Starting receiver
Thread-87 INFO  receiver.BlockGenerator - Started block pushing thread
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.EOFException: 
        java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
        java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
        java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
        java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
        org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
        org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.EOFException: 
        java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2325)
        java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
        java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
        java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
        org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
        org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:235)
        org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:169)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Starting Kafka Consumer Stream with group: stratioStreaming
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Connecting to Zookeeper: node.stratio.com:2181
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.MappedDStream - Time 1414657344000 ms is valid
sparkDriver-akka.actor.default-dispatcher-4 DEBUG dstream.FilteredDStream - Time 1414657344000 ms is valid
Result resolver thread-0 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
Result resolver thread-0 ERROR scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.ReceiverTracker - Registered receiver for stream 0 from akka://sparkDriver
Result resolver thread-0 INFO  scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result resolver thread-0 INFO  scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
sparkDriver-akka.actor.default-dispatcher-3 DEBUG storage.BlockManagerMasterActor - [actor] received message BlockManagerHeartbeat(BlockManagerId(<driver>, ajn-stratio.local, 53377, 0)) from Actor[akka://sparkDriver/temp/$d]
sparkDriver-akka.actor.default-dispatcher-3 DEBUG storage.BlockManagerMasterActor - [actor] handled message (0.197908 ms) BlockManagerHeartbeat(BlockManagerId(<driver>, ajn-stratio.local, 53377, 0)) from Actor[akka://sparkDriver/temp/$d]
sparkDriver-akka.actor.default-dispatcher-5 DEBUG spark.HeartbeatReceiver - [actor] handled message (169.804965 ms) Heartbeat(localhost,[Lscala.Tuple2;@1a94802,BlockManagerId(<driver>, ajn-stratio.local, 53377, 0)) from Actor[akka://sparkDriver/temp/$c]
{code}
> Changing from TorrentBroadcast to HTTPBroadcast (""spark.broadcast.factory"", ""org.apache.spark.broadcast.HttpBroadcastFactory"").
{code}
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.DAGScheduler - Missing parents: List()
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - submitStage(Stage 1)
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - missing: List()
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.DAGScheduler - Submitting Stage 1 (FilteredRDD[6] at filter at FilteredDStream.scala:35), which has no missing parents
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - submitMissingTasks(Stage 1)
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710006, packet:: clientPath:null serverPath:null finished:false header:: 7,4  replyHeader:: 7,41,0  request:: '/consumers/stratioStreaming/ids/stratioStreaming_ajn-stratio-1414657757842-d7a2ca15,F  response:: #7b2276657273696f6e223a312c22737562736372697074696f6e223a7b227374726174696f5f73747265616d696e675f7265717565737473223a317d2c227061747465726e223a22737461746963222c2274696d657374616d70223a2231343134363537373538303535227d,s{41,41,1414657758409,1414657758409,0,0,0,92710854385008646,108,0,41} 
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
qtp1571833412-35 DEBUG http.HttpParser - filled 167/167
RecurringTimer - BlockGenerator DEBUG util.RecurringTimer - Callback for BlockGenerator called at time 1414657758400
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.MappedDStream - Time 1414657758000 ms is valid
sparkDriver-akka.actor.default-dispatcher-12 DEBUG dstream.FilteredDStream - Time 1414657758000 ms is valid
qtp1571833412-35 - /broadcast_0 DEBUG server.Server - REQUEST /broadcast_0 on BlockingHttpConnection@7cbd5b8f,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=-5,l=10,c=0},r=1
sparkDriver-akka.actor.default-dispatcher-12 DEBUG streaming.DStreamGraph - Generated 14 jobs for time 1414657758000 ms
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Added jobs for time 1414657758000 ms
sparkDriver-akka.actor.default-dispatcher-3 INFO  scheduler.JobScheduler - Starting job streaming job 1414657758000 ms.0 from job set of time 1414657758000 ms
sparkDriver-akka.actor.default-dispatcher-12 DEBUG scheduler.JobGenerator - Got event DoCheckpoint(1414657758000 ms)
qtp1571833412-35 - /broadcast_0 DEBUG server.Server - RESPONSE /broadcast_0  404 handled=true
pool-7-thread-1 INFO  spark.SparkContext - Starting job: collect at ActionBaseFunction.java:65
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710006, packet:: clientPath:null serverPath:null finished:false header:: 8,8  replyHeader:: 8,41,0  request:: '/consumers/stratioStreaming/ids,T  response:: v{'stratioStreaming_ajn-stratio-1414657757842-d7a2ca15} 
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710006, packet:: clientPath:null serverPath:null finished:false header:: 9,4  replyHeader:: 9,41,0  request:: '/consumers/stratioStreaming/ids/stratioStreaming_ajn-stratio-1414657757842-d7a2ca15,F  response:: #7b2276657273696f6e223a312c22737562736372697074696f6e223a7b227374726174696f5f73747265616d696e675f7265717565737473223a317d2c227061747465726e223a22737461746963222c2274696d657374616d70223a2231343134363537373538303535227d,s{41,41,1414657758409,1414657758409,0,0,0,92710854385008646,108,0,41} 
pool-7-thread-1 INFO  spark.SparkContext - Job finished: collect at ActionBaseFunction.java:65, took 3.9409E-5 s
Executor task launch worker-0 DEBUG storage.BlockManager - Getting local block broadcast_0
Executor task launch worker-0 DEBUG storage.BlockManager - Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
Executor task launch worker-0 DEBUG storage.BlockManager - Getting block broadcast_0 from memory
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Finished job streaming job 1414657758000 ms.0 from job set of time 1414657758000 ms
Executor task launch worker-0 INFO  storage.BlockManager - Found block broadcast_0 locally
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Starting job streaming job 1414657758000 ms.1 from job set of time 1414657758000 ms
Executor task launch worker-0 DEBUG executor.Executor - Task 0's epoch is 0
Executor task launch worker-0 ERROR executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.io.FileNotFoundException: http://172.17.42.1:34477/broadcast_0
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
	at org.apache.spark.broadcast.HttpBroadcast$.org$apache$spark$broadcast$HttpBroadcast$$read(HttpBroadcast.scala:197)
	at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:89)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
pool-7-thread-1 INFO  spark.SparkContext - Starting job: collect at ActionBaseFunction.java:65
pool-7-thread-1 INFO  spark.SparkContext - Job finished: collect at ActionBaseFunction.java:65, took 3.1765E-5 s
Executor task launch worker-0 INFO  util.RecurringTimer - Started timer for BlockGenerator at time 1414657758600
Executor task launch worker-0 INFO  receiver.BlockGenerator - Started BlockGenerator
Executor task launch worker-0 INFO  receiver.ReceiverSupervisorImpl - Starting receiver
sparkDriver-akka.actor.default-dispatcher-2 INFO  storage.MemoryStore - ensureFreeSpace(3136) called with curMem=1216, maxMem=991470551
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Finished job streaming job 1414657758000 ms.1 from job set of time 1414657758000 ms
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Starting job streaming job 1414657758000 ms.2 from job set of time 1414657758000 ms
sparkDriver-akka.actor.default-dispatcher-5 DEBUG local.LocalActor - [actor] received message StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=1868 cap=1868]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-2 INFO  storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 945.5 MB)
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710006, packet:: clientPath:null serverPath:null finished:false header:: 10,8  replyHeader:: 10,41,0  request:: '/brokers/ids,F  response:: v{'7} 
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManager - Put block broadcast_1 locally took  7 ms
sparkDriver-akka.actor.default-dispatcher-2 DEBUG storage.BlockManager - Putting block broadcast_1 without replication took  7 ms
pool-7-thread-1 INFO  spark.SparkContext - Starting job: collect at ActionBaseFunction.java:65
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Starting Kafka Consumer Stream with group: stratioStreaming
sparkDriver-akka.actor.default-dispatcher-3 INFO  receiver.ReceiverSupervisorImpl - Registered receiver 0
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Connecting to Zookeeper: node.stratio.com:2181
Executor task launch worker-0 INFO  utils.VerifiableProperties - Verifying properties
Executor task launch worker-0 INFO  utils.VerifiableProperties - Property group.id is overridden to stratioStreaming
Executor task launch worker-0 INFO  utils.VerifiableProperties - Property zookeeper.connect is overridden to node.stratio.com:2181
Executor task launch worker-0 INFO  utils.VerifiableProperties - Property zookeeper.connection.timeout.ms is overridden to 10000
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414657758445-7b49bb3b], Connecting to zookeeper instance at node.stratio.com:2181
sparkDriver-akka.actor.default-dispatcher-4 INFO  scheduler.ReceiverTracker - Registered receiver for stream 0 from akka://sparkDriver
Thread-99 INFO  receiver.BlockGenerator - Started block pushing thread
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.DAGScheduler - Submitting 2 missing tasks from Stage 1 (FilteredRDD[6] at filter at FilteredDStream.scala:35)
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.DAGScheduler - New pending tasks: Set(ResultTask(1, 1), ResultTask(1, 0))
sparkDriver-akka.actor.default-dispatcher-2 INFO  scheduler.TaskSchedulerImpl - Adding task set 1.0 with 2 tasks
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
sparkDriver-akka.actor.default-dispatcher-2 DEBUG scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Reading reply sessionid:0x1496007e6710006, packet:: clientPath:null serverPath:null finished:false header:: 11,4  replyHeader:: 11,41,0  request:: '/brokers/ids/7,F  response:: #7b226a6d785f706f7274223a393939392c2274696d657374616d70223a2231343134363535333735373234222c22686f7374223a226e6f64652e7374726174696f2e636f6d222c2276657273696f6e223a312c22706f7274223a393039327d,s{18,18,1414655375792,1414655375792,0,0,0,92710854385008640,95,0,18} 
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] received message ReviveOffers from Actor[akka://sparkDriver/deadLetters]
pool-7-thread-1 INFO  spark.SparkContext - Job finished: collect at ActionBaseFunction.java:65, took 3.0385E-5 s
Executor task launch worker-0 DEBUG zkclient.ZkConnection - Creating new ZookKeeper instance to connect to node.stratio.com:2181.
Executor task launch worker-0 INFO  zookeeper.ZooKeeper - Initiating client connection, connectString=node.stratio.com:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2fdc4517
ZkClient-EventThread-189-node.stratio.com:2181 INFO  zkclient.ZkEventThread - Starting ZkClient event thread.
sparkDriver-akka.actor.default-dispatcher-5 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 1
sparkDriver-akka.actor.default-dispatcher-5 DEBUG local.LocalActor - [actor] handled message (4.883443 ms) StatusUpdate(0,FAILED,java.nio.HeapByteBuffer[pos=0 lim=1868 cap=1868]) from Actor[akka://sparkDriver/deadLetters]
sparkDriver-akka.actor.default-dispatcher-3 DEBUG scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1, runningTasks: 0
sparkDriver-akka.actor.default-dispatcher-3 INFO  scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 880 bytes)
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Opening socket connection to server node.stratio.com/172.19.0.96:2181. Will not attempt to authenticate using SASL (unknown error)
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Socket connection established to node.stratio.com/172.19.0.96:2181, initiating session
Executor task launch worker-0-SendThread(node.stratio.com:2181) DEBUG zookeeper.ClientCnxn - Session establishment request sent on node.stratio.com/172.19.0.96:2181
Executor task launch worker-0-SendThread(node.stratio.com:2181) INFO  zookeeper.ClientCnxn - Session establishment complete on server node.stratio.com/172.19.0.96:2181, sessionid = 0x1496007e6710007, negotiated timeout = 6000
Executor task launch worker-0 DEBUG zkclient.ZkClient - Awaiting connection to Zookeeper server
Executor task launch worker-0 DEBUG zkclient.ZkClient - Waiting for keeper state SyncConnected
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Finished job streaming job 1414657758000 ms.2 from job set of time 1414657758000 ms
Executor task launch worker-0-EventThread DEBUG zkclient.ZkClient - Received event: WatchedEvent state:SyncConnected type:None path:null
Executor task launch worker-0-EventThread INFO  zkclient.ZkClient - zookeeper state changed (SyncConnected)
sparkDriver-akka.actor.default-dispatcher-12 INFO  scheduler.JobScheduler - Starting job streaming job 1414657758000 ms.3 from job set of time 1414657758000 ms
Executor task launch worker-0 DEBUG zkclient.ZkClient - State is SyncConnected
Result resolver thread-0 WARN  scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.FileNotFoundException: http://172.17.42.1:34477/broadcast_0
        sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
        org.apache.spark.broadcast.HttpBroadcast$.org$apache$spark$broadcast$HttpBroadcast$$read(HttpBroadcast.scala:197)
        org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:89)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:159)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Executor task launch worker-0-EventThread DEBUG zkclient.ZkClient - Leaving process event
Executor task launch worker-0 DEBUG utils.KafkaScheduler - Initializing task scheduler.
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] handled message (7.610459 ms) ReviveOffers from Actor[akka://sparkDriver/deadLetters]
Executor task launch worker-0 INFO  consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414657758445-7b49bb3b], starting auto committer every 60000 ms
Executor task launch worker-1 INFO  executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
Executor task launch worker-0 DEBUG utils.KafkaScheduler - Scheduling task kafka-consumer-autocommit with initial delay 60000 ms and period 60000 ms.
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] received message StatusUpdate(1,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
Executor task launch worker-0 INFO  kafka.KafkaReceiver - Connected to node.stratio.com:2181
Executor task launch worker-0 DEBUG consumer.ZookeeperConsumerConnector - [stratioStreaming_ajn-stratio-1414657758445-7b49bb3b], entering consume 
sparkDriver-akka.actor.default-dispatcher-3 DEBUG local.LocalActor - [actor] handled message (0.07141 ms) StatusUpdate(1,RUNNING,java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]) from Actor[akka://sparkDriver/deadLetters]
{code}
but with no luck for the moment.

",,ajnavarro,derrickburns,DjvuLee,joshrosen,lev,maropu,mkim,smolav,tdas,vitalii.migov@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4641,,,,,,,,SPARK-3630,SPARK-3958,,,,,,,,,,,,,"04/Nov/14 16:03;vitalii.migov@gmail.com;spark_ex.logs;https://issues.apache.org/jira/secure/attachment/12679241/spark_ex.logs",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 18:56:42 UTC 2015,,,,,,,,,,"0|i21pr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/14 11:58;ajnavarro;Existing Spark Streaming app can not be upgraded from 1.0.2 to 1.1.0;;;","29/Oct/14 17:48;joshrosen;Since you mentioned that you see a similar issue when using HTTPBroadcast, could you post the stacktrace from that case, too?  Similarly, can you post the stacktrace when broadcast compression is disabled?;;;","29/Oct/14 19:15;joshrosen;Also, can you paste more of the log leading up to the error?  It would be helpful to see any other log messages from broadcast, such as messages about it fetching pieces / blocks.;;;","29/Oct/14 21:20;joshrosen;Also, could you enable debug logging and share the executor logs?  If you're able to reliably reproduce this bug, please email me at joshrosen@databricks.com and I'd be glad to hop on Skype to help you configure logging, etc.;;;","30/Oct/14 09:01;ajnavarro;Hi Josh,
I edited the issue to add more logs.
Thanks.;;;","03/Nov/14 22:56;joshrosen;Do you happen to be creating multiple running SparkContexts in the same JVM by any chance?;;;","04/Nov/14 08:09;ajnavarro;Yes, We are creating one local SparkContext for each SparkStreamingContext.;;;","04/Nov/14 16:04;vitalii.migov@gmail.com;The same issue observed in the similar spark streaming application.
Note: Broadcast factory changed to the HttpBroadcastFactory as suggested, so there is 
 ""java.io.FileNotFoundException: http://10.8.0.22:44907/broadcast_0"" exception instead of the ""Snappy java.io.IOException: PARSING_ERROR(2)""

Full logs attached to the issue: spark_ex.logs

I think that something wrong with the handing of the ""broadcast_0"" in the BlockManager:
14/11/04 17:20:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1216.0 B, free 983.1 MB)
14/11/04 17:20:39 DEBUG BlockManager: Put block broadcast_0 locally took  84 ms
14/11/04 17:20:39 DEBUG BlockManager: Putting block broadcast_0 without replication took  86 ms
14/11/04 17:20:39 DEBUG BlockManager: Getting local block broadcast_0
14/11/04 17:20:39 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
14/11/04 17:20:39 DEBUG BlockManager: Getting block broadcast_0 from memory
14/11/04 17:20:39 INFO BlockManager: Found block broadcast_0 locally
14/11/04 17:20:57 WARN BlockManager: Block broadcast_0 already exists on this machine; not re-adding it
14/11/04 17:20:57 DEBUG BlockManager: Getting local block broadcast_0
14/11/04 17:20:57 DEBUG BlockManager: Block broadcast_0 not registered locally
14/11/04 17:20:57 DEBUG BlockManager: Getting remote block broadcast_0
14/11/04 17:20:57 DEBUG BlockManagerMasterActor: [actor] received message GetLocations(broadcast_0) from Actor[akka://sparkDriver/temp/$l]
14/11/04 17:20:57 DEBUG BlockManager: Block broadcast_0 not found
14/11/04 17:20:57 DEBUG BlockManagerMasterActor: [actor] handled message (0.117676 ms) GetLocations(broadcast_0) from Actor[akka://sparkDriver/temp/$l]
14/11/04 17:20:57 INFO HttpBroadcast: Started reading broadcast variable 0
14/11/04 17:20:57 DEBUG HttpBroadcast: broadcast read server: http://10.8.0.22:44907 id: broadcast-0
14/11/04 17:20:57 DEBUG HttpBroadcast: broadcast not using security
14/11/04 17:20:57 DEBUG RecurringTimer: Callback for BlockGenerator called at time 1415114457200
14/11/04 17:20:57 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.FileNotFoundException: http://10.8.0.22:44907/broadcast_0


;;;","04/Nov/14 17:22;vitalii.migov@gmail.com;After additional investigation of this issue i have found that SparkContext mistakenly created by twice.

 This code will lead to the double creation of the SparkContext and as are result of this - lead to the described error.
  val sparkConf = sparkCtxBuilder.createSparkConf()
  val sparkContext = new SparkContext(sparkConf)
  val ssc =  new StreamingContext(sparkConf, Seconds(5))  <!---  sparkConf passed

 This is corrected initialization sequence. After changing initial code to this - error is gone:
  val sparkConf = sparkCtxBuilder.createSparkConf()
  val sparkContext = new SparkContext(sparkConf)
  val ssc =  new StreamingContext(sparkContext, Seconds(5)) <!-- sparkContext passed
;;;","04/Nov/14 17:56;joshrosen;Spark doesn't support multiple active SparkContexts in the same JVM, although this isn't well-documented and there's no error-checking for this (PySpark has checks for this, though).  This isn't to say that we can't / won't eventually support multiple contexts per JVM (see SPARK-2243), but that could be somewhat difficult in the very short term because there may be several baked-in assumptions that we'll have to address (the (effectively) global SparkEnv, for example).  In both this issue and the issue reported in a comment on SPARK-4080, I think the symptoms are being caused by the two SparkContexts' block / broadcast managers becoming mixed up so that executors belonging to one SparkContext are somehow fetching blocks added by another SparkContext (or are deleting each others' blocks; see SPARK-3148).

SPARK-4180 will add proper error-detection to detect when multiple contexts have been created and to help debug where this is happening (e.g. by printing the callsite that created the currently active context).;;;","05/Nov/14 06:06;vitalii.migov@gmail.com;The problem is that if we mistakenly pass SparkConf to the  StreamingContext instead of the previously created SparkContext then StreamingContext  will silently create new ( second ) SparkContext . This error is hard to find and it can lead to the spontaneous errors related to the block / broadcast managers which in turn can manifest itself by cryptic exceptions like : ""java.io.IOException: PARSING_ERROR(2)"".
This can be easily prevented if StreamingContext will require to pass SparkContext as argument to constructor and will not try to create new SparkContext if SparkConf   passed.
;;;","06/Nov/14 19:45;joshrosen;We can't remove that StreamingContext constructor since we don't want to break binary compatibility, but we can add a warning / raise an error.  I'm working on a patch for this now.;;;","25/Nov/14 13:39;tdas;[~joshrosen] Any more comments regarding this error based on patches that have gone in?;;;","20/Dec/14 22:04;derrickburns;I am getting a similar error with Spark 1.1.1. I am not creating a second SparkContext.

2014-12-20 20:04:16,233 WARN  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logWarning(71)) - Lost task 30.0 in stage 242.0 (TID 65228, ip-10-215-133-219.us-west-2.compute.internal): java.io.IOException: failed to uncompress the chunk: PARSING_ERROR(2)
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:383)
        java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)
        java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)
        java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1171)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:89)
        org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
        org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745);;;","24/Dec/14 21:34;tdas;[~derrickburns] Are you sure you are not creating multiple Spark Contexts? See previous posts on this JIRA to find how you might be accidentally creating to SparkContexts.;;;","29/Jan/15 19:27;tdas;[~derrickburns] Any thoughts? If there arent any more thoughts, I am going to close this JIRA. As of now, I am going to remove the blocker priority. 

;;;","29/Jan/15 23:47;derrickburns;I worked around it, so feel free....

On Thu, Jan 29, 2015 at 11:28 AM, Tathagata Das (JIRA) <jira@apache.org>

;;;","05/Feb/15 18:56;joshrosen;I'm going to close this issue for now, since I think this was caused by multiple SparkContexts and not a Spark bug, per-se.  Please comment here or open a new issue if you see {{PARSING_ERROR(2)}} on newer versions of Spark without creating multiple active SparkContexts.

Note that {{FAILED_TO_UNCOMPRESS(5)}} is a distinct issue, which is being addressed at SPARK-4105.  That issue is still open because we've seen reports of it in newer versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loadLibSVMFile does not handle extra whitespace,SPARK-4130,12751323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jegonzal,jegonzal,jegonzal,29/Oct/14 07:05,30/Oct/14 07:06,14/Jul/23 06:26,30/Oct/14 07:06,,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"When testing MLlib on the splice site data (http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#splice-site) the loadSVM.  To reproduce in spark shell:

{code:scala}
import org.apache.spark.mllib.util.MLUtils
val data =  MLUtils.loadLibSVMFile(sc, ""hdfs://ec2-54-200-69-227.us-west-2.compute.amazonaws.com:9000/splice_site.t"")
{code}

generates the error:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:73 failed 4 times, most recent failure: Exception failure in TID 335 on host ip-172-31-31-54.us-west-2.compute.internal: java.lang.NumberFormatException: For input string: """"
        java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        java.lang.Integer.parseInt(Integer.java:504)
        java.lang.Integer.parseInt(Integer.java:527)
        scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
        scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4$$anonfun$5.apply(MLUtils.scala:81)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4$$anonfun$5.apply(MLUtils.scala:79)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4.apply(MLUtils.scala:79)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4.apply(MLUtils.scala:76)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,jegonzal,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 07:06:35 UTC 2014,,,,,,,,,,"0|i21pin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/14 07:37;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2996;;;","30/Oct/14 07:06;mengxr;Issue resolved by pull request 2996
[https://github.com/apache/spark/pull/2996];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify serialization and call API in MLlib Python,SPARK-4124,12751258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,28/Oct/14 22:34,31/Oct/14 05:25,14/Jul/23 06:26,31/Oct/14 05:25,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,0,,,,,,"There are much repeated code  to similar things, convert RDD into Java object, convert arguments into java, convert to result rdd/object into python, they could be simplified to share the same code.",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 05:25:43 UTC 2014,,,,,,,,,,"0|i21p47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/14 07:26;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2995;;;","31/Oct/14 05:25;mengxr;Issue resolved by pull request 2995
[https://github.com/apache/spark/pull/2995];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master build failures after shading commons-math3,SPARK-4121,12751232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,mengxr,mengxr,28/Oct/14 20:54,24/Nov/14 19:44,14/Jul/23 06:26,01/Nov/14 22:22,1.2.0,,,,,,,1.2.0,,,,,,Build,MLlib,Spark Core,,0,,,,,,"The Spark master Maven build kept failing after we replace colt with commons-math3 and shade the latter:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/

The error message is:

{code}
KMeansClusterSuite:
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- task size should be small in both training and prediction *** FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 9, localhost): java.io.InvalidClassException: org.apache.spark.util.random.PoissonSampler; local class incompatible: stream classdesc serialVersionUID = -795011761847245121, local class serialVersionUID = 4249244967777318419
        java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:186)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

This test passed in local sbt build. So the issue should be caused by shading. Maybe there are two versions of commons-math3 (hadoop depends on it), or MLlib doesn't use the shaded version at compile.

[~srowen] Could you take a look? Thanks!",,apachespark,joshrosen,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4022,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 01 22:22:16 UTC 2014,,,,,,,,,,"0|i21oyn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"28/Oct/14 21:01;pwendell;[~srowen] - can you help with this? This is likely happening because the PoissonSampler on the driver is using the classpath from Maven (with the unmodified version of PoissonSampler) and the executors are using the version from the assembly jar, which has package relocations of the commons math dependency in the byte code. This is a test that uses ""local-cluster"" mode.

Is there a reason we are doing these relocations in the assembly only? Would it be better to actually shade-and-inline commons-math in both the spark-core and spark-mllib package jars?

Having discrepancies between the assmebly and package jars I'm guessing could lead to problems other than just this test issue. It also means that applications which compile against Spark's dependencies rather than running through the Spark assembly packages won't get the benefit of the shading we've done.;;;","28/Oct/14 21:03;joshrosen;Here's an easy command to reproduce this:

{code}
mvn -DskipTests package
mvn test -DwildcardSuites=org.apache.spark.mllib.clustering.KMeansClusterSuite
{code};;;","28/Oct/14 21:12;srowen;Yeah I was seeing this locally, but not on the Jenkins test build, so chalked it up to weirdness in my build.
I think the answer may indeed be to do the relocating in core/mllib itself. I'll get on that.;;;","30/Oct/14 18:04;mengxr;[~srowen] Are you working on this? I played with the pom files yesterday. I found the serialVersionUID mismatch was because the `rng` in `PoissonSampler` is compiled to a public API in Java. Even I changed it to `private[this] val`, it is still public in Java. The problem is similar to:

http://www.scala-lang.org/old/node/11418.html

If I declare it as an `AnyRef`, it solved the serialVersionUID problem, then I got ClassNotFound from executors, because commons-math3 is not in the assembly jar nor on worker classpath. To not block the release, I suggest that we do not shade commons-math3 but set its version based on the hadoop profile. I can prepare a PR, but please let us know if you have a better solution.;;;","30/Oct/14 19:52;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3023;;;","30/Oct/14 19:57;srowen;Sorry I have been traveling without internet. When I tried revising the shading last night, tests hung. Probably due to network weirdness. 

My concern was that by shading just 2 modules you have to watch for other modules that accidentally start using Math3. 

I think a less drastic solution like modulating the version with hadoop profile sounds fine in principle. I'd have to look up what version goes with what. I think we are not using any very new methods. 

Sorry about that and please proceed as you see fit though I will have another look tonight. ;;;","30/Oct/14 20:22;mengxr;No, we are not using fancy methods. v3.1.1 is good for us. I just sent out the PR. Please help review. Thanks!;;;","01/Nov/14 22:22;pwendell;Okay I merged this. Let's see how it goes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Join of multiple tables with syntax like SELECT .. FROM T1,T2,T3.. does not work in SparkSQL",SPARK-4120,12751194,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,ravi.pesala,ravi.pesala,28/Oct/14 18:45,31/Oct/14 00:16,14/Jul/23 06:26,31/Oct/14 00:16,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The queries with more than like 2 tables does not work. 
{code}
sql(""SELECT * FROM records1 as a,records2 as b,records3 as c where a.key=b.key and a.key=c.key"")
{code}

The above query gives following exception.
{code}
Exception in thread ""main"" java.lang.RuntimeException: [1.40] failure: ``UNION'' expected but `,' found

SELECT * FROM records1 as a,records2 as b,records3 as c where a.key=b.key and a.key=c.key
                                       ^
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:33)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:75)

{code}",,apachespark,glenn.strycker@gmail.com,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 00:16:07 UTC 2014,,,,,,,,,,"0|i21oqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/14 03:00;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2987;;;","31/Oct/14 00:16;marmbrus;Issue resolved by pull request 2987
[https://github.com/apache/spark/pull/2987];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyhon UDF on ArrayType,SPARK-4113,12750996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,28/Oct/14 06:20,29/Oct/14 05:17,14/Jul/23 06:26,29/Oct/14 02:48,1.2.0,,,,,,,1.2.0,,,,,,PySpark,SQL,,,0,,,,,,"from Matei:

I have a table where column c is of type array<int>. However the following set of commands fails:
sqlContext.registerFunction(""py_func"", lambda a: len(a))

%sql select py_func(c) from some_temp

Error in SQL statement: java.lang.RuntimeException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 252.0 failed 4 times, most recent failure: Lost task 2.3 in stage 252.0 (TID 8454, ip-10-0-157-104.us-west-2.compute.internal): net.razorvine.pickle.PickleException: couldn't introspect javabean: java.lang.IllegalArgumentException: wrong number of arguments
        net.razorvine.pickle.Pickler.put_javabean(Pickler.java:603)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:299)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:392)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:195)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:392)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:195)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.dump(Pickler.java:95)
The same function works if I select a Row from my table into Python and call it on its third column.",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 29 05:17:02 UTC 2014,,,,,,,,,,"0|i21njr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/14 05:17;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2973;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong comments about default settings in spark-daemon.sh,SPARK-4110,12750990,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,28/Oct/14 05:49,28/Oct/14 19:29,14/Jul/23 06:26,28/Oct/14 19:29,1.2.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"In spark-daemon.sh, thare are following comments.

{code}
#   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_PREFIX}/conf.
#   SPARK_LOG_DIR   Where log files are stored.  PWD by default.
{code}

But, I think the default value for SPARK_CONF_DIR is ${SPARK_HOME}/conf and for SPARK_LOG_DIR is ${SPARK_HOME}/logs.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 05:53:26 UTC 2014,,,,,,,,,,"0|i21nif:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"28/Oct/14 05:53;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2972;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task.stageId is not been deserialized correctly,SPARK-4109,12750987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,luluorta,luluorta,28/Oct/14 05:42,03/Nov/14 05:55,14/Jul/23 06:26,03/Nov/14 05:55,1.0.0,1.0.2,,,,,,1.0.3,,,,,,Spark Core,,,,0,,,,,,"The two subclasses of Task, ShuffleMapTask and ResultTask, do not correctly deserialize stageId. Therefore, the accessing of TaskContext.stageId always returns zero value to the user.",,apachespark,luluorta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 05:50:25 UTC 2014,,,,,,,,,,"0|i21nhr:",9223372036854775807,,,,,,,,,,,,,,1.0.3,,,,,,,,,,,"28/Oct/14 05:50;apachespark;User 'luluorta' has created a pull request for this issue:
https://github.com/apache/spark/pull/2971;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect handling of Channel.read()'s return value may lead to data truncation,SPARK-4107,12750969,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,28/Oct/14 03:31,29/Oct/14 20:11,14/Jul/23 06:26,28/Oct/14 19:30,1.1.0,1.1.1,1.2.0,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"When using {{Channel.read()}}, we need to properly handle the return value and account for the case where we've read fewer bytes than expected.  There are a few places where we don't do this properly, which may lead to incorrect data truncation in rare circumstances.  I've opened a PR to fix this.",,apachespark,cfregly,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4105,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 07:23:12 UTC 2014,,,,,,,,,,"0|i21ndr:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"28/Oct/14 03:32;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2969;;;","28/Oct/14 07:23;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2974;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FAILED_TO_UNCOMPRESS(5) errors when fetching shuffle data with sort-based shuffle,SPARK-4105,12750940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,joshrosen,joshrosen,28/Oct/14 00:19,14/Dec/21 02:39,14/Jul/23 06:26,09/Dec/16 23:45,1.2.0,1.2.1,1.3.0,1.4.1,1.5.1,1.6.1,2.0.0,2.2.0,,,,,,Shuffle,Spark Core,,,42,,,,,,"We have seen non-deterministic {{FAILED_TO_UNCOMPRESS(5)}} errors during shuffle read.  Here's a sample stacktrace from an executor:

{code}
14/10/23 18:34:11 ERROR Executor: Exception in task 1747.3 in stage 11.0 (TID 33053)
java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:427)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:127)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1090)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1.apply(ShuffleBlockFetcherIterator.scala:116)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1.apply(ShuffleBlockFetcherIterator.scala:115)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:243)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:129)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

Here's another occurrence of a similar error:

{code}
java.io.IOException: failed to read chunk
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:348)
        org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:159)
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
        java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)
        java.io.ObjectInputStream$BlockDataInputStream.read(ObjectInputStream.java:2712)
        java.io.ObjectInputStream$BlockDataInputStream.readFully(ObjectInputStream.java:2742)
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1687)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:129)
        org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
        org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
        org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

The first stacktrace was reported by a Spark user.  The second stacktrace occurred when running

{code}
import java.util.Random


val numKeyValPairs=1000
val numberOfMappers=200
val keySize=10000

for (i <- 0 to 19) {
val pairs1 = sc.parallelize(0 to numberOfMappers, numberOfMappers).flatMap(p=>{
  val randGen = new Random
  val arr1 = new Array[(Int, Array[Byte])](numKeyValPairs)
  for (i <- 0 until numKeyValPairs){
    val byteArr = new Array[Byte](keySize)
    randGen.nextBytes(byteArr)
    arr1(i) = (randGen.nextInt(Int.MaxValue),byteArr)
  }
  arr1
})
  pairs1.groupByKey(numberOfMappers).count
}
{code}

This job frequently runs without any problems, but when it fails it seem that every post-shuffle task fails with either PARSING_ERROR(2), FAILED_TO_UNCOMPRESS(5), or some other decompression error.  I've seen reports of similar problems when using LZF compression, so I think that this is caused by some sort of general stream corruption issue. 

This issue has been observed even when no spilling occurs, so I don't believe that this is due to a bug in spilling code.

I was unable to reproduce this when running this code in a fresh Spark EC2 cluster and we've been having a hard time finding a deterministic reproduction.",,aakash.mandlik,andyliu1227,ankitBhardwaj12,apachespark,ashrowty,ASRRAJ,asukhenko,bherta,bpasley,BranY,cryptoe,daniel.siegmann.aol,deepujain,devaraj,dhananjaydp,Dhruve Ashar,diederik,DjvuLee,DoingDone9,douglaz,dweeks,edonato,feiwang,geynard,glenn.strycker@gmail.com,H4ml3t,himanish,irashid,jasonr,jean,jelly,jincheng,jkirsch,jkleckner,jonathak,joshrosen,kalle,kamilmroczek,kexinxie,lasanthafdo,lianhuiwang,mannswinky,maropu,mmitsuto,mmoroz,Mr.黄,MSeal,mvherweg,nadenf,neelesh77,nezihyigitbasi,nishanthps,ourui521314,parmesan,pwendell,rdub,renuyadav,richardatcloudera,romi-totango,rrag,silvermast,ssonker,SuYan,tcondie,tenstriker,tgraves,umesh9794@gmail.com,vadim_ps,vgirijira,vijay.saikam,waldoppper,wy90021,Xia Hu,yashwanth.rao11@gmail.com,zhaoyunjiong,zheng.tan,zhpengg,zhuqi,zzhan,,,,,,,,,,,,,,,SPARK-12418,,,,,,,,SPARK-7660,SPARK-3630,SPARK-4107,SPARK-27562,SPARK-26089,,,,,,,,,,"07/May/15 14:08;geynard;JavaObjectToSerialize.java;https://issues.apache.org/jira/secure/attachment/12731180/JavaObjectToSerialize.java","07/May/15 14:08;geynard;SparkFailedToUncompressGenerator.scala;https://issues.apache.org/jira/secure/attachment/12731179/SparkFailedToUncompressGenerator.scala",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 14 02:37:14 UTC 2021,,,,,,,,,,"0|i21n7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/14 20:11;joshrosen;It seems plausible that SPARK-4107 could have caused this issue, but I'm waiting for confirmation that it fixes this issue.;;;","17/Dec/14 03:09;silvermast;Similarly, I hit this in 1.1.1:

{code}
Job aborted due to stage failure: Task 0 in stage 3919.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3919.0 (TID 2467, ...): java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:362)
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:384)
        java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)
        java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)
        java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1171)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:144)
        org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
        org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:48)
        org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
{code};;;","18/Dec/14 18:43;nishanthps;I hit this error when using ALS in Mllib. I am not using the KryoSerializer but I am using the default serializer. I faced this error when running Spark 1.1.1.

        java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:362)
        org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:159)
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
        java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2283)
        java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2296)
        java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:2986)
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1672)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1341)
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1341)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1171)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:144)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
        scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        java.lang.Thread.run(Thread.java:722);;;","21/Jan/15 22:28;pwendell;At this point I'm not aware of people still hitting this set of issues in newer releases, so per discussion with [~joshrosen], I'd like to close this. Please comment on this JIRA if you are having some variant of this issue in a newer version of Spark, and we'll continue to investigate.;;;","25/Jan/15 02:46;ding;I hit this error when using pagerank(It cannot be consistent repro as I only hit once). I am not using the KryoSerializer but I am using the default serializer. The Spark code is get from chunk at 2015/1/19 which should be later than spark 1.2.0. 

15/01/23 23:32:57 WARN scheduler.TaskSetManager: Lost task 347.0 in stage 9461.0 (TID 302687, sr213): FetchFailed(BlockManagerId(13, sr207, 49805), shuffleId=399, mapId=461, reduceId=347, message=
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
    at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)
    at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)
    at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at org.apache.spark.graphx.impl.VertexPartitionBaseOps.aggregateUsingIndex(VertexPartitionBaseOps.scala:207)
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$5$$anonfun$apply$4.apply(VertexRDDImpl.scala:171)
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$5$$anonfun$apply$4.apply(VertexRDDImpl.scala:171)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:113)
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:111)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:65)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:64)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:192)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
    at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
    at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
    at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)
    at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)
    at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135)
    at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92)
    at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
    at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:143)
    at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1165)
    at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)
    at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:299)
    at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
    at scala.util.Try$.apply(Try.scala:161)
    at scala.util.Success.map(Try.scala:206)
    at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:299)
    at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)

;;;","05/Feb/15 18:25;joshrosen;I'm re-opening this issue because I've seen another recent occurrence of this error in Spark 1.2.0+.

From what I've seen, occurrences of this error seem to cluster together: things will be working okay for a little while, then a ton of instances of this error will occur.  Around the Spark 1.2.0 release period, I spent a bunch of time investigating potential causes of this bug but wasn't able to track it down.  I'd appreciate any help in auditing the Spark code to see if we can figure out what's causing this.  One strategy might be to purposely introduce bugs or corruption into different parts of the shuffle write and read path to see how that corruption manifests itself as errors; this might help us to narrow down the range of potential causes.;;;","10/Feb/15 06:04;mkman84;We're running 1.2.1-rc2 on our cluster and running into the exact same problem. Several different jobs by different users will typically run perfectly fine, and then another identical run randomly throws the FAILED_TO_UNCOMPRESS(5) error, which causes the job to fail altogether actually. I'll try to re-produce this somehow, though it is a tricky one!;;;","23/Feb/15 23:13;yashwanth.rao11@gmail.com;I have a similar issue. I have a job which occasionally fails with this error. I am using Spark 1.2.0 with HDP 2.1.1. ;;;","23/Feb/15 23:53;ding;I am taking annual leave from 2/14 - 2/26. Sorry to bring the inconvience.

Best regards
DingDing
;;;","20/Mar/15 02:35;airhorns;Would just like to add that we are seeing this occasionally as well on 1.3 and master from github. It seems mostly to manifest when there is high IO contention. We were at one point using spinning disks for {{spark.local.dir}} and that seems to exacerbate the issue. Anybody have any leads? ;;;","20/Mar/15 02:54;mkman84;Can also confirm it's happened once so far since moving to 1.3-rc3. We're using SSDs for scratch space in our case however. I did notice that it's only happened (from 1.2 as well) on rare occasions, only when multiple frameworks were running heavy tasks at the same time (which ones exactly I'm not positive about though).;;;","27/Apr/15 09:05;geynard;I have this issue using spark1.2.0+ shipped with Cloudera CDH5.3. Some more information:
 - This issue showed when I tried to augment the property ""spark.default.parallelism"" in order to have 2 or 3 tasks per executor (in my case 36 tasks for 18 executors).
 - I reproduce it almost every time on my configuration, with Snappy or any other compression mechanism.
 - My cluster is composed of 4 VMs with 6 cores, 32GB RAM on each.
 - My use case is performing joins on about 50GB of data compressed with Snappy.
 - I use KryoSerialization.
 - It generally occurs on just once shuffle partition, but once it has, this partition will throw the error 4 times and fail the job.;;;","03/May/15 17:01;joshrosen;While working on some new shuffle code, I managed to trigger a {{FAILED_TO_UNCOMPRESS(5)}} error by trying to decompress data which was not compressed or which was compressed with the wrong compression codec. This is kind of a long shot, but I wonder if there's a rarely-hit branch in the sort-shuffle write path that doesn't properly wrap an output stream for compression (or that does so with the wrong compression codec).;;;","07/May/15 14:08;geynard;I have been able to reproduce this bug several times on my cluster composed of 4 VMs (6 proc, 32GB ram) using the classes I join. These classes are based on my business use case on which I first ran into the issue and the example given on this page, it is basically joining two flows with massive spilling on disk.
I reproduce the bug starting from 8000 mappers generating each 100000 objects (first two arguments). This is equivalent to about 36GB of spilled data. With 2000 mappers, the bug did not happen. This issue occurs clearly for me when a massive volume of data is involved. The job takes 1 hour to run on my cluster with these parameters.

Other parameters: 18 executors, parallelism level = 36.
Spar version: 1.3.0, cdh5.4

Stack trace:
com.esotericsoftware.kryo.KryoException: java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
	at com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
	at com.esotericsoftware.kryo.io.Input.require(Input.java:169)
	at com.esotericsoftware.kryo.io.Input.readAscii_slow(Input.java:580)
	at com.esotericsoftware.kryo.io.Input.readAscii(Input.java:558)
	at com.esotericsoftware.kryo.io.Input.readString(Input.java:436)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:132)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:610)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:721)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:138)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at com.esotericsoftware.kryo.io.Input.fill(Input.java:140)
	... 42 more;;;","07/May/15 14:49;bherta;I can add a few more pieces of information, since I see this problem consistently in a program on my cluster.
- I see this this both in Spark 1.3.1, and in a build of the head as of May 6th.
- I am running on only three executors, and it occurs in all three of them around the same time.

Here's an example stack taken from the trunk build:

FetchFailed(BlockManagerId(0, XXX.XXX.XXX.XXX, 40817), shuffleId=69, mapId=3, reduceId=5, message=
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:127)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:169)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:168)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:168)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:785)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:160)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1167)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:301)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Success.map(Try.scala:206)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	... 37 more

);;;","07/May/15 16:46;pwendell;[~bherta] can you give a bit more information about your environment? E.g. what Java version, OS versions and whether you are running Spark inside of containers or VM's? This bug involves data corruption caused by a race condition somewhere and we aren't sure if it might be in OS or JVM libraries or in Spark.;;;","07/May/15 17:19;bherta;Yes, sorry, I should have included that information in my earlier post.

I'm running on an up-to-date install of CentOS 7 x64, not in a VM or other type of container, but installed directly on the servers.  I have two physical machines, with two workers on one machine (40 cores, 256Gb memory, divided in half for the two workers), and a single worker on the other (32 cores, 128Gb memory).   The JVM is IBM's Java 8, SR1 (http://www.ibm.com/developerworks/java/jdk/linux/download.html).

Removing some calls to persist/cache appears to help prevent this from occurring.;;;","09/May/15 20:34;douglaz;Got something like this but using:
- Java serializer
- Snappy
- Also Lz4
- Spark 1.3.0 (most things on default settings)

We got the error many times on the same cluster (which was doing fine for days) but after recreating it the problem disappeared again. Stack traces (the first two are from two different runs using Snappy and the third from an execution using Lz4):

15/05/09 13:05:55 ERROR Executor: Exception in task 234.2 in stage 27.1 (TID 876507)
java.io.IOException: PARSING_ERROR(2)
		at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
		at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
		at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:594)
		at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:358)
		at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:387)
		at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)
		at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)
		at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
		at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)
		at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
		at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
		at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
		at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
		at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60)
		at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
		at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
		at org.apache.spark.scheduler.Task.run(Task.scala:64)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
		at java.lang.Thread.run(Thread.java:745)



15/05/08 20:46:28 WARN scheduler.TaskSetManager: Lost task 9559.0 in stage 55.0 (TID 424644, ip-172-24-36-214.ec2.internal): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
		at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
		at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
		at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)
		at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)
		at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:362)
		at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:387)
		at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)
		at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)
		at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
		at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
		at scala.collection.immutable.$colon$colon.readObject(List.scala:366)
		at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:606)
		at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
		at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)
		at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
		at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
		at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
		at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
		at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60)
		at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
		at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
		at org.apache.spark.scheduler.Task.run(Task.scala:64)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
		at java.lang.Thread.run(Thread.java:745)
		
		
		
15/05/09 16:27:01 WARN scheduler.TaskSetManager: Lost task 1365.0 in stage 52.0 (TID 812600, ip-172-24-33-2.ec2.internal): FetchFailed(BlockManagerId(19, ip-172-24-33-64.ec2.internal, 34224), shuffleId=14, mapId=662, reduceId=1365, message=
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
		at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)
		at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)
		at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)
		at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
		at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)
		at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)
		at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
		at org.apache.spark.scheduler.Task.run(Task.scala:64)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
		at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Stream is corrupted
		at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:152)
		at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:116)
		at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)
		at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2323)
		at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
		at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
		at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
		at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
		at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
		at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:102)
		at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:302)
		at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)
		at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
		at scala.util.Try$.apply(Try.scala:161)
		at scala.util.Success.map(Try.scala:206)
		at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
		at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
		... 51 more


The cluster was 15 d2.4xlarge using customized spark_ec2.py (but on stock image).

After the problem started it happened in 18 from 20 executions (i.e only 2 executions finished successfully). Curiously it happened after a deploy which updated the application jar (but this wasn't the cause because rolling back didn't fix it). My guess is that killing the running job (as consequence of the deploy) left the cluster in a inconsistent state which new executions couldn't handle.;;;","14/May/15 21:53;joshrosen;I found a bug in snappy-java that can lead to stream corruption issues and have submitted an upstream patch to fix it.  See https://github.com/xerial/snappy-java/issues/107 for more details.  Once snappy-java publishes a snapshot / release using this patch, it would be great if someone encountering this issue could try upgrading Snappy to see if this fixes the bug.;;;","15/May/15 06:22;joshrosen;I've opened SPARK-7660 to track progress on the fix for the snappy-java buffer sharing bug.;;;","15/May/15 13:57;geynard;I think I add the bug using another compression codec. I will try to reproduce it as soon as I can.;;;","15/May/15 18:11;joshrosen;For FAILED_TO_UNCOMPRESS(5), here's a reproduction that reliably triggers the bug when using Snappy compression:

{code}
import java.io.ByteArrayOutputStream
import org.apache.spark.io.SnappyCompressionCodec
import org.apache.spark.SparkConf
sc.parallelize(1 to 1000, 100).mapPartitions { p =>
  val snappyCodec = new SnappyCompressionCodec(new SparkConf())
  val baos = new ByteArrayOutputStream()
  val snappyStream = snappyCodec.compressedOutputStream(baos)
  (1 to 100).foreach { _ => snappyStream.close() }
  p
}.coalesce(2, shuffle=true).count()
{code}

This example is designed to hit the bug that SPARK-7660 fixes.;;;","15/May/15 19:52;bherta;I haven't had a chance to test the patch for snappy as I am having trouble building it, and the latest snappy-java 1.1.2-SNAPSHOT is a few days old, so does not yet include this patch.  Once a new 1.1.2-SNAPSHOT becomes available I'll give it a try.

However, I DID confirm that when I set ""spark.io.compression.codec"" to ""lzf"" instead of the default (which is snappy), my program runs correctly, and I do not see the FAILED_TO_UNCOMPRESS(5) errors.  This means that I have a workaround for now, and high hopes when the new snappy build becomes available.;;;","15/May/15 19:54;joshrosen;Check out my patch at https://issues.apache.org/jira/browse/SPARK-7660, which should fix this bug without needing to upgrade Snappy: https://github.com/apache/spark/pull/6176;;;","15/May/15 19:54;joshrosen;Check out my patch at https://issues.apache.org/jira/browse/SPARK-7660, which should fix this bug without needing to upgrade Snappy: https://github.com/apache/spark/pull/6176;;;","18/May/15 17:32;bherta;Unfortunately, patch 6176 does not resolve the issue, at least on my system.  I verified that using lzf still works correctly.  Here's an updated stack trace, using the latest code as of May 15th, with 6176 applied:

org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:127)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:169)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:168)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:168)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(Thread.java:785)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:140)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:91)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:160)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1168)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:301)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Success.map(Try.scala:206)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	... 37 more;;;","18/May/15 18:10;joshrosen;I just noticed something interesting, although perhaps it's a red herring: in most of the stacktraces posted here, it looks like shuffled data is being processed with CoGroupedRDD. In most (all?) of these cases, it looks like the error is occurring when inserting an iterator of values into an external append-only map inside of CoGroupedRDD.  https://github.com/apache/spark/pull/1607 was one of the last PRs to touch this file in 1.1, so I wonder whether there could be some sort of odd corner-case that we're hitting there; might be a lead worth exploring further.;;;","22/May/15 08:03;geynard;Ok, my bad, I tried to reproduce the bug using lz4, but I was unable to get it, even with a test shuffling 500GB of data. So I must have been mistaken.
Using another compression codec seems to be a good workaround.;;;","01/Jun/15 14:58;deepujain;I see this issue when reading sequence file stored in Sequence File format (SEQorg.apache.hadoop.io.Textorg.apache.hadoop.io.Text'org.apache.hadoop.io.compress.GzipCodec?v?
)

All i do is 
sc.sequenceFile(dwTable, classOf[Text], classOf[Text]).partitionBy(new org.apache.spark.HashPartitioner(2053))
.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
      .set(""spark.kryoserializer.buffer.mb"", arguments.get(""buffersize"").get)
      .set(""spark.kryoserializer.buffer.max.mb"", arguments.get(""maxbuffersize"").get)
      .set(""spark.driver.maxResultSize"", arguments.get(""maxResultSize"").get)
      .set(""spark.yarn.maxAppAttempts"", ""0"")
      //.set(""spark.akka.askTimeout"", arguments.get(""askTimeout"").get)
      //.set(""spark.akka.timeout"", arguments.get(""akkaTimeout"").get)
      //.set(""spark.worker.timeout"", arguments.get(""workerTimeout"").get)
      .registerKryoClasses(Array(classOf[com.ebay.ep.poc.spark.reporting.process.model.dw.SpsLevelMetricSum]))


and values are 
buffersize=128 maxbuffersize=1068 maxResultSize=200G ;;;","07/Aug/15 02:21;DjvuLee;Is there any progress on this bug? 

I have the same error on spark1.1.0, I occurred when I use the GroupByKey just as posted above or a sql join. when the data is not very big,  this bug do not occurred, but when the data is bigger, the the FAILED_TO_UNCOMPRESS throwed.

by the way, I just using the Hash-based shuffle.;;;","27/Aug/15 09:18;mvherweg;Just hit this error in pyspark code, using Spark 1.4.0
Same context as others have posted before:
- happens sometimes
- happens in environments where the exact same code has been executed in the past without issues

Part of stacktrace:
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:160)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1176)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:301)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Success.map(Try.scala:206)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	... 11 more;;;","27/Aug/15 19:36;mmitsuto;mapPartitions at Exchange.scala:60 +details 
 org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:635)
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:60)
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:49)
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:48)
org.apache.spark.sql.execution.joins.HashOuterJoin.execute(HashOuterJoin.scala:188)
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:60)
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:49)
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:48)
org.apache.spark.sql.execution.joins.HashOuterJoin.execute(HashOuterJoin.scala:188)
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:60)
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:49)
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:48)
org.apache.spark.sql.execution.joins.HashOuterJoin.execute(HashOuterJoin.scala:188)
org.apache.spark.sql.execution.Project.execute(basicOperators.scala:40)
org.apache.spark.sql.parquet.InsertIntoParquetTable.execute(ParquetTableOperations.scala:265)
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1099)
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1099)
 2015/08/27 19:32:12  2 s 
4/2000 (29 failed)

   61.8 MB 6.6 MB org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)+details ;;;","04/Sep/15 21:02;irashid;[~mvherweg] Do you know if the error occurred after there was already a stage retry?  If so, then this might just be a symptom of SPARK-8029.  You would know if earlier in the logs, you see a FetchFailedException which is *not* related to snappy exceptions.  I think that is the first report of this bug since SPARK-7660, which we were really hoping fixed this issue, so it would be great to capture more information about it.

[~mmitsuto] Can you do the same check, and also tell us which version of Spark you are using?;;;","06/Oct/15 07:13;nadenf;Can confirm this also on Spark 1.4.2 HEAD.

Had a lot of these org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 41

But that was not on the same node.;;;","08/Oct/15 13:50;irashid;[~nadenf] The FetchFailures don't need to be on the same node as the snappy exceptions for the root cause to be SPARK-8029.  Of course, we can't be sure that is the cause either, but it is at least a working hypothesis for now.;;;","15/Oct/15 17:39;rrag;I get this error consistently when using spark-shell on 1.5.1 (win 7)

{code}
scala> sc.textFile(""README.md"", 1).flatMap(x => x.split("" "")).countByValue()
{code}

Happens for any {code}countByValue/groupByKey/reduceByKey{code} operations. The affected versions tag on this issue mentions 1.2.0, 1.2.1, 1.3.0, 1.4.1 but not 1.5.1

Can someone help me if I am doing something wrong or is this a problem in 1.5.1 also;;;","27/Oct/15 00:53;BranY;I Have the same problem in Spark 1.4.0, it gives me these error:
1. org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
2. org.apache.spark.shuffle.FetchFailedException: PARSING_ERROR(2)
3. org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 10
;;;","04/Jan/16 15:06;renuyadav;Hi ,
I am also getting similar exception on spark 1.4.1 when shuffling above 500GB data,but this exception occurs sometimes
Please help.;;;","17/Jan/16 19:20;alpinegizmo;I've seen this with 1.6.0. Sorry, but I don't have a reproducible case.
;;;","17/Jan/16 21:50;alpinegizmo;Increasing the heap size has got me out of trouble, which seems consistent with the theory that this is related to spilling.;;;","18/Jan/16 20:12;daniel.siegmann.aol;I had this happen in Spark 1.5.0. It only happened once and I haven't reproduced it (so far).

{noformat}
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
{noformat};;;","10/Mar/16 02:46;DoingDone9;I  had this happen in Spark 1.5.2 [~joshrosen] [~daniel.siegmann.aol];;;","28/Mar/16 19:07;vgirijira;This is the blocker in Spark 1.5.2 also. I tried using LZ4 or LZF. But I got different exception.
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.1 failed 4 times, most recent failure: Lost task 1.3 in stage 8.1 (TID 2741, p01bdl870.aap.csaa.com): java.io.IOException: Stream is corrupted;;;","01/Apr/16 04:18;vgirijira;As a temporary work around, I fixed the issue in the code itself to reduce the huge volume that is shuffling in executor nodes. One of the fix is identify bad data and filter it, skewed data and partition it,etc.;;;","04/Apr/16 16:56;wy90021;This happened in Spark Streaming 1.3.1 by using direct kafka api

{code}
16/04/04 12:41:01 ERROR scheduler.JobScheduler: Error running job streaming job 1459773240000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 0.0 failed 4 times, most recent failure: Lost task 19.3 in stage 0.0 (TID 66, xvac-d25.xv.dc.openx.org): java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at java.io.InputStream.read(InputStream.java:101)
	at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply$mcI$sp(ByteBufferMessageSet.scala:68)
	at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply(ByteBufferMessageSet.scala:68)
	at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply(ByteBufferMessageSet.scala:68)
	at scala.collection.immutable.Stream$.continually(Stream.scala:1129)
	at kafka.message.ByteBufferMessageSet$.decompress(ByteBufferMessageSet.scala:68)
	at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:178)
	at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:191)
	at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:145)
	at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
	at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:847)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:615)
	at org.apache.spark.streaming.kafka.KafkaRDD$KafkaRDDIterator.getNext(KafkaRDD.scala:161)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code};;;","21/Apr/16 03:49;DoingDone9;I find that some task recompute before this problem happened,and I  think that retry operation Corrupted shuffle file that caused this problem. I debug the code and corrupted the shuffle file before it has been readed, this problem happened every time.maybe we can regenerate the shuffle file when it is corrupted

code like this 

BlockStoreShuffleReader.scala
{code:title=Bar.java|borderStyle=solid}
     val wrappedStreams = blockFetcherItr.map { case (blockId, inputStream) =>
-      serializerManager.wrapForCompression(blockId, inputStream)
+      try {
+        serializerManager.wrapForCompression(blockId, inputStream)  
+      } catch {
+        case e: IOException => {
+          if ((e.getMessage.contains(""FAILED_TO_UNCOMPRESS(5)"") ||
+              e.getMessage.contains(""PARSING_ERROR(2)"") ||
+              e.getMessage.contains(""Stream is corrupted"")) && blockId.isShuffle) {
+            val shuffleBlockId = blockId.asInstanceOf[ShuffleBlockId]
+            throw new FetchFailedException(
+              blockManager.blockManagerId, shuffleBlockId.shuffleId,
+              shuffleBlockId.mapId, shuffleBlockId.reduceId, e)
+          } else {
+            throw e
+          }
+        }
+      }
     }
{code}
[~joshrosen] [~wy90021] [~vgirijira][~alpinegizmo];;;","26/Apr/16 12:37;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/12700;;;","17/May/16 19:25;jasonr;FWIW - I am able to reproduce this error consistently for a job, even when there are no failures in the preceding stage.  In my case, a viable workaround was to increase the number of partitions for the preceding stage such that none of the tasks had to spill to memory or disk.  So it would seem that the corruption may be happening during a spill operation.;;;","18/May/16 02:03;DoingDone9;Can you suggest how I reproduce this error?;;;","20/May/16 19:00;vijay.saikam;I've started to hit this exact same issue. It is not intermittent and doesn't always happen. We use Spark 1.5 

{code}
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:427)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:127)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1189)
	at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$3.apply(HashShuffleReader.scala:53)
	at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$3.apply(HashShuffleReader.scala:52)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:355)
	... 8 more
{code};;;","31/May/16 03:32;ASRRAJ;This issue got fixed in 1.5.1 version but I was looking if it can be ported back to 1.3.1 version,as i am not using 1.5.1 as of now .or any workaround for this ?;;;","01/Jul/16 21:11;mmoroz;How do you know this issue is fixed? And if it is, should this issue be CLOSED?;;;","11/Jul/16 14:07;ssonker;I'm using 1.5.1 and was also facing this exception. The issue got resolved when I switched to nio (instead of the default netty). To switch to nio, set 'spark.shuffle.blockTransferService' to 'nio'. Please see if that helps.;;;","31/Aug/16 18:48;MSeal;Producible on 1.6.1 from a count() call after cache(df.repartition(2000)) against a dataset built from a PySpark generated RDD once I grew it into GB sizes (a few million rows). Smaller dataset sizes have consistently not triggered this issue.

{code}
java.io.IOException: failed to uncompress the chunk: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.spark-project.guava.io.ByteStreams.read(ByteStreams.java:899)
	at org.spark-project.guava.io.ByteStreams.readFully(ByteStreams.java:733)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:119)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:102)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:168)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:90)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:64)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code};;;","30/Sep/16 12:41;H4ml3t;I have exactly the same problem. It occurs when I process more than one TB of data.

Splitting the computation in halves it works perfectly, but I need to process the whole thing together.

{noformat}
java.io.IOException: failed to read chunk
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:347)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.spark-project.guava.io.ByteStreams.read(ByteStreams.java:899)
	at org.spark-project.guava.io.ByteStreams.readFully(ByteStreams.java:733)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:119)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:102)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:512)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

as a consequence, since I am using SparkSQL, the job will fail, reporting

{noformat}
16/09/30 14:40:51 ERROR LiveListenerBus: Listener SQLListener threw an exception
java.lang.NullPointerException
        at org.apache.spark.sql.execution.ui.SQLListener.onTaskEnd(SQLListener.scala:167)
        at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)
        at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
        at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
        at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)
        at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
{noformat}

the command is really simple, and I presume the dropDistinct is causing this:

{noformat}
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

import sqlContext.implicits._

sqlContext.read
    .option(""mode"",""DROPMALFORMED"")
    .schema(StructType(StructField(""body"", StringType, true) :: StructField(""timestamp"", StringType, true) :: Nil))
    .json(""myData"")
    .select(""timestamp"",""body"")
    .dropDuplicates(""body"")
    .where($""body"".isNotNull && $""timestamp"".isNotNull)
    .withColumn(""timestamp"",$""timestamp"".multiply(.001).cast(TimestampType).cast(DateType))
    .groupBy(""timestamp"").count()
    .collect()
{noformat}

Expected output, something like
{noformat}
+----------+---------+
|        ts|    count|
+----------+---------+
|2016-09-01| 76682822|
|2016-09-02|107701926|
|2016-09-03| 32859100|
|2016-09-04| 30218800|
|2016-09-05| 59807028|
|2016-09-06| 52250275|
|2016-09-07| 52187300|
|2016-09-08| 51205733|
|2016-09-09| 64345768|
|2016-09-10| 43249106|
|2016-09-11| 31924421|
|2016-09-12| 78127144|
|2016-09-13|194981002|
|2016-09-14|167246581|
|2016-09-15| 56367223|
|2016-09-16| 55473385|
|2016-09-17|214722399|
|2016-09-18| 65054069|
|2016-09-19| 64932135|
|2016-09-20| 54006489|
|2016-09-21| 54140656|
|2016-09-22| 61518266|
|2016-09-23| 47547635|
|2016-09-24| 24848697|
|2016-09-25| 24111800|
|2016-09-26| 44870596|
|2016-09-27| 25653917|
|2016-09-28| 32558282|
|2016-09-29| 32465916|
|2016-09-30|  1227087|
+----------+---------+
{noformat};;;","30/Sep/16 13:20;vadim_ps;we had gotten the same behavior but the problem was in not enough physical memory for executors. We reduced executor's memory and the problem was fixed.;;;","30/Sep/16 18:24;MSeal;Backing off executor memory away from boundary of physical memory did not solve the problem for my above crash report.;;;","05/Oct/16 08:54;jean;We've just hit this training RandomForest on spark 1.6.1

{noformat}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 72 in stage 12980.0 failed 4 times, most recent failure: Lost task 72.3 in stage 12980.0 (TID 757443, *********.com): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:465)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:504)
        at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:147)
        at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:99)
        at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:59)
        at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159)
        at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1186)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:53)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
        at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:58)
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:741)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:740)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)
        at org.apache.spark.mllib.tree.DecisionTree$.findBestSplits(DecisionTree.scala:651)
        at org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:233)
        at org.apache.spark.mllib.tree.RandomForest$.trainRegressor(RandomForest.scala:378)
        at org.apache.spark.mllib.tree.RandomForest.trainRegressor(RandomForest.scala)
        at com.*****.sparkws.core.mlib.RegressorImpl.trainActivityModel(RegressorImpl.java:144)
        at com.*****.sparkws.core.mlib.RegressorImpl$$FastClassBySpringCGLIB$$9f2cc95.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:651)
        ... 17 more

Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:465)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:504)
        at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:147)
        at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:99)
        at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:59)
        at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159)
        at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1186)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:53)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
        at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:58)
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        ... 3 more
{noformat};;;","05/Oct/16 19:28;himanish;Hi,

I am also seeing this error {{java.io.IOException: FAILED_TO_UNCOMPRESS(5)}} on Spark 2.0.0 intermittently with {{snappy}}. 

Changing {{spark.io.compression.codec}} to {{lz4}} causes intermittent {{java.io.IOException: Stream is corrupted}} errors. 

I have been setting {{spark.rdd.compress}} to {{true}} along with setting  {{spark.io.compression.codec}}.As a workaround will disabling {{spark.rdd.compress}} fix the issue ?;;;","06/Oct/16 13:57;himanish;
The only way I could make this work for 500GB+ data using the {{sort}} based shuffle manager is by disabling shuffle compressions altogether using {{--conf spark.shuffle.compress=false --conf spark.shuffle.spill.compress=false}}. This is definitely not desirable.

Appreciate if someone could suggest any other better workaround. I will keep trying other things ( like switching {{spark.shuffle.manager}} to {{hash}} , using LZF etc.) and update here.;;;","07/Oct/16 11:39;cryptoe;I also ran into this error :java.io.IOException: FAILED_TO_UNCOMPRESS(5) while shuffling 3.5 TB using spark version 1.5.2 on yarn version : HDP 2.2.4.2-2.  
The fix was bumping up the snappy version to 1.1.2.6 from snappy-java-1.0.4.1.;;;","19/Oct/16 20:52;tenstriker;I hit this error as well but I also noticed lot of  `java.lang.OutOfMemoryError: Java heap space` or and high GC pressure. My guess is having no memory might have caused snappy to fail on uncompress.;;;","24/Oct/16 10:49;dhananjaydp;I see this error intermittently.
I am using
spark             :1.6.2
hadoop/yarn  :2.7.3

{quote}
 Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 4 times, most recent failure: Lost task 40.3 in stage 7.0 (TID 411, 10.0.0.12): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:422)
	at org.xerial.snappy.SnappyInputStream.available(SnappyInputStream.java:469)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:353)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.spark-project.guava.io.ByteStreams.read(ByteStreams.java:899)
	at org.spark-project.guava.io.ByteStreams.readFully(ByteStreams.java:733)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:119)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:102)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
{quote};;;","18/Nov/16 00:46;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15923;;;","16/Nov/18 15:31;irashid;fyi I opened SPARK-26089 for handling issues in large corrupt blocks, as we recently ran into that, and I dont' see any other issues for it.;;;","12/Mar/19 06:03;feiwang;Is there anyone see these errors in latest version, such as spark-2.3?

Can anyone provide a reproducible case?;;;","25/Apr/19 09:30;aakash.mandlik;I am facing similar issue while persisting to S3 for Spark-2.4. Earlier the same code was working for spark-2.2. 

 

19/04/25 14:42:36 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 55.0 (TID 1345, node4, executor 1): org.xerial.snappy.SnappyIOExceptio
n: [EMPTY_INPUT] Cannot decompress empty stream
 at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:94)
 at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:59)
 at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:164)
 at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
 at org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:209)
 at org.apache.spark.storage.BlockManager$$anonfun$getRemoteValues$1.apply(BlockManager.scala:698)
 at org.apache.spark.storage.BlockManager$$anonfun$getRemoteValues$1.apply(BlockManager.scala:696)
 at scala.Option.map(Option.scala:146)
 at org.apache.spark.storage.BlockManager.getRemoteValues(BlockManager.scala:696)
 at org.apache.spark.storage.BlockManager.get(BlockManager.scala:820)
 at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:875)
 at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324);;;","12/Feb/21 14:46;tcondie;[~joshrosen] We just experienced a similar stack trace / issue last week in Spark 2.4.4. Could you please share your thoughts on whether it could be related? 

 Job aborted due to stage failure: Task 5639 in stage 87235.0 failed 4 times, most recent failure: Lost task 5639.3 in stage 87235.0 (TID 3466497, executor 82): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:439)
	at org.xerial.snappy.SnappyInputStream.available(SnappyInputStream.java:486)
	at org.apache.spark.storage.BufferReleasingInputStream.available(ShuffleBlockFetcherIterator.scala:581)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:353)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.spark_project.guava.io.ByteStreams.read(ByteStreams.java:910)
	at org.spark_project.guava.io.ByteStreams.readFully(ByteStreams.java:787)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:127)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110)
	at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:94)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:84)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
 ;;;","14/Dec/21 02:37;Mr.黄;[~joshrosen] Hello, I still encountered this problem in the company's project today, but its error level in the log is WARN. The spark version we used was 2.4.0.cloudera2. I'm not sure this issue has anything to do with Cloudera.
The following is the log information about the error
{code:java}
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:565)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:470)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:65)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown So
urce)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:216)
        at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
        at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:121)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)
        at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:147)
        at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:99)
        at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:59)
        at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:164)
        at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
        at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:455)
        ... 27 more)
{code}
 ;;;"
Race condition in org.apache.spark.ComplexFutureAction.cancel,SPARK-4097,12750695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,27/Oct/14 08:22,29/Oct/14 21:44,14/Jul/23 06:26,29/Oct/14 21:44,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,bug,race-condition,,,,"There is a chance that `thread` is null when calling `thread.interrupt()`.

{code:java}
  override def cancel(): Unit = this.synchronized {
    _cancelled = true
    if (thread != null) {
      thread.interrupt()
    }
  }
{code}

Should put `thread = null` into a `synchronized` block to fix the race condition.

{code:java}
      try {
        p.success(func)
      } catch {
        case e: Exception => p.failure(e)
      } finally {
        thread = null
      }
{code}",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 27 08:24:35 UTC 2014,,,,,,,,,,"0|i21lpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/14 08:24;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2957;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Input metrics don't work for coalesce()'d RDD's,SPARK-4092,12750627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kostas,pwendell,pwendell,26/Oct/14 19:48,16/Jan/15 02:49,14/Jul/23 06:26,16/Jan/15 02:49,,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"In every case where we set input metrics (from both Hadoop and block storage) we currently assume that exactly one input partition is computed within the task. This is not a correct assumption in the general case. The main example in the current API is coalesce(), but user-defined RDD's could also be affected.

To deal with the most general case, we would need to support the notion of a single task having multiple input sources. A more surgical and less general fix is to simply go to HadoopRDD and check if there are already inputMetrics defined for the task with the same ""type"". If there are, then merge in the new data rather than blowing away the old one.

This wouldn't cover case where, e.g. a single task has input from both on-disk and in-memory blocks. It _would_ cover the case where someone calls coalesce on a HadoopRDD... which is more common.",,aash,apachespark,hammer,kostas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2630,SPARK-4157,,,,,SPARK-5225,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 22:20:58 UTC 2014,,,,,,,,,,"0|i21laf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"30/Oct/14 22:47;kostas;The current patch I'm working on does the simplest thing to address this issue. In the hadoop rdds and cache manager if the task already has input metrics of the same read method it will increment them instead of overriding. This simple solution should handle the common case of:
{code}
sc.textFile(..).coalesce(5).collect()
{code}
In addition it will cover the situation where blocks are coming from the cache. 

What it won't handle properly is if coalesce (or other rdds with similar properties) reads from multiple blocks of mixed read methods (memory vs. hadoop). In that case, one input metric will override the other. We have several options:
# We create a MIXED readMethod and if we see input metrics from different methods, we change to MIXED. This will loose some information because now we don't know where the blocks were read from.
# We store multiple inputMetrics for each TaskContext. Up the stack (eg. JsonProtocol) we send the array of InputMetrics to the caller. We have to worry about backwards compatibility in that case so we can't just remove the single inputMetric. We might have to send back a MIXED metric and in addition the array for newer clients.
# We punt on this issue for now since it can be argued is not common.
What are people's thoughts on this?

;;;","31/Oct/14 06:26;pwendell;I'd be fine, as a first step, just doing (3). It will solve many of the use cases and will be simple.;;;","05/Nov/14 21:53;apachespark;User 'ksakellis' has created a pull request for this issue:
https://github.com/apache/spark/pull/3120;;;","06/Nov/14 09:31;aash;Hi Kostas,

This looks to be touching some of the same code that SPARK-2630 aims to fix.  Your fix looks to have better testing so might be better to build on if these two are duplicates.

Does your work also fix SPARK-2630 as a side effect?;;;","11/Nov/14 22:20;kostas;[~aash], yes this should solve a superset of the same problems that SPARK-2630 aims to fix. I say superset because https://github.com/apache/spark/pull/3120 also includes a similar fix when hadoop 2.5 is used with the bytesReadCallback. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in snappy-java 1.1.1.4/5,SPARK-4090,12750584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,davies,davies,26/Oct/14 07:47,24/Nov/14 19:46,14/Jul/23 06:26,26/Oct/14 08:08,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"There is memory-leak bug in snappy-java 1.1.1.4/5, we should rollback to 1.1.1.3 or wait for bugfix.

The jenkins tests timeouted or OOM multiple times recently. While test it locally, I got the heap dump of leaked JVM:



Then found that it's a bug in recent releases of snappy-java:
{code}
+        inputBuffer = inputBufferAllocator.allocate(inputSize);
+        outputBuffer = inputBufferAllocator.allocate(outputSize);
{code}

The outputBuffer is allocated from inputBufferAllocator but released to outputBufferAllocator: https://github.com/xerial/snappy-java/issues/91

",,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4056,,,,,,,,,,,,,,,,"26/Oct/14 07:48;davies;screenshot-12.png;https://issues.apache.org/jira/secure/attachment/12677160/screenshot-12.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 17:41:42 UTC 2014,,,,,,,,,,"0|i21l0v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"26/Oct/14 07:50;joshrosen;I rolled back earlier today, so the build should be fixed now.;;;","26/Oct/14 17:41;davies;[~joshrosen] 1.1.1.6 is released.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The version number of Spark in _config.yaml is wrong.,SPARK-4089,12750522,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,25/Oct/14 08:10,28/Oct/14 19:44,14/Jul/23 06:26,28/Oct/14 19:44,1.2.0,,,,,,,1.2.0,,,,,,Documentation,,,,0,,,,,,The version number of Spark in docs/_config.yaml for master branch should be 1.2.0 for now.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 25 08:13:29 UTC 2014,,,,,,,,,,"0|i21knj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Oct/14 08:13;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2943;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python worker should exit after socket is closed by JVM,SPARK-4088,12750518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,25/Oct/14 07:04,25/Oct/14 08:21,14/Jul/23 06:26,25/Oct/14 08:21,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"In case of take() or exception in Python, python worker may exit before JVM read() all the response, then the write thread may raise ""Connection reset"" exception.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 25 08:21:09 UTC 2014,,,,,,,,,,"0|i21kmn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Oct/14 07:15;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2941;;;","25/Oct/14 08:21;joshrosen;Issue resolved by pull request 2941
[https://github.com/apache/spark/pull/2941];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job will fail if a shuffle file that's read locally gets deleted,SPARK-4085,12750472,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,kayousterhout,kayousterhout,24/Oct/14 21:33,04/Dec/14 00:30,14/Jul/23 06:26,04/Dec/14 00:30,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"This commit: https://github.com/apache/spark/commit/665e71d14debb8a7fc1547c614867a8c3b1f806a changed the behavior of fetching local shuffle blocks such that if a shuffle block is not found locally, the shuffle block is no longer marked as failed, and a fetch failed exception is not thrown (this is because the ""catch"" block here won't ever be invoked: https://github.com/apache/spark/commit/665e71d14debb8a7fc1547c614867a8c3b1f806a#diff-e6e1631fa01e17bf851f49d30d028823R202 because the exception called from getLocalFromDisk() doesn't get thrown until next() gets called on the iterator).

[~rxin] [~matei] it looks like you guys changed the test for this to catch the new exception that gets thrown (https://github.com/apache/spark/commit/665e71d14debb8a7fc1547c614867a8c3b1f806a#diff-9c2e1918319de967045d04caf813a7d1R93).  Was that intentional?  Because the new exception is a SparkException and not a FetchFailedException, jobs with missing local shuffle data will now fail, rather than having the map stage get retried.

This problem is reproducible with this test case:
{code}
  test(""hash shuffle manager recovers when local shuffle files get deleted"") {
    val conf = new SparkConf(false)
    conf.set(""spark.shuffle.manager"", ""hash"")
    sc = new SparkContext(""local"", ""test"", conf)
    val rdd = sc.parallelize(1 to 10, 2).map((_, 1)).reduceByKey(_+_)
    rdd.count()

    // Delete one of the local shuffle blocks.
    sc.env.blockManager.diskBlockManager.getFile(new ShuffleBlockId(0, 0, 0)).delete()
    rdd.count()
  }
{code}

which will fail on the second rdd.count().

This is a regression from 1.1.",,apachespark,glenn.strycker@gmail.com,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 10:00:46 UTC 2014,,,,,,,,,,"0|i21kcv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Oct/14 21:36;kayousterhout;Upgraded to critical on [~pwendell]'s request;;;","03/Dec/14 10:00;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3579;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""IOException: unexpected exception type"" while deserializing tasks",SPARK-4080,12750411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,24/Oct/14 18:29,04/Nov/14 15:33,14/Jul/23 06:26,24/Oct/14 22:22,1.1.0,1.2.0,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"When deserializing tasks on executors, we sometimes see {{IOException: unexpected exception type}}:

{code}
 java.io.IOException: unexpected exception type
        java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538)
        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1025)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:163)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

Here are some occurrences of this bug reported on the mailing list and GitHub:

- https://www.mail-archive.com/user@spark.apache.org/msg12129.html
- http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201409.mbox/%3CCAEaWm8UOp9TGarm5scEpPZEy5qxO+H8hU8UjzaH5s-ajyzZB_g@mail.gmail.com%3E
- https://github.com/yieldbot/flambo/issues/13
- https://www.mail-archive.com/user@spark.apache.org/msg13283.html

This is probably caused by throwing exceptions other than IOException from our custom {{readExternal}} methods (see http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7u40-b43/java/io/ObjectStreamClass.java#1022).  [~davies] spotted an instance of this in TorrentBroadcast, where a failed {{require}} throws a different exception, but this issue has been reported in Spark 1.1.0 as well.  To fix this, I'm going to add try-catch blocks around all of our {{readExternal}} and {{writeExternal}} methods to re-throw caught exceptions as IOException.

This fix should allow us to determine the actual exceptions that are causing deserialization failures.",,apachespark,joshrosen,kul,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 02 08:57:00 UTC 2014,,,,,,,,,,"0|i21jzb:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"24/Oct/14 19:15;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2932;;;","24/Oct/14 22:22;joshrosen;Issue resolved by pull request 2932
[https://github.com/apache/spark/pull/2932];;;","31/Oct/14 04:51;kul;Hello Josh,
I did some testing with 1.1.1-SNAPSHOT branch, I am working on mesos cluster and i am able to consistently reproduce this when i create more than one spark contexts. The new exceptions look like the following on executors logs.
{code}
14/10/31 10:07:51 INFO TorrentBroadcast: Started reading broadcast variable 0
14/10/31 10:07:51 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 0)
java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:930)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:155)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:160)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:124)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:104)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:104)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:104)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readObject$1.apply$mcV$sp(TorrentBroadcast.scala:165)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:927)
	... 20 more
14/10/31 10:07:51 INFO Executor: Running task 0.1 in stage 1.0 (TID 1)
14/10/31 10:07:51 INFO TorrentBroadcast: Started reading broadcast variable 
{code};;;","31/Oct/14 16:28;joshrosen;Hi [~kul],

Thanks for trying this out!  I'm glad to see that my patch improved the error reporting here.

What do you mean by ""creating more than one SparkContext""?  Are you creating multiple concurrently-running SparkContexts in the same driver JVM?;;;","01/Nov/14 06:13;kul;Yes [~joshrosen], two sparkcontexts are being created from same jvm process to connect to a local mesos cluster with single master and slave.;;;","01/Nov/14 07:49;joshrosen;Spark does not currently support multiple concurrently-running SparkContexts in the same JVM (see SPARK-2243).  In a nutshell, there are a few components that have global registries (e.g. broadcast variables, accumulators, etc.) or are (effectively) global objects (SparkEnv), so having multiple active SparkContexts may cause unexpected behavior.  We do not test Spark with multiple concurrently-running SparkContexts in a single JVM, so I'm not sure what happens when you do that; the behavior is effectively undefined.

We should display a loud warning / error when attempting to initialize a SparkContext when another one is running and hasn't been {{stop()}}'d.  PySpark already does this, but the Scala API should do this as well, so I've opened SPARK-4180 to add this error-checking.;;;","02/Nov/14 08:57;kul;Thanks [~joshrosen], I was not aware of that. Although i did not face this in the standalone mode but i guess i would not have tested extensively enough.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy bundled with Spark does not work on older Linux distributions,SPARK-4079,12750401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kostas,vanzin,vanzin,24/Oct/14 17:47,22/Dec/14 21:07,14/Jul/23 06:26,22/Dec/14 21:07,1.0.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"This issue has existed at least since 1.0, but has been made worse by 1.1 since snappy is now the default compression algorithm. When trying to use it on a CentOS 5 machine, for example, you'll get something like this:

{noformat}
      java.lang.reflect.InvocationTargetException
           at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
           at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
           at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
           at java.lang.reflect.Method.invoke(Method.java:606)
           at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:319)
           at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:226)
           at org.xerial.snappy.Snappy.<clinit>(Snappy.java:48)
           at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)
           at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)
           at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:207)
       ...
       Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5.3-af72bf3c-9dab-43af-a662-f9af657f06b1-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /tmp/snappy-1.0.5.3-af72bf3c-9dab-43af-a662-f9af657f06b1-libsnappyjava.so)
           at java.lang.ClassLoader$NativeLibrary.load(Native Method)
           at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1957)
           at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1882)
           at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1843)
           at java.lang.Runtime.load0(Runtime.java:795)
           at java.lang.System.load(System.java:1061)
           at org.xerial.snappy.SnappyNativeLoader.load(SnappyNativeLoader.java:39)
           ... 29 more
{noformat}

There are two approaches I can see here (well, 3):

* Declare CentOS 5 (and similar OSes) not supported, although that would suck for the people who are still on it and already use Spark
* Fallback to another compression codec if Snappy cannot be loaded
* Ask the Snappy guys to compile the library on an older OS...

I think the second would be the best compromise.",,apachespark,hammer,kostas,pwendell,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 21:33:59 UTC 2014,,,,,,,,,,"0|i21jx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/14 19:09;pwendell;What about just catching the exception and throwing a better one that tells users to switch to use LZF? In my experience users are fine with this kind of thing if you just give them an actionable and simple step to work around it.

I think it's a bit extreme to declare that Spark doesn't support CentOS5 because of this - if people want to package Spark to work on CentOS5 they can just change the default compression codec.

Could be a good idea to ask Snappy to add older versions as well. The maintainer of that library is very responsive. ;;;","30/Oct/14 23:17;kostas;[~pwendell] I'm in favor of checking if Snappy is supported and if not, log a warning and switch to LZF. I feel this is a better user experience than just throwing an exception and telling the user to go modify ""spark.io.compression.codec"".  Especially since this is a performance issue not a correctness one. Thoughts?;;;","31/Oct/14 06:24;pwendell;Yeah that sounds like a good call. Did you want to do this?;;;","31/Oct/14 18:15;kostas;yes, I'm taking this over from Marcelo.;;;","05/Nov/14 21:33;apachespark;User 'ksakellis' has created a pull request for this issue:
https://github.com/apache/spark/pull/3119;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New FsPermission instance w/o FsPermission.createImmutable in eventlog,SPARK-4078,12750368,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grace.huang,grace.huang,grace.huang,24/Oct/14 15:09,30/Oct/14 22:53,14/Jul/23 06:26,30/Oct/14 22:53,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"By default, Spark builds its package against Hadoop 1.0.4 version. In that version, it has some FsPermission bug (see HADOOP-7629 by Todd Lipcon). This bug got fixed since 1.1 version. By using that FsPermission.createImmutable() API, end-user may see some RPC exception like below (if turn on eventlog over HDFS). 
{quote}
Exception in thread ""main"" java.io.IOException: Call to sr484/10.1.2.84:54310 failed on local exception: java.io.EOFException
        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1150)
        at org.apache.hadoop.ipc.Client.call(Client.java:1118)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
        at $Proxy6.setPermission(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
        at $Proxy6.setPermission(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.setPermission(DFSClient.java:1285)
        at org.apache.hadoop.hdfs.DistributedFileSystem.setPermission(DistributedFileSystem.java:572)
        at org.apache.spark.util.FileLogger.createLogDir(FileLogger.scala:138)
        at org.apache.spark.util.FileLogger.start(FileLogger.scala:115)
        at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:74)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:324)
{quote}",,apachespark,grace.huang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4132,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 15:11:01 UTC 2014,,,,,,,,,,"0|i21jpz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Oct/14 15:11;apachespark;User 'GraceH' has created a pull request for this issue:
https://github.com/apache/spark/pull/2892;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A broken string timestamp value can Spark SQL return wrong values for valid string timestamp values,SPARK-4077,12750361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gvramana,yhuai,yhuai,24/Oct/14 14:37,31/Oct/14 18:30,14/Jul/23 06:26,31/Oct/14 18:30,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The following case returns wrong results.
The text file is 
{code}
2014-12-11 00:00:00,1
2014-12-11astring00:00:00,2
{code}
The DDL statement and the query are shown below...
{code}
sql(""""""
create external table date_test(my_date timestamp, id int)
row format delimited
fields terminated by ','
lines terminated by '\n'
LOCATION 'dateTest'
"""""")
sql(""select * from date_test"").collect.foreach(println)
{code}
The result is 
{code}
[1969-12-31 19:00:00.0,1]
[null,2]
{code}

If I change the data to 
{code}
2014-12-11 00:00:00,1
2014-12-11 00:00:00,2
{code}
The result is fine.

For the data with broken string timestamp value, I tried runSqlHive. The result is fine.",,apachespark,gvramana,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 18:30:44 UTC 2014,,,,,,,,,,"0|i21jof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/14 13:02;gvramana;In org.apache.hadoop.hive.serde2.io.TimestampWritable.set , if the next entry is null then current time stamp object is being reset. 
Not sure why it is done like that in hive. We also can raise a bug in hive.

However because of this hiveinspectors:unwrap cannot use the same timestamp object without creating a copy. ;;;","30/Oct/14 18:17;yhuai;[~gvramana] Thank you for looking at it. Seems for text source, Hive use LazyTimestamp to deserialize the value and set it to TimestampWriteable. So, this issue only affects text sources, right?

[The code|https://github.com/apache/hive/blob/trunk/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java#L130] mentioned by [~gvramana] is shown as follows.
{code:java}
public void set(Timestamp t) {
  if (t == null) {
    timestamp.setTime(0);
    timestamp.setNanos(0);
    return;
  }
  this.timestamp = t;
  bytesEmpty = true;
  timestampEmpty = false;
}
{code}

btw, why the result of runSqlHive(ask hive to run the query) is not affected?;;;","30/Oct/14 18:19;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/3019;;;","31/Oct/14 03:53;gvramana;I could not find this behaviour for non text source.
I case of runSqlHive it is serializing row by row, so this problem will not be visible.
org.apache.hadoop.hive.ql.exec.FetchTask.fetch;;;","31/Oct/14 18:30;marmbrus;Issue resolved by pull request 3019
[https://github.com/apache/spark/pull/3019];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter expansion in spark-config is wrong,SPARK-4076,12750358,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,24/Oct/14 14:25,24/Oct/14 20:06,14/Jul/23 06:26,24/Oct/14 20:06,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In sbin/spark-config.sh, parameter expansion is used to extract source root as follows.

{code}
this=""${BASH_SOURCE-$0}""
{code}

I think, the parameter expansion should be "":-"" instead of ""-"".
If we use ""-"" and BASH_SOURCE="""", (empty character is set, not unset),
"""" (empty character) is set to $this.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 14:28:30 UTC 2014,,,,,,,,,,"0|i21jnr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Oct/14 14:28;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2930;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jar url validation is not enough for Jar file,SPARK-4075,12750326,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,24/Oct/14 10:09,18/Nov/14 02:41,14/Jul/23 06:26,24/Oct/14 20:19,1.2.0,,,,,,,1.1.1,1.2.0,,,,,Deploy,,,,0,,,,,,"In deploy.ClientArguments.isValidJarUrl, the url is checked as follows.

{code}
def isValidJarUrl(s: String): Boolean = s.matches(""(.+):(.+)jar"")
{code}

So, it allows like 'hdfs:file.jar' (no authority).",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 18 02:41:03 UTC 2014,,,,,,,,,,"0|i21jgn:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"24/Oct/14 10:12;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2925;;;","18/Nov/14 02:41;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Storage UI does not reflect memory usage by streaming blocks,SPARK-4072,12750247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,tdas,tdas,24/Oct/14 00:05,14/Jul/15 20:59,14/Jul/23 06:26,14/Jul/15 20:59,1.0.0,1.0.1,1.0.2,1.1.0,,,,1.5.0,,,,,,DStreams,Web UI,,,1,,,,,,"The storage page in the web ui does not show the memory usage of non-RDD, non-Broadcast blocks. In other words, the memory used by data received through Spark Streaming is not shown on the web ui. ",,apachespark,Haopu Wang,otis,rdub,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 15:35:03 UTC 2015,,,,,,,,,,"0|i21iz3:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,"24/Oct/14 00:11;tdas;[~joshrosen] [~andrewor14] How hard would it be to add this?

;;;","01/Jun/15 21:00;tdas;[~zsxwing] Can you take a crack at this? This is pretty important for debugging. ;;;","04/Jun/15 07:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6634;;;","05/Jun/15 15:35;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6672;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unroll fails silently if BlockManager size is small,SPARK-4071,12750243,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,23/Oct/14 23:43,26/Oct/14 03:08,14/Jul/23 06:26,26/Oct/14 03:08,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In tests, we may want to have BlockManagers of size < 1MB (spark.storage.unrollMemoryThreshold). However, these BlockManagers are useless because we can't unroll anything in them ever. At the very least we need to log a warning.",,andrewor14,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 03:08:20 UTC 2014,,,,,,,,,,"0|i21iy7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Oct/14 23:44;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2917;;;","26/Oct/14 03:08;joshrosen;Issue resolved by pull request 2917
[https://github.com/apache/spark/pull/2917];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in jsonRDD schema inference,SPARK-4068,12750220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,marmbrus,marmbrus,23/Oct/14 21:57,26/Oct/14 23:32,14/Jul/23 06:26,26/Oct/14 23:32,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"{code}
val jsonData = """"""{""data"":[[null], [[[""Test""]]]]}"""""" :: """"""{""other"": """"}"""""" :: Nil
sqlContext.jsonRDD(sc.parallelize(jsonData))
{code}

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 5.0 failed 4 times, most recent failure: Lost task 13.3 in stage 5.0 (TID 347, ip-10-0-234-152.us-west-2.compute.internal): java.lang.NullPointerException: 
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$allKeysWithValueTypes$1.org$apache$spark$sql$json$JsonRDD$$anonfun$$buildKeyPathForInnerStructs$1(JsonRDD.scala:252)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$allKeysWithValueTypes$1$$anonfun$org$apache$spark$sql$json$JsonRDD$$anonfun$$buildKeyPathForInnerStructs$1$3.apply(JsonRDD.scala:253)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$allKeysWithValueTypes$1$$anonfun$org$apache$spark$sql$json$JsonRDD$$anonfun$$buildKeyPathForInnerStructs$1$3.apply(JsonRDD.scala:253)
        scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
        scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
...
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 23:32:20 UTC 2014,,,,,,,,,,"0|i21it3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Oct/14 00:36;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/2918;;;","26/Oct/14 23:32;marmbrus;Issue resolved by pull request 2918
[https://github.com/apache/spark/pull/2918];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark will not use ipython on Windows,SPARK-4065,12750130,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michael.griffiths,michael.griffiths,michael.griffiths,23/Oct/14 17:39,28/Oct/14 19:49,14/Jul/23 06:26,28/Oct/14 19:49,1.1.0,,,,,,,1.1.1,1.2.0,,,,,PySpark,,,,0,,,,,,"pyspark2.cmd will not launch ipython, even if the environment variables are set. It doesn't check for the existence of ipython environment variables - in all cases, it will just launch python.",,apachespark,michael.griffiths,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 17:47:34 UTC 2014,,,,,,,,,,"0|i21i9z:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"23/Oct/14 17:47;apachespark;User 'msjgriffiths' has created a pull request for this issue:
https://github.com/apache/spark/pull/2910;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NioBlockTransferService should deal with empty messages correctly,SPARK-4064,12750077,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gq,gq,gq,23/Oct/14 14:46,28/Oct/14 06:32,14/Jul/23 06:26,28/Oct/14 06:32,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"When I test [the PR 1983|https://github.com/apache/spark/pull/1983], The probability of a third, spark hangs",,apachespark,gq,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/14 14:46;gq;executor.log;https://issues.apache.org/jira/secure/attachment/12676599/executor.log","23/Oct/14 15:31;gq;jstack.txt;https://issues.apache.org/jira/secure/attachment/12676605/jstack.txt","23/Oct/14 14:48;gq;screenshot.png;https://issues.apache.org/jira/secure/attachment/12676600/screenshot.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 14:00:16 UTC 2014,,,,,,,,,,"0|i21hz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/14 14:00;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2929;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We cannot use EOL character in the operand of LIKE predicate.,SPARK-4061,12750024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,23/Oct/14 10:13,26/Oct/14 23:54,14/Jul/23 06:26,26/Oct/14 23:54,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"We cannot use EOL character like \n or \r in the operand of LIKE predicate.
So following condition is never true.

{code}
-- someStr is 'hoge\nfuga'
where someStr LIKE 'hoge_fuga'
{code}",,apachespark,marmbrus,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 23:54:29 UTC 2014,,,,,,,,,,"0|i21hnj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Oct/14 10:14;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2908;;;","26/Oct/14 19:23;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2946;;;","26/Oct/14 23:54;marmbrus;Issue resolved by pull request 2908
[https://github.com/apache/spark/pull/2908];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent spelling 'MLlib' and 'MLLib',SPARK-4055,12749986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sarutak,sarutak,sarutak,23/Oct/14 05:24,23/Oct/14 16:20,14/Jul/23 06:26,23/Oct/14 16:20,1.2.0,,,,,,,1.2.0,,,,,,Documentation,MLlib,,,0,,,,,,Thare are some inconsistent spellings 'MLlib' and 'MLLib' in some documents and source codes.,,apachespark,mengxr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 16:20:19 UTC 2014,,,,,,,,,,"0|i21hg7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Oct/14 05:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2903;;;","23/Oct/14 16:20;mengxr;Issue resolved by pull request 2903
[https://github.com/apache/spark/pull/2903];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Block generator throttling in NetworkReceiverSuite is flaky,SPARK-4053,12749945,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tdas,tdas,tdas,23/Oct/14 00:46,19/Dec/14 16:16,14/Jul/23 06:26,30/Oct/14 00:59,1.1.0,,,,,,,1.2.0,,,,,,DStreams,,,,0,flaky-test,,,,,"In the unit test that checked whether blocks generated by throttled block generator had expected number of records, the thresholds are too tight, which sometimes led to the test failing. ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 00:48:09 UTC 2014,,,,,,,,,,"0|i21h73:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Oct/14 00:48;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2900;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use scala.collection.Map for pattern matching instead of using Predef.Map (it is scala.collection.immutable.Map),SPARK-4052,12749923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yhuai,yhuai,yhuai,22/Oct/14 23:34,26/Oct/14 23:31,14/Jul/23 06:26,26/Oct/14 23:31,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Seems ScalaReflection and InsertIntoHiveTable only take scala.collection.immutable.Map as the value type of MapType. Here are test cases showing errors.
{code}
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
import sqlContext.createSchemaRDD
val rdd = sc.parallelize((""key"", ""value"") :: Nil)

// Test1: This one fails.
case class Test1(m: scala.collection.Map[String, String])
val rddOfTest1 = rdd.map { case (k, v) => Test1(Map(k->v)) }
rddOfTest1.registerTempTable(""t1"")
/* Stack trace
scala.MatchError: scala.collection.Map[String,String] (of class scala.reflect.internal.Types$TypeRef$$anon$5)
	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:53)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:64)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:62)
...
*/

// Test2: This one is fine.
case class Test2(m: scala.collection.immutable.Map[String, String])
val rddOfTest2 = rdd.map { case (k, v) => Test2(Map(k->v)) }
rddOfTest2.registerTempTable(""t2"")
sqlContext.sql(""SELECT m FROM t2"").collect
sqlContext.sql(""SELECT m['key'] FROM t2"").collect

// Test3: This one fails.
val schema = StructType(StructField(""m"", MapType(StringType, StringType), true) :: Nil)
val rowRDD = rdd.map { case (k, v) =>  Row(scala.collection.mutable.HashMap(k->v)) }
val schemaRDD = sqlContext.applySchema(rowRDD, schema)
schemaRDD.registerTempTable(""t3"")
sqlContext.sql(""SELECT m FROM t3"").collect
sqlContext.sql(""SELECT m['key'] FROM t3"").collect
sqlContext.sql(""CREATE TABLE testHiveTable1(m MAP <STRING, STRING>)"")
sqlContext.sql(""INSERT OVERWRITE TABLE testHiveTable1 SELECT m FROM t3"")
/* Stack trace
14/10/22 19:30:56 INFO DAGScheduler: Job 4 failed: runJob at InsertIntoHiveTable.scala:124, took 1.384579 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4.0 (TID 12, yins-mbp): java.lang.ClassCastException: scala.collection.mutable.HashMap cannot be cast to scala.collection.immutable.Map
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$wrapperFor$5.apply(InsertIntoHiveTable.scala:96)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$wrapperFor$5.apply(InsertIntoHiveTable.scala:96)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:148)
        org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:145)
*/

// Test4: This one is fine.
val rowRDD = rdd.map { case (k, v) =>  Row(Map(k->v)) }
val schemaRDD = sqlContext.applySchema(rowRDD, schema)
schemaRDD.registerTempTable(""t4"")
sqlContext.sql(""SELECT m FROM t4"").collect
sqlContext.sql(""SELECT m['key'] FROM t4"").collect
sqlContext.sql(""CREATE TABLE testHiveTable1(m MAP <STRING, STRING>)"")
sqlContext.sql(""INSERT OVERWRITE TABLE testHiveTable1 SELECT m FROM t4"")
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 00:37:03 UTC 2014,,,,,,,,,,"0|i21h2n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Oct/14 23:47;yhuai;I searched our sql code base with
{code}
grep -r ""typeOf\[Map"" ./sql/
grep -r ""asInstanceOf\[Map"" ./sql/
{code}
Only ScalaReflection and InsertIntoHiveTable are affected.;;;","23/Oct/14 00:37;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/2899;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching of temporary tables with projects fail when the final query projects fewer columns,SPARK-4050,12749884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,22/Oct/14 21:16,24/Oct/14 17:52,14/Jul/23 06:26,24/Oct/14 17:52,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"{code}
import sqlContext._
import org.apache.spark.sql._
val data = sc.parallelize(1 to 100, 10).map(i => (i, i)).sortByKey()
val data2: SchemaRDD = data.orderBy('_1.asc).select('_1)
data2.registerTempTable(""data3"")
table(""data3"")
sql(""cache table data3"")
{code}

{code}
// Note that a count(*) plan does not use the in-memory cached relation
​
sql(""select count(*) from data3"")
res2: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[69] at RDD at SchemaRDD.scala:104
== Query Plan ==
== Physical Plan ==
Aggregate false, [], [SUM(PartialCount#17L) AS c_0#15L]
 Exchange SinglePartition
  Aggregate true, [], [COUNT(1) AS PartialCount#17L]
   Project []
    Sort [_1#2 ASC], true
     Exchange (RangePartitioning [_1#2 ASC], 200)
      PhysicalRDD [_1#2,_2#3], MapPartitionsRDD[38] at mapPartitions at ExistingRDD.scala:37
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 17:52:46 UTC 2014,,,,,,,,,,"0|i21gs7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Oct/14 20:21;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2912;;;","24/Oct/14 17:52;marmbrus;Issue resolved by pull request 2912
[https://github.com/apache/spark/pull/2912];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Java example on site,SPARK-4046,12749743,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rxin,bobrik,bobrik,22/Oct/14 10:01,23/Apr/15 18:14,14/Jul/23 06:26,27/Nov/14 00:36,1.1.0,,,,,,,,,,,,,Documentation,Java API,,,0,,,,,,"https://spark.apache.org/examples.html

Here word count example for java is incorrect. It should be mapToPair instead of map. Correct example is here:

https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java",Web,bobrik,boyork,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7096,,,,SPARK-7096,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 16:15;boyork;SPARK-4046.diff;https://issues.apache.org/jira/secure/attachment/12683858/SPARK-4046.diff",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 27 00:36:25 UTC 2014,,,,,,,,,,"0|i21fxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 16:15;boyork;Pulled down the site at the [Apache repo|https://svn.apache.org/repos/asf/spark/site], made the change, and attached the .diff file to resolve the issue.;;;","27/Nov/14 00:36;rxin;Thanks - I pushed the change and updated the website.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thriftserver fails to start when JAVA_HOME points to JRE instead of JDK,SPARK-4044,12749696,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,joshrosen,joshrosen,22/Oct/14 05:17,30/Apr/15 15:56,14/Jul/23 06:26,13/Mar/15 17:59,1.1.0,1.2.0,,,,,,1.3.1,,,,,,Documentation,SQL,,,0,,,,,,"If {{JAVA_HOME}} points to a JRE instead of a JDK, e.g.

{code}
JAVA_HOME=/usr/lib/jvm/java-7-oracle/jre/ 
{code}

instead of 

{code}
JAVA_HOME=/usr/lib/jvm/java-7-oracle/
{code}

Then start-thriftserver.sh will fail with Datanucleus JAR errors:

{code}
Caused by: java.lang.ClassNotFoundException: org.datanucleus.api.jdo.JDOPersistenceManagerFactory
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at javax.jdo.JDOHelper$18.run(JDOHelper.java:2018)
	at javax.jdo.JDOHelper$18.run(JDOHelper.java:2016)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.forName(JDOHelper.java:2015)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1162)
{code}

The root problem seems to be that {{compute-classpath.sh}} uses {{JAVA_HOME}} to find the path to the {{jar}} command, which isn't present in JRE directories.  This leads to silent failures when adding the Datanucleus JARs to the classpath.

This same issue presumably affects the command that checks whether Spark was built on Java 7 but run on Java 6.

We should probably add some error-handling that checks whether the {{jar}} command is actually present and throws an error otherwise, and also update the documentation to say that `JAVA_HOME` must point to a JDK and not a JRE.",,advancedxy,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 18:00:14 UTC 2015,,,,,,,,,,"0|i21fnr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/14 06:04;srowen;How about using {{unzip -l}} to probe the contents of the .jar files? They're just zip files after all. You check the exit status to see if it contained the entry in question -- 0 if it did, non-0 otherwise.

I am not sure how this will interact with the check for an invalid JAR file that is also in the script though.;;;","03/Mar/15 13:28;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4873;;;","11/Mar/15 13:45;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4981;;;","13/Mar/15 17:59;srowen;Issue resolved by pull request 4981
[https://github.com/apache/spark/pull/4981];;;","13/Mar/15 18:00;srowen;Note that this is also fixed, by being obsoleted, in 1.4.x, by SPARK-4924;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
append columns ids and names before broadcast,SPARK-4042,12749624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,21/Oct/14 21:19,26/Oct/14 23:56,14/Jul/23 06:26,26/Oct/14 23:56,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"appended columns ids and names will not broadcast because we append them after creating table reader. This leads to the config broadcasted to executor side dose not contain the configs of appended columns and names. 
",,apachespark,marmbrus,scwf,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 23:56:53 UTC 2014,,,,,,,,,,"0|i21f7b:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"21/Oct/14 21:23;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2885;;;","22/Oct/14 01:54;yhuai;Can you add an explanation about the problem in Description? ;;;","22/Oct/14 14:14;yhuai;Can you also add some test results? Like the amount of data read from HDFS when we read all columns and only a single column.;;;","26/Oct/14 23:56;marmbrus;Issue resolved by pull request 2885
[https://github.com/apache/spark/pull/2885];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convert attributes names in table scan lowercase when compare with relation attributes,SPARK-4041,12749621,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,21/Oct/14 21:11,28/Oct/14 03:47,14/Jul/23 06:26,28/Oct/14 03:47,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 03:47:29 UTC 2014,,,,,,,,,,"0|i21f6n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Oct/14 21:19;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2884;;;","28/Oct/14 03:47;marmbrus;Issue resolved by pull request 2884
[https://github.com/apache/spark/pull/2884];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in JDBC server when calling SET,SPARK-4037,12749579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,21/Oct/14 18:31,12/Dec/14 12:46,14/Jul/23 06:26,01/Nov/14 22:03,,,,,,,,1.2.0,,,,,,SQL,,,,1,,,,,,"{code}
SET spark.sql.shuffle.partitions=10;
{code}

{code}
14/10/21 18:00:47 ERROR server.SparkSQLOperationManager: Error executing query:
java.lang.NullPointerException
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:309)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:272)
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:244)
	at org.apache.spark.sql.execution.SetCommand.sideEffectResult$lzycompute(commands.scala:64)
...
{code}",,apachespark,dongxu,lian cheng,marmbrus,yanakad,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3939,SPARK-4103,,,,,,,SPARK-3729,SPARK-2814,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 01 22:03:25 UTC 2014,,,,,,,,,,"0|i21exb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Oct/14 23:26;lian cheng;This is a regression of SPARK-2814, added in SPARK-3729.

One solution is let HiveContext always reuse SessionState started within the same thread, if there's none, create a new one. In this way, we don't need to override the SessionState field in HiveThriftServer2, thus eliminate this issue.;;;","22/Oct/14 05:18;lian cheng;I think we can safely remove the global singleton SessionState created in HiveThriftServer2, and replace it with the SessionState field instance in HiveContext.

The current global singleton SessionState design dates back to Shark (SharkContext.sessionState). This basically breaks session isolation in Shark, and later HiveThriftServer2 because all connections/sessions share a single SessionState instance. For example, switching current database in one connection also affects other concurrent connections (SPARK-3552). Fixing this properly requires major refactoring of HiveContext. Considering 1.2.0 release is approaching, and Hive support will probably be rewritten against the newly introduced external data source API, I'd like to fix this specific SessionState initialization issue for Spark 1.2.0 release, and leave multi-user support to a later release.;;;","22/Oct/14 06:52;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2887;;;","01/Nov/14 22:03;marmbrus;Issue resolved by pull request 2887
[https://github.com/apache/spark/pull/2887];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong format specifier in BlockerManager.scala,SPARK-4035,12749479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,zsxwing,zsxwing,21/Oct/14 10:01,21/Oct/14 17:37,14/Jul/23 06:26,21/Oct/14 17:37,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,typo,,,,,"Should not use ""%f"" for Long.",,apachespark,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 17:37:52 UTC 2014,,,,,,,,,,"0|i21ebb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/14 10:03;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2875;;;","21/Oct/14 17:37;joshrosen;Issue resolved by pull request 2875
[https://github.com/apache/spark/pull/2875];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflow when SparkPi is called with more than 25000 slices,SPARK-4033,12749469,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carlmartin,carlmartin,carlmartin,21/Oct/14 09:24,12/Jan/15 00:34,14/Jul/23 06:26,12/Jan/15 00:34,1.2.0,,,,,,,1.3.0,,,,,,Examples,,,,0,,,,,,"If input of the SparkPi args is larger than the 25000, the integer 'n' inside the code will be overflow, and may be a negative number.
And it causes  the (0 until n) Seq as an empty seq, then doing the action 'reduce'  will throw the UnsupportedOperationException(""empty collection"").",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 09:28:29 UTC 2014,,,,,,,,,,"0|i21e93:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"21/Oct/14 09:28;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2874;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read broadcast variables on use,SPARK-4031,12749436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,21/Oct/14 05:40,28/Oct/14 17:16,14/Jul/23 06:26,28/Oct/14 17:16,,,,,,,,1.2.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"This is a proposal to change the broadcast variable implementations in Spark to only read values when they are used rather than on deserializing.

This change will be very helpful (and in our use cases required) for complex applications which have a large number of broadcast variables. For example if broadcast variables are class members, they are captured in closures even when they are not used.

We could also consider cleaning closures more aggressively, but that might be a more complex change.",,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 17:16:09 UTC 2014,,,,,,,,,,"0|i21e1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/14 05:43;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/2871;;;","28/Oct/14 17:16;shivaram;Issue resolved by pull request 2871
https://github.com/apache/spark/pull/2871;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark's stat.Statistics is broken,SPARK-4023,12749399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,mengxr,mengxr,21/Oct/14 01:00,21/Oct/14 16:30,14/Jul/23 06:26,21/Oct/14 16:30,1.2.0,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,0,,,,,,"{code}
from pyspark.mllib.stat import Statistics
from pyspark.mllib.random import RandomRDDs
data = RandomRDDs.uniformVectorRDD(sc, 100000, 10, 10)
Statistics.colStats(data)
{code}

throws 

{code}

Py4JJavaError: An error occurred while calling o37.colStats.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
        net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
        net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
        net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
        net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
        net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
        org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:695)
        org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:694)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
        scala.collection.AbstractIterator.foldLeft(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:201)
        scala.collection.AbstractIterator.aggregate(Iterator.scala:1157)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:744)
{code}",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3491,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 16:30:01 UTC 2014,,,,,,,,,,"0|i21dtr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Oct/14 04:38;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2870;;;","21/Oct/14 16:30;mengxr;Issue resolved by pull request 2870
[https://github.com/apache/spark/pull/2870];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace colt dependency (LGPL) with commons-math,SPARK-4022,12749398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,pwendell,pwendell,21/Oct/14 00:56,28/Oct/14 20:57,14/Jul/23 06:26,27/Oct/14 17:53,,,,,,,,1.2.0,,,,,,MLlib,Spark Core,,,0,,,,,,"The colt library we use is LGPL-licensed:
http://acs.lbl.gov/ACSSoftware/colt/license.html

We need to swap this out for commons-math. It is also a very old library that hasn't been updated since 2004.",,apachespark,bcantoni,dorx,josephkb,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4121,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 27 17:53:35 UTC 2014,,,,,,,,,,"0|i21dtj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"21/Oct/14 01:03;srowen;Yeah, looks like it is only in examples at least, so I don't know if Colt ever got technically distributed (? I forget whether it goes out with transitive deps). But best to change it. I can try it, since I know Commons Math well, unless someone's already on it.;;;","21/Oct/14 01:10;mengxr;[~srowen] We use its random number generators and the stat.Probability package in core and mllib. It would be great if you can work on this JIRA. There are couple places we need to change:

1) sampling
2) chi-sq tests
3) random forests
4) some unit tests

We also need to shade the commons-math3 jar because hadoop depends on it. Fortunately, commons-math3 doesn't have any run-time deps.;;;","21/Oct/14 01:22;srowen;Ah right there is Jet too, not just Colt.

The LGPL license actually only pertains to a few parts of Colt, in hep.aida.*, which aren't used by Spark.
Another solution is just to make sure these classes never become part of the distribution. Colt and Jet themselves don't appear to be LGPL, in the main.

Of course, if there was a desire to just stop using Colt+Jet anyway, I'm cool with that too.;;;","21/Oct/14 01:26;mengxr;Colt is really old. Even the download link (http://acs.lbl.gov/software/colt/colt-download) is broken. It is good if we switch to an alternative that is active and has no license issues.;;;","23/Oct/14 16:17;srowen;I have begun work on this. You can see the base change here:

https://github.com/srowen/spark/commits/SPARK-4022
https://github.com/srowen/spark/commit/8246dbd39be7ff162392c59c28dee74a1419e236

There are 4 potential problems, each of which might need some assistance as to how to proceed.

*HypothesisTestSuite failure*

CC [~dorx]

{code}
HypothesisTestSuite:
  ...
- chi squared pearson RDD[LabeledPoint] *** FAILED ***
  org.apache.commons.math3.exception.NotStrictlyPositiveException: shape (0)
  at org.apache.commons.math3.distribution.GammaDistribution.<init>(GammaDistribution.java:168)
  ...
  at org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredMatrix(ChiSqTest.scala:241)
  at org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$chiSquaredFeatures$4.apply(ChiSqTest.scala:134)
  at org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$chiSquaredFeatures$4.apply(ChiSqTest.scala:125)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
{code}

The commons-math3 implementation complains that a chi squared distribution is created with 0 degrees of freedom. It looks like that for col 645 in this test, there is just one feature, 0, and two labels. The contingency table should be at least 2x2 but it's 1x2 only, and that's not valid AFAICT. I spent some time staring at this and don't quite know what to make of fixing it.

*KMeansClusterSuite failure*

CC [~mengxr]

{code}
KMeansClusterSuite:
- task size should be small in both training and prediction *** FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 8, localhost): java.io.InvalidClassException: org.apache.spark.util.random.PoissonSampler; local class incompatible: stream classdesc serialVersionUID = -795011761847245121, local class serialVersionUID = 4249244967777318419
{code}

I understand what it's saying. PoissonSampler did indeed change and its serialized form changed, but, I don't see how two incompatible versions are turning up as a result of one clean build.

*RandomForestSuite failure*

CC [~josephkb]

{code}
RandomForestSuite:
...
- alternating categorical and continuous features with multiclass labels to test indexing *** FAILED ***
  java.lang.AssertionError: assertion failed: validateClassifier calculated accuracy 0.75 but required 1.0.
  at scala.Predef$.assert(Predef.scala:179)
  at org.apache.spark.mllib.tree.RandomForestSuite$.validateClassifier(RandomForestSuite.scala:227)
{code}

My guess on this one is that something is sampled differently as a result of this change, and happens to make the decision forest come out differently on this toy data set, and it happens to get 3/4 instead of 4/4 right now. This may be ignorable, meaning, the test was actually a little too strict and optimistic.

*Less efficient seeded sampling for series of Poisson variables*

CC [~dorx]

Colt had a way to seed the RNG, then generate a one-off sample from a Poisson distribution with mean m. commons-math3 lets you seed an instance of a Poisson distribution with mean m, but then not change that mean. To simulate, it's necessary to recreate a Poisson distribution with each successive mean with a deterministic series of seeds.

See here: https://github.com/srowen/spark/commit/8246dbd39be7ff162392c59c28dee74a1419e236#diff-0544248063499d8688c21f49be0918c8R285

This isn't a problem per se but could be slower. I am not sure if this code can be changed to not require constant reinitialization of the distribution.;;;","23/Oct/14 19:00;josephkb;Hi Sean, Thanks for picking this up!  W.r.t. the RandomForestSuite failure, I agree that the test is likely flaky since it relies on choosing random subsets of the features.  That test actually does not really need to call validateClassifier; the test was to check for a bug in training which caused an exception previously.  It should be fine to change (in that test in RandomForestSuite.scala):
from {code}RandomForestSuite.validateClassifier(model, arr, 1.0){code}
to {code}RandomForestSuite.validateClassifier(model, arr, 0.0){code}
;;;","23/Oct/14 20:20;mengxr;Hi [~srowen],

ChiSqTest:

This is an independence test. Since a constant distribution is independent of any distribution by definition, I think we should return pValue = 1.0 and statistic = 0.0 in that case.

KMeansClusterSuite:

It passed on my local machine. This test requires the assembly jar. Did you rebuild? Btw, breeze also depends on commons-math3. Even we plan to shade the jar, we should try to use the same version.

Poisson sampler:

The workaround won't generate good Poisson sequence because we change seed too frequently. We need two different mean values in stratified sampling. Refactoring the code is independent of this JIRA. So I would recommend the following:

1) maintain a map of mean -> PoissonDistribution in RandomDataGenerator
2) if the request mean is in the map, use the RNG there, otherwise, insert a new RNG into the map (and check the size of the map)
;;;","24/Oct/14 13:48;srowen;[~mengxr] [~josephkb] Great, most of this is resolved now. 

{{KMeansSuite}} still fails for me, yes, after a clean build. I wonder if the problem is that the commons-math3 code is in a different package in the assembly? Before thinking too hard about it, let me open a PR to see what Jenkins makes of it.

I also implemented the different approach to Poisson sampler seeding. It would be good to cap the size of the cache, although, I wonder if that could lead to problems. If a sampler is removed and recreated, it will start generating the same sequence again from the same seed. If it is not seeded, it will be nondeterministic. It looks like {{RandomDataGenerator}} instances are short-lived and applied to a fixed set of mean values, which suggests this won't blow up readily. I admit I just glanced at the usages though.;;;","24/Oct/14 13:50;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2928;;;","27/Oct/14 17:53;mengxr;Issue resolved by pull request 2928
[https://github.com/apache/spark/pull/2928];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed executor not properly removed if it has not run tasks,SPARK-4020,12749391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,20/Oct/14 23:50,23/Oct/14 06:42,14/Jul/23 06:26,21/Oct/14 18:46,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"If an executor has not run tasks, then the DAG scheduler won't remove the block manager that belongs to this executor. This is because we don't add the executor to `activeExecutorIds` in `TaskSchedulerImpl` until we have successfully scheduled a task on the executor.

The motivation for fixing this is to avoid having the `BlockManagerMasterActor` to remove the associated block manager through a timeout when it can just do so right away.",,andrewor14,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 18:46:50 UTC 2014,,,,,,,,,,"0|i21drz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 23:57;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2865;;;","21/Oct/14 18:46;kayousterhout;Fixed by https://github.com/apache/spark/commit/61ca7742d21dd66f5a7b3bb826e3aaca6f049b68;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffling with more than 2000 reducers may drop all data when partitons are mostly empty or cause deserialization errors if at least one partition is empty,SPARK-4019,12749371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,mengxr,mengxr,20/Oct/14 22:38,23/Oct/14 23:40,14/Jul/23 06:26,23/Oct/14 23:40,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"{code}
sc.makeRDD(0 until 10, 1000).repartition(2001).collect()
{code}

returns `Array()`.

1.1.0 doesn't have this issue. Tried both HASH and SORT manager.

This problem can also manifest itself as Snappy deserialization errors if the average map output status size is non-zero but there is at least one empty partition, e.g. 

sc.makeRDD(0 until 100000, 1000).repartition(2001).collect()
",,apachespark,hammer,joshrosen,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3740,,SPARK-3630,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 23:40:04 UTC 2014,,,,,,,,,,"0|i21dnr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Oct/14 23:06;joshrosen;This issue is caused by a bug in HighlyCompressedMapStatus.  I think that's we're compressing a bunch of blocks whose average size is small, so this gets averaged down to zero.  As a result, we skip these blocks as empty even though they contain data.

I'm going to work on a fix ASAP, but first I'm going to use ScalaCheck to write a property-based test that would have caught this.  The invariant that we need to maintain: ""if an uncompressed map output size is greater than zero, then compressing and decompressing should continue to report the map output as non-empty."";;;","21/Oct/14 01:44;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2866;;;","21/Oct/14 07:05;pwendell;Great work getting to the root cause of this [~joshrosen] - this was a tricky issue.;;;","22/Oct/14 23:31;joshrosen;This also explains another occurrence of the Snappy PARSING_ERROR(2) error.  If the average block size is non-zero but there is at least one zero-sized block, then HighlyCompressedMapStatus would cause us to fetch empty blocks, leading to the PARSING_ERROR(2) when Snappy tries to decompress this empty block.

Thanks to [~ilikerps] for helping to figure this out.;;;","23/Oct/14 23:40;pwendell;Fixed by Josh's patch:
https://github.com/apache/spark/pull/2866;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation in the streaming context references non-existent function,SPARK-4015,12749336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,holden,holden,20/Oct/14 20:05,21/Oct/14 16:37,14/Jul/23 06:26,21/Oct/14 16:37,,,,,,,,1.2.0,,,,,,DStreams,,,,0,,,,,,The current streaming scaladoc/javadoc references context.awaitTransformation when it should be context.awaitTermination.,,holden,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 16:37:48 UTC 2014,,,,,,,,,,"0|i21dfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/14 16:37;joshrosen;Issue resolved by pull request 2861
[https://github.com/apache/spark/pull/2861];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskContext.attemptId returns taskId,SPARK-4014,12749326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,yhuai,yhuai,20/Oct/14 19:20,20/Jan/15 18:12,14/Jul/23 06:26,14/Jan/15 19:45,,,,,,,,1.3.0,,,,,,Spark Core,,,,0,backport-needed,,,,,"In TaskRunner, we assign the taskId of a task to the attempId of the corresponding TaskContext. Should we rename attemptId to taskId to avoid confusion?",,apachespark,joshrosen,koeninger,pwendell,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5333,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 18:12:47 UTC 2015,,,,,,,,,,"0|i21ddz:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.2,1.2.1,,,,,,,,,"21/Oct/14 07:02;pwendell;[~joshrosen] what do you think about renaming this field (see associated discussion on dev list). I agree that it's confusingly named at present.;;;","30/Dec/14 22:07;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3849;;;","14/Jan/15 19:45;joshrosen;Issue resolved by pull request 3849
[https://github.com/apache/spark/pull/3849];;;","20/Jan/15 18:12;joshrosen;Note to self: when backporting this to any branches, also backport SPARK-5333 (since that fixes a bug introduced here).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not run multiple actor systems on each executor,SPARK-4013,12749325,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,20/Oct/14 19:17,24/Oct/14 20:32,14/Jul/23 06:26,24/Oct/14 20:32,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,This causes many more error messages to be logged on the driver than necessary when an executor is disassociated. We shouldn't have two concurrently running actor systems anyway.,,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 21:56:00 UTC 2014,,,,,,,,,,"0|i21ddr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Oct/14 21:56;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2863;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught OOM in ContextCleaner,SPARK-4012,12749322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,codingcat,20/Oct/14 19:07,19/Mar/15 06:50,14/Jul/23 06:26,19/Mar/15 06:50,,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"When running an ""might-be-memory-intensive""  application locally, I received the following exception


Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""Spark Context Cleaner""

Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- the VM may need to be forcibly terminated

Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""Driver Heartbeater""

Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- 
the VM may need to be forcibly terminated

Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- the VM may need to be forcibly terminated
Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- the VM may need to be forcibly terminated
Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- the VM may need to be forcibly terminated
Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- the VM may need to be forcibly terminated
Java HotSpot(TM) 64-Bit Server VM warning: Exception java.lang.OutOfMemoryError occurred dispatching signal SIGINT to handler- the VM may need to be forcibly terminated


I looked at the code, we might want to call Utils.tryOrExit instead of Utils.logUncaughtExceptions

",,apachespark,codingcat,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 17:47:51 UTC 2015,,,,,,,,,,"0|i21dd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 22:32;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/2864;;;","10/Mar/15 15:37;srowen;Andrew suggested this is maybe WontFix in his review, and the PR was not revived. Is this WontFix?;;;","10/Mar/15 16:27;codingcat;[~srowen], actually I got more understanding on the scenario involved in the patch...let me resubmit that within the week for more feedbacks;;;","12/Mar/15 17:47;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/5004;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI returns 500 in yarn-client mode ,SPARK-4010,12749271,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,gq,gq,20/Oct/14 15:00,25/Feb/15 15:52,14/Jul/23 06:26,20/Oct/14 18:04,1.2.0,,,,,,,1.1.1,1.2.0,,,,,Web UI,,,,0,,,,,,"http://host/proxy/application_id/stages/   returns this result:
{noformat}
HTTP ERROR 500

Problem accessing /proxy/application_1411648907638_0281/stages/. Reason:

    Connection refused
Caused by:

java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:280)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:185)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:336)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
	at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1183)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Powered by Jetty://

{noformat}",,apachespark,gq,HanchenSu,m.capuccini,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3067,,SPARK-5837,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 15:52:25 UTC 2015,,,,,,,,,,"0|i21d07:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"20/Oct/14 15:36;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2858;;;","25/Feb/15 15:46;HanchenSu;I still have the problem in Spark 1.2.1;;;","25/Feb/15 15:52;srowen;[~Hanchen] see SPARK-5837 for a possible explanation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Driver crashes whenever an Executor is registered twice,SPARK-4006,12749189,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tsliwowicz,tsliwowicz,tsliwowicz,20/Oct/14 08:42,09/Jan/15 20:07,14/Jul/23 06:26,27/Dec/14 08:02,0.9.2,1.0.2,1.1.0,1.2.0,,,,0.9.3,1.0.3,1.1.1,1.2.0,,,Block Manager,Spark Core,,,0,,,,,,"This is a huge robustness issue for us (Taboola), in mission critical , time sensitive (real time) spark jobs.

We have long running spark drivers and even though we have state of the art hardware, from time to time executors disconnect. In many cases, the RemoveExecutor is not received, and when the new executor registers, the driver crashes. In mesos coarse grained, executor ids are fixed. 

The issue is with the System.exit(1) in BlockManagerMasterActor

{code}
private def register(id: BlockManagerId, maxMemSize: Long, slaveActor: ActorRef) {
    if (!blockManagerInfo.contains(id)) {
      blockManagerIdByExecutor.get(id.executorId) match {
        case Some(manager) =>
          // A block manager of the same executor already exists.
          // This should never happen. Let's just quit.
          logError(""Got two different block manager registrations on "" + id.executorId)
          System.exit(1)
        case None =>
          blockManagerIdByExecutor(id.executorId) = id
      }

      logInfo(""Registering block manager %s with %s RAM"".format(
        id.hostPort, Utils.bytesToString(maxMemSize)))

      blockManagerInfo(id) =
        new BlockManagerInfo(id, System.currentTimeMillis(), maxMemSize, slaveActor)
    }
    listenerBus.post(SparkListenerBlockManagerAdded(id, maxMemSize))
  }
{code}","Mesos, Coarse Grained",apachespark,joshrosen,pwendell,sliwo,tsliwowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 27 08:02:42 UTC 2014,,,,,,,,,,"0|i21cif:",9223372036854775807,,,,,,,,,,,,,,0.9.3,1.0.3,1.1.1,1.2.0,,,,,,,,"20/Oct/14 10:13;tsliwowicz;Fixed in - https://github.com/apache/spark/pull/2854;;;","20/Oct/14 10:42;apachespark;User 'tsliwowicz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2854;;;","21/Oct/14 23:54;apachespark;User 'tsliwowicz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2886;;;","22/Oct/14 06:17;joshrosen;Thanks for the bug report + patch!  I'd like to see if I can find a way to reproduce this issue in a fault-injection tool that I'm building.  I'll give it a quick try tomorrow before reviewing (and hopefully merging) your fix.;;;","22/Oct/14 20:47;sliwo;Cool! Would be very interesting to know.
For us it's hard to force reproduce this (because you need two registers without a remove in between), but it always happens eventually, so I can tell for sure that the fix resolved our issue.;;;","23/Oct/14 21:18;apachespark;User 'tsliwowicz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2914;;;","23/Oct/14 21:28;apachespark;User 'tsliwowicz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2915;;;","23/Oct/14 21:32;sliwo;After the fix was merged to master, created a PR for 1.0, 1.1 and updated the original 0.9 PR (the merges were not clean).;;;","27/Dec/14 08:02;pwendell;I verified that this is present in branch 1.2, 1.1, and 1.0, so I'm resolving it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
handle message replies in receive instead of in the individual private methods,SPARK-4005,12749172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,20/Oct/14 07:15,05/Dec/14 20:01,14/Jul/23 06:26,05/Dec/14 20:00,1.1.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"In BlockManagermasterActor, when handling message type UpdateBlockInfo, the message replies is in handled in individual private methods, should handle it in receive of Akka.",,apachespark,codingcat,joshrosen,liyezhang556520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 05 20:00:59 UTC 2014,,,,,,,,,,"0|i21cen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 07:33;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/2853;;;","05/Dec/14 20:00;joshrosen;Issue resolved by pull request 2853
[https://github.com/apache/spark/pull/2853];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add {Big Decimal, Timestamp, Date} types to Java SqlContext",SPARK-4003,12749146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,20/Oct/14 03:11,22/Jul/16 05:12,14/Jul/23 06:26,29/Oct/14 19:11,,,,,,,,1.2.0,,,,,,SQL,,,,1,,,,,,"in JavaSqlContext, we need to let java program use big decimal, timestamp, date types.",,adrian-wang,apachespark,marmbrus,stevenmz,yaoge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 22 05:12:18 UTC 2016,,,,,,,,,,"0|i21c8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 03:13;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2850;;;","29/Oct/14 19:11;marmbrus;Issue resolved by pull request 2850
[https://github.com/apache/spark/pull/2850];;;","22/Jul/16 05:12;adrian-wang;DataTypes.TimestampType is not using java.sql.Timestamp internally. you should only use exposed API.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scalastyle should output the error location,SPARK-3997,12749033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gq,gq,18/Oct/14 06:08,26/Oct/14 23:25,14/Jul/23 06:26,26/Oct/14 23:25,,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"{{./dev/scalastyle}} =>
{noformat}
Scalastyle checks failed at following occurrences:
java.lang.RuntimeException: exists error
	at scala.sys.package$.error(package.scala:27)
	at scala.Predef$.error(Predef.scala:142)
[error] (mllib/*:scalastyle) exists error
{noformat}

scalastyle should output the error location:
{noformat}
[error] /Users/witgo/work/code/java/spark/mllib/src/main/scala/org/apache/spark/mllib/feature/TopicModeling.scala:413: File line length exceeds 100 characters
{noformat}
",,apachespark,gq,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 23:25:14 UTC 2014,,,,,,,,,,"0|i21bjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/14 08:22;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2846;;;","26/Oct/14 23:25;joshrosen;Issue resolved by pull request 2846
[https://github.com/apache/spark/pull/2846];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[PYSPARK] PySpark's sample methods do not work with NumPy 1.9,SPARK-3995,12748990,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,freeman-lab,freeman-lab,freeman-lab,17/Oct/14 22:02,27/Nov/14 05:11,14/Jul/23 06:26,22/Oct/14 16:33,1.1.0,,,,,,,1.2.0,,,,,,PySpark,Spark Core,,,0,,,,,,"There is a breaking bug in PySpark's sampling methods when run with NumPy v1.9. This is the version of NumPy included with the current Anaconda distribution (v2.1); this is a popular distribution, and is likely to affect many users.

Steps to reproduce are:

{code:python}
foo = sc.parallelize(range(1000),5)
foo.takeSample(False, 10)
{code}

Returns:

{code}
PythonException: Traceback (most recent call last):
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/worker.py"", line 79, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/serializers.py"", line 196, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/serializers.py"", line 127, in dump_stream
    for obj in iterator:
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/serializers.py"", line 185, in _batched
    for item in iterator:
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/rddsampler.py"", line 116, in func
    if self.getUniformSample(split) <= self._fraction:
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/rddsampler.py"", line 58, in getUniformSample
    self.initRandomGenerator(split)
  File ""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/rddsampler.py"", line 44, in initRandomGenerator
    self._random = numpy.random.RandomState(self._seed)
  File ""mtrand.pyx"", line 610, in mtrand.RandomState.__init__ (numpy/random/mtrand/mtrand.c:7397)
  File ""mtrand.pyx"", line 646, in mtrand.RandomState.seed (numpy/random/mtrand/mtrand.c:7697)
ValueError: Seed must be between 0 and 4294967295
{code}

In PySpark's {{RDDSamplerBase}} class from {{pyspark.rddsampler}} we use:

{code:python}
self._seed = seed if seed is not None else random.randint(0, sys.maxint)
{code}

In previous versions of NumPy a random seed larger than 2 ** 32 would silently get truncated to 2 ** 32. This was fixed in a recent patch (https://github.com/numpy/numpy/commit/6b1a1205eac6fe5d162f16155d500765e8bca53c). But sampling {{(0, sys.maxint)}} often yields ints larger than 2 ** 32, which effectively breaks sampling operations in PySpark (unless the seed is set manually).

I am putting a PR together now (the fix is very simple!).


",,andrewor14,apachespark,freeman-lab,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 27 05:11:03 UTC 2014,,,,,,,,,,"0|i21b9z:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"20/Oct/14 06:30;mengxr;[~freeman-lab] Thanks for catching the bug! Please create a PR and ping me.;;;","22/Oct/14 06:55;apachespark;User 'freeman-lab' has created a pull request for this issue:
https://github.com/apache/spark/pull/2889;;;","22/Oct/14 16:33;mengxr;Issue resolved by pull request 2889
[https://github.com/apache/spark/pull/2889];;;","26/Nov/14 03:02;andrewor14;Hey [~mengxr] do we want this in branch-1.1? If so we should re-open it.;;;","26/Nov/14 08:11;mengxr;I think we can backport SPARK-4477, which removes numpy from sampling.;;;","27/Nov/14 05:11;freeman-lab;Agree with [~mengxr], that's a better strategy.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
countByKey / countByValue do not go through Aggregator,SPARK-3994,12748989,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,17/Oct/14 21:50,21/Oct/14 20:16,14/Jul/23 06:26,21/Oct/14 20:16,1.0.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"The implementations of these methods are historical remnants of Spark from a time when the shuffle may have been nonexistent. Now, they can be simplified by plugging into reduceByKey(), potentially seeing performance and stability improvements.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 22:16:52 UTC 2014,,,,,,,,,,"0|i21b9r:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Oct/14 22:16;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/2839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python worker may hang after reused from take(),SPARK-3993,12748974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,17/Oct/14 21:05,06/Feb/15 10:23,14/Jul/23 06:26,24/Oct/14 00:21,,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"After take(), maybe there are some garbage left in the socket, then next task assigned to this worker will hang because of corrupted data.

We should make sure the socket is clean before reuse it.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5363,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 00:21:30 UTC 2014,,,,,,,,,,"0|i21b6n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Oct/14 21:08;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2838;;;","24/Oct/14 00:21;joshrosen;Issue resolved by pull request 2838
[https://github.com/apache/spark/pull/2838];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kryo.KryoException caused by ALS.trainImplicit in pyspark,SPARK-3990,12748895,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,gen,gen,17/Oct/14 14:17,07/Jul/15 23:39,14/Jul/23 06:26,24/Oct/14 07:01,1.1.0,,,,,,,1.1.1,1.2.0,,,,,MLlib,PySpark,,,0,test,,,,,"When we tried ALS.trainImplicit() in pyspark environment, it only works for iterations = 1. What is more strange, it is that if we try the same code in Scala, it works very well.(I did several test, by now, in Scala ALS.trainImplicit works)

For example, the following code:
{code:title=test.py|borderStyle=solid}
  from pyspark.mllib.recommendation import *
  r1 = (1, 1, 1.0) 
  r2 = (1, 2, 2.0) 
  r3 = (2, 1, 2.0) 
  ratings = sc.parallelize([r1, r2, r3]) 
  model = ALS.trainImplicit(ratings, 1) 
'''by default iterations = 5 or model = ALS.trainImplicit(ratings, 1, 2)'''
{code}


It will cause the failed stage at count at ALS.scala:314 Info as:
{code:title=error information provided by ganglia}
Job aborted due to stage failure: Task 6 in stage 90.0 failed 4 times, most recent failure: Lost task 6.3 in stage 90.0 (TID 484, ip-172-31-35-238.ec2.internal): com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
Serialization trace:
shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)
        com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
        com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:137)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
        scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
{code}
In the log of slave which failed the task, it has:

{code:title=error information in the log of slave}
14/10/17 13:20:54 ERROR executor.Executor: Exception in task 6.0 in stage 90.0 (TID 465)
com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
Serialization trace:
shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:137)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	... 36 more
{code}

","5 slaves cluster(m3.large) in AWS launched by spark-ec2
Linux
Python 2.6.8",apachespark,davies,gen,glenn.strycker@gmail.com,mengxr,tzhang101@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2652,SPARK-1977,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 23:39:11 UTC 2015,,,,,,,,,,"0|i21apj:",9223372036854775807,,,,,mengxr,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"17/Oct/14 14:38;gen;I think this problem is related to https://issues.apache.org/jira/browse/SPARK-1977;;;","17/Oct/14 22:58;mengxr;In PySpark 1.1, we switched to Kryo serialization by default. However, ALS code requires special registration with Kryo in order to work. The error happens when there is not enough memory and ALS needs to store ratings or in/out blocks to disk. I will work on this issue. For now, the workaround is to use a cluster with enough memory.;;;","18/Oct/14 08:36;gen;Thanks a lot for your reply. Right now, I use scala to realize my algorithm.
However, I don't think the problem is caused by the lack of memory, because my data has just 3 lines and 9 numbers and the slave node of cluster has 5 GB memory(I tried the same code with m3.xlarge this morning). For more details, the stage fails when certain slave tries to take the second task of count() but other slaves can finish the task. Hoping this information can help you (I don't know much about Scala.)

;;;","20/Oct/14 06:07;mengxr;[~gen] Could you try the following and see whether it solves the problem or not:

{code}
bin/pyspark --master local-cluster[2,1,512] --conf 'spark.kryo.registrator=org.apache.spark.examples.mllib.MovieLensALS$ALSRegistrator' --jars lib/spark-examples-1.1.0-hadoop2.4.0.jar
{code}

Please replace master and the filename of the example jar to match yours.;;;","20/Oct/14 10:27;gen;[~mengxr] I tried the code that you provided and it worked. Thanks a lot. 
In fact, I created a topic in the Apache Spark User List and I wanted to put your solution in this topic. It is OK for you?
;;;","20/Oct/14 16:41;mengxr;Sure, please link to this JIRA so we can keep the discussion here. This is a temp fix. We should move `ALSRegistrator` back to `ALS.scala`. So if people want to use it, they don't need the examples jar. Are you interested in fixing it?;;;","20/Oct/14 19:40;gen;Yeah, I will try. 
But I am afraid that I couldn't fix this problem efficiently, as I don't know much about programming.  
Thanks again for your help.
;;;","20/Oct/14 21:54;davies;The default serializer change was introduced by https://issues.apache.org/jira/browse/SPARK-2652?filter=-2, This assume that all the data used in pyspark are array[byte], but this is not correct for MLlib, so I would like to revert it.

cc [~matei];;;","20/Oct/14 22:59;mengxr;[~davies] Is there a JIRA that we can link?;;;","24/Oct/14 07:01;mengxr;This is fixed by reverting the default SerDe for PySpark in SPARK-2652.;;;","10/Nov/14 22:22;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3190;;;","11/Nov/14 06:41;mengxr;In Spark 1.1.1, I put a note on ALS and ask users to use the default serializer if they want to run ALS.;;;","07/Jul/15 23:39;tzhang101@yahoo.com;Hi, I am using spark streaming 1.3.0 and I use scala. I am hitting a similar issue.
As long as I have List in my class definition, this happens here is the stack trace.
Serialization trace:
underlying (scala.collection.convert.Wrappers$JListWrapper)
bandwidth_kbps (com.oncue.rna.realtime.streaming.models.QosErrorEvent)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:138)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.avro.generic.GenericData$Array.add(GenericData.java:200)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	... 42 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/07/07 22:42:06 INFO spark.SparkContext: Starting job: foreachRDD at QosErrorExtractorJob2.scala:595
15/07/07 22:42:06 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 17.0 (TID 81, ip-10-118-10-71.ec2.internal): org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NNLS generates incorrect result,SPARK-3987,12748796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coderxiang,debasish83,debasish83,17/Oct/14 04:28,16/May/15 11:55,14/Jul/23 06:26,16/May/15 11:55,1.1.0,,,,,,,1.1.1,1.2.0,,,,,MLlib,,,,0,,,,,,"Hi,

Please see the example gram matrix and linear term:

val P2 = new DoubleMatrix(20, 20, 333907.312770, -60814.043975, 207935.829941, -162881.367739, -43730.396770, 17511.428983, -243340.496449, -225245.957922, 104700.445881, 32430.845099, 336378.693135, -373497.970207, -41147.159621, 53928.060360, -293517.883778, 53105.278068, 0.000000, -85257.781696, 84913.970469, -10584.080103, -60814.043975, 13826.806664, -38032.612640, 33475.833875, 10791.916809, -1040.950810, 48106.552472, 45390.073380, -16310.282190, -2861.455903, -60790.833191, 73109.516544, 9826.614644, -8283.992464, 56991.742991, -6171.366034, 0.000000, 19152.382499, -13218.721710, 2793.734234, 207935.829941, -38032.612640, 129661.677608, -101682.098412, -27401.299347, 10787.713362, -151803.006149, -140563.601672, 65067.935324, 20031.263383, 209521.268600, -232958.054688, -25764.179034, 33507.951918, -183046.845592, 32884.782835, 0.000000, -53315.811196, 52770.762546, -6642.187643, -162881.367739, 33475.833875, -101682.098412, 85094.407608, 25422.850782, -5437.646141, 124197.166330, 116206.265909, -47093.484134, -11420.168521, -163429.436848, 189574.783900, 23447.172314, -24087.375367, 148311.355507, -20848.385466, 0.000000, 46835.814559, -38180.352878, 6415.873901, -43730.396770, 10791.916809, -27401.299347, 25422.850782, 8882.869799, 15.638084, 35933.473986, 34186.371325, -10745.330690, -974.314375, -43537.709621, 54371.010558, 7894.453004, -5408.929644, 42231.381747, -3192.010574, 0.000000, 15058.753110, -8704.757256, 2316.581535, 17511.428983, -1040.950810, 10787.713362, -5437.646141, 15.638084, 2794.949847, -9681.950987, -8258.171646, 7754.358930, 4193.359412, 18052.143842, -15456.096769, -253.356253, 4089.672804, -12524.380088, 5651.579348, 0.000000, -1513.302547, 6296.461898, 152.427321, -243340.496449, 48106.552472, -151803.006149, 124197.166330, 35933.473986, -9681.950987, 182931.600236, 170454.352953, -72361.174145, -19270.461728, -244518.179729, 279551.060579, 33340.452802, -37103.267653, 219025.288975, -33687.141423, 0.000000, 67347.950443, -58673.009647, 8957.800259, -225245.957922, 45390.073380, -140563.601672, 116206.265909, 34186.371325, -8258.171646, 170454.352953, 159322.942894, -66074.960534, -16839.743193, -226173.967766, 260421.044094, 31624.194003, -33839.612565, 203889.695169, -30034.828909, 0.000000, 63525.040745, -53572.741748, 8575.071847, 104700.445881, -16310.282190, 65067.935324, -47093.484134, -10745.330690, 7754.358930, -72361.174145, -66074.960534, 35869.598076, 13378.653317, 106033.647837, -111831.682883, -10455.465743, 18537.392481, -88370.612394, 20344.288488, 0.000000, -22935.482766, 29004.543704, -2409.461759, 32430.845099, -2861.455903, 20031.263383, -11420.168521, -974.314375, 4193.359412, -19270.461728, -16839.743193, 13378.653317, 6802.081898, 33256.395091, -30421.985199, -1296.785870, 7026.518692, -24443.378205, 9221.982599, 0.000000, -4088.076871, 10861.014242, -25.092938, 336378.693135, -60790.833191, 209521.268600, -163429.436848, -43537.709621, 18052.143842, -244518.179729, -226173.967766, 106033.647837, 33256.395091, 339200.268106, -375442.716811, -41027.594509, 54636.778527, -295133.248586, 54177.278365, 0.000000, -85237.666701, 85996.957056, -10503.209968, -373497.970207, 73109.516544, -232958.054688, 189574.783900, 54371.010558, -15456.096769, 279551.060579, 260421.044094, -111831.682883, -30421.985199, -375442.716811, 427793.208465, 50528.074431, -57375.986301, 335203.382015, -52676.385869, 0.000000, 102368.307670, -90679.792485, 13509.390393, -41147.159621, 9826.614644, -25764.179034, 23447.172314, 7894.453004, -253.356253, 33340.452802, 31624.194003, -10455.465743, -1296.785870, -41027.594509, 50528.074431, 7255.977434, -5281.636812, 39298.355527, -3440.450858, 0.000000, 13717.870243, -8471.405582, 2071.812204, 53928.060360, -8283.992464, 33507.951918, -24087.375367, -5408.929644, 4089.672804, -37103.267653, -33839.612565, 18537.392481, 7026.518692, 54636.778527, -57375.986301, -5281.636812, 9735.061160, -45360.674033, 10634.633559, 0.000000, -11652.364691, 15039.566630, -1202.539106, -293517.883778, 56991.742991, -183046.845592, 148311.355507, 42231.381747, -12524.380088, 219025.288975, 203889.695169, -88370.612394, -24443.378205, -295133.248586, 335203.382015, 39298.355527, -45360.674033, 262923.925938, -42012.606885, 0.000000, 79810.919951, -71657.856143, 10464.327491, 53105.278068, -6171.366034, 32884.782835, -20848.385466, -3192.010574, 5651.579348, -33687.141423, -30034.828909, 20344.288488, 9221.982599, 54177.278365, -52676.385869, -3440.450858, 10634.633559, -42012.606885, 13238.686902, 0.000000, -8739.845698, 16511.872845, -530.252003, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 118.430000, 0.000000, 0.000000, 0.000000, -85257.781696, 19152.382499, -53315.811196, 46835.814559, 15058.753110, -1513.302547, 67347.950443, 63525.040745, -22935.482766, -4088.076871, -85237.666701, 102368.307670, 13717.870243, -11652.364691, 79810.919951, -8739.845698, 0.000000, 26878.133950, -18588.407734, 3894.934299, 84913.970469, -13218.721710, 52770.762546, -38180.352878, -8704.757256, 6296.461898, -58673.009647, -53572.741748, 29004.543704, 10861.014242, 85996.957056, -90679.792485, -8471.405582, 15039.566630, -71657.856143, 16511.872845, 0.000000, -18588.407734, 23649.538538, -1951.083671, -10584.080103, 2793.734234, -6642.187643, 6415.873901, 2316.581535, 152.427321, 8957.800259, 8575.071847, -2409.461759, -25.092938, -10503.209968, 13509.390393, 2071.812204, -1202.539106, 10464.327491, -530.252003, 0.000000, 3894.934299, -1951.083671, 738.955915)
    val q2 = new DoubleMatrix(20, 1, 31755.057100, -13047.148129, 20191.244430, -25993.775800, -11963.550172, -4272.425977, -33569.856044, -33451.387021, 2320.764250, -5333.136834, 30633.764272, -49513.939049, -10351.230305, 872.276714, -37634.078430, -4628.338543, -0.000000, -18109.093788, 1856.725521, -3397.693211)

NNLS result:

0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0

PR result: https://github.com/apache/spark/pull/2705

QuadraticMinimizer result [0.130104; 0.126840; 0.072565; 0.154153; 0.144728; 0.129937; 0.121166; 0.161450; 0.199197; 0.187168; 0.159467; 0.144269; 0.117404; 0.109298; 0.086901; 0.221391; 0.000000; 0.174044; 0.162080; 0.045439]

Octave result:
octave:7> qp(x0, H, f, [], [], lb, ub)
ans =
   0.13010
   0.12684
   0.07256
   0.15415
   0.14473
   0.12994
   0.12117
   0.16145
   0.19920
   0.18717
   0.15947
   0.14427
   0.11740
   0.10930
   0.08690
   0.22139
   0.00000
   0.17404
   0.16208
   0.04544",,apachespark,coderxiang,debasish83,donnchadh,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 11:55:53 UTC 2015,,,,,,,,,,"0|i21a3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/14 05:05;coderxiang;[~debasish83] By using a finer config (change 1e-6 on line 82 of NNLS.scala to 1e-7), NNLS will produce the expected result. ;;;","22/Oct/14 15:07;debasish83;I will test it but this is how I called NNLS...assuming P2 and q2 are jblas matrices as mentioned up...

val nnlsResult2 = NNLS.solve(P2, q2.mul(-1), ws)
println(s""NNLS iters ${ws.iterations} result ${nnlsResult2.toList.mkString("","")}"")

val (posResult2, posConverged2) = qpIters.solve(P2, q2)
println(s""QuadraticMinimizer iters ${qpIters.iterations} result ${posResult2.toString()}"")

I did multiply q2 with -1 before it goes to NNLS...

Is that the right way to call NNLS ?;;;","22/Oct/14 15:44;coderxiang;[~debasish83] I think you are correct. Could you please help test whether changing the stopping criterion works on your machine/data?;;;","22/Oct/14 18:49;debasish83;[~coderxiang] changing to 1e-6 to 1e-7 fixes this error but why bound step at 1e-6 or 1e-7...why not let the algorithm converge using the standard techniques used in BFGS / OWLQN (gradient norm and last 5 iterates)...you can also extend Breeze linear CG to implement NNLS...

NNLS iters 36 result 0.13010360222362655,0.1268399356245685,0.07256472682635416,0.15415258739697485,0.14472814692821925,0.12993720335014108,0.12116579552952525,0.16145040854270917,0.19919730253363563,0.18716812848138634,0.1594670311402431,0.1442692338314524,0.11740410727778867,0.10929848737016828,0.08690057753031168,0.22139114605899224,0.0,0.17404384335673376,0.16208039794069887,0.04543896291399707

QuadraticMinimizer iters 259 result [0.130104; 0.126840; 0.072565; 0.154153; 0.144728; 0.129937; 0.121166; 0.161450; 0.199197; 0.187168; 0.159467; 0.144269; 0.117404; 0.109298; 0.086901; 0.221391; 0.000000; 0.174044; 0.162080; 0.045439]

I will run more tests...Basically based on this PR I am testing https://issues.apache.org/jira/browse/SPARK-2426 userFeatures as SMOOTH and productFeatures as POSITIVE...and this particular case showed up there...For POSITIVE I use both NNLS and QuadraticMinimizer to see the iterations and optimize on that...;;;","27/Oct/14 16:59;coderxiang;[~debasish83] IMHO, the current implementation of NNLS also alopts various terminating criteria, including small gradient and small step-size. I agree that general QP solver will definitely solve NNLS problem, it might be the case that a special solver may be advantageous in numerical consideration. For example, if you consider this case:

    val n = 5
    val ata = new DoubleMatrix(5, 5
      , 517399.13534, 242529.67289, -153644.98976, 130802.84503, -798452.29283
      , 242529.67289, 126017.69765, -75944.21743, 81785.36128, -405290.60884
      , -153644.98976, -75944.21743, 46986.44577, -45401.12659, 247059.51049
      , 130802.84503, 81785.36128, -45401.12659, 67457.31310, -253747.03819
      , -798452.29283, -405290.60884, 247059.51049, -253747.03819, 1310939.40814
    )
    val atb = new DoubleMatrix(5, 1,
      -31755.05710, 13047.14813, -20191.24443, 25993.77580, 11963.55017)

run MLlib/nnls, Matlab/quadprog and Matlab/lsqnonneg and use objective value to evaluate the algorithm, I got:
 * MLlib/nnls with step = 1e-6: *-18962.751251160757*
 * MLlib/nnls with step = 1e-7:  *-845478.0828914623*
 * Matlab/quadprog with all default settings: *-781762.406480181*
 * Matlab/quadprog with more iterations: *-845372.586681944*
 * Matlab/lsqnonneg with all default settings: *-845478.058414435*
;;;","27/Oct/14 21:31;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2965;;;","28/Oct/14 02:44;mengxr;Issue resolved by pull request 2965
[https://github.com/apache/spark/pull/2965];;;","31/Oct/14 03:07;debasish83;I can send you a further list of failures...this is one more example...I strongly suggest moving to a robust convergence criteria inside NNLS than adding hacks on step sizes...

P = [0.986619, 0.639909, 0.748906, 0.900377, 0.688079, 0.734711, 0.835164, 0.723973, 0.822436, 0.852591, 0.699979, 0.609533, 0.559504, 0.708015, 0.544744, 0.658359, 0.632510, 0.751316, 0.653993, 0.642734, 0.799106, 0.898689, 0.712825, 0.878405, 0.849565; 0.639909, 1.055175, 0.884940, 0.975502, 0.815121, 0.845699, 0.899780, 0.709264, 0.960949, 1.021108, 0.896508, 0.692635, 0.659746, 0.809355, 0.539466, 0.730501, 0.639971, 0.881502, 0.840159, 0.628515, 0.917052, 0.950677, 0.823301, 1.022355, 0.994935; 0.748906, 0.884940, 1.421868, 1.175203, 0.986093, 1.028669, 1.091437, 0.849192, 1.204776, 1.249037, 1.115160, 0.815680, 0.772715, 0.971263, 0.621197, 0.875925, 0.757031, 1.034483, 1.022001, 0.705077, 1.115237, 1.164796, 0.983688, 1.226847, 1.180170; 0.900377, 0.975502, 1.175203, 1.676666, 1.065369, 1.109756, 1.181625, 0.944176, 1.220418, 1.328803, 1.156144, 0.984967, 0.916500, 1.046903, 0.728221, 0.991042, 0.855095, 1.181719, 1.095485, 0.901193, 1.214851, 1.277434, 1.077374, 1.372354, 1.356724; 0.688079, 0.815121, 0.986093, 1.065369, 1.249908, 0.953824, 1.004765, 0.771115, 1.082255, 1.161322, 1.021400, 0.757474, 0.736266, 0.923406, 0.598005, 0.812629, 0.706870, 1.011984, 0.968135, 0.682813, 1.034818, 1.039625, 0.937088, 1.152792, 1.121475; 0.734711, 0.845699, 1.028669, 1.109756, 0.953824, 1.334869, 1.091564, 0.824709, 1.157992, 1.226413, 1.045522, 0.731169, 0.709382, 0.980030, 0.634635, 0.853988, 0.758256, 1.070744, 0.997542, 0.692832, 1.118828, 1.119519, 0.977618, 1.202464, 1.152186; 0.835164, 0.899780, 1.091437, 1.181625, 1.004765, 1.091564, 1.566492, 0.994066, 1.307932, 1.301575, 1.075206, 0.697477, 0.670553, 1.068449, 0.725819, 0.908391, 0.877151, 1.083915, 0.993647, 0.735423, 1.202928, 1.275592, 1.040266, 1.242863, 1.150213; 0.723973, 0.709264, 0.849192, 0.944176, 0.771115, 0.824709, 0.994066, 1.291185, 1.084907, 0.934737, 0.802436, 0.559117, 0.510659, 0.801938, 0.624240, 0.682461, 0.761152, 0.633965, 0.633248, 0.622034, 0.826798, 1.084260, 0.777897, 0.868127, 0.764337; 0.822436, 0.960949, 1.204776, 1.220418, 1.082255, 1.157992, 1.307932, 1.084907, 1.821667, 1.386725, 1.220398, 0.710929, 0.683088, 1.116057, 0.719380, 0.925482, 0.907590, 1.010205, 1.041999, 0.673723, 1.224967, 1.373886, 1.082813, 1.261720, 1.130003; 0.852591, 1.021108, 1.249037, 1.328803, 1.161322, 1.226413, 1.301575, 0.934737, 1.386725, 1.831419, 1.289097, 0.885418, 0.868685, 1.190024, 0.740180, 1.034238, 0.884778, 1.342867, 1.252762, 0.811797, 1.374303, 1.319088, 1.190385, 1.478896, 1.427420; 0.699979, 0.896508, 1.115160, 1.156144, 1.021400, 1.045522, 1.075206, 0.802436, 1.220398, 1.289097, 1.501713, 0.831799, 0.807718, 0.994071, 0.599941, 0.876646, 0.730630, 1.074102, 1.088886, 0.677923, 1.126406, 1.127440, 1.013104, 1.257194, 1.217591; 0.609533, 0.692635, 0.815680, 0.984967, 0.757474, 0.731169, 0.697477, 0.559117, 0.710929, 0.885418, 0.831799, 1.206659, 0.820404, 0.673313, 0.477307, 0.698907, 0.529141, 0.853290, 0.812957, 0.721698, 0.775190, 0.790003, 0.746242, 0.994816, 1.053857; 0.559504, 0.659746, 0.772715, 0.916500, 0.736266, 0.709382, 0.670553, 0.510659, 0.683088, 0.868685, 0.807718, 0.820404, 1.103586, 0.666515, 0.455429, 0.667077, 0.500250, 0.855895, 0.807716, 0.674035, 0.759131, 0.732206, 0.732658, 0.965557, 1.021876; 0.708015, 0.809355, 0.971263, 1.046903, 0.923406, 0.980030, 1.068449, 0.801938, 1.116057, 1.190024, 0.994071, 0.673313, 0.666515, 1.293629, 0.631919, 0.822011, 0.745899, 1.060809, 0.965440, 0.672388, 1.090059, 1.069719, 0.959136, 1.162012, 1.109763; 0.544744, 0.539466, 0.621197, 0.728221, 0.598005, 0.634635, 0.725819, 0.624240, 0.719380, 0.740180, 0.599941, 0.477307, 0.455429, 0.631919, 0.802572, 0.551340, 0.549393, 0.654652, 0.567237, 0.531461, 0.684655, 0.749979, 0.627718, 0.744096, 0.710310; 0.658359, 0.730501, 0.875925, 0.991042, 0.812629, 0.853988, 0.908391, 0.682461, 0.925482, 1.034238, 0.876646, 0.698907, 0.667077, 0.822011, 0.551340, 1.075722, 0.642962, 0.955470, 0.862026, 0.658120, 0.955639, 0.944118, 0.835790, 1.057347, 1.042521; 0.632510, 0.639971, 0.757031, 0.855095, 0.706870, 0.758256, 0.877151, 0.761152, 0.907590, 0.884778, 0.730630, 0.529141, 0.500250, 0.745899, 0.549393, 0.642962, 0.976258, 0.726041, 0.658693, 0.579842, 0.811285, 0.917944, 0.731996, 0.859613, 0.798469; 0.751316, 0.881502, 1.034483, 1.181719, 1.011984, 1.070744, 1.083915, 0.633965, 1.010205, 1.342867, 1.074102, 0.853290, 0.855895, 1.060809, 0.654652, 0.955470, 0.726041, 1.779551, 1.210339, 0.817870, 1.289494, 1.030990, 1.082950, 1.419900, 1.451896; 0.653993, 0.840159, 1.022001, 1.095485, 0.968135, 0.997542, 0.993647, 0.633248, 1.041999, 1.252762, 1.088886, 0.812957, 0.807716, 0.965440, 0.567237, 0.862026, 0.658693, 1.210339, 1.442263, 0.688333, 1.139661, 0.988895, 0.991869, 1.276105, 1.281761; 0.642734, 0.628515, 0.705077, 0.901193, 0.682813, 0.692832, 0.735423, 0.622034, 0.673723, 0.811797, 0.677923, 0.721698, 0.674035, 0.672388, 0.531461, 0.658120, 0.579842, 0.817870, 0.688333, 1.044625, 0.753991, 0.788593, 0.710741, 0.906241, 0.935628; 0.799106, 0.917052, 1.115237, 1.214851, 1.034818, 1.118828, 1.202928, 0.826798, 1.224967, 1.374303, 1.126406, 0.775190, 0.759131, 1.090059, 0.684655, 0.955639, 0.811285, 1.289494, 1.139661, 0.753991, 1.621034, 1.201765, 1.081841, 1.365902, 1.323655; 0.898689, 0.950677, 1.164796, 1.277434, 1.039625, 1.119519, 1.275592, 1.084260, 1.373886, 1.319088, 1.127440, 0.790003, 0.732206, 1.069719, 0.749979, 0.944118, 0.917944, 1.030990, 0.988895, 0.788593, 1.201765, 1.692948, 1.052744, 1.265542, 1.169388; 0.712825, 0.823301, 0.983688, 1.077374, 0.937088, 0.977618, 1.040266, 0.777897, 1.082813, 1.190385, 1.013104, 0.746242, 0.732658, 0.959136, 0.627718, 0.835790, 0.731996, 1.082950, 0.991869, 0.710741, 1.081841, 1.052744, 1.291439, 1.189060, 1.159221; 0.878405, 1.022355, 1.226847, 1.372354, 1.152792, 1.202464, 1.242863, 0.868127, 1.261720, 1.478896, 1.257194, 0.994816, 0.965557, 1.162012, 0.744096, 1.057347, 0.859613, 1.419900, 1.276105, 0.906241, 1.365902, 1.265542, 1.189060, 1.847657, 1.521522; 0.849565, 0.994935, 1.180170, 1.356724, 1.121475, 1.152186, 1.150213, 0.764337, 1.130003, 1.427420, 1.217591, 1.053857, 1.021876, 1.109763, 0.710310, 1.042521, 0.798469, 1.451896, 1.281761, 0.935628, 1.323655, 1.169388, 1.159221, 1.521522, 1.887527];

q = [-3.640583; -5.563638; -7.040787; -7.387618; -6.410455; -6.452543; -5.698284; -2.604581; -6.184568; -8.450254; -7.746985; -6.173699; -6.072464; -5.954310; -2.867163; -5.751653; -3.362224; -8.631145; -8.238493; -4.198840; -7.653769; -5.764195; -6.387377; -8.917469; -9.348541];

mosekx =

    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    1.0951
    0.5516
    0.6727
    0.0000
    0.0000
    0.0000
    0.0000
    1.0555
    2.0667
    0.0000
    0.0000
    0.0000
    0.0000
    0.2235
    1.1788

QuadraticMinimizer https://github.com/apache/spark/pull/2705

x =

         0
         0
         0
         0
         0
         0
         0
         0
         0
         0
    1.0951
    0.5516
    0.6727
         0
         0
         0
         0
    1.0555
    2.0666
         0
         0
         0
         0
    0.2235
    1.1787

I added (step < 1e-7) // too small or negative;;;","31/Oct/14 03:10;debasish83;NNLS iters 36 result 0.13010360222362655,0.1268399356245685,0.07256472682635416,0.15415258739697485,0.14472814692821925,0.12993720335014108,0.12116579552952525,0.16145040854270917,0.19919730253363563,0.18716812848138634,0.1594670311402431,0.1442692338314524,0.11740410727778867,0.10929848737016828,0.08690057753031168,0.22139114605899224,0.0,0.17404384335673376,0.16208039794069887,0.04543896291399707;;;","31/Oct/14 03:31;coderxiang;[~debasish83] Thanks for the follow up. Wondering can you provide your full test script, as I cannot replicate your result in my following tests:

{code:title=NNLSuite.scala|borderStyle=solid}
  test(""NNLS: extra test"") {
    val n = 25
    val ata = new DoubleMatrix(25, 25,
      0.986619, 0.639909, 0.748906, 0.900377, 0.688079, 0.734711, 0.835164, 0.723973, 0.822436, 0.852591,
      0.699979, 0.609533, 0.559504, 0.708015, 0.544744, 0.658359, 0.632510, 0.751316, 0.653993, 0.642734, 0.799106, 0.898689, 0.712825, 0.878405, 0.849565, 0.639909, 1.055175, 0.884940, 0.975502, 0.815121, 0.845699, 0.899780, 0.709264, 0.960949, 1.021108, 0.896508, 0.692635, 0.659746, 0.809355, 0.539466, 0.730501, 0.639971, 0.881502, 0.840159, 0.628515, 0.917052, 0.950677, 0.823301, 1.022355, 0.994935, 0.748906, 0.884940, 1.421868, 1.175203, 0.986093, 1.028669, 1.091437, 0.849192, 1.204776, 1.249037, 1.115160, 0.815680, 0.772715, 0.971263, 0.621197, 0.875925, 0.757031, 1.034483, 1.022001, 0.705077, 1.115237, 1.164796, 0.983688, 1.226847, 1.180170, 0.900377, 0.975502, 1.175203, 1.676666, 1.065369, 1.109756, 1.181625, 0.944176, 1.220418, 1.328803, 1.156144, 0.984967, 0.916500, 1.046903, 0.728221, 0.991042, 0.855095, 1.181719, 1.095485, 0.901193, 1.214851, 1.277434, 1.077374, 1.372354, 1.356724, 0.688079, 0.815121, 0.986093, 1.065369, 1.249908, 0.953824, 1.004765, 0.771115, 1.082255, 1.161322, 1.021400, 0.757474, 0.736266, 0.923406, 0.598005, 0.812629, 0.706870, 1.011984, 0.968135, 0.682813, 1.034818, 1.039625, 0.937088, 1.152792, 1.121475, 0.734711, 0.845699, 1.028669, 1.109756, 0.953824, 1.334869, 1.091564, 0.824709, 1.157992, 1.226413, 1.045522, 0.731169, 0.709382, 0.980030, 0.634635, 0.853988, 0.758256, 1.070744, 0.997542, 0.692832, 1.118828, 1.119519, 0.977618, 1.202464, 1.152186, 0.835164, 0.899780, 1.091437, 1.181625, 1.004765, 1.091564, 1.566492, 0.994066, 1.307932, 1.301575, 1.075206, 0.697477, 0.670553, 1.068449, 0.725819, 0.908391, 0.877151, 1.083915, 0.993647, 0.735423, 1.202928, 1.275592, 1.040266, 1.242863, 1.150213, 0.723973, 0.709264, 0.849192, 0.944176, 0.771115, 0.824709, 0.994066, 1.291185, 1.084907, 0.934737, 0.802436, 0.559117, 0.510659, 0.801938, 0.624240, 0.682461, 0.761152, 0.633965, 0.633248, 0.622034, 0.826798, 1.084260, 0.777897, 0.868127, 0.764337, 0.822436, 0.960949, 1.204776, 1.220418, 1.082255, 1.157992, 1.307932, 1.084907, 1.821667, 1.386725, 1.220398, 0.710929, 0.683088, 1.116057, 0.719380, 0.925482, 0.907590, 1.010205, 1.041999, 0.673723, 1.224967, 1.373886, 1.082813, 1.261720, 1.130003, 0.852591, 1.021108, 1.249037, 1.328803, 1.161322, 1.226413, 1.301575, 0.934737, 1.386725, 1.831419, 1.289097, 0.885418, 0.868685, 1.190024, 0.740180, 1.034238, 0.884778, 1.342867, 1.252762, 0.811797, 1.374303, 1.319088, 1.190385, 1.478896, 1.427420, 0.699979, 0.896508, 1.115160, 1.156144, 1.021400, 1.045522, 1.075206, 0.802436, 1.220398, 1.289097, 1.501713, 0.831799, 0.807718, 0.994071, 0.599941, 0.876646, 0.730630, 1.074102, 1.088886, 0.677923, 1.126406, 1.127440, 1.013104, 1.257194, 1.217591, 0.609533, 0.692635, 0.815680, 0.984967, 0.757474, 0.731169, 0.697477, 0.559117, 0.710929, 0.885418, 0.831799, 1.206659, 0.820404, 0.673313, 0.477307, 0.698907, 0.529141, 0.853290, 0.812957, 0.721698, 0.775190, 0.790003, 0.746242, 0.994816, 1.053857, 0.559504, 0.659746, 0.772715, 0.916500, 0.736266, 0.709382, 0.670553, 0.510659, 0.683088, 0.868685, 0.807718, 0.820404, 1.103586, 0.666515, 0.455429, 0.667077, 0.500250, 0.855895, 0.807716, 0.674035, 0.759131, 0.732206, 0.732658, 0.965557, 1.021876, 0.708015, 0.809355, 0.971263, 1.046903, 0.923406, 0.980030, 1.068449, 0.801938, 1.116057, 1.190024, 0.994071, 0.673313, 0.666515, 1.293629, 0.631919, 0.822011, 0.745899, 1.060809, 0.965440, 0.672388, 1.090059, 1.069719, 0.959136, 1.162012, 1.109763, 0.544744, 0.539466, 0.621197, 0.728221, 0.598005, 0.634635, 0.725819, 0.624240, 0.719380, 0.740180, 0.599941, 0.477307, 0.455429, 0.631919, 0.802572, 0.551340, 0.549393, 0.654652, 0.567237, 0.531461, 0.684655, 0.749979, 0.627718, 0.744096, 0.710310, 0.658359, 0.730501, 0.875925, 0.991042, 0.812629, 0.853988, 0.908391, 0.682461, 0.925482, 1.034238, 0.876646, 0.698907, 0.667077, 0.822011, 0.551340, 1.075722, 0.642962, 0.955470, 0.862026, 0.658120, 0.955639, 0.944118, 0.835790, 1.057347, 1.042521, 0.632510, 0.639971, 0.757031, 0.855095, 0.706870, 0.758256, 0.877151, 0.761152, 0.907590, 0.884778, 0.730630, 0.529141, 0.500250, 0.745899, 0.549393, 0.642962, 0.976258, 0.726041, 0.658693, 0.579842, 0.811285, 0.917944, 0.731996, 0.859613, 0.798469, 0.751316, 0.881502, 1.034483, 1.181719, 1.011984, 1.070744, 1.083915, 0.633965, 1.010205, 1.342867, 1.074102, 0.853290, 0.855895, 1.060809, 0.654652, 0.955470, 0.726041, 1.779551, 1.210339, 0.817870, 1.289494, 1.030990, 1.082950, 1.419900, 1.451896, 0.653993, 0.840159, 1.022001, 1.095485, 0.968135, 0.997542, 0.993647, 0.633248, 1.041999, 1.252762, 1.088886, 0.812957, 0.807716, 0.965440, 0.567237, 0.862026, 0.658693, 1.210339, 1.442263, 0.688333, 1.139661, 0.988895, 0.991869, 1.276105, 1.281761, 0.642734, 0.628515, 0.705077, 0.901193, 0.682813, 0.692832, 0.735423, 0.622034, 0.673723, 0.811797, 0.677923, 0.721698, 0.674035, 0.672388, 0.531461, 0.658120, 0.579842, 0.817870, 0.688333, 1.044625, 0.753991, 0.788593, 0.710741, 0.906241, 0.935628, 0.799106, 0.917052, 1.115237, 1.214851, 1.034818, 1.118828, 1.202928, 0.826798, 1.224967, 1.374303, 1.126406, 0.775190, 0.759131, 1.090059, 0.684655, 0.955639, 0.811285, 1.289494, 1.139661, 0.753991, 1.621034, 1.201765, 1.081841, 1.365902, 1.323655, 0.898689, 0.950677, 1.164796, 1.277434, 1.039625, 1.119519, 1.275592, 1.084260, 1.373886, 1.319088, 1.127440, 0.790003, 0.732206, 1.069719, 0.749979, 0.944118, 0.917944, 1.030990, 0.988895, 0.788593, 1.201765, 1.692948, 1.052744, 1.265542, 1.169388, 0.712825, 0.823301, 0.983688, 1.077374, 0.937088, 0.977618, 1.040266, 0.777897, 1.082813, 1.190385, 1.013104, 0.746242, 0.732658, 0.959136, 0.627718, 0.835790, 0.731996, 1.082950, 0.991869, 0.710741, 1.081841, 1.052744, 1.291439, 1.189060, 1.159221, 0.878405, 1.022355, 1.226847, 1.372354, 1.152792, 1.202464, 1.242863, 0.868127, 1.261720, 1.478896, 1.257194, 0.994816, 0.965557, 1.162012, 0.744096, 1.057347, 0.859613, 1.419900, 1.276105, 0.906241, 1.365902, 1.265542, 1.189060, 1.847657, 1.521522, 0.849565, 0.994935, 1.180170, 1.356724, 1.121475, 1.152186, 1.150213, 0.764337, 1.130003, 1.427420, 1.217591, 1.053857, 1.021876, 1.109763, 0.710310, 1.042521, 0.798469, 1.451896, 1.281761, 0.935628, 1.323655, 1.169388, 1.159221, 1.521522, 1.887527)

    val atb = new DoubleMatrix(25, 1, -3.640583, -5.563638, -7.040787, -7.387618, -6.410455, -6.452543, -5.698284, -2.604581, -6.184568, -8.450254, -7.746985, -6.173699, -6.072464, -5.954310, -2.867163, -5.751653, -3.362224, -8.631145, -8.238493, -4.198840, -7.653769, -5.764195, -6.387377, -8.917469, -9.348541)

    val refx = new DoubleMatrix(Array(0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0951, 0.5516, 0.6727, 0.0000, 0.0000,
    0.0000, 0.0000, 1.0555, 2.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 1.1788))
    val refObj = computeObjectiveValue(ata, atb.mul(-1.0), refx)

    val ws = NNLS.createWorkspace(n)
    val ans = NNLS.solve(ata, atb.mul(-1.0), ws)
    val x = new DoubleMatrix(ans)
    val obj = computeObjectiveValue(ata, atb.mul(-1.0), x)

    println((refObj, obj))
    println(ans.mkString("" ""))
    assert(obj < refObj + 1E-5)
  }
{code}

The test is OK with output:

[info] NNLSSuite:
[info] - NNLS: exact solution cases (145 milliseconds)
[info] - NNLS: nonnegativity constraint active (3 milliseconds)
[info] - NNLS: objective value test (0 milliseconds)
(-27.561557962647033,-27.561557968417716)
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0950760986499062 0.5515642980855557 0.6727115532538089 0.0 0.0 0.0 0.0 1.0555025979837525 2.066667142460158 0.0 0.0 0.0 0.0 0.2235312185469203 1.178752675112592
[info] - NNLS: extra test (4 milliseconds)
;;;","31/Oct/14 03:35;debasish83;Was there more changes that step size in your checkin ? I still have not updated my branch...changed the step size and ran...Let me update the branch and re-run full tests...;;;","31/Oct/14 04:59;mengxr;[~debasish83] For the sample data, did you check the condition number? The NNLS is to solve ALS sub-problems, where we assume that they are well-conditioned.;;;","31/Oct/14 05:27;debasish83;[~mengxr] this came out of an internal dataset while running ALS...I can't point to the dataset...I might not have got all the changes so I am updating my branch and re-run this validation over the dataset...Basically I check cases where NNLS and QuadraticMinimization don't match and dump them out for further analysis...;;;","31/Oct/14 06:24;mengxr;Please check the condition number of the matrix you sent. Did you run ALS with a very small lambda?;;;","31/Oct/14 06:54;debasish83;Nope...standard ALS...same as netflix params...0.065 as L2...My ratings are
not within 1-5 but more like 1-10...

Also what's a good condition number for NNLS ?

On Thu, Oct 30, 2014 at 11:25 PM, Xiangrui Meng (JIRA) <jira@apache.org>

;;;","31/Oct/14 17:59;coderxiang;[~debasish83][~mengxr] The condition number for the latest test case is 74.5 and  the test case I put in my PR was 20000.;;;","05/Apr/15 22:52;mengxr;[~coderxiang] [~debasish83] What is the status of this issue? Is the NNLS solver implemented in MLlib correct (or enough accurate)?;;;","06/Apr/15 00:42;coderxiang;[~mengxr] I think it was an accuracy issue and it should be done now.;;;","08/Apr/15 03:19;debasish83;[~mengxr] for this testcase it was fixed but I remember there was someone in user list who mentioned that he got incorrect result compared to some other tool...may be it's a good idea to ask for testcases...;;;","08/Apr/15 23:28;coderxiang;[~debasish83] could you point me to those test cases?;;;","16/May/15 11:55;srowen;From the discussion it sounds like the issue that this JIRA concerns was actually OK.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix package names to fit their directory names.,SPARK-3986,12748791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,17/Oct/14 03:42,20/Oct/14 18:34,14/Jul/23 06:26,20/Oct/14 18:33,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Package names of 2 test suites are different from their directory names.

- {{GeneratedEvaluationSuite}}
- {{GeneratedMutableEvaluationSuite}}",,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 18:33:42 UTC 2014,,,,,,,,,,"0|i21a2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/14 03:46;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2835;;;","20/Oct/14 18:33;marmbrus;Issue resolved by pull request 2835
[https://github.com/apache/spark/pull/2835];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json file path is not right,SPARK-3985,12748790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adrian-wang,adrian-wang,adrian-wang,17/Oct/14 02:59,21/Oct/14 05:58,14/Jul/23 06:26,17/Oct/14 21:50,1.2.0,,,,,,,1.2.0,,,,,,Examples,SQL,,,0,,,,,,"in examples/src/main/python/sql.py, we just add SPARK_HOME and ""examples/..."" together instead of using ""os.path.join"", would cause a problem.",,adrian-wang,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 21:50:00 UTC 2014,,,,,,,,,,"0|i21a27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/14 03:02;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2834;;;","17/Oct/14 21:50;joshrosen;Issue resolved by pull request 2834
[https://github.com/apache/spark/pull/2834];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler delay (shown in the UI) is incorrect,SPARK-3983,12748763,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,16/Oct/14 23:25,06/Nov/14 07:56,14/Jul/23 06:26,06/Nov/14 07:56,,,,,,,,1.2.0,,,,,,Web UI,,,,0,,,,,,"The reported scheduler delay includes time to get a new thread (from a threadpool) in order to start the task, time to deserialize the task, and time to serialize the result.  None of these things are delay caused by the scheduler; including them as such is misleading.

This is especially problematic when debugging performance of short tasks (that run in 10s of milliseconds), when the scheduler delay can be very large relative to the task duration.

cc [~sparks] [~shivaram]",,apachespark,codingcat,kayousterhout,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 01:11:41 UTC 2014,,,,,,,,,,"0|i219wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/14 01:11;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/2832;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn backend's default file replication should match HDFS's default one,SPARK-3979,12748694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,16/Oct/14 19:59,17/Oct/14 18:45,14/Jul/23 06:26,17/Oct/14 18:45,,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"This code in ClientBase.scala sets the replication used for files uploaded to HDFS:

{code}
    val replication = sparkConf.getInt(""spark.yarn.submit.file.replication"", 3).toShort
{code}

Instead of a hardcoded ""3"" (which is the default value for HDFS), it should be using the default value from the HDFS conf (""dfs.replication"").",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 16 20:54:59 UTC 2014,,,,,,,,,,"0|i219i7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"16/Oct/14 20:00;vanzin;BTW, this would avoid issues like this:

{noformat}
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException(java.io.IOException): file /user/systest/.sparkStaging/application_1413485082283_0001/spark-assembly-1.2.0-SNAPSHOT-hadoop2.3.0.jar.
Requested replication 3 exceeds maximum 1
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:943)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplicationInt(FSNamesystem.java:2243)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:2233)
        ...
        at org.apache.spark.deploy.yarn.ClientBase$class.copyFileToRemote(ClientBase.scala:101)
{noformat};;;","16/Oct/14 20:54;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2831;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema change on Spark-Hive (Parquet file format) table not working,SPARK-3978,12748691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,arov,barge.nilesh,barge.nilesh,16/Oct/14 19:50,16/Sep/15 21:40,14/Jul/23 06:26,15/Sep/15 21:02,1.1.0,,,,,,,1.5.0,,,,,,SQL,,,,1,,,,,,"On following releases: 
Spark 1.1.0 (built using sbt/sbt -Dhadoop.version=2.2.0 -Phive assembly) , Apache HDFS 2.2 

Spark job is able to create/add/read data in hive, parquet formatted, tables using HiveContext. 
But, after changing schema, spark job is not able to read data and throws following exception: 
java.lang.ArrayIndexOutOfBoundsException: 2 
        at org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getStructFieldData(ArrayWritableObjectInspector.java:127) 
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$1.apply(TableReader.scala:284) 
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$1.apply(TableReader.scala:278) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$class.foreach(Iterator.scala:727) 
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) 
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) 
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) 
        at scala.collection.AbstractIterator.to(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) 
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) 
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) 
        at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774) 
        at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774) 
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121) 
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121) 
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62) 
        at org.apache.spark.scheduler.Task.run(Task.scala:54) 
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
        at java.lang.Thread.run(Thread.java:744)


code snippet in short: 

hiveContext.sql(""CREATE EXTERNAL TABLE IF NOT EXISTS people_table (name String, age INT) ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat' OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'""); 
hiveContext.sql(""INSERT INTO TABLE people_table SELECT name, age FROM temp_table_people1""); 
hiveContext.sql(""SELECT * FROM people_table""); //Here, data read was successful.  
hiveContext.sql(""ALTER TABLE people_table ADD COLUMNS (gender STRING)""); 
hiveContext.sql(""SELECT * FROM people_table""); //Not able to read existing data and ArrayIndexOutOfBoundsException is thrown.

",,arov,barge.nilesh,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 21:40:23 UTC 2015,,,,,,,,,,"0|i219hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/15 16:35;arov;I have verified that altering a table stored as parquet files and then querying the table through spark-sql shell works as expected in master.;;;","15/Sep/15 21:02;marmbrus;Closing as it sound like this is fixed in at least Spark 1.5.;;;","16/Sep/15 21:26;barge.nilesh;Thanks for resolving this, I also verified on my end and now it is working fine....
;;;","16/Sep/15 21:29;arov;[~barge.nilesh] What version of Spark have you tested with?;;;","16/Sep/15 21:40;barge.nilesh;I tested with the latest Spark 1.5 release... 
I got the source (http://www.apache.org/dyn/closer.lua/spark/spark-1.5.0/spark-1.5.0.tgz) and then build with ""mvn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package"" command... and then ran my original tests...
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print callSite information for broadcast variables,SPARK-3973,12748670,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivaram,shivaram,shivaram,16/Oct/14 18:34,17/Oct/14 02:43,14/Jul/23 06:26,17/Oct/14 02:43,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Printing call site information for broadcast variables will help in debugging which variables are used, when they are used etc.",,apachespark,joshrosen,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 02:43:53 UTC 2014,,,,,,,,,,"0|i219d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/14 18:37;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/2829;;;","17/Oct/14 02:43;joshrosen;Issue resolved by pull request 2829
[https://github.com/apache/spark/pull/2829];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to deserialize Vector in cluster mode,SPARK-3971,12748661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,16/Oct/14 17:56,05/Nov/14 15:59,14/Jul/23 06:26,16/Oct/14 21:57,1.2.0,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,0,,,,,,"The serialization of Vector/Rating did not work in cluster mode, because the initializer is not called in executor.",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3981,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 15:59:44 UTC 2014,,,,,,,,,,"0|i219bb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"16/Oct/14 19:44;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2830;;;","16/Oct/14 21:57;mengxr;Issue resolved by pull request 2830
[https://github.com/apache/spark/pull/2830];;;","05/Nov/14 15:59;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3113;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove duplicate removal of local dirs,SPARK-3970,12748615,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,16/Oct/14 15:23,27/Oct/14 01:40,14/Jul/23 06:26,27/Oct/14 01:02,1.1.0,,,,,,,1.2.0,,,,,,,,,,0,,,,,,The shutdown hook of DiskBlockManager would remove localDirs. So do not need to register them with Utils.registerShutdownDeleteDir. It causes duplicate removal of these local dirs and corresponding exceptions.,,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4091,,,,,,SPARK-4091,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 16 15:23:59 UTC 2014,,,,,,,,,,"0|i2192f:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"16/Oct/14 15:23;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/2826;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark applications fail in yarn-cluster mode when the directories configured in yarn.nodemanager.local-dirs are located on different disks/partitions,SPARK-3967,12748544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,preaudc,preaudc,preaudc,16/Oct/14 09:16,15/May/15 13:35,14/Jul/23 06:26,15/May/15 13:35,1.1.0,,,,,,,1.2.0,,,,,,YARN,,,,2,,,,,,"Spark applications fail from time to time in yarn-cluster mode (but not in yarn-client mode) when yarn.nodemanager.local-dirs (Hadoop YARN config) is set to a comma-separated list of directories which are located on different disks/partitions.

Steps to reproduce:
1. Set yarn.nodemanager.local-dirs (in yarn-site.xml) to a list of directories located on different partitions (the more you set, the more likely it will be to reproduce the bug):
(...)
<property>
  <name>yarn.nodemanager.local-dirs</name>
  <value>file:/d1/yarn/local/nm-local-dir,file:/d2/yarn/local/nm-local-dir,file:/d3/yarn/local/nm-local-dir,file:/d4/yarn/local/nm-local-dir,file:/d5/yarn/local/nm-local-dir,file:/d6/yarn/local/nm-local-dir,file:/d7/yarn/local/nm-local-dir</value>
</property>
(...)
2. Launch (several times) an application in yarn-cluster mode, it will fail (apparently randomly) from time to time",,apachespark,djice,hammer,jeanlyn,joshrosen,preaudc,rdub,SuYan,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4896,SPARK-4764,,,,,,,,,,,,,"17/Oct/14 22:29;rdub;spark-1.1.0-utils-fetch.patch;https://issues.apache.org/jira/secure/attachment/12675599/spark-1.1.0-utils-fetch.patch","16/Oct/14 09:23;preaudc;spark-1.1.0-yarn_cluster_tmpdir.patch;https://issues.apache.org/jira/secure/attachment/12675251/spark-1.1.0-yarn_cluster_tmpdir.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 19 23:58:50 UTC 2014,,,,,,,,,,"0|i218mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/14 09:23;preaudc;Ensure that the temporary file which the jar file is fetched in is located in the same directory than the target jar file
;;;","16/Oct/14 09:24;preaudc;After investigating, it turns out that the problem is when the executor fetches a jar file: the jar is downloaded in a temporary file, always in /d1/yarn/local/nm-local-dir (first directory of yarn.nodemanager.local-dirs), and then moved in one of the directories of yarn.nodemanager.local-dirs:
--> if it is the same than the temporary file (i.e. /d1/yarn/local/nm-local-dir), then the application continues normally
--> if it is another one (i.e. /d2/yarn/local/nm-local-dir, /d3/yarn/local/nm-local-dir,...), it fails with the following error:
14/10/10 14:33:51 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 0)
java.io.FileNotFoundException: ./logReader-1.0.10.jar (Permission denied)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at com.google.common.io.Files$FileByteSink.openStream(Files.java:223)
        at com.google.common.io.Files$FileByteSink.openStream(Files.java:211)
        at com.google.common.io.ByteSource.copyTo(ByteSource.java:203)
        at com.google.common.io.Files.copy(Files.java:436)
        at com.google.common.io.Files.move(Files.java:651)
        at org.apache.spark.util.Utils$.fetchFile(Utils.scala:440)
        at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:325)
        at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:323)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:323)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:158)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

I have no idea why the move fails when the source and target files are not on the same partition (it is no more atomic, but it should succeed anyway), for the moment I have worked around the problem with the attached patch (i.e. I ensure that the temp file and the moved file are always on the same partition).;;;","17/Oct/14 22:28;rdub;I've been debugging this issue as well and I think I've found an issue in {{org.apache.spark.util.Utils}} that is contributing to / causing the problem:

{{Files.move}} on [line 390|https://github.com/apache/spark/blob/v1.1.0/core/src/main/scala/org/apache/spark/util/Utils.scala#L390] is called even if {{targetFile}} exists and {{tempFile}} and {{targetFile}} are equal.

The check on [line 379|https://github.com/apache/spark/blob/v1.1.0/core/src/main/scala/org/apache/spark/util/Utils.scala#L379] seems to imply the desire to skip a redundant overwrite if the file is already there and has the contents that it should have.

Gating the {{Files.move}} call on a further {{if (!targetFile.exists)}} fixes the issue for me; attached is a patch of the change.

In practice all of my executors that hit this code path are finding every dependency JAR to already exist and be exactly equal to what they need it to be, meaning they were all needlessly overwriting all of their dependency JARs, and now are all basically no-op-ing in {{Utils.fetchFile}}; I've not determined who/what is putting the JARs there, why the issue only crops up in {{yarn-cluster}} mode (or {{--master yarn --deploy-mode cluster}}), etc., but it seems like either way this patch is probably desirable.
;;;","17/Oct/14 22:29;rdub;Don't redundantly copy executor dependency files in {{Utils.fetchFile}}.;;;","18/Oct/14 00:32;srowen;You guys should make PRs for these. I am also not sure if it's so necessary to download the file into a temp directory and move it... it may cause a copy instead of rename, and in fact does here, and so is not like the file appears in the target dir atomically anyway. I'm not sure the code here cleans up the partially downloaded file in case of error and that could leave a broken file in the target dir instead of just a temp dir.

The change to not copy the file when identical looks sound; I bet you can avoid checking if it exists twice.;;;","19/Oct/14 23:11;apachespark;User 'ryan-williams' has created a pull request for this issue:
https://github.com/apache/spark/pull/2848;;;","20/Oct/14 10:14;apachespark;User 'preaudc' has created a pull request for this issue:
https://github.com/apache/spark/pull/2855;;;","20/Oct/14 10:22;preaudc;Hi Ryan,
Thanks for your help. You should probably add the same test on the {{Files.move}} on [line 437|https://github.com/apache/spark/blob/v1.1.0/core/src/main/scala/org/apache/spark/util/Utils.scala#L437];;;","20/Oct/14 15:40;rdub;Cool, I'll add it there as well [~preaudc], and take further discussion to [the PR|https://github.com/apache/spark/pull/2848] unless you prefer having it here;;;","20/Oct/14 15:59;preaudc;That's fine, thanks!;;;","21/Oct/14 10:09;jeanlyn;This issue also hapens to me.but i want to know why this issue don't happens to the *yarn-client* mode;;;","21/Oct/14 11:34;jeanlyn;I think you this pull request also can be referenced:
https://github.com/apache/spark/pull/1616;;;","08/Dec/14 19:58;joshrosen;To give a quick update on this: I've merged [~preaudc]'s patch into {{master}} and {{branch-1.1}} (and will backport it into {{branch-1.2}} after the 1.2.0 release passes).

I'd also like to include [~rdub]'s patch, too, since it's a pretty good refactoring of this code.  That patch is getting a lot closer to merging; I'm going to try to loop back soon to provide more review feedback.;;;","19/Dec/14 23:58;joshrosen;I've merged [~rdub]'s patch (SPARK-4896) into {{master}}, {{branch-1.1}}, and {{branch-1.2}} and have backported the other patch to {{branch-1.2}}.

It would be great if folks could confirm whether these fixes have resolved this issue, or whether there's still more work to be done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullabilities of Cast related to DateType.,SPARK-3966,12748504,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,16/Oct/14 05:39,20/Oct/14 22:51,14/Jul/23 06:26,20/Oct/14 22:51,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 22:51:20 UTC 2014,,,,,,,,,,"0|i218dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/14 05:43;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2820;;;","20/Oct/14 22:51;marmbrus;Issue resolved by pull request 2820
[https://github.com/apache/spark/pull/2820];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Mark spark dependency as ""provided"" in external libraries",SPARK-3962,12748489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,pwendell,pwendell,16/Oct/14 03:23,07/Feb/20 17:23,14/Jul/23 06:26,19/Nov/14 22:20,,,,,,,,1.2.0,,,,,,DStreams,,,,0,,,,,,"Right now there is not an easy way for users to link against the external streaming libraries and not accidentally pull Spark into their assembly jar. We should mark Spark as ""provided"" in the external connector pom's so that user applications can simply include those like any other dependency in the user's jar.

This is also the best format for third-party libraries that depend on Spark (of which there will eventually be many) so it would be nice for our own build to conform to this nicely.",,apachespark,prashant,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3951,,,,,,,,SPARK-3951,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 10:08:26 UTC 2014,,,,,,,,,,"0|i218af:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"26/Oct/14 18:11;pwendell;[~prashant] can you take a crack at this? It's pretty simple, we just want the streaming external projects to mark spark-core and spark-streaming as provided.;;;","27/Oct/14 09:50;prashant;Yes sure.;;;","27/Oct/14 10:38;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/2959;;;","17/Nov/14 10:08;pwendell;I think this is causing the build to fail because of this bug:
https://jira.codehaus.org/browse/MSHADE-148

Would it be possible to remove the test-jar dependency on streaming for these modules (/cc @tdas)? If we need to have a moderate amount of code duplication I think it's fine because as it stands now these aren't really usable for people. I looked and I only found some minor dependencies that could be inlined into the modules.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We can apply unary minus only to literal.,SPARK-3960,12748419,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sarutak,sarutak,15/Oct/14 21:05,27/Oct/14 23:17,14/Jul/23 06:26,27/Oct/14 23:17,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Because of the wrong syntax definition, we cannot apply unary minus only to literal. So, we cannot write such expressions.

{code}
-(value1 + value2) // Parenthesized expressions
-column // Columns
-MAX(column) // Functions
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 19:51:18 UTC 2014,,,,,,,,,,"0|i217vr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Oct/14 21:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2816;;;","26/Oct/14 19:51;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2949;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlParser fails to parse literal -9223372036854775808 (Long.MinValue).,SPARK-3959,12748418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,sarutak,sarutak,15/Oct/14 21:01,26/Oct/14 23:41,14/Jul/23 06:26,26/Oct/14 23:41,1.2.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"SqlParser fails to parse -9223372036854775808 (Long.MinValue) so we cannot write queries such like as follows.

{code}
SELECT value FROM someTable WHERE value > -9223372036854775808
{code}",,apachespark,marmbrus,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 23:41:01 UTC 2014,,,,,,,,,,"0|i217vj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Oct/14 21:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2816;;;","26/Oct/14 19:51;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2949;;;","26/Oct/14 23:41;marmbrus;Issue resolved by pull request 2816
[https://github.com/apache/spark/pull/2816];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible stream-corruption issues in TorrentBroadcast,SPARK-3958,12748414,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,15/Oct/14 20:54,25/Jan/16 08:46,14/Jul/23 06:26,21/Jan/15 22:28,1.1.0,1.2.0,,,,,,,,,,,,Spark Core,,,,4,,,,,,"TorrentBroadcast deserialization sometimes fails with decompression errors, which are most likely caused by stream-corruption exceptions.  For example, this can manifest itself as a Snappy PARSING_ERROR when deserializing a broadcasted task:

{code}
14/10/14 17:20:55.016 DEBUG BlockManager: Getting local block broadcast_8
14/10/14 17:20:55.016 DEBUG BlockManager: Block broadcast_8 not registered locally
14/10/14 17:20:55.016 INFO TorrentBroadcast: Started reading broadcast variable 8
14/10/14 17:20:55.017 INFO TorrentBroadcast: Reading broadcast variable 8 took 5.3433E-5 s
14/10/14 17:20:55.017 ERROR Executor: Exception in task 2.0 in stage 8.0 (TID 18)
java.io.IOException: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
	at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:594)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:125)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:216)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:170)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:164)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

SPARK-3630 is an umbrella ticket for investigating all causes of these Kryo and Snappy deserialization errors.  This ticket is for a more narrowly-focused exploration of the TorrentBroadcast version of these errors, since the similar errors that we've seen in sort-based shuffle seem to be explained by a different cause (see SPARK-3948).",,aash,apachespark,chlam4,gmaas,jerryshao,joshrosen,pwendell,vitalii.migov@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4133,,SPARK-3630,,,,,,,,,,,,,,"04/Nov/14 16:09;vitalii.migov@gmail.com;spark_ex.logs;https://issues.apache.org/jira/secure/attachment/12679242/spark_ex.logs",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 08:46:01 UTC 2016,,,,,,,,,,"0|i217un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/14 21:11;joshrosen;Digging into this stacktrace in more detail:

Snappy-java's {{SnappyOutputStream}} writes its own 8-byte header at the beginning of the serialized output.  This header consists of an 8-byte magic value followed by two 4-byte version numbers.  This header is distinct from Snappy's own 6-byte magic number / header.

{{org.xerial.snappy.SnappyInputStream.readHeader}} is implemented like this (in Snappy-Java 1.1.1.3):

{code}
protected void readHeader() throws IOException {
        byte[] header = new byte[SnappyCodec.headerSize()];
        int readBytes = 0;
        while (readBytes < header.length) {
            int ret = in.read(header, readBytes, header.length - readBytes);
            if (ret == -1)
                break;
            readBytes += ret;
        }

        // Quick test of the header 
        if (readBytes < header.length || header[0] != SnappyCodec.MAGIC_HEADER[0]) {
            // do the default uncompression
            readFully(header, readBytes);
            return;
        }

        SnappyCodec codec = SnappyCodec.readHeader(new ByteArrayInputStream(header));
        if (codec.isValidMagicHeader()) {
            // The input data is compressed by SnappyOutputStream
            if (codec.version < SnappyCodec.MINIMUM_COMPATIBLE_VERSION) {
                throw new IOException(String.format(
                        ""compressed with imcompatible codec version %d. At least version %d is required"",
                        codec.version, SnappyCodec.MINIMUM_COMPATIBLE_VERSION));
            }
        }
        else {
            // (probably) compressed by Snappy.compress(byte[])
            readFully(header, readBytes);
            return;
        }
    }
{code}

It starts by attempting to read the 8-byte header.  The first {{while}} loop exits when we've either read 8 bytes of header data or if the input stream was closed before it could read a complete header.  The following code checks whether the header is unexpectedly short or whether it doesn't match the snappy-java magic header.  In our case, we end up taking this branch and calling {{readFully(header, readBytes)}} in order to perform the default Snappy decompression  This is the wrong branch to take (since our data was compressed with a SnappyOutputStream), leading to the PARSING_ERROR.

Based on this, I think that the input data to the SnappyInputStream is somehow being corrupted.  It's not obvious whether this corruption is causing the input data to be too short or whether the start of the stream has the wrong contents.  I'll keep digging and look into adding some size-checking assertions throughout our code.;;;","15/Oct/14 22:08;joshrosen;Removing 1.1.0 as an affected version for now, since the stacktrace that I posted here was from a recent build of master (1.2).  If anyone can reproduce this in branch-1.1 or Spark 1.1.0, please let me know.;;;","15/Oct/14 23:49;joshrosen;I think that I can safely rule out problems in TorrentBroadcast's (de-)blockification code: I used ScalaCheck to write some tests to ensure that blockifyObject and unblockifyObject are inverses, plus a similar test for ByteArrayChunkOutputStream: https://github.com/JoshRosen/spark/commit/413be7f6a8d4eb14c69c7db87e2564ed4d776c42?diff=unified;;;","16/Oct/14 01:16;jerryshao;Hi Josh, have you tried other compression like LZO to narrow down the problem?;;;","16/Oct/14 01:26;joshrosen;Hi [~jerryshao],

I don't have a reliable reproduction for this issue yet, so I haven't tried switching compression schemes or broadcast implementations.  I'm working with the user who provided this stack trace to see if we can get more logs to provide additional context.;;;","17/Oct/14 02:29;joshrosen;[~davies] ran across this exception while testing a pull request that modifies TorrentBroadcast: https://github.com/apache/spark/pull/2681#issuecomment-59120483

That PR's reproductioncould be a valuable debugging clue for this issue.;;;","19/Oct/14 07:08;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2844;;;","29/Oct/14 17:50;joshrosen;Adding 1.1.0 as an affected version, since a user has observed this in 1.1.0, too; see SPARK-4133.;;;","04/Nov/14 16:10;vitalii.migov@gmail.com;Observed the same exception ( Spark 1.1.0 ). After changing broadcast factory to the HttpBroadcastFactory ( as suggested in the SPARK-4133 ), exception looks like:
java.io.FileNotFoundException: http://10.8.0.22:44907/broadcast_0

Full logs attached to issue: spark_ex.logs 

Seems that something wrong with the handling of the ""broadcast_0"" in the BlockManager:
I think that something wrong with the handing of the ""broadcast_0"" in the BlockManager:
14/11/04 17:20:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1216.0 B, free 983.1 MB)
14/11/04 17:20:39 DEBUG BlockManager: Put block broadcast_0 locally took 84 ms
14/11/04 17:20:39 DEBUG BlockManager: Putting block broadcast_0 without replication took 86 ms
14/11/04 17:20:39 DEBUG BlockManager: Getting local block broadcast_0
14/11/04 17:20:39 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
14/11/04 17:20:39 DEBUG BlockManager: Getting block broadcast_0 from memory
14/11/04 17:20:39 INFO BlockManager: Found block broadcast_0 locally
14/11/04 17:20:57 WARN BlockManager: Block broadcast_0 already exists on this machine; not re-adding it
14/11/04 17:20:57 DEBUG BlockManager: Getting local block broadcast_0
14/11/04 17:20:57 DEBUG BlockManager: Block broadcast_0 not registered locally
14/11/04 17:20:57 DEBUG BlockManager: Getting remote block broadcast_0
14/11/04 17:20:57 DEBUG BlockManagerMasterActor: [actor] received message GetLocations(broadcast_0) from Actor[akka://sparkDriver/temp/$l]
14/11/04 17:20:57 DEBUG BlockManager: Block broadcast_0 not found
14/11/04 17:20:57 DEBUG BlockManagerMasterActor: [actor] handled message (0.117676 ms) GetLocations(broadcast_0) from Actor[akka://sparkDriver/temp/$l]
14/11/04 17:20:57 INFO HttpBroadcast: Started reading broadcast variable 0
14/11/04 17:20:57 DEBUG HttpBroadcast: broadcast read server: http://10.8.0.22:44907 id: broadcast-0
14/11/04 17:20:57 DEBUG HttpBroadcast: broadcast not using security
14/11/04 17:20:57 DEBUG RecurringTimer: Callback for BlockGenerator called at time 1415114457200
14/11/04 17:20:57 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.FileNotFoundException: http://10.8.0.22:44907/broadcast_0
;;;","21/Jan/15 22:28;pwendell;At this point I'm not aware of people still hitting this set of issues in newer releases, so per discussion with [~joshrosen], I'd like to close this. Please comment on this JIRA if you are having some variant of this issue in a newer version of Spark, and we'll continue to investigate.;;;","25/Jan/16 08:46;gmaas;[~pwendell] [~joshrosen] We just hit this bug in one of our production jobs using Spark Streaming 1.4.1. Each task spawned by the streaming job fails down the road.
This jobs has been working fine for months, so I'm not clear on whether we can narrow down the conditions to reproduce it.

Here's the exception:

{code}
[Stage 16049:(0 + 0) / 24][Stage 16056:(0 + 0) / 24][Stage 16058:(0 + 0) / 24]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 17478.0 failed 6 times, most recent failure: Lost task 23.5 in stage 17478.0 (TID 172352, dnode-6.hdfs.private): java.io.IOException: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
	at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:594)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:358)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:387)
	at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2296)
	at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2589)
	at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2599)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1319)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.DeserializationStream.readKey(Serializer.scala:169)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:200)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:197)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:127)
	at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)
	at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:90)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

(Murphy's law: Bugs reappear the moment you close them);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Different versions between jackson-mapper-asl and jackson-core-asl,SPARK-3955,12748221,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jongyoul,jongyoul,jongyoul,15/Oct/14 07:46,29/Dec/14 18:18,14/Jul/23 06:26,27/Dec/14 07:00,1.1.0,,,,,,,1.3.0,,,,,,Build,Spark Core,SQL,,0,,,,,,"In the parent pom.xml, specified a version of jackson-mapper-asl. This is used by sql/hive/pom.xml. When mvn assembly runs, however, jackson-mapper-asl is not same as jackson-core-asl. This is because other libraries use several versions of jackson, so other version of jackson-core-asl is assembled. Simply, fix this problem if pom.xml has a specific version information of jackson-core-asl. If it's not set, a version 1.9.11 is merged info assembly.jar and we cannot use jackson library properly.

{code}
[INFO] Including org.codehaus.jackson:jackson-mapper-asl:jar:1.8.8 in the shaded jar.
[INFO] Including org.codehaus.jackson:jackson-core-asl:jar:1.9.11 in the shaded jar.
{code}",,apachespark,jongyoul,jvwilge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 29 18:18:06 UTC 2014,,,,,,,,,,"0|i216p3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"15/Oct/14 08:41;srowen;Looks like Jackson is managed to version 1.8.8 for Avro reasons. I think core just needs to be managed the same way. I'll try it locally to make sure that works.;;;","16/Oct/14 00:04;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/2818;;;","21/Oct/14 11:34;jvwilge;Is it possible to include the latest version (1.9.13) of jackson for both jackson-core-asl and jackson-mapper-asl? Version 1.8.8 is pretty old (january 2012). We're having some issues with methods that are available in 1.9.x and not in 1.8.x. 

I think parquet-hadoop includes the old jar, when you do a
{{mvn dependency:tree -Dincludes=org.codehaus.jackson}}
on the assembly project it shows this tree (we are compiling with Hadoop 2.5.1, Yarn and the Hadoop 2.4 profile) :
{noformat}
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ spark-assembly_2.10 ---
[INFO] org.apache.spark:spark-assembly_2.10:pom:1.1.0
[INFO] \- org.apache.spark:spark-sql_2.10:jar:1.1.0:compile
[INFO]    \- com.twitter:parquet-hadoop:jar:1.4.3:compile
[INFO]       +- org.codehaus.jackson:jackson-mapper-asl:jar:1.8.8:compile
[INFO]       \- org.codehaus.jackson:jackson-core-asl:jar:1.9.11:compile
{noformat};;;","21/Oct/14 11:37;srowen;I think the issue is matching the version used by Hadoop. Your usage of Jackson shouldn't affect what Spark uses, but I imagine you're saying this is a case of dependency leakage?;;;","21/Oct/14 11:49;jvwilge;Is is indeed a dependency leakage. I have a few suggestions/options : 
- 1) mark the dependency of jackson as provided, since it is included in Hadoop, 
- 2) Add a dependency for mapper and core with version 1.9.x so the version 1.8.8 of parquet-hadoop is overridden. 
- 3) Upgrade parquet-hadoop, there is a newer version with a dependency to jackson 1.9.11;;;","22/Oct/14 03:08;jongyoul;[~srowen], yes, This is about dependency leakage.

[~jvwilge], #2 is my patch, and I'm agree that the jackson version is pretty old.;;;","16/Dec/14 08:20;jongyoul;Please mark this issue as Resolved. It's fixed by PR 3379.;;;","16/Dec/14 11:48;srowen;[~jongyoul] That PR was never merged though: https://github.com/apache/spark/pull/3379
You closed yours too: https://github.com/apache/spark/pull/2818
I don't think this is resolved.;;;","17/Dec/14 02:29;jongyoul;[~srowen] Oops. That's my mistake. I tried to reopen my PR on github, but that repository is not available, thus I'll take a pull request again. Sorry for confusing you.;;;","17/Dec/14 02:56;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/3716;;;","29/Dec/14 18:18;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3829;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python examples in Streaming Programming Guide,SPARK-3952,12748152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,davies,davies,14/Oct/14 22:10,19/Oct/14 02:15,14/Jul/23 06:26,19/Oct/14 02:15,,,,,,,,1.2.0,,,,,,DStreams,PySpark,,,0,,,,,,Having Python examples in Streaming Programming Guide.,,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 19 02:15:12 UTC 2014,,,,,,,,,,"0|i216a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/14 00:17;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2808;;;","19/Oct/14 02:15;joshrosen;Issue resolved by pull request 2808
[https://github.com/apache/spark/pull/2808];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort-based shuffle can lead to assorted stream-corruption exceptions,SPARK-3948,12748040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jerryshao,jerryshao,jerryshao,14/Oct/14 14:18,17/May/20 18:30,14/Jul/23 06:26,20/Oct/14 17:22,1.2.0,,,,,,,1.1.1,1.2.0,,,,,Shuffle,Spark Core,,,0,,,,,,"Several exceptions occurred when running TPC-DS queries against latest master branch with sort-based shuffle enable, like PARSING_ERROR(2) in snappy, deserializing error in Kryo and offset out-range in FileManagedBuffer, all these exceptions are gone when we changed to hash-based shuffle.

With deep investigation, we found that some shuffle output file is unexpectedly smaller than the others, as the log shows:

{noformat}
14/10/14 18:25:06 INFO shuffle.IndexShuffleBlockManager: Block id: shuffle_6_9_11, offset: 3055635, length: 236708, file length: 47274167
14/10/14 18:25:06 INFO shuffle.IndexShuffleBlockManager: Block id: shuffle_6_10_11, offset: 2986484, length: 222755, file length: 47174539
14/10/14 18:25:06 INFO shuffle.IndexShuffleBlockManager: Block id: shuffle_6_11_11, offset: 2995341, length: 259871, file length: 383405
14/10/14 18:25:06 INFO shuffle.IndexShuffleBlockManager: Block id: shuffle_6_12_11, offset: 2991030, length: 268191, file length: 47478892
14/10/14 18:25:06 INFO shuffle.IndexShuffleBlockManager: Block id: shuffle_6_13_11, offset: 3016292, length: 230694, file length: 47420826
14/10/14 18:25:06 INFO shuffle.IndexShuffleBlockManager: Block id: shuffle_6_14_11, offset: 3061400, length: 241136, file length: 47395509
{noformat}

As you can see the total file length of shuffle_6_11_11 is much smaller than other same stage map output results.

And we also dump the map outputs in map side to see if this small size output is correct or not, below is the log:

{noformat}
 In bypass merge sort, file name: /mnt/DP_disk1/animal/spark/spark-local-
20141014182142-8345/22/shuffle_6_11_0.data, file length: 383405length:
 274722 262597 291290 272902 264941 270358 291005 295285 252482 
287142 232617 259871 233734 241439 228897 234282 253834 235619 
233803 255532 270739 253825 262087 266404 234273 250120 262983 
257024 255947 254971 258908 247862 221613 258566 245399 251684 
274843 226150 264278 245279 225656 235084 239466 212851 242245 
218781 222191 215500 211548 234256 208601 204113 191923 217895 
227020 215331 212313 223725 250876 256875 239276 266777 235520 
237462 234063 242270 246825 255888 235937 236956 233099 264508 
260303 233294 239061 254856 257475 230105 246553 260412 210355 
211201 219572 206636 226866 209937 226618 218208 206255 248069 
221717 222112 215734 248088 239207 246125 239056 241133 253091 
246738 233128 242794 231606 255737 221123 252115 247286 229688 
251087 250047 237579 263079 256251 238214 208641 201120 204009 
200825 211965 200600 194492 226471 194887 226975 215072 206008 
233288 222132 208860 219064 218162 237126 220465 201343 225711 
232178 233786 212767 211462 213671 215853 227822 233782 214727 
247001 228968 247413 222674 214241 184122 215643 207665 219079 
215185 207718 212723 201613 216600 212591 208174 204195 208099 
229079 230274 223373 214999 256626 228895 231821 383405 229646 
220212 245495 245960 227556 213266 237203 203805 240509 239306 
242365 218416 238487 219397 240026 251011 258369 255365 259811 
283313 248450 264286 264562 257485 279459 249187 257609 274964 
292369 273826
{noformat}

Here I dump the file name, length and each partition's length, obviously the sum of all partition lengths is not equal to file length. So I think there may be a situation paritionWriter in ExternalSorter not always append to the end of previous written file, the file's content is overwritten in some parts, and this lead to the exceptions I mentioned before.

Also I changed the code of copyStream by disable transferTo, use the previous one, all the issues are gone. So I think there maybe some flushing problems in transferTo when processed data is large.

",,aash,apachespark,codingcat,hammer,jerryshao,joshrosen,koeninger,mridulm80,pwendell,rdub,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3630,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 17:56:06 UTC 2014,,,,,,,,,,"0|i215m7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/14 14:22;jerryshao;I think the issue of SPARK-3630 is related to this one;;;","14/Oct/14 18:13;joshrosen;Hi [~jerryshao],

Could you share the changes that you made to copyStream (maybe by attaching a {{git diff}} to this ticket)?  I want to play around with your fix to see whether I can spot the problem.;;;","15/Oct/14 01:14;jerryshao;Hi Josh, according to my observation, this bug is occurred in this branch of writePartitionedFile (ExternalSorter), but I cannot reproduce this in my local mode with small data size,

{code}
if (bypassMergeSort && partitionWriters != null) {
      // We decided to write separate files for each partition, so just concatenate them. To keep
      // this simple we spill out the current in-memory collection so that everything is in files.
      spillToPartitionFiles(if (aggregator.isDefined) map else buffer)
      partitionWriters.foreach(_.commitAndClose())
      var out: FileOutputStream = null
      var in: FileInputStream = null
      try {
        out = new FileOutputStream(outputFile)
        for (i <- 0 until numPartitions) {
          in = new FileInputStream(partitionWriters(i).fileSegment().file)
          val size = org.apache.spark.util.Utils.copyStream(in, out, false)
          in.close()
          in = null
          lengths(i) = size
        }
      } finally {
        if (out != null) {
          out.close()
        }
        if (in != null) {
          in.close()
        }
      }
{code}

Currently I just revert this commit (https://github.com/apache/spark/commit/246cb3f158686348a698d1c0da3001c314727129) in copyStream to workaround this issue. I still need to investigate why transferTo will get wrong result.

Besides, would you mind assign this JIRA to me, thanks a lot.;;;","15/Oct/14 01:40;joshrosen;Thanks for narrowing the problem down to a single commit; this is extremely helpful.  I've assigned this JIRA to you, but I'll help investigate, too.;;;","15/Oct/14 01:42;jerryshao;Thanks for your help :).;;;","15/Oct/14 02:54;pwendell;Great thanks [~jerryshao] - I updated the description. It sounds like this is just an issue with something in the sort-based code that can cause all kinds of random stream exceptions.;;;","15/Oct/14 05:42;joshrosen;I've been looking over the code in https://github.com/apache/spark/pull/1884 and I haven't spotted anything glaringly wrong.  One hunch, though: in the {{transferTo}} code, we read the size of the file up front and then copy that many bytes, whereas the old, non-{{transferTo}} code keeps reading until it hits the end of the stream.  This could potentially account for different behavior if the size of the underlying file increased while the copy was taking place.;;;","15/Oct/14 06:19;jerryshao;Hi Josh, thanks for your help. I don't think it's un-flushed input file which makes output file copy length error, since bunch of {{partitionWriters}} are flushed and closed before copying to output file, as you can see line 704 in ExternalSorter {{partitionWriters.foreach(_.commitAndClose())}}.

I still doubt that current use of {{transferTo}} may have some problems which will ruin the output file, I try to add append flag for the output file in line 708, seems the problem is gone.;;;","15/Oct/14 06:38;joshrosen;Hi [~jerryshao],

To make sure that I understand, are you saying that changing

{code}
out = new FileOutputStream(outputFile)
{code}

to

{code}
out = new FileOutputStream(outputFile, append=True)
{code}

seems to solve the issue, even while continuing to use the {{transferTo}} code?;;;","15/Oct/14 06:45;jerryshao;Yes, that's correct. Also please see the link (http://stackoverflow.com/questions/1651832/java-filechannel-transferfrom-questions), I guess we may face the similar problem.;;;","15/Oct/14 07:11;joshrosen;I'm surprised that the lack of {{append}} didn't cause problems prior to the {{transferTo}} patch.  To try to reason this out:

*Old code, no append*: We use same FileOutputStream object to write all of the partitions, copying each partition's data by calling {{FileOutputStream.write}} on this same FileOutputStream object.  Since we're not in append mode, the first partition is written at the beginning of the file and all subsequent partitions are appended to it.

*New {{transferTo}} code, no append:* We call {{FileOutputStream.getChannel()}} for each partition; which [""returns the unique FileChannel object associated with this file output stream""|http://docs.oracle.com/javase/7/docs/api/java/io/FileOutputStream.html#getChannel()].  Since we're not in append mode, this channel's initial position should ""be equal to the number of bytes written to the file so far"", which should initially be 0 since we just created a new FileOutputStream and haven't written anything using it.  According to [its docs|http://docs.oracle.com/javase/6/docs/api/java/nio/channels/FileChannel.html#transferTo(long, long, java.nio.channels.WritableByteChannel)], {{transferTo}} should increment the target channel's position by the number of bytes written.  So, in this case it also seems like the first partition should be written to the beginning of the file while others are appended after it.

Am I missing an obvious bug here?  Is there a reason why these two copy implementations don't behave identically when append is disabled? ;;;","15/Oct/14 07:55;mridulm80;[~joshrosen] Assuming there are no VM bugs being hit for inChannel.size() or some other concurrent writes to these files, I dont see any issues with the code - as you elaborated.
On other hand, external sort code is slightly loose w.r.t use of file api - not sure if that is causing the observed problems : example, use of skip() in SortShuffleManager.scala.
We will need to investigate in detail if some of these are causing the observed problems.;;;","15/Oct/14 07:57;jerryshao;Hi Josh,

I think for old code without append, the behavior is correct even without {{append}} flag, because we never close the output stream between different partitions, so input streams for each partition will always write to the end of output stream, I think that's correct.

For new {{transferTo}} code without append, theoretically it's correct that target channel's position should increment by the bytes written, but according to my investigation through logs, I found that for the subsequent partitions, output channel's position unexpectedly pointed to 0, not the previous output file size, that's also the unexplained behavior which makes me confuse a lot.;;;","15/Oct/14 08:16;mridulm80;[~jerryshao] Just to clarify, what exactly is the behavior you are observing ?
- Is it that getChannel is returning a channel which has position == 0 after writing bytes to the stream ? (size > 0)
If yes, what is the channel's length you are observing in that case ?

Also, how ""large"" are the file sizes ?

The documentation of getChannel and transferTo are fairly unambigous ... so our code, as written, is conforment to that. Ofcourse, it is always possible we are hitting some bugs in some scenarios !
What is the environment you are running this on btw ? OS/jvm version ? Thanks.
;;;","15/Oct/14 08:44;jerryshao;Hi [~mridul], what I observed is that, after writing bytes to channel, still get the position 0 in the next calling, here is one of the log that I traced:

{noformat}
java.lang.AssertionError: assertion failed: outChannel before size, 272314, before position: 0. Current size: 284167, current position: 0, expected write data size: 284167
{noformat}

I just print out the channel size and position *before* and *after* the {{transferTo}} action. As you can see the output file size is not 0 before written, but the position is 0, not the end of channel. After written to it use {{transferTo}}, the file size is changed, but not 272314 + 284167 as expected, and the position also not move to the end.

Also I add append flag as I mentioned before, here the log changes to:

{noformat}
14/10/15 13:14:12 INFO util.Utils: outChannel before size, 0, before position: 0. Current size: 294896, current position: 294896, expected write data size: 294896
14/10/15 13:14:12 INFO util.Utils: outChannel before size, 294896, before position: 294896. Current size: 551216, current position: 551216, expected write data size: 256320
{noformat}

And my OS Redhat 6.2, and JVM is 1.8.0_20-ea.

Appreciate your suggestions, thanks a lot.
;;;","15/Oct/14 10:15;mridulm80;That is weird, I tried a stripped down version to test just this - and it seems to be working fine.

In scala interpreter, this seems to work as expected.
{code:java}

import java.io._
import java.nio.ByteBuffer
import java.nio._
import java.nio.channels._
  def copyStream(in: InputStream,
                 out: OutputStream,
                 closeStreams: Boolean = false): Long =
  {
    var count = 0L
    try {
      if (in.isInstanceOf[FileInputStream] && out.isInstanceOf[FileOutputStream]) {
        // When both streams are File stream, use transferTo to improve copy performance.
        val inChannel = in.asInstanceOf[FileInputStream].getChannel()
        val outChannel = out.asInstanceOf[FileOutputStream].getChannel()
println(""size = "" + outChannel.size)
println(""position = "" + outChannel.position)
        val size = inChannel.size()
        // In case transferTo method transferred less data than we have required.
        while (count < size) {
          count += inChannel.transferTo(count, size - count, outChannel)
        }
      } else {
        val buf = new Array[Byte](8192)
        var n = 0
        while (n != -1) {
          n = in.read(buf)
          if (n != -1) {
            out.write(buf, 0, n)
            count += n
          }
        }
}
      count
   } finally {
      if (closeStreams) {
        try {
          in.close()
        } finally {
          out.close()
        }
      }
    }
  }
val out = new FileOutputStream(""output"")
for (i <- 0 until 10) {
val in = new FileInputStream(""t"")
         val size = copyStream(in, out, false)
println(""size = "" + size + "" for i = "" + i)
in.close()
}
out.close()

{code}


Scenarios tried :
a) No ""output"" file.
b) Empty ""output"" file.
c) Non empty ""output"" file.

And it seemed to work fine (and as expected) for all the cases.
Can you try this at your end ? I want to eliminate any potential environment issues.
I tried this with 1.7.0_55 and 1.8.0_25 ...;;;","15/Oct/14 10:17;mridulm80;Note, ""t"" is just some file with a few strings in it - simply generate something locally.;;;","15/Oct/14 13:51;jerryshao;Hi [~mridulm80], thanks a lot for your suggestions, I tested with different Java version and two OS: Redhat 6.2 and Ubuntu 14.04, I think this behavior is OS or kernel related, not jvm version, you can see the output as below:

First is the result under Redhat, I tried Java 1.8.0_20, 1.7.0_60 and 1.7.0_04, the result is:

{noformat}
size = 0
position = 0
size = 118 for i = 0
size = 118
position = 0
size = 118 for i = 1
size = 118
position = 0
size = 118 for i = 2
size = 118
position = 0
size = 118 for i = 3
size = 118
position = 0
size = 118 for i = 4
size = 118
position = 0
size = 118 for i = 5
size = 118
position = 0
size = 118 for i = 6
size = 118
position = 0
size = 118 for i = 7
size = 118
position = 0
size = 118 for i = 8
size = 118
position = 0
size = 118 for i = 9
{noformat}

Obvious the position is always 0, so the final output file size is just 118.

Then changed to Ubuntu 14.04, with Java 1.7.0_04 and 1.7.0_51, the result shows as below:

{noformat}
size = 0
position = 0
size = 118 for i = 0
size = 118
position = 118
size = 118 for i = 1
size = 236
position = 236
size = 118 for i = 2
size = 354
position = 354
size = 118 for i = 3
size = 472
position = 472
size = 118 for i = 4
size = 590
position = 590
size = 118 for i = 5
size = 708
position = 708
size = 118 for i = 6
size = 826
position = 826
size = 118 for i = 7
size = 944
position = 944
size = 118 for i = 8
size = 1062
position = 1062
size = 118 for i = 9
{noformat}

This is the result we expected, and position also seek to the right end, so the final output file size is 1180 as expected.

My Ubuntu machine's kernel version is: 3.13.0-37-generic; and Redhat machine's kernel version is: 2.6.32-220.el6.x86_64.

So I guess maybe the difference relies on kernel version or OS version, but still need to verify.

Besides I think to keep the consistency, we can add *append=true* flag to enforce the same output even in different platforms.
;;;","15/Oct/14 14:07;jerryshao;So probably a kernel bug in 2.6.32, as you can see https://bugs.openjdk.java.net/browse/JDK-7052359?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ;;;","15/Oct/14 16:49;joshrosen;That kernel bug seems like it could explain why the {{transferTo}} change caused problems in sort-based shuffle.  It looks like SPARK-3630 also describes some other places where the Kryo PARSING_ERROR(2) has occurred, so I'm going to try to figure out which of these other cases might also hit this {{transferTo}} code path.

In terms of a bug fix / workaround: maybe we can open the file in append mode, since that seems to solve the problem, and also add an internal configuration option to disable the transferTo code; this configuration / feature-flag would provide an ""escape hatch"" for users in case the append fix doesn't work.;;;","15/Oct/14 16:50;mridulm80;[~jerryshao] great work !
I agree, append might be a workaround to consider (given the semantics of getChannel when stream is opened with append).
On other hand, since this piece of code might be used in general context also (the copyStreams) - what about logging a warning in case position != initialPosition + size at end of the transferTo loop ? Warning users that they should upgrade kernal ? (and explicitly modifying position as workaround)

;;;","15/Oct/14 20:38;joshrosen;[~mridulm80], that explicit position-checking and modification idea is a clever workaround.

I think that this JIRA explains the cause of the stream-corruption issues that we've seen in the sort-based shuffle code, but after some investigation I think that it doesn't explain the similar errors that we've seen when deserializing tasks that were broadcasted with TorrentBroadcast; I'll keep investigating this.;;;","16/Oct/14 04:32;jerryshao;Thanks for your suggestions. I think it would be better to add append flag and do some position-checking works after {{transferTo}}. 

I'm not sure about position modification, is that means by calling {{position(outChannel.size())}} to seek to the end of file? If so, I think we still face the same problem as I tested before in kernel 2.6.32; unless we add *append=true* flag, we cannot get the right file size even with explicit position modification, I just do a round of test with [~mridulm80] provided code by adding {{position(outChannel.size())}} before transferTo. But this is OK in my Ubuntu machine.

So I think we can only add append flag to workaround this issue. Besides, explicit position-modification may not meet user's requirement where user want to overwrite the outputstream by calling copyStream function.;;;","16/Oct/14 06:09;mridulm80;Not exactly, what I was suggesting was :

a) At begining of the transferTo method, add
{code: java}
  val initialPos = channel.position()
{code}
b) At bottom of the transferTo method, before returning size, add
{code: java}
  val finalPos = channel.position()
  if (finalPos == initialPos) {
    logWarning(""Hit kernal bug, upgrade kernal. Attempting workaround"")
    channel.position(initialPos + size)
  } else {
    assert(finalPos == initialPos + size)
  }
{code}


What I understand from the javadoc, this should alleviate the problem : ofcourse, will need verification on the setup you have where it is currently failing !
Note that the reason I would prefer this to append is for simple reason : the method is generic method to copy streams - and it might be used (currnetly, or in future) in scenarios where append is not true. So would be good to be defensive about the final state.;;;","16/Oct/14 06:30;jerryshao;Hi [~mridulm80], thanks a lot for your suggestions, here is the snippet I changed:

{code}
       val inChannel = in.asInstanceOf[FileInputStream].getChannel()
        val outChannel = out.asInstanceOf[FileOutputStream].getChannel()
        val initialPos = outChannel.position()
        println(""size = "" + outChannel.size)
        println(""initial position = "" + initialPos)
        val size = inChannel.size()
        // In case transferTo method transferred less data than we have required.
        while (count < size) {
          count += inChannel.transferTo(count, size - count, outChannel)
        }
        val finalPos = outChannel.position()
        println(""final position = "" + finalPos)
        if (initialPos == finalPos) {
          outChannel.position(initialPos + count)
        } else {
          assert(finalPos == initialPos + count)
        }
{code}

And the result shows as below:

{noformat}
size = 0
initial position = 0
final position = 0
size = 118 for i = 0
size = 118
initial position = 118
final position = 118
size = 118 for i = 1
size = 118
initial position = 236
final position = 236
size = 118 for i = 2
size = 118
initial position = 354
final position = 354
size = 118 for i = 3
size = 118
initial position = 472
final position = 472
size = 118 for i = 4
size = 118
initial position = 590
final position = 590
size = 118 for i = 5
size = 118
initial position = 708
final position = 708
size = 118 for i = 6
size = 118
initial position = 826
final position = 826
size = 118 for i = 7
size = 118
initial position = 944
final position = 944
size = 118 for i = 8
size = 118
initial position = 1062
final position = 1062
size = 118 for i = 9
{noformat}

Still has problem in my 2.6.32 machine, though position is moving forward, the file size is still 118. But it is OK in my Ubuntu machine, so probably this is not feasible.;;;","16/Oct/14 06:38;jerryshao;Besides I think we can add configuration as [~joshrosen] suggested, if the problem is met when using transferTo, we can log out the issue and warn people to disable transferTo, what's your suggestion.;;;","16/Oct/14 07:38;mridulm80;Damn, this sucks : the transferTo is not using the position of the channel to write output to ... while it is doing so when append is true (which is effectively setting position to end of file on call to getChannel).
The state of the channel, based on what we see above, is the same in both cases - since we can see the position is updated - and is persisted and returned when we call getChannel in next invocation of copyStreams. Not sure if sync() will help ...
So there is some other set of issues at play which we might not be able to workaround from the jvm.


Given this, I think we should 
a) add a logError when initialPosition == finalPosition when inChannel.size > 0 asking users to upgrade to a newer linux kernal
b) ofcourse use append = true : to workaround immediate issues.

(a) will ensure that developers and users/admins will be notified of issues in case other codepaths (currently or in future) hit the same issue.

;;;","16/Oct/14 08:28;jerryshao;Hi [~mridulm80],

Thanks a lot for your suggestion. I think currently for other codes in Spark which use copyStream will not be affected by this issue, since they only copy one input file to the output file. But for some use cases like ExternalSorter  will indeed be affected by it. I will submit a PR according to your suggestions, thanks a lot.;;;","16/Oct/14 13:58;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/2824;;;","20/Oct/14 17:22;joshrosen;Issue resolved by pull request 2824
[https://github.com/apache/spark/pull/2824];;;","20/Oct/14 17:42;rxin;How often does this bug manifest? If it is often enough, maybe we want to set the option to false by default instead of true. ;;;","20/Oct/14 17:45;joshrosen;[~rxin] The patch here should actually fix the issue; that option is just an ""escape hatch"" in case we were mistaken and our fix doesn't always work.  Users should not have to change any configuration options when applying this fix _unless_ the fix doesn't work for some reason.;;;","20/Oct/14 17:56;rxin;Ok then it sounds good. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.gitignore in /python includes wrong directory.,SPARK-3946,12747997,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tsudukim,tsudukim,tsudukim,14/Oct/14 11:01,14/Oct/14 21:10,14/Jul/23 06:26,14/Oct/14 21:10,1.1.0,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,".gitignore in /python includes {{docs/}} .
But /python/docs/* are the normal script or template files.
It seems to be indended to ignore output files of sphinx build, so it should be modified as {{docs/_build/}} .",,apachespark,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 11:07:50 UTC 2014,,,,,,,,,,"0|i215cv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"14/Oct/14 11:07;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/2796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write properties of hive-site.xml to HiveContext when initilize session state In SparkSQLEnv.scala,SPARK-3945,12747978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,luogankun,luogankun,luogankun,14/Oct/14 10:38,21/Oct/14 07:19,14/Jul/23 06:26,20/Oct/14 23:51,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Write properties of hive-site.xml to HiveContext when initilize session state In SparkSQLEnv.scala

The method of SparkSQLEnv.init() in HiveThriftServer2.scala cann't write the properties of hive-site.xml to HiveContext

example: 
hive-site.xml
<property>
	<name>spark.sql.shuffle.partititions</name>
	<value>99</value>
</property>
",centos6.3  hadoop-2.3.0-cdh5.0.0,apachespark,luogankun,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 07:19:10 UTC 2014,,,,,,,,,,"0|i2158n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/14 11:36;apachespark;User 'luogankun' has created a pull request for this issue:
https://github.com/apache/spark/pull/2800;;;","20/Oct/14 23:51;marmbrus;Issue resolved by pull request 2800
[https://github.com/apache/spark/pull/2800];;;","21/Oct/14 07:19;pwendell;Hey @luogankun would you mind adding a first and last name in your JIRA account? It makes it easier for us to give credit when writing release notes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
_remainingMem should not increase twice when updateBlockInfo,SPARK-3941,12747928,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,14/Oct/14 05:28,17/Oct/14 02:10,14/Jul/23 06:26,17/Oct/14 02:10,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In BlockManagermasterActor, _remainingMem would increase memSize for twice when updateBlockInfo if new storageLevel is invalid.",,apachespark,liyezhang556520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 05:46:34 UTC 2014,,,,,,,,,,"0|i214xj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"14/Oct/14 05:46;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/2792;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL console prints error messages three times,SPARK-3940,12747923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wangxj8,wangxj8,wangxj8,14/Oct/14 03:58,21/Oct/14 08:05,14/Jul/23 06:26,21/Oct/14 00:16,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,patch,,,,,"if an error of SQL，the console print Error three times。
eg：
{noformat}
spark-sql> show tablesss;
show tablesss;
14/10/13 20:56:29 INFO ParseDriver: Parsing command: show tablesss
NoViableAltException(26@[598:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | dropViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | createRoleStatement | dropRoleStatement | grantPrivileges | revokePrivileges | showGrants | showRoleGrants | grantRole | revokeRole );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:1962)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1298)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:938)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:190)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:161)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:218)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:226)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:50)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:49)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:184)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:183)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:221)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:98)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:274)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:209)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
14/10/13 20:56:30 ERROR SparkSQLDriver: Failed in [show tablesss]
org.apache.spark.sql.hive.HiveQl$ParseException: Failed to parse: show tablesss
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:225)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:50)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:49)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:184)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:183)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:221)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:98)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:274)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:209)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
Caused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:5 cannot recognize input near 'show' 'tablesss' '<EOF>' in ddl statement

	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:193)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:161)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:218)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:226)
	... 47 more
org.apache.spark.sql.hive.HiveQl$ParseException: Failed to parse: show tablesss
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:225)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:50)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:49)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:184)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:183)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:221)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:98)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:274)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:209)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
Caused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:5 cannot recognize input near 'show' 'tablesss' '<EOF>' in ddl statement

	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:193)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:161)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:218)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:226)
	... 47 more

14/10/13 20:56:30 ERROR CliDriver: org.apache.spark.sql.hive.HiveQl$ParseException: Failed to parse: show tablesss
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:225)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:50)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:49)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.hive.HiveQl$$anonfun$3.apply(HiveQl.scala:130)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:184)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:183)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:221)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:98)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:274)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:209)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
Caused by: org.apache.hadoop.hive.ql.parse.ParseException: line 1:5 cannot recognize input near 'show' 'tablesss' '<EOF>' in ddl statement

	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:193)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:161)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:218)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:226)
	... 47 more
{noformat}",,apachespark,marmbrus,pwendell,wangxj8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,180,180,,0%,180,180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 08:02:38 UTC 2014,,,,,,,,,,"0|i214wf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"14/Oct/14 04:05;apachespark;User 'wangxiaojing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2790;;;","21/Oct/14 00:16;marmbrus;Issue resolved by pull request 2790
[https://github.com/apache/spark/pull/2790];;;","21/Oct/14 07:21;pwendell;Hey [~wangxj8] can you add a first and last name in your JIRA account? It makes it easier for us to write release notes with proper credits.;;;","21/Oct/14 08:02;wangxj8;Hi [~pwendell] ,thank for your reminder,i have corrected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove auto join elimination and introduce aggregateMessages,SPARK-3936,12747868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ankurd,pedrorodriguez,pedrorodriguez,13/Oct/14 22:53,26/Nov/14 05:43,14/Jul/23 06:26,12/Nov/14 07:47,,,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,"This is actually a ticket with two separate problems:

1. Remove auto join elimination

2. Introduce a new fundamental primitive aggregateMessages

For the first one, description provided by Pedro:

There seems to be a bug with the GraphX byte code inspection, specifically in BytecodeUtils. 

These are the unit tests I wrote to expose the problem:
https://github.com/EntilZha/spark/blob/a3c38a8329545c034fae2458df134fa3829d08fb/graphx/src/test/scala/org/apache/spark/graphx/util/BytecodeUtilsSuite.scala#L93-L121

The first two tests pass, the second two tests fail. This exposes a problem with inspection of methods in closures, in this case within maps. Specifically, it seems like there is a problem with inspection of non-inline methods in a closure.


For the 2nd one, see pull request https://github.com/apache/spark/pull/3100",,apachespark,maropu,pedrorodriguez,prateekrungta,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3664,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 05:43:59 UTC 2014,,,,,,,,,,"0|i214lj:",9223372036854775807,,,,,ankurd,,,,,,,,,,,,,,,,,,,,"15/Oct/14 20:17;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2815;;;","05/Nov/14 01:57;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/3100;;;","26/Nov/14 05:43;rxin;BTW message from Taobao: 根据团队小伙伴的测试，[SPARK-3936] Add aggregateMessages, which supersedes mapReduceTriplets这个issue确实Work，可以提升30%的性能。以KCore为例，在standord的100w的数据集上运行作业，运行时间由 37mins减到22mins。使用GraphX的同学可以早日升级。#给周末此时还在加班做Spark和图计算的同学#

i.e. switching from mapReduceTriplets to aggregateMessages improved performance by 30% in their kcore implementation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unused variable in PairRDDFunctions.scala,SPARK-3935,12747832,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,scwf,scwf,scwf,13/Oct/14 20:46,18/Oct/14 01:08,14/Jul/23 06:26,18/Oct/14 01:08,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"There is a unused variable (count) in saveAsHadoopDataset function in PairRDDFunctions.scala. 
It is better to add a log statement to record the line of output. ",,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 04:57:14 UTC 2014,,,,,,,,,,"0|i214dz:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Oct/14 04:57;apachespark;User 'jackylk' has created a pull request for this issue:
https://github.com/apache/spark/pull/2791;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomForest bug in sanity check in DTStatsAggregator,SPARK-3934,12747807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,13/Oct/14 18:45,17/Oct/14 22:03,14/Jul/23 06:26,17/Oct/14 22:03,,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"When run with a mix of unordered categorical and continuous features, on multiclass classification, RandomForest fails.  The bug is in the sanity checks in getFeatureOffset and getLeftRightFeatureOffsets, which use the wrong indices for checking whether features are unordered.

Proposal: Remove the sanity checks since they are not really needed, and since they would require DTStatsAggregator to keep track of an extra set of indices (for the feature subset).",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 22:03:38 UTC 2014,,,,,,,,,,"0|i2149j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 18:51;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/2785;;;","13/Oct/14 23:24;srowen;Yep that fixes the issue I was seeing. Thanks! I can confirm it did not affect DecisionTree too, so it seems to match your analysis.;;;","17/Oct/14 22:03;mengxr;Issue resolved by pull request 2785
[https://github.com/apache/spark/pull/2785];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
result of JavaRDD collectAsMap() is not serializable,SPARK-3926,12747731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,aamend,aamend,13/Oct/14 14:44,17/Dec/14 20:14,14/Jul/23 06:26,17/Dec/14 20:14,1.0.2,1.1.0,1.1.1,1.2.0,,,,1.2.1,1.3.0,,,,,Java API,,,,0,,,,,,"Using the Java API, I want to collect the result of a RDD<String, String> as a HashMap using collectAsMap function:
Map<String, String> map = myJavaRDD.collectAsMap();
This works fine, but when passing this map to another function, such as...
myOtherJavaRDD.mapToPair(new CustomFunction(map))
...this leads to the following error:

Exception in thread ""main"" org.apache.spark.SparkException: Task not serializable

	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)

	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)

	at org.apache.spark.SparkContext.clean(SparkContext.scala:1242)

	at org.apache.spark.rdd.RDD.map(RDD.scala:270)

	at org.apache.spark.api.java.JavaRDDLike$class.mapToPair(JavaRDDLike.scala:99)

	at org.apache.spark.api.java.JavaPairRDD.mapToPair(JavaPairRDD.scala:44)

	../.. MY CLASS ../..

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328)

	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)

	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Caused by: java.io.NotSerializableException: scala.collection.convert.Wrappers$MapWrapper

	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)

	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)

	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)

	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)

	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)

	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)

	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)

	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)

	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)

	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)

at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:164)

This seems to be due to WrapAsJava.scala being non serializable
../..
  implicit def mapAsJavaMap[A, B](m: Map[A, B]): ju.Map[A, B] = m match {
    //case JConcurrentMapWrapper(wrapped) => wrapped
    case JMapWrapper(wrapped) => wrapped.asInstanceOf[ju.Map[A, B]]
    case _ => new MapWrapper(m)
  }
../..

The workaround is to manually wrapper this map into another one (serialized)
Map<String, String> map = myJavaRDD.collectAsMap();
Map<String, String> tmp = new HashMap<String, String>(map);
myOtherJavaRDD.mapToPair(new CustomFunction(tmp))
",CentOS / Spark 1.1 / Hadoop Hortonworks 2.4.0.2.1.2.0-402,aamend,apachespark,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:14:24 UTC 2014,,,,,,,,,,"0|i213t3:",9223372036854775807,,,,,,,,,,,,,,1.2.1,,,,,,,,,,,"13/Oct/14 15:16;srowen;Yeah, seems fine to just let {{MapWrapper}} implement {{Serializable}}, because standard Java {{Map}} implementations are as well. It's backwards-compatible so seems like an easy PR to submit if you like.;;;","14/Oct/14 19:52;srowen;Oops, embarrassed to say I didn't realize {{WrapAsJava.scala}} is a Scala class. Can't change that.
This requires subclassing {{MapWrapper}} to add {{java.io.Serializable}}. It still basically seems worthwhile to support this use case, so I'll propose it as a PR.;;;","14/Oct/14 19:55;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2805;;;","15/Oct/14 01:05;joshrosen;I opened a Scala JIRA to try to fix this: https://issues.scala-lang.org/browse/SI-8911.  In the meantime, I suppose that our own wrapper subclass works, although I wonder whether it's potentially serializing the wrapper's parent object or other undesirable things.;;;","15/Oct/14 07:29;srowen;Nice one, yes it should probably be changed upstream. {{MapWrapper}} extends {{java.util.AbstractMap}} and does not add fields, and this Spark wrapper is just another subclass that implements {{java.io.Serializable}} without adding anything else. It looks safe to serialize.;;;","18/Oct/14 19:43;joshrosen;This has been fixed by Sean's patch in 1.1.1 and 1.2.0.;;;","01/Dec/14 01:25;joshrosen;Just saw a bug report on the mailing list that looks possily related to this: http://apache-spark-user-list.1001560.n3.nabble.com/java-io-InvalidClassException-org-apache-spark-api-java-JavaUtils-SerializableMapWrapper-no-valid-cor-td20034.html

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in
stage 815081.0 failed 4 times, most recent failure: Lost task 0.3 in stage
815081.0 (TID 4751, ns2.xxxxx.net): java.io.InvalidClassException:
org.apache.spark.api.java.JavaUtils$SerializableMapWrapper; no valid
constructor
        at
java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)
        at
java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1775)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
        at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        at
org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:216)
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:177)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1000)
        at
org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
        at
org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
        at
org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
        at
org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:87)
        at
com.xxxxxxx.common.chores.kafka.kafkaListenerChore$4$1.call(kafkaListenerChore.java:418)
        at
com.xxxxxxx.common.chores.kafka.kafkaListenerChore$4$1.call(kafkaListenerChore.java:406)
        at
org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
        at
org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
        at
org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:775)
        at
org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:775)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

Any idea what's going on here?;;;","01/Dec/14 09:44;zsxwing;{quote}
To allow subtypes of non-serializable classes to be serialized, the subtype may assume responsibility for saving and restoring the state of the supertype's public, protected, and (if accessible) package fields. The subtype may assume this responsibility only if the class it extends has an accessible no-arg constructor to initialize the class's state. It is an error to declare a class Serializable if this is not the case. The error will be detected at runtime.
{quote}
From https://docs.oracle.com/javase/7/docs/api/java/io/Serializable.html

So it requires that `MapWrapper` must be serializable or has an accessible no-arg constructor.;;;","01/Dec/14 10:10;srowen;I see, there may be even two issues here -- constructor, and embedded field reference. Let me write a unit test to exercise this and see if this is something that can be amended to work or whether some other approach is needed.;;;","01/Dec/14 17:02;srowen;Yep, you're right, it's the constructor. I think there are two fixes:

- Copy the return value to a {{HashMap}} and return that. This requires a copy (of references only, note), so introduces some overhead, but is simple and works
- Copy the {{MapWrapper}} source and make it {{Serializable}} with a no-arg constructor.

I have the latter working but both are simple. Any preference?;;;","02/Dec/14 07:15;zsxwing;I checked the usage of `mapAsSerializableJavaMap` and found it's used in org.apache.spark.sql.api.java.Row: https://github.com/apache/spark/blob/23f966f47523f85ba440b4080eee665271f53b5e/sql/core/src/main/scala/org/apache/spark/sql/api/java/Row.scala#L122

It's called from `Row.get(i: Int)` -> `Row.toJavaValue`. Since `Row.get` may be used in some iteration and called frequently, the overhead of copying may be a lot. I suggest option 2): Copy the MapWrapper source and make it Serializable with a no-arg constructor. ;;;","03/Dec/14 19:59;srowen;I am reopening as it is not actually serializable without a no-arg constructor. PR coming shortly, that copies the implementation from Scala's Wrappers.MapWrapper.;;;","03/Dec/14 20:03;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3587;;;","09/Dec/14 00:16;joshrosen;I've merged [~srowen]'s patch into {{master}} and {{branch-1.1}} and have tagged this for a {{branch-1.2}} backport.  Thanks!;;;","17/Dec/14 20:14;joshrosen;I've merged this into {{branch-1.2}} for inclusion in 1.2.1, so I'm marking this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not consider the ordering of qualifiers during comparison,SPARK-3925,12747726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,viirya,viirya,13/Oct/14 13:53,26/Oct/14 21:29,14/Jul/23 06:26,26/Oct/14 21:29,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The qualifiers orderings should not be considered during the comparison between old qualifiers and new qualifiers when calling 'withQualifiers'.
",,apachespark,marmbrus,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 26 21:29:41 UTC 2014,,,,,,,,,,"0|i213rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 13:54;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/2783;;;","26/Oct/14 21:29;marmbrus;Issue resolved by pull request 2783
[https://github.com/apache/spark/pull/2783];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All Standalone Mode services time out with each other,SPARK-3923,12747662,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,adav,ilikerps,ilikerps,13/Oct/14 07:29,17/Oct/14 01:58,14/Jul/23 06:26,17/Oct/14 01:58,1.2.0,,,,,,,1.2.0,,,,,,Deploy,,,,0,,,,,,"I'm seeing an issue where it seems that components in Standalone Mode (Worker, Master, Driver, and Executor) all seem to time out with each other after around 1000 seconds. Here is an example log:

{code}
14/10/13 06:43:55 INFO Master: Registering worker ip-10-0-147-189.us-west-2.compute.internal:38922 with 4 cores, 29.0 GB RAM
14/10/13 06:43:55 INFO Master: Registering worker ip-10-0-175-214.us-west-2.compute.internal:42918 with 4 cores, 59.0 GB RAM
14/10/13 06:43:56 INFO Master: Registering app Databricks Shell
14/10/13 06:43:56 INFO Master: Registered app Databricks Shell with ID app-20141013064356-0000

... precisely 1000 seconds later ...

14/10/13 07:00:35 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkWorker@ip-10-0-147-189.us-west-2.compute.internal:38922] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
14/10/13 07:00:35 INFO Master: akka.tcp://sparkWorker@ip-10-0-147-189.us-west-2.compute.internal:38922 got disassociated, removing it.
14/10/13 07:00:35 INFO LocalActorRef: Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.147.189%3A54956-1#1529980245] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/10/13 07:00:35 INFO Master: akka.tcp://sparkWorker@ip-10-0-175-214.us-west-2.compute.internal:42918 got disassociated, removing it.
14/10/13 07:00:35 INFO Master: Removing worker worker-20141013064354-ip-10-0-175-214.us-west-2.compute.internal-42918 on ip-10-0-175-214.us-west-2.compute.internal:42918
14/10/13 07:00:35 INFO Master: Telling app of lost executor: 1
14/10/13 07:00:35 INFO Master: akka.tcp://sparkWorker@ip-10-0-175-214.us-west-2.compute.internal:42918 got disassociated, removing it.
14/10/13 07:00:35 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkWorker@ip-10-0-175-214.us-west-2.compute.internal:42918] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
14/10/13 07:00:35 INFO LocalActorRef: Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.175.214%3A35958-2#314633324] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/10/13 07:00:35 INFO LocalActorRef: Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.175.214%3A35958-2#314633324] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/10/13 07:00:36 INFO ProtocolStateActor: No response from remote. Handshake timed out or transport failure detector triggered.
14/10/13 07:00:36 INFO Master: akka.tcp://sparkDriver@ip-10-0-175-215.us-west-2.compute.internal:58259 got disassociated, removing it.
14/10/13 07:00:36 INFO LocalActorRef: Message [akka.remote.transport.AssociationHandle$InboundPayload] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.175.215%3A41987-3#1944377249] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/10/13 07:00:36 INFO Master: Removing app app-20141013064356-0000
14/10/13 07:00:36 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkDriver@ip-10-0-175-215.us-west-2.compute.internal:58259] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
14/10/13 07:00:36 INFO LocalActorRef: Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.175.215%3A41987-3#1944377249] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/10/13 07:00:36 INFO LocalActorRef: Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.175.215%3A41987-3#1944377249] was not delivered. [7] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/10/13 07:00:36 INFO Master: akka.tcp://sparkDriver@ip-10-0-175-215.us-west-2.compute.internal:58259 got disassociated, removing it.
{code}

Note that the driver and master are living on the same machine, and there is no load to speak of at the time (so no GC). Also everything disconnecting exactly 1000 seconds after initial connection is pretty suspicious.",,apachespark,huangjs,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 20:37:04 UTC 2014,,,,,,,,,,"0|i213dr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Oct/14 07:43;ilikerps;I did a little digging hoping to find some post about this, no particular luck. I did find [this post|https://groups.google.com/forum/#!topic/akka-user/X3xzpTCbEFs] which recommends using an interval time < pause, which we are not doing. This doesn't seem to explain the services all timing out after the heartbeat interval time (which is currently 1000 seconds), but may be good to know in the future.;;;","13/Oct/14 18:21;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/2784;;;","13/Oct/14 20:37;huangjs;I have similar problem in YARN-client mode. Setting spark.akka.heartbeat.interval to 100 fixes the problem.

This is a critical bug.

P.S.
One thing made me very confused during debuggin is the error message. The important one

  WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkDriver@xxx:50278] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].

is of Log Level WARN.


Jianshi
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WorkerWatcher in Standalone mode fail to come up due to invalid workerUrl,SPARK-3921,12747656,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilikerps,ilikerps,ilikerps,13/Oct/14 06:17,14/Oct/14 06:32,14/Jul/23 06:26,14/Oct/14 06:32,1.2.0,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"As of [this commit|https://github.com/apache/spark/commit/79e45c9323455a51f25ed9acd0edd8682b4bbb88#diff-79391110e9f26657e415aa169a004998R153], standalone mode appears to have lost its WorkerWatcher, because of the swapped workerUrl and appId parameters. We still put workerUrl before appId when we start standalone executors, and the Executor misinterprets the appId as the workerUrl and fails to create the WorkerWatcher.

Note that this does not seem to crash the Standalone executor mode, despite the failing of the WorkerWatcher during its constructor.",,andrewor14,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 07:40:27 UTC 2014,,,,,,,,,,"0|i213cf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Oct/14 07:40;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/2779;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forget Unpersist in RandomForest.scala(train Method),SPARK-3918,12747573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,Junlong Liu,Junlong Liu,12/Oct/14 11:59,11/Dec/14 17:02,14/Jul/23 06:26,11/Dec/14 17:02,1.1.0,,,,,,,1.2.0,,,,,,MLlib,,,,0,decisiontree,train,unpersist,,,"   In version 1.1.0 DecisionTree.scala, train Method, treeInput has been persisted in Memory, but without unpersist. It caused heavy DISK usage.
   In github version(1.2.0 maybe), RandomForest.scala, train Method, baggedInput has been persisted but without unpersisted too.",All,apachespark,josephkb,Junlong Liu,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 11 17:02:37 UTC 2014,,,,,,,,,,"0|i212uf:",9223372036854775807,,,,,paulbits,,,,,,,,,1.2.0,,,,,,,,,,,"13/Oct/14 18:51;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/2785;;;","10/Dec/14 06:48;srowen;[~josephkb] The PR you opened for this an another issue was merged, and notes indicate it should remove an unpersist call, but I didn't see that in the change? https://github.com/apache/spark/pull/2785/files

Is this just obsoleted or was the change not in the PR actually?;;;","10/Dec/14 20:14;josephkb;Oops!  I forgot to update that PR's name.  It was originally in that PR, but [~Junlong Liu] sent a PR with the change first:
[https://github.com/apache/spark/commit/942847fd94c920f7954ddf01f97263926e512b0e]

(The PR linked above was not tagged with this JIRA.);;;","11/Dec/14 17:02;srowen;Great, looks like this was in fact fixed for 1.2 then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryRelation should inherit statistics of its child to enable broadcast join,SPARK-3914,12747559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,12/Oct/14 03:58,02/Nov/14 15:41,14/Jul/23 06:26,02/Nov/14 15:41,1.1.0,,,,,,,,,,,,,SQL,,,,0,,,,,,"When a table/query is cached, {{InMemoryRelation}} stores the physical plan rather than the logical plan of the original table/query, thus loses the statistics information and disables broadcast join optimization.

Sample {{spark-shell}} session to reproduce this issue:
{code}
val sparkContext = sc

import org.apache.spark.sql._
import sparkContext._

val sqlContext = new SQLContext(sparkContext)

import sqlContext._

case class Sale(year: Int)
makeRDD((1 to 100).map(Sale(_))).registerTempTable(""sales"")
sql(""select distinct year from sales limit 10"").registerTempTable(""tinyTable"")
cacheTable(""tinyTable"")
sql(""select * from sales join tinyTable on sales.year = tinyTable.year"").queryExecution.executedPlan

...

res3: org.apache.spark.sql.execution.SparkPlan =
Project [year#4,year#5]
 ShuffledHashJoin [year#4], [year#5], BuildRight
  Exchange (HashPartitioning [year#4], 200)
   PhysicalRDD [year#4], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:37
  Exchange (HashPartitioning [year#5], 200)
   InMemoryColumnarTableScan [year#5], [], (InMemoryRelation [year#5], false, 1000, StorageLevel(true, true, false, true, 1), (Limit 10))
{code}
A workaround for this is to add a {{LIMIT}} operator above the {{InMemoryColumnarTableScan}} operator:
{code}
sql(""select * from sales join (select * from tinyTable limit 10) tiny on sales.year = tiny.year"").queryExecution.executedPlan

...

res8: org.apache.spark.sql.execution.SparkPlan =
Project [year#12,year#13]
 BroadcastHashJoin [year#12], [year#13], BuildRight
  PhysicalRDD [year#12], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:37
  Limit 10
   InMemoryColumnarTableScan [year#13], [], (InMemoryRelation [year#13], false, 1000, StorageLevel(true, true, false, true, 1), (Limit 10))
{code}",,apachespark,lian cheng,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 17:36:09 UTC 2014,,,,,,,,,,"0|i212rb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"20/Oct/14 17:36;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2860;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FlumeStreamSuite is flaky, fails either with port binding issues or data not being reliably sent",SPARK-3912,12747545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,12/Oct/14 00:19,19/Dec/14 16:13,14/Jul/23 06:26,14/Oct/14 05:47,1.1.0,,,,,,,1.2.0,,,,,,DStreams,,,,0,flaky-test,,,,,"Two problems.

1. Attempts to start the service to start on different possible ports (to avoid bind failures) was incorrect as the service is actually start lazily (when receiver starts, not when the flume input stream is created). 
2. Lots of Thread.sleep was used to improve the probabilities that data sent through avro to flume receiver was being sent. However, the sending may fail for various unknown reasons, causing the test to fail.
3. Thread.sleep was also used to send one record per batch and checks were made on whether only one records was received in every batch. This was an overkill because all we need to test in this unit test is whether data is being sent and received or not, not about timings.",,apachespark,nchammas,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 21:22:56 UTC 2014,,,,,,,,,,"0|i212o7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Oct/14 21:22;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A corrupted format in Sphinx documents and building warnings,SPARK-3909,12747494,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,cocoatomo,cocoatomo,11/Oct/14 10:17,11/Oct/14 18:52,14/Jul/23 06:26,11/Oct/14 18:52,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,docstrings,pyspark,,,,"Sphinx documents contains a corrupted ReST format and have some warnings.

The purpose of this issue is same as https://issues.apache.org/jira/browse/SPARK-3773.

commit: 0e8203f4fb721158fb27897680da476174d24c4b

output
{noformat}
$ cd ./python/docs
$ make clean html
rm -rf _build/*
sphinx-build -b html -d _build/doctrees   . _build/html
Making output directory...
Running Sphinx v1.2.3
loading pickled environment... not yet created
building [html]: targets for 4 source files that are out of date
updating environment: 4 added, 0 changed, 0 removed
reading sources... [100%] pyspark.sql                                                                                                                                                                     
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/feature.py:docstring of pyspark.mllib.feature.Word2VecModel.findSynonyms:4: WARNING: Field list ends without a blank line; unexpected unindent.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/feature.py:docstring of pyspark.mllib.feature.Word2VecModel.transform:3: WARNING: Field list ends without a blank line; unexpected unindent.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/sql.py:docstring of pyspark.sql:4: WARNING: Bullet list ends without a blank line; unexpected unindent.
looking for now-outdated files... none found
pickling environment... done
checking consistency... done
preparing documents... done
writing output... [100%] pyspark.sql                                                                                                                                                                      
writing additional files... (12 module code pages) _modules/index search
copying static files... WARNING: html_static_path entry u'/Users/<user>/MyRepos/Scala/spark/python/docs/_static' does not exist
done
copying extra files... done
dumping search index... done
dumping object inventory... done
build succeeded, 4 warnings.

Build finished. The HTML pages are in _build/html.
{noformat}","Mac OS X 10.9.5, Python 2.7.8, Jinja2==2.7.3, MarkupSafe==0.23, Pygments==1.6, Sphinx==1.2.3, docutils==0.12, numpy==1.9.0, wsgiref==0.1.2",cocoatomo,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 11 18:52:16 UTC 2014,,,,,,,,,,"0|i212db:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"11/Oct/14 18:52;joshrosen;Issue resolved by pull request 2766
[https://github.com/apache/spark/pull/2766];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The keys for sorting the columns of Executor page ,Stage page Storage page  are incorrect",SPARK-3905,12747481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,11/Oct/14 07:10,13/Oct/14 07:43,14/Jul/23 06:26,13/Oct/14 05:51,1.0.2,1.1.0,1.2.0,,,,,1.1.1,1.2.0,,,,,Web UI,,,,0,,,,,,,,apachespark,gq,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 07:43:15 UTC 2014,,,,,,,,,,"0|i212af:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 05:51;joshrosen;Fixed by https://github.com/apache/spark/pull/2763;;;","13/Oct/14 07:43;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HQL doesn't support the ConstantObjectInspector to pass into UDFs,SPARK-3904,12747480,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,11/Oct/14 06:45,07/May/15 07:49,14/Jul/23 06:26,29/Oct/14 02:12,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"In HQL, we convert all of the data type into normal ObjectInspectors for UDFs, most of cases it work, however, some of the UDF actually requires the input ObjectInspector to be the ConstantObjectInspector, which will cause exception.
e.g.
{panel}
select named_struct(""x"", ""str"") from src limit 1
{panel}

It will throws exception like
{panel}
14/10/10 16:25:17 INFO parse.ParseDriver: Parsing command: select named_struct(""x"", ""str"") from src
14/10/10 16:25:17 INFO parse.ParseDriver: Parse Completed
14/10/10 16:25:17 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=src
14/10/10 16:25:17 INFO HiveMetaStore.audit: ugi=hcheng	ip=unknown-ip-addr	cmd=get_table : db=default tbl=tmp2	
14/10/10 16:25:17 ERROR thriftserver.SparkSQLDriver: Failed in [select named_struct(""x"", ""str"") from src]
org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: Even arguments to NAMED_STRUCT must be a constant STRING.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector@2f2dbcfc
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.initialize(GenericUDFNamedStruct.java:55)
	at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector$lzycompute(hiveUdfs.scala:129)
	at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector(hiveUdfs.scala:129)
	at org.apache.spark.sql.hive.HiveGenericUdf.eval(hiveUdfs.scala:158)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$6$$anonfun$applyOrElse$2.applyOrElse(Optimizer.scala:267)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$6$$anonfun$applyOrElse$2.applyOrElse(Optimizer.scala:260)
{panel}",,apachespark,baishuo,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 07:49:34 UTC 2015,,,,,,,,,,"0|i212a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/14 14:41;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2762;;;","29/Oct/14 02:12;marmbrus;Issue resolved by pull request 2762
[https://github.com/apache/spark/pull/2762];;;","07/May/15 07:49;baishuo;please referrence https://github.com/apache/spark/pull/5660, thanks:);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationMaster's shutdown hook fails and IllegalStateException is thrown.,SPARK-3900,12747329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,sarutak,sarutak,10/Oct/14 16:15,02/Dec/22 22:52,14/Jul/23 06:26,24/Oct/14 13:52,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"ApplicationMaster registers a shutdown hook and it calls ApplicationMaster#cleanupStagingDir.

cleanupStagingDir invokes FileSystem.get(yarnConf) and it invokes FileSystem.getInternal. FileSystem.getInternal also registers shutdown hook.
In FileSystem of hadoop 0.23, the shutdown hook registration does not consider whether shutdown is in progress or not (In 2.2, it's considered).

{code}
// 0.23 
if (map.isEmpty() ) {
  ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);
}
{code}

{code}
// 2.2
if (map.isEmpty()
            && !ShutdownHookManager.get().isShutdownInProgress()) {
   ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);
}
{code}

Thus, in 0.23, another shutdown hook can be registered when ApplicationMaster's shutdown hook run.

This issue cause IllegalStateException as follows.

{code}
java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2306)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:162)
        at org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$cleanupStagingDir(ApplicationMaster.scala:307)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:118)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{code}",Hadoop 0.23,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-41313,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 29 05:59:04 UTC 2022,,,,,,,,,,"0|i211ef:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Oct/14 08:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2755;;;","24/Oct/14 08:34;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2924;;;","29/Nov/22 05:58;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","29/Nov/22 05:58;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","29/Nov/22 05:59;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong links in streaming doc,SPARK-3899,12747245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,10/Oct/14 09:13,13/Oct/14 06:36,14/Jul/23 06:26,13/Oct/14 06:36,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Documentation,,,,0,,,,,,,,joshrosen,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 06:36:27 UTC 2014,,,,,,,,,,"0|i210vz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"13/Oct/14 06:36;joshrosen;Issue resolved by pull request 2749
[https://github.com/apache/spark/pull/2749];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Hive Percentile UDAF with array of percentile values,SPARK-3891,12747188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gvramana,chinnitv,chinnitv,10/Oct/14 02:33,17/Dec/14 23:41,14/Jul/23 06:26,17/Dec/14 23:41,1.2.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Spark PR 2620 brings in the support of Hive percentile UDAF.
However Hive percentile and percentile_approx UDAFs also support returning an array of percentile values with the syntax
percentile(BIGINT col, array(p1 [, p2]...)) or 
percentile_approx(DOUBLE col, array(p1 [, p2]...) [, B])

These queries are failing with the below error:

0: jdbc:hive2://dev-uuppala.sfohi.philips.com> select name, percentile(turnaroundtime,array(0,0.25,0.5,0.75,1)) from exam group by name;

Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 25.0 failed 4 times, most recent failure: Lost task 1.3 in stage 25.0 (TID 305, Dev-uuppala.sfohi.philips.com): java.lang.ClassCastException: scala.collection.mutable.ArrayBuffer cannot be cast to [Ljava.lang.Object;
        org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListLength(StandardListObjectInspector.java:83)
        org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$ListConverter.convert(ObjectInspectorConverters.java:259)
        org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper.convertIfNecessary(GenericUDFUtils.java:349)
        org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge$GenericUDAFBridgeEvaluator.iterate(GenericUDAFBridge.java:170)
        org.apache.spark.sql.hive.HiveUdafFunction.update(hiveUdfs.scala:342)
        org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:167)
        org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:151)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace: (state=,code=0)","Spark 1.2.0 trunk (ac302052870a650d56f2d3131c27755bb2960ad7) on
CDH 5.1.0
Centos 6.5
8x 2GHz, 24GB RAM",apachespark,chinnitv,gvramana,marmbrus,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4263,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 23:41:47 UTC 2014,,,,,,,,,,"0|i210jb:",9223372036854775807,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,"14/Oct/14 14:49;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/2802;;;","14/Oct/14 14:53;gvramana;Following problems need to be fixed to passing array to percentile and percentile_approx UDAFs
1. percentile UDAF the parameters are not wrapped before passing to UDAF
2. percentile_approx takes only constant inspector as parameter, so constant inspectors support needs to be added to GenericUDAF.;;;","17/Dec/14 23:41;marmbrus;Issue resolved by pull request 2802
[https://github.com/apache/spark/pull/2802];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JVM dies with SIGBUS, resulting in ConnectionManager failed ACK",SPARK-3889,12747167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilikerps,ilikerps,ilikerps,10/Oct/14 00:50,01/Mar/15 06:58,14/Jul/23 06:26,10/Oct/14 08:44,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Here's the first part of the core dump, possibly caused by a job which shuffles a lot of very small partitions.

{code}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0x7) at pc=0x00007fa5885fcdb0, pid=488, tid=140343502632704
#
# JRE version: 7.0_25-b30
# Java VM: OpenJDK 64-Bit Server VM (23.7-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# v  ~StubRoutines::jbyte_disjoint_arraycopy
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   https://bugs.launchpad.net/ubuntu/+source/openjdk-7/
#

---------------  T H R E A D  ---------------

Current thread (0x00007fa4b0631000):  JavaThread ""Executor task launch worker-170"" daemon [_thread_in_Java, id=6783, stack(0x00007fa4448ef000,0x00007fa4449f0000)]

siginfo:si_signo=SIGBUS: si_errno=0, si_code=2 (BUS_ADRERR), si_addr=0x00007fa428f79000
{code}

Here is the only useful content I can find related to JVM and SIGBUS from Google: https://bugzilla.redhat.com/show_bug.cgi?format=multiple&id=976664

It appears it may be related to disposing byte buffers, which we do in the ConnectionManager -- we mmap shuffle files via ManagedBuffer and dispose of them in BufferMessage.",,cfregly,fryz,idanzalz,ilikerps,mridulm80,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 01 06:58:57 UTC 2015,,,,,,,,,,"0|i210f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/14 18:57;mridulm80;The status says fixed - what was done to resolve this ? I did not see a PR ...;;;","10/Oct/14 22:14;ilikerps;Sorry, it was not linked: https://github.com/apache/spark/pull/2742;;;","20/Oct/14 20:10;fryz;Any chance of getting this backported to 1.1.1? 
If not, what about the possibility of investigation another fix for 1.1.1? 

Its a pretty big bummer that this just causes the JVM to crash, and I was wondering if this is something that would be considered for backport. 

 
;;;","21/Oct/14 04:25;pwendell;[~fryz] I think the relevant code path here did not exist in Spark 1.1, so this can't be backported. Have you observed SIGBUS errors in Spark 1.1?;;;","10/Feb/15 04:33;idanzalz;I actually got this error on 1.1:
{noformat}
 #
 # A fatal error has been detected by the Java Runtime Environment:
 #
 #  SIGBUS (0x7) at pc=0x00007f8c7d0420a0, pid=57386, tid=140240086370048
 #
 # JRE version: Java(TM) SE Runtime Environment (7.0_55-b13) (build 1.7.0_55-b13)
 # Java VM: Java HotSpot(TM) 64-Bit Server VM (24.55-b03 mixed mode linux-amd64 compressed oops)
 # Problematic frame:
 # v  ~StubRoutines::jshort_disjoint_arraycopy
 #
 # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
 #
{noformat};;;","10/Feb/15 18:51;ilikerps;The only place we memory map in 1.1 is this method: https://github.com/apache/spark/blob/branch-1.1/core/src/main/scala/org/apache/spark/storage/DiskStore.scala#L106

This threshold is configurable with ""spark.storage.memoryMapThreshold"" -- we upped the default from 2 KB to 2 MB in 1.2, which you could try here as well.;;;","01/Mar/15 04:04;idanzalz;Hi,
I am still getting the same error with spark 1.2.1 (sporadically):
{noformat}
#
# A fatal error has been detected by the Java Runtime Environment:
# 
#  SIGBUS (0x7) at pc=0x00007ff5ed042220, pid=3694, tid=140692916811520
#
# JRE version: Java(TM) SE Runtime Environment (7.0_55-b13) (build 1.7.0_55-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.55-b03 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# v  ~StubRoutines::jint_disjoint_arraycopy
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
{noformat}

Should we re-open this one, or open a new ticket?;;;","01/Mar/15 06:13;ilikerps;This may be a new issue. I would open a new ticket, especially because the ""ConnectionManager failed ACK"" thing shouldn't be happening in 1.2.1; there should be different symptoms and perhaps a different cause as well.

A last ditch thing to try, by the way, is to up spark.storage.memoryMapThreshold to a very large number (e.g., 1 GB in bytes) and see if it still occurs -- if so, then please report more details about your workload and any other possible symptoms you see.;;;","01/Mar/15 06:58;mridulm80;[~adav] I have seen this a lot in my recent tests with the latest 1.3 and with the stable 1.2.1 version.
Though usually the reasons are legitimate upon investigation - like remote side died via SIGTERM from yarn, etc.

So would be good to know if this happened legitimately or due to some other bug/issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide mechanism to remove accumulators once they are no longer used,SPARK-3885,12747130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilganeli,joshrosen,joshrosen,09/Oct/14 21:26,28/Feb/15 19:54,14/Jul/23 06:26,23/Feb/15 06:58,1.0.2,1.1.0,1.2.0,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Spark does not currently provide any mechanism to delete accumulators after they are no longer used.  This can lead to OOMs for long-lived SparkContexts that create many large accumulators.

Part of the problem is that accumulators are registered in a global {{Accumulators}} registry.  Maybe the fix would be as simple as using weak references in the Accumulators registry so that accumulators can be GC'd once they can no longer be used.

In the meantime, here's a workaround that users can try:

Accumulators have a public setValue() method that can be called (only by the driver) to change an accumulator’s value.  You might be able to use this to reset accumulators’ values to smaller objects (e.g. the “zero” object of whatever your accumulator type is, or ‘null’ if you’re sure that the accumulator will never be accessed again).

This issue was originally reported by [~nkronenfeld] on the dev mailing list: http://apache-spark-developers-list.1001551.n3.nabble.com/Fwd-Accumulator-question-td8709.html",,apachespark,codingcat,ian.springer,ilganeli,joshrosen,nkronenfeld,ravi.pesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4030,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 28 19:54:52 UTC 2015,,,,,,,,,,"0|i21073:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/14 05:53;nkronenfeld;I tried reusing accumulators and clearing them, but I'm still running out of memory.

According to the profiler, there is a lot still held by Accumulators$.localAccums.

If I read things right, this is where the workers hold on to their versions of the accumulator?

It looks like there is a clear call, but it looks to me like it's only run at the beginning of a task (which means it should be keeping these around at the end of a job, until another task is run on that thread - which makes it a bit tough to profile.

Am I reading all this correctly? If so, I can see why it isn't cleared out at the end of a job - there's probably no way of doing that safely, since the workers don't know when the end is.

Ideally, I'd love a call whereby I can explicitly release an accumulator.  It seems to me that would require a parallel map in Accumulators$ that kept track of the threads on which each accumulator was stored, so it could clear them.

Am I understanding this all correctly?  If so, I think I could put together the fix I describe pretty easily.;;;","07/Jan/15 19:32;ilganeli;Hi [~joshrosen], I can knock this one out - could you please assign it to me. One minor question, both here and in the code there is a TODO recommending using soft references. However, a soft reference will not be released by default, only when the garbage collector explicitly needs more memory. Is there any reason it can't be made a weak reference instead?;;;","13/Jan/15 17:54;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/4021;;;","23/Feb/15 06:58;joshrosen;Issue resolved by pull request 4021
[https://github.com/apache/spark/pull/4021];;;","28/Feb/15 19:54;joshrosen;I found a correctness issue in this patch, which I'll fix shortly: see SPARK-6075;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If deploy mode is cluster, --driver-memory shouldn't apply to client JVM",SPARK-3884,12747128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,sandyr,sandyr,09/Oct/14 21:18,16/Jul/15 00:52,14/Jul/23 06:26,01/Apr/15 11:35,1.1.0,1.2.0,1.3.0,,,,,1.4.0,,,,,,Spark Submit,,,,0,,,,,,,,glenn.strycker@gmail.com,jimbobhickville,rnirmal,sandyr,skrasser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4299,SPARK-9088,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 11:35:35 UTC 2015,,,,,,,,,,"0|i2106n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/14 21:27;sandyr;Accidentally assigned this to myself, but others should feel free to pick it up;;;","05/Nov/14 19:45;jimbobhickville;For example:

{noformat}
spark-submit --master yarn-cluster --driver-memory 12424m --class org.apache.spark.examples.SparkPi /usr/lib/spark-yarn/lib/spark-examples*.jar 1000
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000006fd280000, 4342677504, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 4342677504 bytes for committing reserved memory
{noformat}

The client machine didn't have that much memory available, but the YARN nodes where the driver will be loaded do.  It shouldn't be trying to allocate the memory locally.;;;","01/Apr/15 11:35;srowen;This is fixed in 1.4 due to the new launcher implementation. I verified that in yarn-cluster mode the SparkSubmit JVM is not run with -Xms / -Xmx set, but instead passes through spark.driver.memory in --conf. In yarn-client mode, it does set -Xms / -Xmx.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The exit code of spark-submit is still 0 when an yarn application fails,SPARK-3877,12746990,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,09/Oct/14 10:52,17/Feb/17 23:20,14/Jul/23 06:26,22/Oct/14 22:08,1.1.0,,,,,,,1.1.1,1.2.0,,,,,YARN,,,,0,yarn,,,,,"When an yarn application fails (yarn-cluster mode), the exit code of spark-submit is still 0. It's hard for people to write some automatic scripts to run spark jobs in yarn because the failure can not be detected in these scripts.",,apachespark,j_caplan,sams,tgraves,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11854,,,,SPARK-11854,,,,,,,,,,,,,,SPARK-19649,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 17 23:20:46 UTC 2017,,,,,,,,,,"0|i20zcv:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"09/Oct/14 11:00;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2732;;;","09/Oct/14 13:36;tgraves;this looks like a dup of SPARK-2167.  Or actually perhaps a subset of that since I think you only handle the yarn mode.   Does this cover both client and cluster mode?

;;;","17/Oct/14 00:08;vanzin;[~tgraves] this can be seen as a subset of SPARK-2167, but as I mentioned on that bug, I don't think it's fixable for all cases. SparkSubmit is executing user code, so it can only report errors when the user code does. 

e.g., a job like this would report an error today

{code}
  val sc = ...
  try {
    // do stuff
    if (somethingBad) throw MyJobFailedException()
  } finally {
    sc.stop()
  }
{code}

But this one wouldn't:

{code}
  val sc = ...
  try {
    // do stuff
    if (somethingBad) throw MyJobFailedException()
  } catch {
    case e: Exception => logError(""Oops, something bad happened."", e)
  } finally {
    sc.stop()
  }
{code}

yarn-client mode will abruptly stop the SparkContext when the Yarn app fails. But depending on how the user's {{main()}} deals with errors, that still may not result in a non-zero exit status.;;;","17/Oct/14 12:29;tgraves;[~vanzin]  I agree. The user code should be exiting with non-zero or throwing on failure.  If they aren't then there is nothing we can do about it, other then tell them to change their code to properly exit if they want to see failure status. Perhaps we should better document what they should do on failure too.   Its basically the same I did for the exit codes in ApplicationMaster. It relies on user code exiting non-zero and throwing.

The only other option would be for us to actually look at the details in the scheduler ourselves to try to determine what happened.  ie we see Stage X failed or Y tasks failed, etc.  I would say we do that later if its needed. 

;;;","22/Oct/14 22:08;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2748;;;","18/Nov/15 19:00;sams;Is this really fixed?? I'm getting this on 1.5.0 using EMR.

[~tgraves]

[~vanzin]

[~zsxwing];;;","19/Nov/15 23:05;sams;Actually ignore, as per comment in duplicate, can't seem to reproduce.;;;","09/Jan/17 20:24;j_caplan;I think you have created a race condition with this fix which I am encountering about 50% of the time, using Spark 1.6.3.  I have configured YARN not to keep *any* recent jobs in memory, as some of my jobs get pretty large.

yarn-site	yarn.resourcemanager.max-completed-applications	0

The once-per-second call to getApplicationReport may thus encounter a RUNNING application followed by a not found application, and report a false negative.

(typical) Executor log:
17/01/09 19:31:23 INFO ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
17/01/09 19:31:23 INFO SparkContext: Invoking stop() from shutdown hook
17/01/09 19:31:24 INFO SparkUI: Stopped Spark web UI at http://10.0.0.168:37046
17/01/09 19:31:24 INFO YarnClusterSchedulerBackend: Shutting down all executors
17/01/09 19:31:24 INFO YarnClusterSchedulerBackend: Asking each executor to shut down
17/01/09 19:31:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/09 19:31:24 INFO MemoryStore: MemoryStore cleared
17/01/09 19:31:24 INFO BlockManager: BlockManager stopped
17/01/09 19:31:24 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/09 19:31:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/09 19:31:24 INFO SparkContext: Successfully stopped SparkContext
17/01/09 19:31:24 INFO ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
17/01/09 19:31:24 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/09 19:31:24 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/09 19:31:24 INFO AMRMClientImpl: Waiting for application to be successfully unregistered.
17/01/09 19:31:24 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.

Client log:
17/01/09 19:31:23 INFO Client: Application report for application_1483983939941_0056 (state: RUNNING)
17/01/09 19:31:24 ERROR Client: Application application_1483983939941_0056 not found.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1483983939941_0056 is killed
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1038)
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1081)
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
;;;","17/Jan/17 17:31;j_caplan;see also https://issues.apache.org/jira/browse/MAPREDUCE-6091;;;","17/Jan/17 18:03;vanzin;[~j_caplan] can you open a new bug for that issue?;;;","17/Feb/17 23:20;j_caplan;Done, as SPARK-19649 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./bin/spark-class miss Java version with _JAVA_OPTIONS set,SPARK-3869,12746888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cocoatomo,cocoatomo,09/Oct/14 00:59,10/Dec/14 06:41,14/Jul/23 06:26,10/Dec/14 06:41,1.2.0,,,,,,,1.2.0,,,,,,Spark Shell,,,,0,,,,,,"When _JAVA_OPTIONS environment variable is set, a command ""java -version"" outputs a message like ""Picked up _JAVA_OPTIONS: -Dfile.encoding=UTF-8"".
./bin/spark-class knows java version from the first line of ""java -version"" output, so it mistakes java version with _JAVA_OPTIONS set.

commit: a85f24accd3266e0f97ee04d03c22b593d99c062","Mac OS X 10.9.5, Python 2.6.8, Java 1.8.0_20",apachespark,cocoatomo,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 10 06:41:42 UTC 2014,,,,,,,,,,"0|i20yq7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"09/Oct/14 01:16;apachespark;User 'cocoatomo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2725;;;","13/Oct/14 05:06;pwendell;Hey [~cocoatomo] would you mind updating your JIRA account with a properly formatted name (e.g. ""First Last""). It is difficult for us when nicknames are used when we want to write release notes. If you'd prefer to contribute anonymously, it's fine to just make up a name.;;;","13/Oct/14 14:16;cocoatomo;Hi [~pwendell], thank you for informing me. Is it OK to use the abbreviated last name (e.g. ""Barack O."") ?;;;","10/Dec/14 06:41;srowen;The PR says it was merged, and is in the 1.2 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up python/run-tests problems,SPARK-3866,12746875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cocoatomo,cocoatomo,09/Oct/14 00:16,16/Feb/15 04:56,14/Jul/23 06:26,16/Feb/15 04:39,1.2.0,,,,,,,,,,,,,PySpark,,,,0,pyspark,testing,,,,"This issue is a overhaul issue to remove problems encountered when I run ./python/run-tests at commit a85f24accd3266e0f97ee04d03c22b593d99c062.
It will have sub-tasks for some kinds of issues.

A test output is contained in the attached file.","Mac OS X 10.9.5, Python 2.7.8, Java 1.8.0_20",cocoatomo,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/14 00:39;cocoatomo;unit-tests.log;https://issues.apache.org/jira/secure/attachment/12673784/unit-tests.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 04:56:18 UTC 2015,,,,,,,,,,"0|i20ynb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"09/Oct/14 00:27;cocoatomo;An output from ./python/run-tests;;;","09/Oct/14 00:39;cocoatomo;An output from ./python/run-tests;;;","16/Feb/15 04:39;joshrosen;It looks like all of this issue's subtasks have been resolved, so I'm going to mark this as fixed.;;;","16/Feb/15 04:56;cocoatomo;Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaRDD.generate ignores alias argument,SPARK-3858,12746857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,NathanHowell,NathanHowell,08/Oct/14 23:18,09/Oct/14 22:03,14/Jul/23 06:26,09/Oct/14 22:03,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,The {{alias}} argument to {{SchemaRDD.generate}} is discarded and a constant {{None}} is supplied to the {{logical.Generate}} constructor.,,apachespark,marmbrus,NathanHowell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 22:03:12 UTC 2014,,,,,,,,,,"0|i20yjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/14 23:30;apachespark;User 'NathanHowell' has created a pull request for this issue:
https://github.com/apache/spark/pull/2721;;;","09/Oct/14 22:03;marmbrus;Issue resolved by pull request 2721
[https://github.com/apache/spark/pull/2721];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Binding Exception when running PythonUDFs,SPARK-3855,12746831,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,08/Oct/14 21:29,17/Oct/14 21:12,14/Jul/23 06:26,17/Oct/14 21:12,1.1.0,,,,,,,1.2.0,,,,,,PySpark,SQL,,,0,,,,,,"{code}
from pyspark import *
from pyspark.sql import *
sc = SparkContext()
sqlContext = SQLContext(sc)
sqlContext.registerFunction(""strlen"", lambda string: len(string))
sqlContext.inferSchema(sc.parallelize([Row(a=""test"")])).registerTempTable(""test"")
srdd = sqlContext.sql(""SELECT strlen(a) FROM test WHERE strlen(a) > 1"")
print srdd._jschema_rdd.baseSchemaRDD().queryExecution().toString()
print srdd.collect()
{code}

output:
{code}
== Parsed Logical Plan ==
Project ['strlen('a) AS c0#1]
 Filter ('strlen('a) > 1)
  UnresolvedRelation None, test, None

== Analyzed Logical Plan ==
Project [c0#1]
 Project [pythonUDF#2 AS c0#1]
  EvaluatePython PythonUDF#strlen(a#0)
   Project [a#0]
    Filter (CAST(pythonUDF#3, DoubleType) > CAST(1, DoubleType))
     EvaluatePython PythonUDF#strlen(a#0)
      SparkLogicalPlan (ExistingRdd [a#0], MapPartitionsRDD[7] at mapPartitions at SQLContext.scala:525)

== Optimized Logical Plan ==
Project [pythonUDF#2 AS c0#1]
 EvaluatePython PythonUDF#strlen(a#0)
  Project [a#0]
   Filter (CAST(pythonUDF#3, DoubleType) > 1.0)
    EvaluatePython PythonUDF#strlen(a#0)
     SparkLogicalPlan (ExistingRdd [a#0], MapPartitionsRDD[7] at mapPartitions at SQLContext.scala:525)

== Physical Plan ==
Project [pythonUDF#2 AS c0#1]
 BatchPythonEvaluation PythonUDF#strlen(a#0), [a#0,pythonUDF#5]
  Project [a#0]
   Filter (CAST(pythonUDF#3, DoubleType) > 1.0)
    BatchPythonEvaluation PythonUDF#strlen(a#0), [a#0,pythonUDF#3]
     ExistingRdd [a#0], MapPartitionsRDD[7] at mapPartitions at SQLContext.scala:525

Code Generation: false
== RDD ==
14/10/08 15:03:00 ERROR Executor: Exception in task 1.0 in stage 4.0 (TID 9)
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: pythonUDF#2
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:47)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:46)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:162)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:191)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:147)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:46)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection$$anonfun$$init$$2.apply(Projection.scala:52)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection$$anonfun$$init$$2.apply(Projection.scala:52)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.<init>(Projection.scala:52)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$newMutableProjection$1.apply(SparkPlan.scala:106)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$newMutableProjection$1.apply(SparkPlan.scala:106)
	at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:43)
	at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Couldn't find pythonUDF#2 in [a#0,pythonUDF#5]
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:53)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:47)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
	... 46 more
14/10/08 15:03:00 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 8)
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: pythonUDF#2
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:47)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:46)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:162)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:191)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:147)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:46)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection$$anonfun$$init$$2.apply(Projection.scala:52)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection$$anonfun$$init$$2.apply(Projection.scala:52)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.<init>(Projection.scala:52)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$newMutableProjection$1.apply(SparkPlan.scala:106)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$newMutableProjection$1.apply(SparkPlan.scala:106)
	at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:43)
	at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Couldn't find pythonUDF#2 in [a#0,pythonUDF#5]
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:53)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:47)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)
	... 46 more
{code}",,apachespark,marmbrus,qiaohaijun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 08 21:35:45 UTC 2014,,,,,,,,,,"0|i20ydr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/14 21:35;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2717;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonRDD does not support converting fields to type Timestamp,SPARK-3853,12746805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mtimper,mtimper,08/Oct/14 20:17,09/Oct/14 21:02,14/Jul/23 06:26,09/Oct/14 21:02,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"create a SchemaRDD using 
eventsSchema = sqlContext.jsonRDD(jsonEventsRdd, schemaWithTimestampField)
eventsSchema.registerTempTable(""events"")
sqlContext.sql(""select max(time_field) from events"")

Throws this exception:
scala.MatchError: TimestampType (of class org.apache.spark.sql.catalyst.types.TimestampType$)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:357)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1$$anonfun$apply$12.apply(JsonRDD.scala:391)
......

",,apachespark,marmbrus,mtimper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 21:02:45 UTC 2014,,,,,,,,,,"0|i20y7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/14 20:24;mtimper;I have a fix, I'll submit a pull request shortly.;;;","08/Oct/14 23:00;apachespark;User 'mtimper' has created a pull request for this issue:
https://github.com/apache/spark/pull/2720;;;","09/Oct/14 21:02;marmbrus;Issue resolved by pull request 2720
[https://github.com/apache/spark/pull/2720];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document spark.driver.extra* configs,SPARK-3852,12746803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,andrewor14,andrewor14,08/Oct/14 20:09,26/Jan/15 16:17,14/Jul/23 06:26,26/Jan/15 16:17,1.1.0,,,,,,,1.3.0,,,,,,Documentation,,,,0,,,,,,They are not documented...,,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 24 13:39:32 UTC 2015,,,,,,,,,,"0|i20y7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/15 13:39;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4185;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn alpha doesn't build on master,SPARK-3848,12746726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,tgraves,tgraves,08/Oct/14 15:11,08/Oct/14 16:54,14/Jul/23 06:26,08/Oct/14 16:54,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"yarn alpha build was broken by https://github.com/apache/spark/pull/2432
as it added an argument to YarnAllocator but not to yarn/alpha YarnAllocationHandler


commit https://github.com/apache/spark/commit/79e45c9323455a51f25ed9acd0edd8682b4bbb88

",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 08 15:45:27 UTC 2014,,,,,,,,,,"0|i20xr3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"08/Oct/14 15:45;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2715;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup scalastyle.txt at the end of running dev/scalastyle,SPARK-3843,12746565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sarutak,sarutak,sarutak,08/Oct/14 00:31,08/Oct/14 22:20,14/Jul/23 06:26,08/Oct/14 22:20,1.2.0,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,dev/scalastyle create a log file 'scalastyle.txt'. it is overwrote per running but never deleted even though dev/mima and dev/lint-python delete their log files.,,apachespark,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 08 22:20:02 UTC 2014,,,,,,,,,,"0|i20wrj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"08/Oct/14 00:39;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2702;;;","08/Oct/14 22:20;pwendell;I've merged the PR to fix this:

https://github.com/apache/spark/pull/2702;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark EC2 templates fail when variables are missing,SPARK-3840,12746533,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,douglaz,douglaz,07/Oct/14 22:53,13/Dec/14 22:28,14/Jul/23 06:26,13/Dec/14 22:28,1.2.0,,,,,,,1.2.0,,,,,,EC2,,,,0,,,,,,"For instance https://github.com/mesos/spark-ec2/pull/58 introduced this problem when AWS_ACCESS_KEY_ID isn't set:

Configuring /root/shark/conf/shark-env.sh
Traceback (most recent call last):
  File ""./deploy_templates.py"", line 91, in <module>
    text = text.replace(""{{"" + key + ""}}"", template_vars[key])
TypeError: expected a character buffer object

This makes all the cluster configuration fail.",,douglaz,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 13 22:28:49 UTC 2014,,,,,,,,,,"0|i20wkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/14 23:39;douglaz;PR: https://github.com/mesos/spark-ec2/pull/74;;;","13/Dec/14 22:28;joshrosen;I think that this should have been fixed in 1.2.0 via a pull request to {{mesos/spark-ec2}} and a freezing of the branch number in different Spark branches.  Feel free to re-open if you still feel that this is an issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backticks not correctly handled in subquery aliases,SPARK-3834,12746477,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ravi.pesala,marmbrus,marmbrus,07/Oct/14 19:36,26/Nov/14 04:00,14/Jul/23 06:26,10/Oct/14 01:41,,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,[~ravi.pesala]  assigning to you since you fixed the last problem here.  Let me know if you don't have time to work on this or if you have any questions.,,andrewor14,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 04:00:09 UTC 2014,,,,,,,,,,"0|i20w8n:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"09/Oct/14 07:22;ravipesala;Ok [~marmbrus] , I will work on it.;;;","09/Oct/14 21:38;ravipesala;https://github.com/apache/spark/pull/2737;;;","10/Oct/14 01:41;marmbrus;Issue resolved by pull request 2737
[https://github.com/apache/spark/pull/2737];;;","26/Nov/14 04:00;andrewor14;Also resolved by https://github.com/apache/spark/pull/3199 in 1.1.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Spark logo image on the header of HistoryPage as a link to HistoryPage's page #1,SPARK-3829,12746334,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,07/Oct/14 07:00,07/Oct/14 23:55,14/Jul/23 06:26,07/Oct/14 23:55,1.2.0,,,,,,,1.1.1,1.2.0,,,,,Web UI,,,,0,,,,,,"There is a Spark logo on the header of HistoryPage.
We can have too many HistoryPages if we run 20+ applications. So I think, it's useful if the logo is as a link to the HistoryPage's page #1.
",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 07:05:33 UTC 2014,,,,,,,,,,"0|i20vdj:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"07/Oct/14 07:05;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2690;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Very long RDD names are not rendered properly in web UI,SPARK-3827,12746324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,falaki,falaki,07/Oct/14 04:02,07/Oct/14 18:48,14/Jul/23 06:26,07/Oct/14 18:47,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Web UI,,,,0,,,,,,With Spark SQL we generate very long RDD names. These names are not properly rendered in the web UI. ,,apachespark,falaki,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2915,SPARK-3297,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 18:47:03 UTC 2014,,,,,,,,,,"0|i20vbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/14 04:05;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/2687;;;","07/Oct/14 18:47;joshrosen;Issue resolved by pull request 2687
[https://github.com/apache/spark/pull/2687];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log more information when unrolling a block fails,SPARK-3825,12746290,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,06/Oct/14 23:46,07/Oct/14 19:53,14/Jul/23 06:26,07/Oct/14 19:53,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Block Manager,Spark Core,,,0,,,,,,"We currently log only the following:
{code}
14/10/06 16:45:42 WARN CacheManager: Not enough space to cache partition rdd_0_2 in memory! Free memory is 481861527 bytes.
{code}
This is confusing, however, because ""free memory"" here means the amount of memory not occupied by blocks. It does not include the amount of memory reserved for unrolling.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 04:45:27 UTC 2014,,,,,,,,,,"0|i20v3z:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"07/Oct/14 04:45;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2688;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL should cache in MEMORY_AND_DISK by default,SPARK-3824,12746286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,pwendell,pwendell,06/Oct/14 23:23,10/Oct/14 01:26,14/Jul/23 06:26,10/Oct/14 01:26,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Spark SQL currently uses MEMORY_ONLY as the default format. Due to the use of column buffers however, there is a huge cost to having to recompute blocks, much more so than Spark core. Especially since now we are more conservative about caching blocks and sometimes won't cache blocks we think might exceed memory, it seems good to keep persisted blocks on disk by default.",,apachespark,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 10 01:26:57 UTC 2014,,,,,,,,,,"0|i20v33:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"07/Oct/14 02:10;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2686;;;","10/Oct/14 01:26;marmbrus;Issue resolved by pull request 2686
[https://github.com/apache/spark/pull/2686];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add configureOutputJobPropertiesForStorageHandler to JobConf in SparkHadoopWriter,SPARK-3816,12746170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,alexliu68,alexliu68,06/Oct/14 15:48,23/Dec/14 01:07,14/Jul/23 06:26,28/Oct/14 03:43,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,2,,,,,,"It's similar to SPARK-2846. We should add PlanUtils.configureInputJobPropertiesForStorageHandler to SparkHadoopWriter, so that writer can add configuration from customized StorageHandler to JobConf",,alexliu68,apachespark,marmbrus,rocco.varela,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 01:07:03 UTC 2014,,,,,,,,,,"0|i20ue7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 16:05;apachespark;User 'alexliu68' has created a pull request for this issue:
https://github.com/apache/spark/pull/2677;;;","07/Oct/14 19:50;apachespark;User 'alexliu68' has created a pull request for this issue:
https://github.com/apache/spark/pull/2677;;;","28/Oct/14 03:43;marmbrus;Issue resolved by pull request 2677
[https://github.com/apache/spark/pull/2677];;;","23/Dec/14 01:07;apachespark;User 'alexliu68' has created a pull request for this issue:
https://github.com/apache/spark/pull/3766;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support for Bitwise AND(&), OR(|) ,XOR(^), NOT(~) in Spark HQL and SQL",SPARK-3814,12746141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ravi.pesala,yanakad,yanakad,06/Oct/14 13:18,28/Oct/14 20:36,14/Jul/23 06:26,28/Oct/14 20:36,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Error: java.lang.RuntimeException: 
Unsupported language features in query: select (case when bit_field & 1=1 then r_end - r_start else NULL end) from mytable where pkey='0178-2014-07' LIMIT 2
TOK_QUERY
  TOK_FROM
    TOK_TABREF
      TOK_TABNAME
       mytable 
  TOK_INSERT
    TOK_DESTINATION
      TOK_DIR
        TOK_TMP_FILE
    TOK_SELECT
      TOK_SELEXPR
        TOK_FUNCTION
          when
          =
            &
              TOK_TABLE_OR_COL
                bit_field
              1
            1
          -
            TOK_TABLE_OR_COL
              r_end
            TOK_TABLE_OR_COL
              r_start
          TOK_NULL
    TOK_WHERE
      =
        TOK_TABLE_OR_COL
          pkey
        '0178-2014-07'
    TOK_LIMIT
      2


SQLState:  null
ErrorCode: 0",,apachespark,marmbrus,ravi.pesala,ravipesala,yanakad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 20:36:24 UTC 2014,,,,,,,,,,"0|i20u7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/14 05:05;ravipesala;Currently there is no support of  Bitwise & in Spark HiveQl and Spark SQL as well.

I am working on this issue.

And as well as we need to support Bitwise | , Bitwise ^ , Bitwise ~. I will add seperate jira for these operations and I will work on these.
Thank you.;;;","08/Oct/14 12:10;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2710;;;","09/Oct/14 21:34;ravipesala;https://github.com/apache/spark/pull/2736;;;","13/Oct/14 10:06;ravi.pesala;https://github.com/apache/spark/pull/2772;;;","13/Oct/14 10:12;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2736;;;","14/Oct/14 02:33;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2789;;;","14/Oct/14 13:53;yanakad;Ravindra,as you said the full list of Bitwise operators missing is &,|, ~ and ^. I don't mind changing the title of this JIRA if you want to fix them all at once -- seems that the fix is so symmetric it's almost a shame to do 1/2 of them. That said, I only need & so I'm happy either way;;;","16/Oct/14 08:25;ravi.pesala;Added support for Bitwise AND(&), OR(|) ,XOR(^), NOT(~) in this PR and I updated the title of this defect.;;;","24/Oct/14 12:23;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2926;;;","27/Oct/14 18:25;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2961;;;","28/Oct/14 20:36;marmbrus;Issue resolved by pull request 2961
[https://github.com/apache/spark/pull/2961];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rule PreInsertionCasts doesn't handle partitioned table properly,SPARK-3810,12746090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,lian cheng,lian cheng,06/Oct/14 10:01,09/Oct/14 01:11,14/Jul/23 06:26,09/Oct/14 01:11,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"This issue can be reproduced by the following {{sbt/sbt hive/console}} session:
{code}
scala> loadTestTable(""src"")
...
scala> loadTestTable(""srcpart"")
...
scala> sql(""INSERT INTO TABLE srcpart PARTITION (ds='1', hr='2') SELECT key, value FROM src"").queryExecution
...
== Parsed Logical Plan ==
InsertIntoTable (UnresolvedRelation None, srcpart, None), Map(ds -> Some(hello), hr -> Some(world)), false
 Project ['key,'value]
  UnresolvedRelation None, src, None

== Analyzed Logical Plan ==
InsertIntoTable (MetastoreRelation default, srcpart, None), Map(ds -> Some(hello), hr -> Some(world)), false
 Project [key#50,value#51]
  Project [key#50,value#51]
   Project [key#50,value#51]
    Project [key#50,value#51]
     Project [key#50,value#51]
      Project [key#50,value#51]
       Project [key#50,value#51]
        Project [key#50,value#51]
         Project [key#50,value#51]
          Project [key#50,value#51]
           Project [key#50,value#51]
            Project [key...
{code}",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 01:11:28 UTC 2014,,,,,,,,,,"0|i20twf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,,,,,,,,,,,"06/Oct/14 10:02;lian cheng;This issue is marked as MINOR because it doesn't affect correctness. All the redundant Projects can be removed by the subsequent optimization phase.;;;","06/Oct/14 11:05;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2672;;;","09/Oct/14 01:11;marmbrus;Issue resolved by pull request 2672
[https://github.com/apache/spark/pull/2672];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make HiveThriftServer2Suite work correctly,SPARK-3809,12746089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,06/Oct/14 09:59,13/Oct/14 20:50,14/Jul/23 06:26,13/Oct/14 20:50,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Currently HiveThriftServer2Suite is a fake one, actually HiveThriftServer not started there",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 20:50:39 UTC 2014,,,,,,,,,,"0|i20tw7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Oct/14 10:25;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2671;;;","07/Oct/14 19:49;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2675;;;","13/Oct/14 20:50;marmbrus;Issue resolved by pull request 2675
[https://github.com/apache/spark/pull/2675];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark fails to start in Windows,SPARK-3808,12746053,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tsudukim,tsudukim,tsudukim,06/Oct/14 07:35,29/Apr/15 06:07,14/Jul/23 06:26,07/Oct/14 18:54,1.2.0,,,,,,,1.2.0,,,,,,PySpark,Windows,,,0,,,,,,"When we execute bin\pyspark.cmd in Windows, it fails to start.
We get following messages.
{noformat}
C:\XXXX>bin\pyspark.cmd
Running C:\XXXX\python.exe with PYTHONPATH=C:\XXXX\bin\..\python\lib\py4j-0.8.2.1-src.zip;C:\XXXX\bin\..\python;
Python 2.7.8 (default, Jun 30 2014, 16:03:49) [MSC v.1500 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
=""x"" was unexpected at this time.
Traceback (most recent call last):
  File ""C:\XXXX\bin\..\python\pyspark\shell.py"", line 45, in <module>
    sc = SparkContext(appName=""PySparkShell"", pyFiles=add_files)
  File ""C:\XXXX\python\pyspark\context.py"", line 103, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""C:\XXXX\python\pyspark\context.py"", line 212, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""C:\XXXX\python\pyspark\java_gateway.py"", line 71, in launch_gateway
    raise Exception(error_msg)
Exception: Launching GatewayServer failed with exit code 255!
Warning: Expected GatewayServer to output a port, but found no output.

>>>
{noformat}",Windows,andrewor14,apachespark,Eminent,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3879,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 06:07:36 UTC 2015,,,,,,,,,,"0|i20to7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Oct/14 07:41;tsudukim;The root cause of this problem is an syntax error in bin\compute-classpath.cmd .

{code}
if ""x%SPARK_CONF_DIR%""!=""x"" (
{code}

Batch script of Windows doesn't have the '!=' operator.
We should use 'not' and '==' instead.
;;;","06/Oct/14 07:45;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/2669;;;","06/Oct/14 08:32;tsudukim;The same problem occurs when we execute spark-shell.cmd and spark-submit.cmd in Windows.;;;","07/Oct/14 18:55;andrewor14;Hey [~tsudukim] can you verify that pyspark, spark-shell and spark-submit work as expected in Windows now?;;;","08/Oct/14 01:56;tsudukim;I verified and it works well! Thank you [~andrewor14];;;","28/Apr/15 07:31;Eminent;I still met the problem even if compute-classpath.cmd is up to day (there is not '!=' in the cmd file)
Is there any other suggestion to solve the problem?


Environment:
Windows 7 64bit
Python 2.7.9
spark-1.2.0-bin-hadoop2.4

log:
Running python with PYTHONPATH=C:\spark\bin\..\python\lib\py4j-0.8.2.1-src.zip;C
:\spark\bin\..\python;
Python 2.7.9 (default, Dec 10 2014, 12:28:03) [MSC v.1500 64 bit (AMD64)] on win
32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
else was unexpected at this time.
Traceback (most recent call last):
  File ""C:\spark\bin\..\python\pyspark\shell.py"", line 45, in <module>
    sc = SparkContext(appName=""PySparkShell"", pyFiles=add_files)
  File ""C:\spark\python\pyspark\context.py"", line 102, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""C:\spark\python\pyspark\context.py"", line 211, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""C:\spark\python\pyspark\java_gateway.py"", line 73, in launch_gateway
    raise Exception(error_msg)
Exception: Launching GatewayServer failed with exit code 255!
Warning: Expected GatewayServer to output a port, but found no output.

>>>;;;","28/Apr/15 09:15;tsudukim;It seems to fail to run jvm.
Perhaps you haven't got java.exe in environment variable %PATH%.
{code}
set PATH=%PATH%;C:\path\to\java.exe
{code};;;","29/Apr/15 06:07;Eminent;Yes. It's the cause. 
After updating the %PATH%, spark was successfully launched. Thanks so much for your help!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSql does not work for tables created using custom serde,SPARK-3807,12746051,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chiragaggarwal,chiragaggarwal,06/Oct/14 07:30,16/Oct/14 06:35,14/Jul/23 06:26,13/Oct/14 20:48,1.1.0,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,"SparkSql crashes on selecting tables using custom serde. 

Example:
----------------

CREATE EXTERNAL TABLE table_name PARTITIONED BY ( a int) ROW FORMAT 'SERDE ""org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer"" with serdeproperties(""serialization.format""=""org.apache.thrift.protocol.TBinaryProtocol"",""serialization.class""=""ser_class"") STORED AS SEQUENCEFILE;

The following exception is seen on running a query like 'select * from table_name limit 1': 

ERROR CliDriver: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException 
at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.initialize(ThriftDeserializer.java:68) 
at org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializer(TableDesc.java:80) 
at org.apache.spark.sql.hive.execution.HiveTableScan.addColumnMetadataToConf(HiveTableScan.scala:86) 
at org.apache.spark.sql.hive.execution.HiveTableScan.<init>(HiveTableScan.scala:100) 
at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$$anonfun$14.apply(HiveStrategies.scala:188) 
at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$$anonfun$14.apply(HiveStrategies.scala:188) 
at org.apache.spark.sql.SQLContext$SparkPlanner.pruneFilterProject(SQLContext.scala:364) 
at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$.apply(HiveStrategies.scala:184) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58) 
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54) 
at org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:280) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58) 
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) 
at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59) 
at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:402) 
at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:400) 
at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:406) 
at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:406) 
at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:406) 
at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:59) 
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:291) 
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413) 
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226) 
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) 
at java.lang.reflect.Method.invoke(Unknown Source) 
at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
Caused by: java.lang.NullPointerException


After fixing this issue, when some columns in the table were referred in the query, sparksql could not resolve those references.",,apachespark,capricornius,chiragaggarwal,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 16 06:35:37 UTC 2014,,,,,,,,,,"0|i20tnr:",9223372036854775807,,,,,,,,,,,,,,1.1.1,,,,,,,,,,,"06/Oct/14 11:22;chiragaggarwal;Pull request for this issue:
https://github.com/apache/spark/pull/2674;;;","06/Oct/14 11:25;apachespark;User 'chiragaggarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2674;;;","13/Oct/14 20:48;marmbrus;Issue resolved by pull request 2674
[https://github.com/apache/spark/pull/2674];;;","16/Oct/14 06:35;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2821;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minor bug in CliSuite,SPARK-3806,12746037,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,06/Oct/14 03:41,09/Oct/14 20:23,14/Jul/23 06:26,09/Oct/14 20:23,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Clisuite throw exception as follows:
Exception in thread ""Thread-6"" java.lang.IndexOutOfBoundsException: 6
	at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
	at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)
	at org.apache.spark.sql.hive.thriftserver.CliSuite.org$apache$spark$sql$hive$thriftserver$CliSuite$$captureOutput$1(CliSuite.scala:67)
	at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$4.apply(CliSuite.scala:78)
	at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$4.apply(CliSuite.scala:78)
	at scala.sys.process.ProcessLogger$$anon$1.out(ProcessLogger.scala:96)
	at scala.sys.process.BasicIO$$anonfun$processOutFully$1.apply(BasicIO.scala:135)
	at scala.sys.process.BasicIO$$anonfun$processOutFully$1.apply(BasicIO.scala:135)
	at scala.sys.process.BasicIO$.readFully$1(BasicIO.scala:175)
	at scala.sys.process.BasicIO$.processLinesFully(BasicIO.scala:179)
	at scala.sys.process.BasicIO$$anonfun$processFully$1.apply(BasicIO.scala:164)
	at scala.sys.process.BasicIO$$anonfun$processFully$1.apply(BasicIO.scala:162)
	at scala.sys.process.ProcessBuilderImpl$Simple$$anonfun$3.apply$mcV$sp(ProcessBuilderImpl.scala:73)
	at scala.sys.process.ProcessImpl$Spawn$$anon$1.run(ProcessImpl.scala:22)",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 20:23:03 UTC 2014,,,,,,,,,,"0|i20tkn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Oct/14 04:00;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2666;;;","09/Oct/14 20:23;marmbrus;Issue resolved by pull request 2666
[https://github.com/apache/spark/pull/2666];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output of Generator expressions is not stable after serialization.,SPARK-3804,12746020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,05/Oct/14 21:10,16/Sep/15 08:04,14/Jul/23 06:26,15/Sep/15 21:04,,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,The output of a generator expression (such as explode) can change after serialization.  This caused a nasty data corruption bug.  While this bug has seen been addressed it would be good to fix this since it violates the general contract of query plans.,,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:04:47 UTC 2015,,,,,,,,,,"0|i20tgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/15 21:04;marmbrus;This has been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException found in executing computePrincipalComponents,SPARK-3803,12745982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,dobachi,dobachi,05/Oct/14 11:27,15/Jan/15 09:08,14/Jul/23 06:26,14/Oct/14 21:42,1.1.0,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"When I executed computePrincipalComponents method of RowMatrix, I got java.lang.ArrayIndexOutOfBoundsException.

{code}
14/10/05 20:16:31 INFO DAGScheduler: Failed to run reduce at RDDFunctions.scala:111
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 611, localhost): java.lang.ArrayIndexOutOfBoundsException: 4878161
        org.apache.spark.mllib.linalg.distributed.RowMatrix$.org$apache$spark$mllib$linalg$distributed$RowMatrix$$dspr(RowMatrix.scala:460)
        org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$3.apply(RowMatrix.scala:114)
        org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$3.apply(RowMatrix.scala:113)
        scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
        scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
        scala.collection.AbstractIterator.foldLeft(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:201)
        scala.collection.AbstractIterator.aggregate(Iterator.scala:1157)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

The RowMatrix instance was generated from the result of TF-IDF like the following.

{code}
scala> val hashingTF = new HashingTF()
scala> val tf = hashingTF.transform(texts)
scala> import org.apache.spark.mllib.feature.IDF
scala> tf.cache()
scala> val idf = new IDF().fit(tf)
scala> val tfidf: RDD[Vector] = idf.transform(tf)

scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix
scala> val mat = new RowMatrix(tfidf)
scala> val pc = mat.computePrincipalComponents(2)
{code}

I think this was because I created HashingTF instance with default numFeatures and Array is used in RowMatrix#computeGramianMatrix method
like the following.

{code}
  /**
   * Computes the Gramian matrix `A^T A`.
   */
  def computeGramianMatrix(): Matrix = {
    val n = numCols().toInt
    val nt: Int = n * (n + 1) / 2

    // Compute the upper triangular part of the gram matrix.
    val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(
      seqOp = (U, v) => {
        RowMatrix.dspr(1.0, v, U.data)
        U
      }, combOp = (U1, U2) => U1 += U2)

    RowMatrix.triuToFull(n, GU.data)
  }
{code} 

When the size of Vectors generated by TF-IDF is too large, it makes ""nt"" to have undesirable value (and undesirable size of Array used in treeAggregate),
since n * (n + 1) / 2 exceeded Int.MaxValue.


Is this surmise correct?

And, of course, I could avoid this situation by creating instance of HashingTF with smaller numFeatures.
But this may not be fundamental solution.
",,apachespark,dobachi,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 21:42:25 UTC 2014,,,,,,,,,,"0|i20t8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/14 16:43;srowen;I agree with your assessment. It would take some work, though not terribly much, to rewrite this method to correctly handle A with more than 46340 columns. At n = 46340, the Gramian already consumes about 8.5GB of memory, so it's kinda getting big to realistically use in core anyway. At the least, an error should be raised if n is too large. Any one else think this should be supported though? Would be nice, but, practically helpful?;;;","07/Oct/14 00:41;mengxr;In `computeCovariance`, we generate a warning message if `numCols > 10000`. 

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/RowMatrix.scala#L307

We could do the same in `Gram`, or we can throw an exception if `numCols` is too big.;;;","07/Oct/14 15:13;dobachi;Thank you for your comments.
I agree with the idea to throw an exception.

This is because exiting with an appropriate exception and messages seems to be kind for users of MLlib.
It helps them to recognize which part of application they should change.

How about using sys.error() to throw RuntimeException in the same way as handling of empty rows.;;;","14/Oct/14 11:58;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2801;;;","14/Oct/14 21:42;mengxr;Issue resolved by pull request 2801
[https://github.com/apache/spark/pull/2801];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala version is wrong in dev/audit-release/blank_sbt_build/build.sbt,SPARK-3802,12745980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,sarutak,sarutak,05/Oct/14 10:49,09/Dec/14 00:22,14/Jul/23 06:26,09/Dec/14 00:22,1.2.0,,,,,,,,,,,,,Build,,,,0,,,,,,"In dev/audit-release/blank_sbt_build/build.sbt, scalaVersion indicates 2.9.3 but I think 2.10.4 is correct.",,apachespark,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 00:22:39 UTC 2014,,,,,,,,,,"0|i20t87:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Oct/14 10:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2661;;;","09/Dec/14 00:22;joshrosen;This was fixed by [~andrewor14] in https://github.com/apache/spark/commit/723a86b04cfbc178fbd57bb78f4a2becc5cb1ef1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BindingException when grouping on nested fields,SPARK-3800,12745967,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,05/Oct/14 03:19,20/Oct/14 22:32,14/Jul/23 06:26,20/Oct/14 22:32,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Query to reproduce by [~ekhliang]
{code}
select attribute, sum(cnt)
from (
  select nested.attribute, count(*) as cnt
  from rows
  group by nested.attribute) a
group by attribute
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 22:32:43 UTC 2014,,,,,,,,,,"0|i20t5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/14 03:26;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2658;;;","20/Oct/14 22:32;marmbrus;Issue resolved by pull request 2658
[https://github.com/apache/spark/pull/2658];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted projection in Generator,SPARK-3798,12745951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,04/Oct/14 21:27,10/Oct/14 00:54,14/Jul/23 06:26,10/Oct/14 00:54,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"In some cases it is possible for the output of a generator to change, resulting in a corrupted projection and thus incorrect data from a query that uses a generator (e.g., LATERAL VIEW explode).",,apachespark,marmbrus,NathanHowell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 10 00:54:11 UTC 2014,,,,,,,,,,"0|i20t1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/14 21:30;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2656;;;","10/Oct/14 00:54;marmbrus;Issue resolved by pull request 2656
[https://github.com/apache/spark/pull/2656];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building spark core fails due to inadvertent dependency on Commons IO,SPARK-3794,12745936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cocoatomo,cocoatomo,04/Oct/14 18:32,06/Oct/14 01:44,14/Jul/23 06:26,06/Oct/14 01:44,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,spark,,,,,"At the commit cf1d32e3e1071829b152d4b597bf0a0d7a5629a2, building spark core result in compilation error when we specify some hadoop versions.

To reproduce this issue, we should execute following command with <hadoop.version>=1.1.0, 1.1.1, 1.1.2, 1.2.0, 1.2.1, or 2.2.0.

{noformat}
$ cd ./core
$ mvn -Dhadoop.version=<hadoop.version> -DskipTests clean compile
...
[ERROR] /Users/tomohiko/MyRepos/Scala/spark/core/src/main/scala/org/apache/spark/util/Utils.scala:720: value listFilesAndDirs is not a member of object org.apache.commons.io.FileUtils
[ERROR]       val files = FileUtils.listFilesAndDirs(dir, TrueFileFilter.TRUE, TrueFileFilter.TRUE)
[ERROR]                             ^
{noformat}

Because that compilation uses commons-io version 2.1 and FileUtils#listFilesAndDirs method was added at commons-io version 2.2, this compilation always fails.

FileUtils#listFilesAndDirs → http://commons.apache.org/proper/commons-io/apidocs/org/apache/commons/io/FileUtils.html#listFilesAndDirs%28java.io.File,%20org.apache.commons.io.filefilter.IOFileFilter,%20org.apache.commons.io.filefilter.IOFileFilter%29

Because a hadoop-client in those problematic version depends on commons-io 2.1 not 2.4, we should have assumption that commons-io is version 2.1.",Mac OS X 10.9.5,apachespark,cocoatomo,donnchadh,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 01:44:30 UTC 2014,,,,,,,,,,"0|i20syf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"04/Oct/14 23:05;pwendell;Are you trying to build from within the core directory? You should try a maven build directly from the root directory.;;;","05/Oct/14 01:34;cocoatomo;Thank you for the comment.

Building from the root directory results in a same error.

{noformat}
$ mvn -Dhadoop.version=1.1.0 -DskipTests clean compile
...
[ERROR] /Users/<user>/MyRepos/Scala/spark/core/src/main/scala/org/apache/spark/util/Utils.scala:720: value listFilesAndDirs is not a member of object org.apache.commons.io.FileUtils
[ERROR]       val files = FileUtils.listFilesAndDirs(dir, TrueFileFilter.TRUE, TrueFileFilter.TRUE)
[ERROR]                             ^
[ERROR] one error found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  2.147 s]
[INFO] Spark Project Core ................................. FAILURE [ 42.550 s]
[INFO] Spark Project Bagel ................................ SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Spark Project External Twitter ..................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project External Flume Sink .................. SKIPPED
[INFO] Spark Project External Flume ....................... SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 45.365 s
[INFO] Finished at: 2014-10-05T10:29:48+09:00
[INFO] Final Memory: 34M/1017M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile (scala-compile-first) on project spark-core_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile failed. CompileFailed -> [Help 1]
{noformat};;;","05/Oct/14 13:47;srowen;The real problem here is that commons-io is not a dependency of Spark, and should not be, but it began to be used a few days ago in commit https://github.com/apache/spark/commit/cf1d32e3e1071829b152d4b597bf0a0d7a5629a2 for SPARK-1860. So it is accidentally depending on the version of Commons IO brought in by third party dependencies. 

I will propose a PR that removes this usage in favor of Guava or Java APIs.;;;","05/Oct/14 14:25;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2662;;;","06/Oct/14 01:44;marmbrus;Issue resolved by pull request 2662
[https://github.com/apache/spark/pull/2662];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveThriftServer2 returns 0.12.0 to ODBC SQLGetInfo call,SPARK-3791,12745900,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,04/Oct/14 07:12,26/Nov/14 04:03,14/Jul/23 06:26,02/Nov/14 23:18,1.1.0,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,"The ""DBMS Server version"" should be Spark version rather than Hive version:
{code}
...
{""ts"":""2014-10-03T07:01:21.679"",""pid"":23188,""tid"":""2034"",""sev"":""info"",""req"":""-"",""sess"":""-"",""site"":""-"",""user"":""-"",""k"":""msg"",""v"":""GenericODBCProtocol: DBMS Server version: 0.12.0""}
...
{code}
",,andrewor14,apachespark,lian cheng,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 04:03:08 UTC 2014,,,,,,,,,,"0|i20sqf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"19/Oct/14 06:04;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2843;;;","02/Nov/14 23:18;marmbrus;Issue resolved by pull request 2843
[https://github.com/apache/spark/pull/2843];;;","26/Nov/14 04:03;andrewor14;Backported into 1.1.1 through https://github.com/apache/spark/pull/3199;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Yarn dist cache code is not friendly to HDFS HA, Federation",SPARK-3788,12745854,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,03/Oct/14 23:01,03/Dec/14 04:04,14/Jul/23 06:26,08/Oct/14 13:51,,,,,,,,1.1.1,1.2.0,,,,,YARN,,,,0,,,,,,"There are two bugs here.

1. The {{compareFs()}} method in ClientBase considers the 'host' part of the URI to be an actual host. In the case of HA and Federation, that's a namespace name, which doesn't resolve to anything. So in those cases, {{compareFs()}} always says the file systems are different.

2. In {{prepareLocalResources()}}, when adding a file to the distributed cache, that is done with the common FileSystem object instantiated at the start of the method. In the case of Federation that doesn't work: the qualified URL's scheme may differ from the non-qualified one, so the FileSystem instance will not work.

Fixes are pretty trivial.",,apachespark,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4712,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 03 23:50:35 UTC 2014,,,,,,,,,,"0|i20sgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/14 23:09;vanzin;Note: ""2"" above only applies to branch-1.1. It was fixed in master by https://github.com/apache/spark/commit/c4022dd5.;;;","03/Oct/14 23:35;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2649;;;","03/Oct/14 23:36;vanzin;Ah, ""2"" was fixed in branch-1.1 as part of SPARK-2577. So only issue 1 remains.;;;","03/Oct/14 23:50;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2650;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assembly jar name is wrong when we build with sbt omitting -Dhadoop.version,SPARK-3787,12745845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,03/Oct/14 22:09,27/Dec/14 06:57,14/Jul/23 06:26,27/Dec/14 06:57,1.2.0,,,,,,,1.2.1,1.3.0,,,,,Build,,,,0,,,,,,"When we build with sbt with profile for hadoop and without property for hadoop version like:

{code}
sbt/sbt -Phadoop-2.2 assembly
{code}

jar name is always used default version (1.0.4).

When we build with maven with same condition for sbt, default version for each profile is used.
For instance, if we  build like:

{code}
mvn -Phadoop-2.2 package
{code}

jar name is used hadoop2.2.0 as a default version of hadoop-2.2.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 01 02:50:03 UTC 2014,,,,,,,,,,"0|i20sev:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,"03/Oct/14 22:15;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2647;;;","01/Nov/14 02:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/3046;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The type parameters for SparkContext.accumulable are inconsistent Accumulable itself,SPARK-3783,12745764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nkronenfeld,nkronenfeld,nkronenfeld,03/Oct/14 16:40,06/Oct/14 04:05,14/Jul/23 06:26,06/Oct/14 04:04,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"SparkContext.accumulable takes type parameters [T, R] - and passes them to accumulable, in that order.
Accumulable takes type parameters [R, T].
So T for SparkContext.accumulable corresponds with R for Accumulable and vice versa.
Minor, but very confusing.",,nkronenfeld,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 04:04:38 UTC 2014,,,,,,,,,,"0|i20ryn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/14 16:42;nkronenfeld;https://github.com/apache/spark/pull/2637;;;","06/Oct/14 04:04;pwendell;https://github.com/apache/spark/pull/2637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Direct use of log4j in AkkaUtils interferes with certain logging configurations ,SPARK-3782,12745752,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,pledge,pledge,03/Oct/14 15:51,25/Jan/15 23:12,14/Jul/23 06:26,25/Jan/15 23:12,1.1.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,AkkaUtils is calling setLevel on Logger from log4j. This causes issues when using another implementation of SLF4J such as logback as log4j-over-slf4j.jars implementation of this class does not contain this method on Logger.,,apachespark,pledge,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1190,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 24 13:18:24 UTC 2015,,,,,,,,,,"0|i20rvz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"06/Oct/14 04:08;pwendell;Would you mind pasting the exact exception? Also, what happens if you set ""spark.akka.logLifecycleEvents"" to ""true"" - does that work as a workaround?;;;","24/Jan/15 13:13;srowen;Aha, I think there's a good point here. 

Looks like this method was added to the log4j shim in slf4j 1.7.6: https://github.com/qos-ch/slf4j/commit/004b5d4879a079f3d6f610b7fe339a0fad7d4831 

So it should be fine if you use log4j-over-slf4j 1.7.6+ in your app. Spark references slf4j 1.7.5 though. Although I don't think it will matter if you use a different version, we could update Spark's slf4j to 1.7.6 at least, to be really consistent. 1.7.10 is the latest in fact.;;;","24/Jan/15 13:18;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4184;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn spark.yarn.applicationMaster.waitTries config should be changed to a time period,SPARK-3779,12745734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,tgraves,tgraves,03/Oct/14 13:46,18/Dec/14 18:19,14/Jul/23 06:26,18/Dec/14 18:19,1.1.0,,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"in pr https://github.com/apache/spark/pull/2577 I added support to use spark.yarn.applicationMaster.waitTries to client mode.  But the time it waits between loops is different so it could be confusing to the user.  We also don't document how long each loop is in the documentation so this config really isn't clear.

We should just changed this config to be time based,  ms or seconds. ",,apachespark,sandyr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4617,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 05:17:42 UTC 2014,,,,,,,,,,"0|i20rrz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"26/Nov/14 05:17;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/3471;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
newAPIHadoopRDD doesn't properly pass credentials for secure hdfs on yarn,SPARK-3778,12745728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,tgraves,03/Oct/14 12:57,26/May/15 06:54,14/Jul/23 06:26,03/Feb/15 06:46,1.1.0,,,,,,,1.3.0,,,,,,Spark Core,,,,1,,,,,,"The newAPIHadoopRDD routine doesn't properly add the credentials to the conf to be able to access secure hdfs.

Note that newAPIHadoopFile does handle these because the org.apache.hadoop.mapreduce.Job automatically adds it for you.",,apachespark,erikvanoosten,joshrosen,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7110,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 06:46:16 UTC 2015,,,,,,,,,,"0|i20rqn:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"06/Oct/14 15:00;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2676;;;","30/Jan/15 17:01;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/4292;;;","02/Feb/15 23:08;pwendell;/cc [~hshreedharan];;;","03/Feb/15 06:46;joshrosen;Issue resolved by pull request 4292
[https://github.com/apache/spark/pull/4292];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong conversion to Catalyst for Option[Product],SPARK-3776,12745711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,r3nat,r3nat,03/Oct/14 11:36,06/Oct/14 00:57,14/Jul/23 06:26,06/Oct/14 00:57,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Method ScalaReflection.convertToCatalyst make wrong conversion for Option[Product] data.
For example:
case class A(intValue: Int)
case class B(optionA: Option[A])
val b = B(Some(A(5)))
ScalaReflection.convertToCatalyst(b) returns Seq(A(5)) instead of Seq(Seq(5))",,apachespark,marmbrus,r3nat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 00:57:17 UTC 2014,,,,,,,,,,"0|i20rmv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/14 11:45;apachespark;User 'r3natko' has created a pull request for this issue:
https://github.com/apache/spark/pull/2641;;;","06/Oct/14 00:57;marmbrus;Issue resolved by pull request 2641
[https://github.com/apache/spark/pull/2641];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sphinx build warnings,SPARK-3773,12745655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,cocoatomo,cocoatomo,03/Oct/14 01:06,06/Oct/14 21:08,14/Jul/23 06:26,06/Oct/14 21:08,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,docs,docstrings,pyspark,,,"When building Sphinx documents for PySpark, we have 12 warnings.
Their causes are almost docstrings in broken ReST format.

To reproduce this issue, we should run following commands on the commit: 6e27cb630de69fa5acb510b4e2f6b980742b1957.

{quote}
$ cd ./python/docs
$ make clean html
...
/Users/<user>/MyRepos/Scala/spark/python/pyspark/__init__.py:docstring of pyspark.SparkContext.sequenceFile:4: ERROR: Unexpected indentation.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/__init__.py:docstring of pyspark.RDD.saveAsSequenceFile:4: ERROR: Unexpected indentation.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/classification.py:docstring of pyspark.mllib.classification.LogisticRegressionWithSGD.train:14: ERROR: Unexpected indentation.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/classification.py:docstring of pyspark.mllib.classification.LogisticRegressionWithSGD.train:16: WARNING: Definition list ends without a blank line; unexpected unindent.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/classification.py:docstring of pyspark.mllib.classification.LogisticRegressionWithSGD.train:17: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/classification.py:docstring of pyspark.mllib.classification.SVMWithSGD.train:14: ERROR: Unexpected indentation.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/classification.py:docstring of pyspark.mllib.classification.SVMWithSGD.train:16: WARNING: Definition list ends without a blank line; unexpected unindent.
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/classification.py:docstring of pyspark.mllib.classification.SVMWithSGD.train:17: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/<user>/MyRepos/Scala/spark/python/docs/pyspark.mllib.rst:50: WARNING: missing attribute mentioned in :members: or __all__: module pyspark.mllib.regression, attribute RidgeRegressionModelLinearRegressionWithSGD
/Users/<user>/MyRepos/Scala/spark/python/pyspark/mllib/tree.py:docstring of pyspark.mllib.tree.DecisionTreeModel.predict:3: ERROR: Unexpected indentation.
...
checking consistency... /Users/<user>/MyRepos/Scala/spark/python/docs/modules.rst:: WARNING: document isn't included in any toctree
...
copying static files... WARNING: html_static_path entry u'/Users/<user>/MyRepos/Scala/spark/python/docs/_static' does not exist
...
build succeeded, 12 warnings.
{quote}
","Mac OS X 10.9.5, Python 2.7.8, IPython 2.2.0, Jinja2==2.7.3, MarkupSafe==0.23, Pygments==1.6, Sphinx==1.2.3, docutils==0.12, numpy==1.9.0",apachespark,cocoatomo,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3420,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 21:08:56 UTC 2014,,,,,,,,,,"0|i20rav:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Oct/14 02:07;cocoatomo;Using Sphinx to generate API docs for PySpark;;;","04/Oct/14 14:20;apachespark;User 'cocoatomo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2653;;;","06/Oct/14 21:08;joshrosen;Issue resolved by pull request 2653
[https://github.com/apache/spark/pull/2653];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD operation on IPython REPL failed with an illegal port number,SPARK-3772,12745645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cocoatomo,cocoatomo,03/Oct/14 00:03,09/Oct/14 23:08,14/Jul/23 06:26,09/Oct/14 23:08,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,pyspark,,,,,"To reproduce this issue, we should execute following commands on the commit: 6e27cb630de69fa5acb510b4e2f6b980742b1957.

{quote}
$ PYSPARK_PYTHON=ipython ./bin/pyspark
...
In [1]: file = sc.textFile('README.md')
In [2]: file.first()
...
14/10/03 08:50:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/10/03 08:50:13 WARN LoadSnappy: Snappy native library not loaded
14/10/03 08:50:13 INFO FileInputFormat: Total input paths to process : 1
14/10/03 08:50:13 INFO SparkContext: Starting job: runJob at PythonRDD.scala:334
14/10/03 08:50:13 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:334) with 1 output partitions (allowLocal=true)
14/10/03 08:50:13 INFO DAGScheduler: Final stage: Stage 0(runJob at PythonRDD.scala:334)
14/10/03 08:50:13 INFO DAGScheduler: Parents of final stage: List()
14/10/03 08:50:13 INFO DAGScheduler: Missing parents: List()
14/10/03 08:50:13 INFO DAGScheduler: Submitting Stage 0 (PythonRDD[2] at RDD at PythonRDD.scala:44), which has no missing parents
14/10/03 08:50:13 INFO MemoryStore: ensureFreeSpace(4456) called with curMem=57388, maxMem=278019440
14/10/03 08:50:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.4 KB, free 265.1 MB)
14/10/03 08:50:13 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[2] at RDD at PythonRDD.scala:44)
14/10/03 08:50:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
14/10/03 08:50:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1207 bytes)
14/10/03 08:50:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
14/10/03 08:50:14 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.IllegalArgumentException: port out of range:1027423549
	at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143)
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:188)
	at java.net.Socket.<init>(Socket.java:244)
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:75)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:90)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:62)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:100)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:71)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{quote}","Mac OS X 10.9.5, Python 2.7.8, IPython 2.2.0",apachespark,cocoatomo,joshrosen,laserson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 23:08:28 UTC 2014,,,,,,,,,,"0|i20r8n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Oct/14 01:16;joshrosen;Can you post the SHA of the commit that you were using when you saw this?;;;","03/Oct/14 02:01;cocoatomo;Thank you for the advice.

I added the commit hash on the description.;;;","03/Oct/14 02:28;joshrosen;Ah, I see the problem:

PythonWorkerFactory also passes the ""-u"" flag when creating Python workers and daemons, which doesn't work in IPython.  I noticed one of the uses of ""-u"" when reviewing your PR, but missed these uses:

https://github.com/apache/spark/blob/42d5077fd3f2c37d1cd23f4c81aa89286a74cb40/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala#L111

https://github.com/apache/spark/blob/42d5077fd3f2c37d1cd23f4c81aa89286a74cb40/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala#L152

I guess we need to apply the same PYTHONUNBUFFERED fix here, too.  Sorry for overlooking this.

It's strange that PythonWorkerFactory didn't report this error in a more graceful way, though.  I think that IPython printed an error message before our code had a chance to run, and in startDaemon() we expected to read an integer from stdout but instead received text.  There are probably less brittle mechanisms for communicating the daemon process's port to its parent (SPARK-2313 is an issue that partially addresses this).

I can fix this and open a PR.  If you'd like to do it yourself, just let me know and I'd be glad to review it.;;;","03/Oct/14 02:31;joshrosen;The reason that we never hit this before is that setting IPYTHON=1 caused Spark to only use IPython on the master; the workers and daemons still launched through regular `python`.  The old behavior might actually be preferable from a performance standpoint, since `ipython` might take longer to start up (this is less of an issue nowadays thanks to the worker re-use patch).;;;","03/Oct/14 02:46;joshrosen;Actually, I'm surprised that nobody reported this issue in SPARK-3265, since that patch actually allowed you to set PYSPARK_PYTHON in order to use a custom ipython executable.;;;","04/Oct/14 02:01;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2651;;;","09/Oct/14 23:08;joshrosen;Issue resolved by pull request 2651
[https://github.com/apache/spark/pull/2651];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AppendingParquetOutputFormat should use reflection to prevent from breaking binary-compatibility.,SPARK-3771,12745635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,02/Oct/14 23:34,13/Oct/14 20:43,14/Jul/23 06:26,13/Oct/14 20:43,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Original problem is [SPARK-3764|https://issues.apache.org/jira/browse/SPARK-3764].

{{AppendingParquetOutputFormat}} uses a binary-incompatible method {{context.getTaskAttemptID}}.
This causes binary-incompatible of Spark itself, i.e. if Spark itself is built against hadoop-1, the artifact is for only hadoop-1, and vice versa.",,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3764,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 20:43:52 UTC 2014,,,,,,,,,,"0|i20r6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/14 23:45;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2638;;;","13/Oct/14 20:43;marmbrus;Issue resolved by pull request 2638
[https://github.com/apache/spark/pull/2638];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The example of building with sbt should be ""sbt assembly"" instead of ""sbt compile""",SPARK-3763,12745444,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sarutak,sarutak,sarutak,02/Oct/14 08:50,03/Oct/14 20:27,14/Jul/23 06:26,03/Oct/14 20:27,1.2.0,,,,,,,1.2.0,,,,,,Documentation,,,,0,,,,,,"In building-spark.md, there are some examples for making assembled package with maven but the example for building with sbt is only about for compiling.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 02 08:55:27 UTC 2014,,,,,,,,,,"0|i20pw7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"02/Oct/14 08:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clear all SparkEnv references after stop,SPARK-3762,12745420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,02/Oct/14 07:05,07/Oct/14 19:06,14/Jul/23 06:26,07/Oct/14 19:06,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"SparkEnv is cached in ThreadLocal object, so after stop and create a new SparkContext, old SparkEnv is still used by some threads, it will trigger many problems.

We should clear all the references after stop a SparkEnv.",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 02 07:15:32 UTC 2014,,,,,,,,,,"0|i20pqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/14 07:15;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2624;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class anonfun$1 not found exception / sbt 13.x / Scala 2.10.4,SPARK-3761,12745284,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,Legart,Legart,01/Oct/14 15:22,07/Oct/14 18:55,14/Jul/23 06:26,07/Oct/14 18:55,1.0.0,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"I have Scala code:

val master = ""spark://<server address>:7077""

    val sc = new SparkContext(new SparkConf()
      .setMaster(master)
      .setAppName(""SparkQueryDemo 01"")
      .set(""spark.executor.memory"", ""512m""))

val count2 = sc         .textFile(""hdfs://<server address>:8020/tmp/data/risk/account.txt"")
      .filter(line  => line.contains(""Word""))
      .count()

I've got such an error:
[error] (run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:0 failed 4 times, most
recent failure: Exception failure in TID 6 on host <server address>: java.lang.ClassNotFoundExcept
ion: SimpleApp$$anonfun$1

My dependencies :

object Version {
  val spark        = ""1.0.0-cdh5.1.0""
}

object Library {
  val sparkCore      = ""org.apache.spark""  % ""spark-assembly_2.10""  % Version.spark
}

My OS is Win 7, sbt 13.5, Scala 2.10.4",,Legart,pwendell,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1410,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 18:54:51 UTC 2014,,,,,,,,,,"0|i20ovz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/14 15:55;Legart;The code in the description does not work under sbt 13.6 too

if I change code to:
val count2 = sc .textFile(""hdfs://<server address>:8020/tmp/data/risk/account.txt"")
//.filter(line => line.contains(""Word""))     it seems it can't work with anonymous function
.count()

Without filter function with lambda inside, it does work!;;;","02/Oct/14 00:03;pwendell;Is this an sbt bug rather than a spark one? If it works in older versions of SBT but doesn't work in 13.x...;;;","02/Oct/14 10:54;Legart;I've tried sbt 12.4, but unfortunately with no luck. I'd like to emphasize that I am using spark-assembly lib with version _2.10;1.0.0-cdh5.1.0 from  repository http://repository.cloudera.com/artifactory/repo/ as we are using cloudera image CDH 5.1.0 and can't use any other version due to serialization issues.;;;","02/Oct/14 11:05;Legart;Created the same bug in Cloudera Jira: 
https://issues.cloudera.org/browse/DISTRO-647;;;","03/Oct/14 18:50;vanzin;How exactly are you packaging and submitting your application? What are the contents of your app's jar file? CDH doesn't support Windows, but still, that information can help figure out where the problem lies.;;;","07/Oct/14 18:54;Legart;After I've added line sc.addJar(""<full path to jar with code for querying spark>"") it works for sbt (but does not for maven project, happily it is enough for me). We can close the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmitDriverBootstrapper should return exit code of driver process,SPARK-3759,12745213,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ericeijkelenboom,ericeijkelenboom,ericeijkelenboom,01/Oct/14 09:53,09/Oct/14 02:15,14/Jul/23 06:26,03/Oct/14 01:06,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Deploy,,,,0,,,,,,"SparkSubmitDriverBootstrapper.scala currently always returns exit code 0. Instead, it should return the exit code of the driver process.

Suggested code change in SparkSubmitDriverBootstrapper, line 157: 

{code}
val returnCode = process.waitFor()
sys.exit(returnCode)
{code}

Workaround for this issue: 
Instead of specifying 'driver.extra*' properties in spark-defaults.conf, pass these properties to spark-submit directly. This will launch the driver program without the use of SparkSubmitDriverBootstrapper. ","Linux, Windows, Scala/Java",apachespark,codingcat,ericeijkelenboom,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 02:15:51 UTC 2014,,,,,,,,,,"0|i20og7:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"02/Oct/14 00:05;pwendell;Thanks for reporting this Eric - do you want to make a pull request with the suggested change? We can merge it.;;;","02/Oct/14 07:12;ericeijkelenboom;Yes, no problem!;;;","02/Oct/14 08:55;apachespark;User 'ericeijkelenboom' has created a pull request for this issue:
https://github.com/apache/spark/pull/2628;;;","09/Oct/14 02:15;codingcat;just passed by....

for merged PR, shall the status of the corresponding JIRA be RESOLVED?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn clean doesn't delete some files,SPARK-3757,12745204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,tsudukim,tsudukim,01/Oct/14 08:58,01/Oct/14 15:55,14/Jul/23 06:26,01/Oct/14 15:55,1.1.0,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"When we build spak using {{mvn package}},
{{/python/lib/py4j-0.8.2.1-src.zip}} is unzipped and some py4j files (*.py) are created into {{/python/build/}}.
But this directory and these files are not deleted while {{mvn clean}}.",,apachespark,pwendell,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 15:55:51 UTC 2014,,,,,,,,,,"0|i20oe7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/14 09:45;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/2613;;;","01/Oct/14 15:55;pwendell;Resolved by:
https://github.com/apache/spark/pull/2613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include possible MultiException when detecting port collisions,SPARK-3756,12745195,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,01/Oct/14 07:51,01/Oct/14 18:55,14/Jul/23 06:26,01/Oct/14 18:55,1.1.0,,,,,,,1.1.1,1.2.0,,,,,,,,,0,,,,,,a tiny bug in method  isBindCollision,,apachespark,pwendell,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 18:55:13 UTC 2014,,,,,,,,,,"0|i20oc7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"01/Oct/14 08:10;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2611;;;","01/Oct/14 18:55;pwendell;https://github.com/apache/spark/pull/2611;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not bind port 1 - 1024 to server in spark,SPARK-3755,12745192,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,01/Oct/14 07:40,11/Nov/14 03:25,14/Jul/23 06:26,03/Oct/14 00:54,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"Non-root user use port 1- 1024 to start jetty server will get the exception "" java.net.SocketException: Permission denied""",,aash,apachespark,pwendell,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4334,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 05 23:11:20 UTC 2014,,,,,,,,,,"0|i20obj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"01/Oct/14 07:45;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2610;;;","01/Oct/14 23:03;pwendell;Original fix was broken so I'm re-opening it.;;;","01/Oct/14 23:30;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2623;;;","03/Oct/14 02:34;scwf;Hi， Andrew Or, i can not change the title now;;;","05/Oct/14 23:11;aash;I think you can only edit the title and description of a ticket when the ticket is open (and this is now closed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in broadcast  of large RDD,SPARK-3749,12745049,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,30/Sep/14 20:23,01/Oct/14 18:22,14/Jul/23 06:26,01/Oct/14 18:22,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"1. broadcast is triggle unexpected
2. fd is leaked in JVM
3. broadcast is not unpersisted in JVM",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 18:22:10 UTC 2014,,,,,,,,,,"0|i20ngn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/14 21:00;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2603;;;","01/Oct/14 18:22;joshrosen;Issue resolved by pull request 2603
[https://github.com/apache/spark/pull/2603];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskResultGetter could incorrectly abort a stage if it cannot get result for a specific task,SPARK-3747,12745034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,30/Sep/14 19:08,01/Oct/14 07:31,14/Jul/23 06:26,01/Oct/14 07:31,,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"There is a ""return"" in logUncaughtExceptions, but we are catching all Exceptions. Instead, we should be catching NonFatal.
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 19:15:56 UTC 2014,,,,,,,,,,"0|i20ndb:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"30/Sep/14 19:15;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2599;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to lock hive client when creating tables,SPARK-3746,12745033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,30/Sep/14 19:00,01/Oct/14 21:37,14/Jul/23 06:26,01/Oct/14 21:37,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 21:37:50 UTC 2014,,,,,,,,,,"0|i20nd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/14 19:05;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2598;;;","01/Oct/14 21:37;marmbrus;Issue resolved by pull request 2598
[https://github.com/apache/spark/pull/2598];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"curl on maven search repo (apache rat) url returns search status, not jar file",SPARK-3745,12745002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,shaneknapp,shaneknapp,30/Sep/14 17:29,30/Sep/14 20:30,14/Jul/23 06:26,30/Sep/14 20:28,,,,,,,,1.1.1,1.2.0,,,,,Build,,,,0,build-failure,easyfix,test,,,"in spark/dev/check-license, there are four attempts to download the apache rat jar from maven:

{noformat}
  URL1=""http://search.maven.org/remotecontent?filepath=org/apache/rat/apache-rat/${RAT_VERSION}/apache-rat-${RAT_VERSION}.jar""
  URL2=""http://repo1.maven.org/maven2/org/apache/rat/apache-rat/${RAT_VERSION}/apache-rat-${RAT_VERSION}.jar""

*snip*

    if hash curl 2>/dev/null; then
      (curl --silent ${URL1} > ""$JAR_DL"" || curl --silent ${URL2} > ""$JAR_DL"") && mv ""$JAR_DL"" ""$JAR""
    elif hash wget 2>/dev/null; then
      (wget --quiet ${URL1} -O ""$JAR_DL"" || wget --quiet ${URL2} -O ""$JAR_DL"") && mv ""$JAR_DL"" ""$JAR""
{noformat}

the first attempt is on the search repo via curl, which returns a ""YEP!  WE FOUND IT!"" html blob:

{noformat}
[root@test01 sknapp]# curl --silent http://search.maven.org/remotecontent?filepath=org/apache/rat/apache-rat/0.10/apache-rat-0.10.jar > test.part
[root@test01 sknapp]# cat test.part
<html>
<head><title>302 Found</title></head>
<body bgcolor=""white"">
<center><h1>302 Found</h1></center>
<hr><center>nginx/0.8.55</center>
</body>
</html>
{noformat}

this is failing to DL for EVERY time the test is run.  i've run curl on the 2nd url, which points at the repo itself and it successfully downloads.  wget does the correct thing for both URLs.

there is also no error checking on the downloaded file, short of file existence.

potential fixes, in no particular order:
1) run unzip -tq ${$JAR}, check for 0 exist status to ensure it's a compressed archive
2) run wget before curl
3) only run curl on the 2nd URL (pointing directly to the repo)",centos 6.5,apachespark,joshrosen,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 20:30:13 UTC 2014,,,,,,,,,,"0|i20n67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/14 17:58;shaneknapp;https://github.com/apache/spark/pull/2596;;;","30/Sep/14 18:00;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/2596;;;","30/Sep/14 18:02;shaneknapp;it works.

from the console log:
{noformat}
=========================================================================
Running Apache RAT checks
=========================================================================
Attempting to fetch rat
Launching rat from /home/jenkins/workspace/SparkPullRequestBuilder/lib/apache-rat-0.10.jar
RAT checks passed.
{noformat}

from the CLI:
{noformat}
[root@test02 lib]# pwd && unzip -tq apache-rat-0.10.jar
/home/jenkins/workspace/SparkPullRequestBuilder/lib
No errors detected in compressed data of apache-rat-0.10.jar.
{noformat};;;","30/Sep/14 20:28;joshrosen;Issue resolved by pull request 2596
[https://github.com/apache/spark/pull/2596];;;","30/Sep/14 20:30;shaneknapp;thanks josh!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlumeStreamSuite will fail during port contention,SPARK-3744,12745001,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,pwendell,pwendell,30/Sep/14 17:19,30/Sep/14 22:19,14/Jul/23 06:26,30/Sep/14 22:19,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,This has been failing the master nightly builds. There are hard-coded ports and if they are contended it will break.,,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 19:40:44 UTC 2014,,,,,,,,,,"0|i20n5z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"30/Sep/14 19:40;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2601;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager.sendMessage may not propagate errors to MessageStatus,SPARK-3741,12744888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,zsxwing,zsxwing,30/Sep/14 08:25,14/Oct/14 09:42,14/Jul/23 06:26,09/Oct/14 18:27,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"If some network exception happens, ConnectionManager.sendMessage won't notify MessageStatus.",,apachespark,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 09:42:14 UTC 2014,,,,,,,,,,"0|i20mh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/14 08:35;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2593;;;","09/Oct/14 18:27;joshrosen;Issue resolved by pull request 2593
[https://github.com/apache/spark/pull/2593];;;","14/Oct/14 09:42;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/2794;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Workers should reconnect to Master if disconnected,SPARK-3736,12744828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mccheah,aash,aash,30/Sep/14 00:16,14/Nov/14 10:44,14/Jul/23 06:26,20/Oct/14 18:35,1.0.2,1.1.0,,,,,,1.2.0,,,,,,Spark Core,,,,1,,,,,,"In standalone mode, when a worker gets disconnected from the master for some reason it never attempts to reconnect.  In this situation you have to bounce the worker before it will reconnect to the master.

The preferred alternative is to follow what Hadoop does -- when there's a disconnect, attempt to reconnect at a particular interval until successful (I think it repeats indefinitely every 10sec).

This has been observed by:

- [~pkolaczk] in http://apache-spark-user-list.1001560.n3.nabble.com/Workers-disconnected-from-master-sometimes-and-never-reconnect-back-td6240.html
- [~romi-totango] in http://apache-spark-user-list.1001560.n3.nabble.com/Re-Workers-disconnected-from-master-sometimes-and-never-reconnect-back-td15335.html
- [~aash]",,aash,apachespark,codingcat,joshrosen,liyezhang556520,mcheah,pwendell,ravipesala,romi-totango,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1231,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 18:35:43 UTC 2014,,,,,,,,,,"0|i20m3r:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"30/Sep/14 00:23;aash;I can't tell for sure but this is possibly related to SPARK-704 or SPARK-1771;;;","09/Oct/14 01:58;pwendell;I spoke a bit offline with [~ilikerps] about this. I think the solution here is pretty simple - if the worker disconnects it should just try to re-initialize the connection to all drivers. It might need some slight refactoring so that on re-connect it will do this for an infinite number of attempts and checking to make sure there aren't races.

A good first step would be to get a grasp of how the general fault tolerance code works here around connections (there is a bit of complexity here around having failover between masters). Checkout the documentation on the Spark website about standalone fault tolerance. Right now the worker will simply hang out and do nothing when it loses the connection to the master, because it's expecting another master to re-connect to it. But this won't occur during the case where there is master failure.;;;","09/Oct/14 18:42;mcheah;Are the two linked cases above different though?

(1) If the worker itself gets locked up, the master sends a heartbeat but the worker doesn't respond, and the master drops the connection with the worker. However the master doesn't send a message to the worker indicating this disconnection, so the worker can't know to reconnect. To repro this I set a breakpoint in the Worker's heartbeat reception code and let the worker time out, and after the worker times out it never receives a DissassociatedEvent, nor is Worker.masterDisconnected() ever called.

(2) If the master crashes, the Worker receives a DissassociatedEvent and sits idly. We can fix this with making the Worker actively attempt to reconnect.

Clearly we can address the second case with the Worker actively trying to reconnect itself. But how can we address the first case?;;;","14/Oct/14 18:29;mcheah;I was curious if anyone had any feedback on my above comment?;;;","14/Oct/14 18:49;codingcat;if the worker itself timeout, the Master will remove the worker from idToWorker, 

when the worker is resumed later and sends heartbeat to Master again, Master detect this by attempting to find worker in idToWorker (search ""logWarning(""Got heartbeat from unregistered worker "" + workerId)"" in Master.scala)

you can simply replace logWarning with the logic of sending a message to worker to ask it to re-register
;;;","14/Oct/14 18:52;codingcat;BTW, master will not send heartbeat to Worker proactively ;;;","16/Oct/14 18:20;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/2828;;;","17/Oct/14 06:00;aash;The configuration for Hadoop's retry policy was added in HDFS-3504

{quote}
+   * Return the default retry policy used in RPC.
+   * 
+   * If dfs.client.retry.policy.enabled == false, use TRY_ONCE_THEN_FAIL.
+   * 
+   * Otherwise, first unwrap ServiceException if possible, and then 
+   * (1) use multipleLinearRandomRetry for
+   *     - SafeModeException, or
+   *     - IOException other than RemoteException, or
+   *     - ServiceException; and
+   * (2) use TRY_ONCE_THEN_FAIL for
+   *     - non-SafeMode RemoteException, or
+   *     - non-IOException.
+   *     
+   * Note that dfs.client.retry.max < 0 is not allowed.
{quote}

From https://github.com/apache/hadoop/commit/45fafc2b8fc1aab0a082600b0d50ad693491ea70#diff-36b19e9d8816002ed9dff8580055d3fbR44 it looks like the default policy is to retry every 10 seconds for 6 attempts, and then every 60 seconds for 10 attempts.;;;","20/Oct/14 18:35;joshrosen;Issue resolved by pull request 2828
[https://github.com/apache/spark/pull/2828];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DriverRunner should not read SPARK_HOME from submitter's environment,SPARK-3734,12744816,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,29/Sep/14 23:19,30/Sep/14 06:37,14/Jul/23 06:26,30/Sep/14 06:37,1.1.0,1.2.0,,,,,,1.1.1,1.2.0,,,,,Deploy,,,,0,,,,,,"If you use {{spark-submit}} in {{cluster}} mode to submit a job to a Spark Standalone cluster, and the {{JAVA_HOME}} environment variable is set on the submitting machine, then DriverRunner will attempt to use the _submitter's_ JAVA_HOME to launch the driver process (instead of the worker's JAVA_HOME), which can cause the job to fail unless the submitter and worker have Java installed in the same location.

This has a pretty simple fix: read JAVA_HOME from {{sys.env}} instead of {{command.environment}}; PR pending shortly.
",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 29 23:30:26 UTC 2014,,,,,,,,,,"0|i20m13:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"29/Sep/14 23:30;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD caching stops working in pyspark after some time,SPARK-3731,12744773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,straka,straka,29/Sep/14 20:03,07/Oct/14 20:09,14/Jul/23 06:26,07/Oct/14 20:09,1.0.2,1.1.0,1.2.0,,,,,1.1.1,1.2.0,,,,,PySpark,Spark Core,,,0,,,,,,"Consider a file F which when loaded with sc.textFile and cached takes up slightly more than half of free memory for RDD cache.

When in PySpark the following is executed:
  1) a = sc.textFile(F)
  2) a.cache().count()
  3) b = sc.textFile(F)
  4) b.cache().count()
and then the following is repeated (for example 10 times):
  a) a.unpersist().cache().count()
  b) b.unpersist().cache().count()
after some time, there are no RDD cached in memory.

Also, since that time, no other RDD ever gets cached (the worker always reports something like ""WARN CacheManager: Not enough space to cache partition rdd_23_5 in memory! Free memory is 277478190 bytes."", even if rdd_23_5 is ~50MB). The Executors tab of the Application Detail UI shows that all executors have 0MB memory used (which is consistent with the CacheManager warning).

When doing the same in scala, everything works perfectly.

I understand that this is a vague description, but I do no know how to describe the problem better.","Linux, 32bit, both in local mode or in standalone cluster mode",apachespark,farrellee,joshrosen,pwendell,straka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/14 07:20;straka;spark-3731.log;https://issues.apache.org/jira/secure/attachment/12672509/spark-3731.log","02/Oct/14 07:20;straka;spark-3731.py;https://issues.apache.org/jira/secure/attachment/12672507/spark-3731.py","02/Oct/14 07:20;straka;spark-3731.txt.bz2;https://issues.apache.org/jira/secure/attachment/12672508/spark-3731.txt.bz2","29/Sep/14 20:09;straka;worker.log;https://issues.apache.org/jira/secure/attachment/12671861/worker.log",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 20:09:05 UTC 2014,,,,,,,,,,"0|i20lrj:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"29/Sep/14 20:09;straka;Sample worker.log showing the problem. For example, consider rdd_1_1. It has size 46.3MB. At the beginning, the caching work, but that stops -- the last time rdd_1_1 does not fit into cache, the following is reported:
{{14/09/29 21:53:10 WARN CacheManager: Not enough space to cache partition rdd_1_1 in memory! Free memory is 148908945 bytes.}}
;;;","02/Oct/14 00:11;pwendell;Thanks for reporting this - can you reproduce by any chance in local mode with a dataset you can attach to the JIRA? That makes it much easier for us to track down.;;;","02/Oct/14 06:18;straka;I will get to it later today and attach a dataset and program which exhibit this behaviour locally. I believe I will find it because I saw this behaviour in many local runs.;;;","02/Oct/14 07:29;straka;I have attached reproducible program, input file and log from the local run. You have to uncompress the input file before executing. I have used unmodified spark-1.1.0-bin-hadoop2.4.tgz.

Before the for loop, 3 out of 4 partitions are cached. After first iteration of the for loop, only 2 are cached, after second iteration only 1 and after third iteration 0 partitions are cached.

I believe this behaviour is not dependent on StorageLevel used, I am using MEMORY_ONLY for the cached partitions to be big even on easily compresible file, but I have encountered the issue when using MEMORY_ONLY_SER.

I also believe that this behaviour is triggered only when some RDD partition does _not_ fit into memory. Before that happens, caching and uncaching work as expected.

An equivalent scala program seems to be working fine.;;;","06/Oct/14 04:47;pwendell;[~davies] any chance you can take a look at this?;;;","06/Oct/14 06:10;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2668;;;","07/Oct/14 20:09;joshrosen;Fixed by Davies' PR, which I backported to 1.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null-pointer when constructing a HiveContext when settings are present,SPARK-3729,12744752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,29/Sep/14 19:00,22/Oct/14 06:55,14/Jul/23 06:26,01/Oct/14 23:35,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"{code}
java.lang.NullPointerException
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:307)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:270)
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:242)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:78)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:78)
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:76)
...
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4037,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 23:35:13 UTC 2014,,,,,,,,,,"0|i20ln3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"29/Sep/14 19:20;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2583;;;","01/Oct/14 23:35;marmbrus;Issue resolved by pull request 2583
[https://github.com/apache/spark/pull/2583];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broadcast Variables above 2GB break in PySpark,SPARK-3721,12744704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,bmiller1,bmiller1,29/Sep/14 15:49,19/Nov/14 00:18,14/Jul/23 06:26,19/Nov/14 00:18,1.1.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"The bug displays 3 unique failure modes in PySpark, all of which seem to be related to broadcast variable size. Note that the tests below ran python 2.7.3 on all machines and used the Spark 1.1.0 binaries.

**BLOCK 1** [no problem]
{noformat}
import cPickle
from pyspark import SparkContext

def check_pre_serialized(size):
    msg = cPickle.dumps(range(2 ** size))
    print 'serialized length:', len(msg)
    bvar = sc.broadcast(msg)
    print 'length recovered from broadcast variable:', len(bvar.value)
    print 'correct value recovered:', msg == bvar.value
    bvar.unpersist()    

def check_unserialized(size):
    msg = range(2 ** size)
    bvar = sc.broadcast(msg)
    print 'correct value recovered:', msg == bvar.value
    bvar.unpersist()

SparkContext.setSystemProperty('spark.executor.memory', '15g')
SparkContext.setSystemProperty('spark.cores.max', '5')
sc = SparkContext('spark://crosby.research.intel-research.net:7077', 'broadcast_bug')
{noformat}

**BLOCK 2**  [no problem]
{noformat}
check_pre_serialized(20)
> serialized length: 9374656
> length recovered from broadcast variable: 9374656
> correct value recovered: True
{noformat}

**BLOCK 3**  [no problem]
{noformat}
check_unserialized(20)
> correct value recovered: True
{noformat}

**BLOCK 4**  [no problem]
{noformat}
check_pre_serialized(27)
> serialized length: 1499501632
> length recovered from broadcast variable: 1499501632
> correct value recovered: True
{noformat}

**BLOCK 5**  [no problem]
{noformat}
check_unserialized(27)
> correct value recovered: True
{noformat}

**BLOCK 6**  **[ERROR 1: unhandled error from cPickle.dumps inside sc.broadcast]**
{noformat}
check_pre_serialized(28)
.....
> /home/spark/greatest/python/pyspark/serializers.py in dumps(self, obj)
>     354
>     355     def dumps(self, obj):
> --> 356         return cPickle.dumps(obj, 2)
>     357
>     358     loads = cPickle.loads
>
> SystemError: error return without exception set
{noformat}

**BLOCK 7**  [no problem]
{noformat}
check_unserialized(28)
> correct value recovered: True
{noformat}

**BLOCK 8**  **[ERROR 2: no error occurs and *incorrect result* is returned]**
{noformat}
check_pre_serialized(29)
> serialized length: 6331339840
> length recovered from broadcast variable: 2036372544
> correct value recovered: False
{noformat}

**BLOCK 9**  **[ERROR 3: unhandled error from zlib.compress inside sc.broadcast]**
{noformat}
check_unserialized(29)
......
> /home/spark/greatest/python/pyspark/serializers.py in dumps(self, obj)
>     418 
>     419     def dumps(self, obj):
> --> 420         return zlib.compress(self.serializer.dumps(obj), 1)
>     421 
>     422     def loads(self, obj):
> 
> OverflowError: size does not fit in an int
{noformat}

**BLOCK 10**  [ERROR 1]
{noformat}
check_pre_serialized(30)
...same as above...
{noformat}

**BLOCK 11**  [ERROR 3]
{noformat}
check_unserialized(30)
...same as above...
{noformat}
",,aash,apachespark,bmiller1,darabos,davies,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 00:18:53 UTC 2014,,,,,,,,,,"0|i20lcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/14 06:29;davies;[~bmiller1] This is an PR for this https://github.com/apache/spark/pull/2659, could you help to test it?;;;","05/Oct/14 06:30;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2659;;;","27/Oct/14 14:34;darabos;We are hitting all kinds of MaxInt and array size limits when broadcasting a 12GB beast from Scala too.
;;;","19/Nov/14 00:18;joshrosen;Issue resolved by pull request 2659
[https://github.com/apache/spark/pull/2659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN integration test is flaky,SPARK-3710,12744516,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,pwendell,pwendell,27/Sep/14 22:23,06/Oct/14 21:15,14/Jul/23 06:26,03/Oct/14 18:37,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"This has been regularly failing the master build:

Example failure: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/738/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.2,label=centos/#showFailuresLink

One thing to look at is whether the YARN mini cluster makes assumptions about being able to bind to specific ports.

{code}
sbt.ForkMain$ForkError: Call From test04.amplab/10.123.1.2 to test04.amplab:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.getClusterMetrics(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:152)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.getClusterMetrics(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:246)
	at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:69)
	at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:69)
	at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
	at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:35)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:68)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:310)
	at org.apache.spark.deploy.yarn.YarnClusterDriver$.main(YarnClusterSuite.scala:140)
	at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply$mcV$sp(YarnClusterSuite.scala:91)
	at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(YarnClusterSuite.scala:89)
	at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(YarnClusterSuite.scala:89)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:158)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1121)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1559)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:155)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:167)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1559)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:200)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:200)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:200)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1559)
	at org.scalatest.Suite$class.run(Suite.scala:1423)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1559)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:204)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:204)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:204)
	at org.apache.spark.deploy.yarn.YarnClusterSuite.org$scalatest$BeforeAndAfterAll$$super$run(YarnClusterSuite.scala:35)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.deploy.yarn.YarnClusterSuite.run(YarnClusterSuite.scala:35)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:444)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:651)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 66 more
{code}",,andrewor14,apachespark,pwendell,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2778,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 21:15:36 UTC 2014,,,,,,,,,,"0|i20k7j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Oct/14 18:38;andrewor14;https://github.com/apache/spark/pull/2605;;;","03/Oct/14 18:54;vanzin;Hmm. For some the e-mail for this bug ended up in my spam box. Anyway, fix was tracked also in SPARK-2778.;;;","03/Oct/14 19:00;vanzin;I filed a Yarn bug (YARN-2642), although we can't get rid of the workaround since we need to support existing versions of Yarn.;;;","06/Oct/14 21:15;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2682;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors don't always report broadcast block removal properly back to the driver,SPARK-3709,12744514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rxin,pwendell,pwendell,27/Sep/14 22:02,22/Oct/14 22:24,14/Jul/23 06:26,22/Oct/14 22:24,,,,,,,,1.0.3,1.1.1,1.2.0,,,,Spark Core,,,,0,,,,,,,,apachespark,joshrosen,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 22 22:24:29 UTC 2014,,,,,,,,,,"0|i20k73:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"29/Sep/14 20:38;rxin;Adding stack trace
{code}
[info] - Unpersisting TorrentBroadcast on executors only in distributed mode *** FAILED ***
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 17, localhost): java.io.IOException: sendMessageReliably failed with ACK that signalled a remote error
[info]         org.apache.spark.network.nio.ConnectionManager$$anonfun$14.apply(ConnectionManager.scala:864)
[info]         org.apache.spark.network.nio.ConnectionManager$$anonfun$14.apply(ConnectionManager.scala:856)
[info]         org.apache.spark.network.nio.ConnectionManager$MessageStatus.markDone(ConnectionManager.scala:61)
[info]         org.apache.spark.network.nio.ConnectionManager.org$apache$spark$network$nio$ConnectionManager$$handleMessage(ConnectionManager.scala:655)
[info]         org.apache.spark.network.nio.ConnectionManager$$anon$10.run(ConnectionManager.scala:515)
[info]         java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]         java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]         java.lang.Thread.run(Thread.java:745)
[info] Driver stacktrace:
[info]   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1192)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1181)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1180)
[info]   at scala.coSpark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
llection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1180)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:695)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:695)
[info]   at scala.Option.foreach(Option.scala:236)
[info]   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:695)
[info]   ...
{code};;;","29/Sep/14 22:05;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2585;;;","30/Sep/14 04:11;rxin;Hanging driver stack trace
{code}
""pool-1-thread-1-ScalaTest-running-BroadcastSuite"" prio=10 tid=0x00007f2114812000 nid=0xc8c in Object.wait() [0x00007f20bb8fd000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:503)
        at org.apache.spark.scheduler.JobWaiter.awaitResult(JobWaiter.scala:73)
        - locked <0x00000007a2ff4bb8> (a org.apache.spark.scheduler.JobWaiter)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:512)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1087)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1104)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1118)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1132)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:775)
        at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:291)
        at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:232)
        at org.apache.spark.broadcast.BroadcastSuite$$anonfun$13.apply$mcV$sp(BroadcastSuite.scala:112)
        at org.apache.spark.broadcast.BroadcastSuite$$anonfun$13.apply(BroadcastSuite.scala:112)
        at org.apache.spark.broadcast.BroadcastSuite$$anonfun$13.apply(BroadcastSuite.scala:112)
        at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
        at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
        at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
        at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
        at org.scalatest.Transformer.apply(Transformer.scala:22)
        at org.scalatest.Transformer.apply(Transformer.scala:20)
        at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:158)
        at org.scalatest.Suite$class.withFixture(Suite.scala:1121)
        at org.scalatest.FunSuite.withFixture(FunSuite.scala:1559)
        at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:155)
        at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
        at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
        at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
        at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:167)
        at org.apache.spark.broadcast.BroadcastSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(BroadcastSuite.scala:26)
        at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
        at org.apache.spark.broadcast.BroadcastSuite.runTest(BroadcastSuite.scala:26)
   ...
{code}

Executor log
{code}
14/09/29 20:35:57.254 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
14/09/29 20:35:57.502 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicabl
e
14/09/29 20:35:57.716 INFO SecurityManager: Changing view acls to: root
14/09/29 20:35:57.717 INFO SecurityManager: Changing modify acls to: root
14/09/29 20:35:57.717 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); u
sers with modify permissions: Set(root)
14/09/29 20:35:58.096 INFO Slf4jLogger: Slf4jLogger started
14/09/29 20:35:58.136 INFO Remoting: Starting remoting
14/09/29 20:35:58.279 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@localhost:42339]
14/09/29 20:35:58.280 INFO Remoting: Remoting now listens on addresses: [akka.tcp://driverPropsFetcher@localhost:42339]
14/09/29 20:35:58.287 INFO Utils: Successfully started service 'driverPropsFetcher' on port 42339.
14/09/29 20:35:58.461 INFO SecurityManager: Changing view acls to: root
14/09/29 20:35:58.461 INFO SecurityManager: Changing modify acls to: root
14/09/29 20:35:58.462 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); u
sers with modify permissions: Set(root)
14/09/29 20:35:58.466 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/09/29 20:35:58.467 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/09/29 20:35:58.493 INFO Slf4jLogger: Slf4jLogger started
14/09/29 20:35:58.499 INFO Remoting: Starting remoting
14/09/29 20:35:58.502 INFO Remoting: Remoting shut down
14/09/29 20:35:58.503 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
14/09/29 20:35:58.540 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@localhost:39122]
14/09/29 20:35:58.540 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkExecutor@localhost:39122]
14/09/29 20:35:58.541 INFO Utils: Successfully started service 'sparkExecutor' on port 39122.
14/09/29 20:35:58.545 INFO CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@localhost:59125/user/CoarseGrainedScheduler
14/09/29 20:35:58.546 INFO WorkerWatcher: Connecting to worker akka.tcp://sparkWorker2@localhost:56210/user/Worker
14/09/29 20:35:58.557 INFO WorkerWatcher: Successfully connected to akka.tcp://sparkWorker2@localhost:56210/user/Worker
14/09/29 20:35:58.562 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
14/09/29 20:35:58.570 INFO SecurityManager: Changing view acls to: root
14/09/29 20:35:58.571 INFO SecurityManager: Changing modify acls to: root
14/09/29 20:35:58.571 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); u
sers with modify permissions: Set(root)
14/09/29 20:35:58.592 INFO Slf4jLogger: Slf4jLogger started
14/09/29 20:35:58.596 INFO Remoting: Starting remoting
14/09/29 20:35:58.610 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@localhost:57639]
14/09/29 20:35:58.610 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkExecutor@localhost:57639]
14/09/29 20:35:58.611 INFO Utils: Successfully started service 'sparkExecutor' on port 57639.
14/09/29 20:35:58.617 INFO AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@localhost:59125/user/MapOutputTracker
14/09/29 20:35:58.643 INFO AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@localhost:59125/user/BlockManagerMaster
14/09/29 20:35:58.672 INFO Utils: Successfully started service 'Connection manager for block manager' on port 50721.
14/09/29 20:35:58.673 INFO ConnectionManager: Bound socket to port 50721 with id = ConnectionManagerId(localhost,50721)
14/09/29 20:35:58.678 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140929203558-9c60
14/09/29 20:35:58.684 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
14/09/29 20:35:58.695 INFO BlockManagerMaster: Trying to register BlockManager
14/09/29 20:35:58.704 INFO BlockManagerMaster: Registered BlockManager
14/09/29 20:35:58.828 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@localhost:59125/user/HeartbeatReceiver
14/09/29 20:35:58.837 INFO CoarseGrainedExecutorBackend: Got assigned task 0
14/09/29 20:35:58.839 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
14/09/29 20:35:58.926 INFO TorrentBroadcast: Started reading broadcast variable 1
14/09/29 20:35:58.969 INFO SendingConnection: Initiating connection to [localhost/127.0.0.1:37826]
14/09/29 20:35:58.971 INFO SendingConnection: Connected to [localhost/127.0.0.1:37826], 1 messages pending
14/09/29 20:35:58.981 INFO ConnectionManager: Accepted connection from [localhost/127.0.0.1:48675]
14/09/29 20:35:59.000 INFO MemoryStore: ensureFreeSpace(1576) called with curMem=0, maxMem=278302556
14/09/29 20:35:59.003 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1576.0 B, free 265.4 MB)
14/09/29 20:35:59.011 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
14/09/29 20:35:59.013 INFO TorrentBroadcast: Reading broadcast variable 1 took 0.085507632 s
14/09/29 20:35:59.104 INFO MemoryStore: ensureFreeSpace(2232) called with curMem=1576, maxMem=278302556
14/09/29 20:35:59.105 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.2 KB, free 265.4 MB)
14/09/29 20:35:59.136 INFO TorrentBroadcast: Started reading broadcast variable 0
14/09/29 20:35:59.141 INFO SendingConnection: Initiating connection to [localhost/127.0.0.1:55386]
14/09/29 20:35:59.141 INFO SendingConnection: Connected to [localhost/127.0.0.1:55386], 1 messages pending
14/09/29 20:35:59.149 INFO ConnectionManager: Accepted connection from [localhost/127.0.0.1:48681]
14/09/29 20:35:59.152 INFO MemoryStore: ensureFreeSpace(278) called with curMem=3808, maxMem=278302556
14/09/29 20:35:59.152 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 278.0 B, free 265.4 MB)
14/09/29 20:35:59.156 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
14/09/29 20:35:59.156 INFO TorrentBroadcast: Reading broadcast variable 0 took 0.019877279 s
14/09/29 20:35:59.158 INFO MemoryStore: ensureFreeSpace(232) called with curMem=4086, maxMem=278302556
14/09/29 20:35:59.158 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 232.0 B, free 265.4 MB)
14/09/29 20:35:59.168 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 715 bytes result sent to driver
14/09/29 20:35:59.173 INFO CoarseGrainedExecutorBackend: Got assigned task 3
14/09/29 20:35:59.173 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
14/09/29 20:35:59.212 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 715 bytes result sent to driver
14/09/29 20:35:59.217 INFO CoarseGrainedExecutorBackend: Got assigned task 5
14/09/29 20:35:59.218 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
14/09/29 20:35:59.247 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 715 bytes result sent to driver
14/09/29 20:35:59.252 INFO CoarseGrainedExecutorBackend: Got assigned task 7
14/09/29 20:35:59.252 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
14/09/29 20:35:59.281 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 715 bytes result sent to driver
14/09/29 20:35:59.286 INFO CoarseGrainedExecutorBackend: Got assigned task 9
14/09/29 20:35:59.287 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
14/09/29 20:35:59.313 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 715 bytes result sent to driver
14/09/29 20:35:59.332 INFO BlockManager: Removing broadcast 0
14/09/29 20:35:59.334 INFO BlockManager: Removing block broadcast_0
14/09/29 20:35:59.335 INFO MemoryStore: Block broadcast_0 of size 232 dropped from memory (free 278298470)
14/09/29 20:35:59.335 INFO BlockManager: Removing block broadcast_0_piece0
14/09/29 20:35:59.336 INFO MemoryStore: Block broadcast_0_piece0 of size 278 dropped from memory (free 278298748)
14/09/29 20:35:59.351 INFO CoarseGrainedExecutorBackend: Got assigned task 11
14/09/29 20:35:59.351 INFO Executor: Running task 1.0 in stage 1.0 (TID 11)
14/09/29 20:35:59.370 INFO TorrentBroadcast: Started reading broadcast variable 2
14/09/29 20:35:59.376 INFO MemoryStore: ensureFreeSpace(1577) called with curMem=3808, maxMem=278302556
14/09/29 20:35:59.377 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1577.0 B, free 265.4 MB)
14/09/29 20:35:59.379 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
14/09/29 20:35:59.380 INFO TorrentBroadcast: Reading broadcast variable 2 took 0.009623772 s
14/09/29 20:35:59.380 INFO MemoryStore: ensureFreeSpace(2232) called with curMem=5385, maxMem=278302556
14/09/29 20:35:59.381 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.2 KB, free 265.4 MB)
14/09/29 20:35:59.384 INFO TorrentBroadcast: Started reading broadcast variable 0
14/09/29 20:35:59.387 INFO SendingConnection: Initiating connection to [localhost/127.0.0.1:50721]
14/09/29 20:35:59.388 INFO ConnectionManager: Accepted connection from [localhost/127.0.0.1:48684]
14/09/29 20:35:59.388 INFO SendingConnection: Connected to [localhost/127.0.0.1:50721], 1 messages pending
14/09/29 20:35:59.390 ERROR NioBlockTransferService: block broadcast_0_piece0 cannot be found !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
14/09/29 20:35:59.390 ERROR NioBlockTransferService: block broadcast_0_piece0 cannot be found !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
14/09/29 20:57:21.717 INFO BlockManager: Removing broadcast 1
14/09/29 20:57:21.718 INFO BlockManager: Removing block broadcast_1_piece0
14/09/29 20:57:21.718 INFO MemoryStore: Block broadcast_1_piece0 of size 1576 dropped from memory (free 278296515)
14/09/29 20:57:21.721 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
14/09/29 20:57:21.722 INFO BlockManager: Removing block broadcast_1
14/09/29 20:57:21.722 INFO MemoryStore: Block broadcast_1 of size 2232 dropped from memory (free 278298747)
{code}
;;;","30/Sep/14 06:25;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2591;;;","22/Oct/14 22:24;joshrosen;It looks like this was fixed in all branches by Reynold's PRs, so I'm going to mark this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backticks aren't handled correctly in aliases,SPARK-3708,12744503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,marmbrus,marmbrus,27/Sep/14 18:25,26/Nov/14 04:01,14/Jul/23 06:26,01/Oct/14 22:44,1.1.0,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,"Here's a failing test case:

{code}
sql(""SELECT k FROM (SELECT `key` AS `k` FROM src) a"")
{code}",,andrewor14,apachespark,marmbrus,qiaohaijun,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 03:57:59 UTC 2014,,,,,,,,,,"0|i20k4n:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"29/Sep/14 21:23;ravipesala;I guess here you mentioned about HiveContext as there is no support of backtick in SqlContext.  I will work on this issue.Thank you.;;;","30/Sep/14 16:50;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2594;;;","01/Oct/14 22:44;marmbrus;Issue resolved by pull request 2594
[https://github.com/apache/spark/pull/2594];;;","01/Nov/14 10:10;qiaohaijun;+1;;;","26/Nov/14 03:57;andrewor14;This is back ported into 1.1.1 through https://github.com/apache/spark/pull/3199;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type Coercion for DIV doesn't work for non-numeric argument,SPARK-3707,12744481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,chenghao,chenghao,27/Sep/14 14:26,09/Oct/14 01:06,14/Jul/23 06:26,09/Oct/14 01:06,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"When do query like ""select key/2 from src"", it throws exception like
{panel}
14/09/27 22:14:36 ERROR SparkSQLDriver: Failed in [select key/2 from src]
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to datatype. Can not resolve due to differing types DoubleType, IntegerType on unresolved object, tree: (CAST(key#3, DoubleType) / 2)
	at org.apache.spark.sql.catalyst.expressions.BinaryArithmetic.dataType(arithmetic.scala:62)
	at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$Division$$anonfun$apply$9.applyOrElse(HiveTypeCoercion.scala:351)
	at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$Division$$anonfun$apply$9.applyOrElse(HiveTypeCoercion.scala:346)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:162)
{panel}",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 01:06:43 UTC 2014,,,,,,,,,,"0|i20jzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/14 15:00;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2559;;;","09/Oct/14 01:06;marmbrus;Issue resolved by pull request 2559
[https://github.com/apache/spark/pull/2559];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot run IPython REPL with IPYTHON set to ""1"" and PYSPARK_PYTHON unset",SPARK-3706,12744455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cocoatomo,cocoatomo,27/Sep/14 03:38,03/Oct/14 17:06,14/Jul/23 06:26,02/Oct/14 18:13,1.2.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,pyspark,,,,,"h3. Problem

The section ""Using the shell"" in Spark Programming Guide (https://spark.apache.org/docs/latest/programming-guide.html#using-the-shell) says that we can run pyspark REPL through IPython.
But a folloing command does not run IPython but a default Python executable.

{quote}
$ IPYTHON=1 ./bin/pyspark
Python 2.7.8 (default, Jul  2 2014, 10:14:46) 
...
{quote}

the spark/bin/pyspark script on the commit b235e013638685758885842dc3268e9800af3678 decides which executable and options it use folloing way.

# if PYSPARK_PYTHON unset
#* → defaulting to ""python""
# if IPYTHON_OPTS set
#* → set IPYTHON ""1""
# some python scripts passed to ./bin/pyspak → run it with ./bin/spark-submit
#* out of this issues scope
# if IPYTHON set as ""1""
#* → execute $PYSPARK_PYTHON (default: ipython) with arguments $IPYTHON_OPTS
#* otherwise execute $PYSPARK_PYTHON

Therefore, when PYSPARK_PYTHON is unset, python is executed though IPYTHON is ""1"".
In other word, when PYSPARK_PYTHON is unset, IPYTHON_OPS and IPYTHON has no effect on decide which command to use.

||PYSPARK_PYTHON||IPYTHON_OPTS||IPYTHON||resulting command||expected command||
|(unset → defaults to python)|(unset)|(unset)|python|(same)|
|(unset → defaults to python)|(unset)|1|python|ipython|
|(unset → defaults to python)|an_option|(unset → set to 1)|python an_option|ipython an_option|
|(unset → defaults to python)|an_option|1|python an_option|ipython an_option|
|ipython|(unset)|(unset)|ipython|(same)|
|ipython|(unset)|1|ipython|(same)|
|ipython|an_option|(unset → set to 1)|ipython an_option|(same)|
|ipython|an_option|1|ipython an_option|(same)|


h3. Suggestion

The pyspark script should determine firstly whether a user wants to run IPython or other executables.

# if IPYTHON_OPTS set
#* set IPYTHON ""1""
# if IPYTHON has a value ""1""
#* PYSPARK_PYTHON defaults to ""ipython"" if not set
# PYSPARK_PYTHON defaults to ""python"" if not set

See the pull request for more detailed modification.","Mac OS X 10.9.5, Python 2.7.8, IPython 2.2.0",apachespark,cocoatomo,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 03 17:06:17 UTC 2014,,,,,,,,,,"0|i20ju7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"27/Sep/14 03:50;apachespark;User 'cocoatomo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2554;;;","01/Oct/14 18:06;joshrosen;Do you know whether this problem occurs in the released version of 1.1.0 or in branch-1.1?  It looks like you tested this with b235e013638685758885842dc3268e9800af3678, which is part of the master branch (1.2.0).

I tried {{IPYTHON=1 ./bin/pyspark}} with one of the 1.1.0 binary distributions and it worked as expected.  Therefore, I'm going to change the ""Affects Versions"" to 1.2.0.  Let me know if you can reproduce this issue on 1.1.0, since we should backport a fix if this represents a regression between 1.0.2 and 1.1.0.;;;","02/Oct/14 17:15;cocoatomo;Thank you for the comment and modification, [~joshrosen].

Taking a quick look, this regression created at the commit [f38fab97c7970168f1bd81d4dc202e36322c95e3|https://github.com/apache/spark/commit/f38fab97c7970168f1bd81d4dc202e36322c95e3#diff-5dbcb82caf8131d60c73e82cf8d12d8aR107] on master branch.
Pushing ""ipython"" aside into a default value force us to set PYSPARK_PYTHON as ""ipython"", since PYSPARK_PYTHON defaults to ""python"" at the top of the ./bin/pyspark script.
This issue is a regression between 1.1.0 and 1.2.0, therefore affects only 1.2.0.;;;","02/Oct/14 18:13;joshrosen;Issue resolved by pull request 2554
[https://github.com/apache/spark/pull/2554];;;","03/Oct/14 17:06;joshrosen;This introduced a problem, since after this patch we now use IPython on the workers; see SPARK-3772 for more details.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add case for VoidObjectInspector in inspectorToDataType,SPARK-3705,12744448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,27/Sep/14 01:13,01/Oct/14 22:55,14/Jul/23 06:26,01/Oct/14 22:55,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 22:55:37 UTC 2014,,,,,,,,,,"0|i20jsn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"27/Sep/14 01:20;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2552;;;","01/Oct/14 22:55;marmbrus;Issue resolved by pull request 2552
[https://github.com/apache/spark/pull/2552];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short (TINYINT) incorrectly handled in thrift JDBC/ODBC server,SPARK-3704,12744443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,27/Sep/14 00:30,27/Nov/14 03:30,14/Jul/23 06:26,01/Oct/14 23:39,1.1.0,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,,,andrewor14,apachespark,marmbrus,pwangjing,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 04:03:45 UTC 2014,,,,,,,,,,"0|i20jrj:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"27/Sep/14 00:40;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2551;;;","01/Oct/14 23:39;marmbrus;Issue resolved by pull request 2551
[https://github.com/apache/spark/pull/2551];;;","15/Oct/14 17:34;pwangjing;Will this fix be merged to branch-v1.1 for the next 1.1.x release?

;;;","15/Oct/14 20:07;marmbrus;Yeah, this should be in 1.1.1.

;;;","26/Nov/14 04:03;andrewor14;Backported into 1.1.1 through https://github.com/apache/spark/pull/3199;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some clean-up work after the refactoring of MLlib's SerDe for PySpark,SPARK-3701,12744395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,mengxr,mengxr,26/Sep/14 20:29,01/Oct/14 00:10,14/Jul/23 06:26,01/Oct/14 00:10,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,0,,,,,,Fix some minor issues came with the refactoring of MLlib's SerDe for PySpark.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 00:10:54 UTC 2014,,,,,,,,,,"0|i20jh3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"26/Sep/14 20:35;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/2548;;;","01/Oct/14 00:10;mengxr;Issue resolved by pull request 2548
[https://github.com/apache/spark/pull/2548];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sbt console tasks don't clean up SparkContext,SPARK-3699,12744364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,willbenton,willbenton,willbenton,26/Sep/14 18:47,28/Sep/14 08:01,14/Jul/23 06:26,28/Sep/14 08:01,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Because the sbt console tasks for the hive and sql projects don't stop the SparkContext upon exit, users are faced with an ugly stack trace.",,apachespark,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 26 18:50:45 UTC 2014,,,,,,,,,,"0|i20ja7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/14 18:50;apachespark;User 'willb' has created a pull request for this issue:
https://github.com/apache/spark/pull/2547;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case sensitive check in spark sql is incompleted.,SPARK-3698,12744267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,cloud_fan,cloud_fan,26/Sep/14 12:19,17/Dec/14 20:45,14/Jul/23 06:26,17/Dec/14 20:45,,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Currently HiveQL is case insensitive. But if we run `SELECT a[0].A.A from t`, we don't do case insensitive check for ""A.A"", which is wrong.",,apachespark,cloud_fan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 17 20:45:41 UTC 2014,,,,,,,,,,"0|i20iov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/14 12:20;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/2543;;;","17/Dec/14 19:03;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3724;;;","17/Dec/14 20:45;marmbrus;Issue resolved by pull request 3724
[https://github.com/apache/spark/pull/3724];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 HistoryServer cann't list event Log when there was a no permissions directory in the $spark.eventLog.dir,SPARK-3697,12744250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,397090770,397090770,26/Sep/14 10:20,21/Feb/17 08:07,14/Jul/23 06:26,20/Dec/14 02:45,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,," HistoryServer cann't list event Log on WEB UI  when there was a no permissions directory in the $spark.eventLog.dir. eg, The following files list   is in the  $spark.eventLog.dir directory:
{code}
[yangping.wu@master spark-1.1.0-bin-2.2.0]$ bin/hadoop fs -ls /spark-logs/eventLog
Found 45 items
drwxrwxrwx   - root        root                0 2014-09-24 18:31 /spark-logs/eventLog/scala.hbasetest-1411552435599
drwxrwxrwx   - root        root                0 2014-09-24 17:56 /spark-logs/eventLog/scala.hbasetest-1411552547780
drwxrwxrwx   - root        root                0 2014-09-24 18:00 /spark-logs/eventLog/scala.hbasetest-1411552747689
drwxrwxrwx   - root        root                0 2014-09-24 18:05 /spark-logs/eventLog/scala.hbasetest-1411553051906
drwxrwxrwx   - root        root                0 2014-09-24 18:09 /spark-logs/eventLog/scala.hbasetest-1411553328706
drwxrwxrwx   - root        root                0 2014-09-24 18:22 /spark-logs/eventLog/scala.hbasetest-1411554132311
drwxrwxrwx   - root        root                0 2014-09-26 10:57 /spark-logs/eventLog/simple-application-1411698604636
drwxrwxrwx   - root        root                0 2014-09-26 14:45 /spark-logs/eventLog/simple-application-1411712643513
drwxrwx---   - yangping.wu supergroup          0 2014-09-26 16:29 /spark-logs/eventLog/simple-application-1411720159090( didn't have permissions for historyServer)
drwxrwx---   - yangping.wu root                0 2014-09-26 16:29 /spark-logs/eventLog/simple-application-1411720190625
{code}
but when I start the historyServer, The historyServer meet  a AccessControlException in the log file as following:
{code}
26 Sep 2014 17:50:26,561 ERROR [LogCheckingThread] (org.apache.spark.Logging$class.logError:96)  - Exception in checking for event log updates
org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=EXECUTE, inode=""/spark-logs/eventLog/simple-application-1411720159090"":yangping.wu:supergroup:drwxrwx---
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:234)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:187)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:150)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5186)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5168)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(FSNamesystem.java:5147)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3286)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:749)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59628)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2048)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2042)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1681)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
        at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1423)
        at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$liftedTree1$1$1.apply(FsHistoryProvider.scala:126)
        at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$liftedTree1$1$1.apply(FsHistoryProvider.scala:125)
        at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at scala.collection.TraversableLike$class.filter(TraversableLike.scala:263)
        at scala.collection.AbstractTraversable.filter(Traversable.scala:105)
        at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$checkForLogs(FsHistoryProvider.scala:123)
        at org.apache.spark.deploy.history.FsHistoryProvider$$anon$1$$anonfun$run$1.apply$mcV$sp(FsHistoryProvider.scala:70)
        at org.apache.spark.deploy.history.FsHistoryProvider$$anon$1$$anonfun$run$1.apply(FsHistoryProvider.scala:61)
        at org.apache.spark.deploy.history.FsHistoryProvider$$anon$1$$anonfun$run$1.apply(FsHistoryProvider.scala:61)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
{code}

Because the HistoryServer don't have permission to read the directory(/spark-logs/eventLog/simple-application-1411720159090). and lead the HistoryServer cann't show other event log file on the WEB UI. but show ""No Completed Applications Found""! I think the HistoryServer can ignore the incorrect directory(eg: /spark-logs/eventLog/simple-application-1411720159090 as mentioned above).",,397090770,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 21 08:07:14 UTC 2017,,,,,,,,,,"0|i20ilb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/14 23:09;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/3391;;;","20/Dec/14 02:45;vanzin;I added the fix for this in the fix for SPARK-2261.;;;","21/Feb/17 08:07;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17011;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable to show host and port in block fetch failure,SPARK-3695,12744200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adrian-wang,adrian-wang,adrian-wang,26/Sep/14 02:49,26/Sep/14 18:27,14/Jul/23 06:26,26/Sep/14 18:27,,,,,,,,1.2.0,,,,,,Input/Output,,,,0,,,,,,,,adrian-wang,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 26 02:55:26 UTC 2014,,,,,,,,,,"0|i20iaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/14 02:55;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Closing shuffle writers we swallow more important exception,SPARK-3690,12744045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,epakhomov,epakhomov,25/Sep/14 14:41,25/Sep/14 21:50,14/Jul/23 06:26,25/Sep/14 21:50,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"ShaffleMapTask: line 75

{code:title=ShaffleMapTask|borderStyle=solid}
 case e: Exception =>
        if (writer != null) {
          writer.stop(success = false)
        }
        throw e
{code}

Exception in writer.stop() swallows the important one. Couldn't find the reason for problems for days. Look up in internet ""java.io.FileNotFoundException: /local/hd2/yarn/local/usercache/epahomov/appcache/application_1411219858924_12991/spark-local-20140924225309-03f5/21/shuffle_4_12_147 (No such file or directory)"" - there are plenty poor guys like me.",,apachespark,epakhomov,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 25 21:50:35 UTC 2014,,,,,,,,,,"0|i20hdb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Sep/14 15:33;epakhomov;https://github.com/apache/spark/pull/2537;;;","25/Sep/14 15:35;apachespark;User 'epahomov' has created a pull request for this issue:
https://github.com/apache/spark/pull/2537;;;","25/Sep/14 21:48;joshrosen;For additional context, here's the mailing list thread: http://apache-spark-user-list.1001560.n3.nabble.com/java-io-FileNotFoundException-in-usercache-td15135.html;;;","25/Sep/14 21:50;joshrosen;Issue resolved by pull request 2537
[https://github.com/apache/spark/pull/2537];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogicalPlan can't resolve column correctlly,SPARK-3688,12743955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tianyi,tianyi,tianyi,25/Sep/14 07:27,11/Feb/15 23:27,14/Jul/23 06:26,11/Feb/15 20:59,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"How to reproduce this problem:
{code}
CREATE TABLE t1(x INT);
CREATE TABLE t2(a STRUCT<x: INT>, k INT);
SELECT a.x FROM t1 a JOIN t2 b ON a.x = b.k;
{code}",,apachespark,marmbrus,tianyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 22:10:52 UTC 2015,,,,,,,,,,"0|i20gtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/14 07:51;tianyi;As we know, the hive support complex colunm datatype like struct.
So it is hard to resolve a reference like a.b.c
I think we should add some judgements on the datatype attributes, like:
{quote}
option.dataType.isInstanceOf[StructType]
{quote};;;","26/Sep/14 07:00;apachespark;User 'tianyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/2542;;;","11/Feb/15 06:00;apachespark;User 'tianyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/4524;;;","11/Feb/15 20:59;marmbrus;Issue resolved by pull request 4524
[https://github.com/apache/spark/pull/4524];;;","11/Feb/15 22:10;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flume.SparkSinkSuite.Success is flaky,SPARK-3686,12743908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hshreedharan,pwendell,pwendell,25/Sep/14 00:48,19/Dec/14 16:13,14/Jul/23 06:26,26/Sep/14 05:57,,,,,,,,1.2.0,,,,,,DStreams,,,,0,flaky-test,,,,,"{code}
Error Message

4000 did not equal 5000
Stacktrace

sbt.ForkMain$ForkError: 4000 did not equal 5000
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:498)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1559)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:416)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite.org$apache$spark$streaming$flume$sink$SparkSinkSuite$$assertChannelIsEmpty(SparkSinkSuite.scala:195)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite$$anonfun$1.apply$mcV$sp(SparkSinkSuite.scala:54)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite$$anonfun$1.apply(SparkSinkSuite.scala:40)
	at org.apache.spark.streaming.flume.sink.SparkSinkSuite$$anonfun$1.apply(SparkSinkSuite.scala:40)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:158)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1121)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1559)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:155)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:167)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1559)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:200)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:200)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:200)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1559)
	at org.scalatest.Suite$class.run(Suite.scala:1423)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1559)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:204)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:204)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:204)
	at org.scalatest.FunSuite.run(FunSuite.scala:1559)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:444)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:651)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

Example test result (this will stop working in a few days):
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/719/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.3,label=centos/testReport/junit/org.apache.spark.streaming.flume.sink/SparkSinkSuite/Success_with_ack/",,apachespark,hshreedharan,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 26 05:57:10 UTC 2014,,,,,,,,,,"0|i20gj3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"25/Sep/14 03:16;hshreedharan;Looking into this.;;;","25/Sep/14 03:19;hshreedharan;Unlike the other tests in this suite, this one does not have a sleep to let the sink commit the transactions back to the channel. So because this does not give enough time for the channel to actually becoming empty. Let me add a sleep - will send a PR and run the pre-commit hook a bunch of times to ensure that it fixes it.;;;","25/Sep/14 03:50;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/2531;;;","26/Sep/14 05:57;pwendell;Resolved by:
https://github.com/apache/spark/pull/2531;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark's local dir should accept only local paths,SPARK-3685,12743906,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,andrewor14,andrewor14,25/Sep/14 00:34,12/Dec/22 18:10,14/Jul/23 06:26,12/Dec/17 08:02,1.1.0,,,,,,,2.3.0,,,,,,Spark Core,YARN,,,0,,,,,,"When you try to set local dirs to ""hdfs:/tmp/foo"" it doesn't work. What it will try to do is create a folder called ""hdfs:"" and put ""tmp"" inside it. This is because in Util#getOrCreateLocalRootDirs we use java.io.File instead of Hadoop's file system to parse this path. We also need to resolve the path appropriately.

This may not have an urgent use case, but it fails silently and does what is least expected.",,andrewor14,apachespark,farrellee,neelesh77,pwendell,stevel@apache.org,tstclair,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1529,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 08:02:30 UTC 2017,,,,,,,,,,"0|i20gin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/14 01:09;andrewor14;Note that this is not meaningful unless we also change the usages of this to use the Hadoop FileSystem. This requires a non-trivial refactor of shuffle and spill code to use the Hadoop API.;;;","28/Sep/14 17:32;farrellee;i'm skeptical. what would be the benefit of using HDFS for temporary storage?;;;","29/Sep/14 04:35;pwendell;[~andrewor14] changing the use of local storage to use HDFS would be a fairly large change. For instance, I think we use the FileChannel API and other types of random access that are not supported by HDFS. The title of this JIRA makes it seem like this is just about interpretation of the filename inside of Spark, but really the proposal is much broader here, suggesting we move to the Hadoop API's instead of the Posix FS Api.;;;","29/Sep/14 05:16;andrewor14;[~farrellee] One of the concerns for SPARK-3174 is that after an executor is removed, any shuffle files it has written are no longer accessible. In my design for that issue, one of my considerations is to use a common DFS to store these files (rather than tying their fates to the executor's). But yes, otherwise there is very little benefit for writing these through HDFS simply because there will be a lot of overhead there.

[~pwendell] Yes, it will be a fairly large change that we may or may not want (depending on the final design of SPARK-3174). Either way Spark should not create a directory called ""hdfs:"" and put the files there. At the very least we should probably throw an exception complaining that it's not supported. Perhaps the title and the description can be renamed to reflect this.;;;","29/Sep/14 13:45;farrellee;[~andrewor] thanks for the info. afaik the executor is also in charge of the shuffle file life-cycle, and breaking that would be complicated. it's probably a cleaner implementation to allow executors to remain and use a policy to prune unused/little-used executors where unused/little-used factors in amount of data they are holding as well as cpu used. you could also go down the path of aging-out executors - let their resources go back to the node's pool for reallocation, but don't kill off the process. however, approaches like that become very complex and push implementation details of the workload, which often don't generalize, into the scheduling system.

[~andrewor] btw, it should be a warning case (""hey you might have messed up, i see you used hdfs:/ in your file name"") instead of an error case.;;;","29/Sep/14 18:58;andrewor14;Yeah there will be a design doc soon on the possible solutions for dealing with shuffles. Note that one of the main motivations of doing this is to free up containers in Yarn when an application is not using it, so maintaining a pool of executor containers does not achieve what we want. Also, DFS shuffle is only one of the solutions we will consider, but we probably won't end up relying on it because of the overhead it adds (i.e. we'll probably need a different solution down the road either way).

It could be a warning, but I think an exception is appropriate here because the user clearly thinks that its shuffle files are going into HDFS when they're not. Also, the fact that it fails-fast means the user knows Spark won't do what he/she wants before even a single shuffle file is written. Either way I don't feel strongly about this.

;;;","29/Sep/14 19:47;farrellee;the root of the resource problem is how they're handed out. yarn is giving you a whole cpu, some amount of memory, some amount of network and some amount of disk to work with. your executor (like any program) uses different amounts of resources throughout its execution. at points in the execution the resource profile changes, call the demarcated regions ""phases"". so an executor may transition from a high resource phase to a low resource phase. in a low resource phase, you may want to free up resources for other executors, but maintain enough to do basic operations (say: serve a shuffle file). this is a problem that should be solved by the resource manager. in my opinion, a solution w/i spark that isn't faciliated by the RN is a workaround/hack and should be avoided. an example of a RN facilitated solution might be a message the executor can send to yarn to indicate its resources can be free'd, except for some minimum amount.;;;","29/Sep/14 20:33;andrewor14;Not sure if I fully understand what you mean. If I'm running an executor and I request 30G from the beginning, my application uses all of it to do computation and all is good. After I decommission the executor, I would like to keep 1G just to serve the shuffle files, but this can't be done easily because we need to start a smaller JVM and a smaller container. (Yarn currently doesn't support scaling the size of a container while it's still running yet). Either way we need to transfer some state from the bigger JVM to the smaller JVM, and that adds some complexity to the design. The simplest alternative then would just to write whatever state to an external location and just terminate the executor JVM / container without starting a smaller one, and then have an external service that is long-running to serve these files.

One proposal here then is to write these shuffle files to a special location and have the Yarn NM shuffle service serve the files. This is an alternative to DFS shuffle that is, however, highly specific to Yarn. I am doing some initial prototyping of this (the Yarn shuffle) approach to see how this will pan out.;;;","29/Sep/14 23:41;farrellee;if you're going to go down this path the best (i'd say correct) way to implement it is to have support from yarn - a way to tell yarn ""i'm only going to need X,Y,Z resources from now on"" without giving up the execution container. i bet there's a way to re-exec the jvm into a smaller form factor.;;;","08/Mar/15 14:49;srowen;This is 90% the same discussion as SPARK-1529, although, this concerns making the current behavior more explicit (e.g. fail an hdfs: URI) whereas SPARK-1529 (and the discussion below) discusses making other FS schemes work. I'd like to potentially address this issue, without prejudicing SPARK-1529. In fact this discussion usefully contains a good use case for putting a local dir on distributed storage, whereas I personally don't see it in the arguments in SPARK-1529.;;;","17/Mar/15 13:09;stevel@apache.org;YARN-1197 covers supporting resizing existing YARN containers: that would be the real solution to altering the memory footprint of an executor in a container -at least if that JVM can change its heap size down. but... that JIRA is ""dormant""; I don't know if anyone is going to pick it up in the near-term.

SPARK-1529 looks at switching to the Hadoop FS APIs, but doesn't mandate remote storage: it just makes it possible

Switching to HDFS storage, as Andrew proposes, risks hitting network performance.
# network traffic unless the replication factor == 1. (though do that & there's only one preferred location for the new container)
# disk IO conflict with other HDFS work going on on the localhost. 
# the overhead of going via the TCP stack unless they are bypassed via unix domain sockets (as HBase does).

There's a risk, therefore, that the performance of all work will suffer just to support a single use case ""flex executor container & JVM size"". That's also ignoring the scheduling risk of the smaller container not being allocated resources

Hooking up the YARN NM shuffle would be the better way to do this. If that shuffle can't handle the wiring-up, it's probably easier to fix that than the whole YARN container-resize problem

;;;","09/Dec/17 13:21;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19934;;;","12/Dec/17 08:02;gurwls223;Issue resolved by pull request 19934
[https://github.com/apache/spark/pull/19934];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to serialized ArrayType or MapType  after accessing them in Python,SPARK-3681,12743862,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,24/Sep/14 21:12,27/Sep/14 19:21,14/Jul/23 06:26,27/Sep/14 19:21,,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"{code}
files_schema_rdd.map(lambda x: x.files).take(1)
{code}

Also it will lose the schema after iterate an ArrayType.

{code}
files_schema_rdd.map(lambda x: [f.batch for f in x.files]).take(1)
{code}",,apachespark,davies,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 27 19:21:52 UTC 2014,,,,,,,,,,"0|i20g9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/14 21:20;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2526;;;","27/Sep/14 19:21;marmbrus;Issue resolved by pull request 2526
[https://github.com/apache/spark/pull/2526];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.Exception: makeCopy when using HiveGeneric UDFs on Converted Parquet Metastore tables,SPARK-3680,12743838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,24/Sep/14 18:53,27/Sep/14 19:10,14/Jul/23 06:26,27/Sep/14 19:10,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 27 19:10:32 UTC 2014,,,,,,,,,,"0|i20g4f:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Sep/14 19:05;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2525;;;","27/Sep/14 19:10;marmbrus;Issue resolved by pull request 2525
[https://github.com/apache/spark/pull/2525];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pickle the exact globals of functions,SPARK-3679,12743831,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,24/Sep/14 18:28,24/Sep/14 20:00,14/Jul/23 06:26,24/Sep/14 20:00,,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"function.func_code.co_names has all the names used in the function, including name of attributes. It will pickle some unnecessary globals if there is a global having the same name with attribute (in co_names).

There is a regression introduced by PR 2114 https://github.com/apache/spark/pull/2144/files

",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 24 20:00:31 UTC 2014,,,,,,,,,,"0|i20g33:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Sep/14 18:30;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2522;;;","24/Sep/14 20:00;joshrosen;Issue resolved by pull request 2522
[https://github.com/apache/spark/pull/2522];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jdk version lead to spark sql test suite error,SPARK-3676,12743722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,24/Sep/14 07:00,27/Sep/14 19:08,14/Jul/23 06:26,27/Sep/14 19:08,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"System.out.println(1/500d)  get different result in diff jdk version
jdk 1.6.0(_31) ---- 0.0020
jdk 1.7.0(_05) ---- 0.002

this will lead to  spark sql hive test suite failed (replay by set jdk version = 1.6.0_31)--- 
[info] - division *** FAILED ***
[info]   Results do not match for division:
[info]   SELECT 2 / 1, 1 / 2, 1 / 3, 1 / COUNT(*) FROM src LIMIT 1
[info]   == Parsed Logical Plan ==
[info]   Limit 1
[info]    Project [(2 / 1) AS c_0#692,(1 / 2) AS c_1#693,(1 / 3) AS c_2#694,(1 / COUNT(1)) AS c_3#695]
[info]     UnresolvedRelation None, src, None
[info]   
[info]   == Analyzed Logical Plan ==
[info]   Limit 1
[info]    Aggregate [], [(CAST(2, DoubleType) / CAST(1, DoubleType)) AS c_0#692,(CAST(1, DoubleType) / CAST(2, DoubleType)) AS c_1#693,(CAST(1, DoubleType) / CAST(3, DoubleType)) AS c_2#694,(CAST(CAST(1, LongType), Doub
leType) / CAST(COUNT(1), DoubleType)) AS c_3#695]
[info]     MetastoreRelation default, src, None
[info]   
[info]   == Optimized Logical Plan ==
[info]   Limit 1
[info]    Aggregate [], [2.0 AS c_0#692,0.5 AS c_1#693,0.3333333333333333 AS c_2#694,(1.0 / CAST(COUNT(1), DoubleType)) AS c_3#695]
[info]     Project []
[info]      MetastoreRelation default, src, None
[info]   
[info]   == Physical Plan ==
[info]   Limit 1
[info]    Aggregate false, [], [2.0 AS c_0#692,0.5 AS c_1#693,0.3333333333333333 AS c_2#694,(1.0 / CAST(SUM(PartialCount#699L), DoubleType)) AS c_3#695]
[info]     Exchange SinglePartition
[info]      Aggregate true, [], [COUNT(1) AS PartialCount#699L]
[info]       HiveTableScan [], (MetastoreRelation default, src, None), None
[info]   
[info]   Code Generation: false
[info]   == RDD ==
[info]   c_0    c_1     c_2     c_3
[info]   !== HIVE - 1 row(s) ==              == CATALYST - 1 row(s) ==
[info]   !2.0   0.5     0.3333333333333333      0.002   2.0     0.5     0.3333333333333333      0.0020 (HiveComparisonTest.scala:370)


[info] - timestamp cast #1 *** FAILED ***
[info]   Results do not match for timestamp cast #1:
[info]   SELECT CAST(CAST(1 AS TIMESTAMP) AS DOUBLE) FROM src LIMIT 1
[info]   == Parsed Logical Plan ==
[info]   Limit 1
[info]    Project [CAST(CAST(1, TimestampType), DoubleType) AS c_0#995]
[info]     UnresolvedRelation None, src, None
[info]   
[info]   == Analyzed Logical Plan ==
[info]   Limit 1
[info]    Project [CAST(CAST(1, TimestampType), DoubleType) AS c_0#995]
[info]     MetastoreRelation default, src, None
[info]   
[info]   == Optimized Logical Plan ==
[info]   Limit 1
[info]    Project [0.0010 AS c_0#995]
[info]     MetastoreRelation default, src, None
[info]   
[info]   == Physical Plan ==
[info]   Limit 1
[info]    Project [0.0010 AS c_0#995]
[info]     HiveTableScan [], (MetastoreRelation default, src, None), None
[info]   
[info]   Code Generation: false
[info]   == RDD ==
[info]   c_0
[info]   !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==
[info]   !0.001                   0.0010 (HiveComparisonTest.scala:370)


",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 27 19:08:54 UTC 2014,,,,,,,,,,"0|i20ff3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Sep/14 08:00;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2517;;;","24/Sep/14 08:09;srowen;(For the interested, I looked it up, since the behavior change sounds surprising. This is in fact a bug in Java 6 that was fixed in Java 7: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4428022 It may even be fixed in later versions of Java 6, but I have a very recent one and it is not.);;;","24/Sep/14 08:24;scwf;hmm, i see, thanks for that.;;;","27/Sep/14 19:08;marmbrus;Issue resolved by pull request 2517
[https://github.com/apache/spark/pull/2517];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.*.memory is ignored in cluster mode,SPARK-3661,12743629,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,23/Sep/14 21:52,30/Oct/14 23:11,14/Jul/23 06:26,30/Oct/14 23:11,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"This is related to https://issues.apache.org/jira/browse/SPARK-3653, but for the config. Note that `spark.executor.memory` is fine only in standalone and mesos mode because we pass the Spark system properties to the driver after it has started.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 22:45:42 UTC 2014,,,,,,,,,,"0|i20eu7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"07/Oct/14 22:45;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2697;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set EC2 version to 1.1.0 in master branch,SPARK-3659,12743577,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivaram,shivaram,shivaram,23/Sep/14 18:52,24/Sep/14 18:35,14/Jul/23 06:26,24/Sep/14 18:35,,,,,,,,1.2.0,,,,,,EC2,,,,0,,,,,,Master branch should be in sync with branch-1.1,,apachespark,pwendell,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 24 18:35:21 UTC 2014,,,,,,,,,,"0|i20ein:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/14 18:55;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/2510;;;","24/Sep/14 18:35;pwendell;https://github.com/apache/spark/pull/2510;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn alpha YarnRMClientImpl throws NPE appMasterRequest.setTrackingUrl starting spark-shell,SPARK-3657,12743547,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,tgraves,tgraves,23/Sep/14 16:19,28/Oct/14 19:42,14/Jul/23 06:26,28/Oct/14 19:42,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"YarnRMClientImpl.registerApplicationMaster can throw null pointer exception when setting the trackingurl if its empty:

    appMasterRequest.setTrackingUrl(new URI(uiAddress).getAuthority())


I hit this just start spark-shell without the tracking url set.

14/09/23 16:18:34 INFO yarn.YarnRMClientImpl: Connecting to ResourceManager at kryptonitered-jt1.red.ygrid.yahoo.com/98.139.154.99:8030
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder.setTrackingUrl(YarnServiceProtos.java:710)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterRequestPBImpl.setTrackingUrl(RegisterApplicationMasterRequestPBImpl.java:132)
        at org.apache.spark.deploy.yarn.YarnRMClientImpl.registerApplicationMaster(YarnRMClientImpl.scala:102)
        at org.apache.spark.deploy.yarn.YarnRMClientImpl.register(YarnRMClientImpl.scala:55)
        at org.apache.spark.deploy.yarn.YarnRMClientImpl.register(YarnRMClientImpl.scala:38)
        at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:168)
        at org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:206)
        at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:120)
",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 16:44:29 UTC 2014,,,,,,,,,,"0|i20ec7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"09/Oct/14 05:45;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2728;;;","28/Oct/14 16:44;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2981;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement all extended HiveQL statements/commands with a separate parser combinator,SPARK-3654,12743420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,lian cheng,lian cheng,23/Sep/14 02:12,13/Oct/14 19:11,14/Jul/23 06:26,03/Oct/14 03:04,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Statements and commands like {{SET}}, {{CACHE TABLE}} and {{ADD JAR}} etc. are currently parsed in a quite hacky way, like this:
{code}
if (sql.trim.toLowerCase.startsWith(""cache table"")) {
  sql.trim.toLowerCase.startsWith(""cache table"") match {
    ...
  }
}
{code}
It would be much better to add an extra parser combinator that parses these syntax extensions first, and then fallback to the normal Hive parser.",,apachespark,lian cheng,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 23:10:50 UTC 2014,,,,,,,,,,"0|i20dk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/14 04:43;ravipesala;https://github.com/apache/spark/pull/2590;;;","30/Sep/14 04:45;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2590;;;","03/Oct/14 03:04;marmbrus;Issue resolved by pull request 2590
[https://github.com/apache/spark/pull/2590];;;","07/Oct/14 23:10;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2698;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triangle Count handles reverse edges incorrectly,SPARK-3650,12743360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,robineast,jegonzal,jegonzal,22/Sep/14 22:22,22/Feb/16 05:37,14/Jul/23 06:26,22/Feb/16 01:07,1.1.0,1.2.0,,,,,,2.0.0,,,,,,GraphX,,,,0,releasenotes,,,,,"The triangle count implementation assumes that edges are aligned in a canonical direction.  As stated in the documentation:

bq. Note that the input graph should have its edges in canonical direction (i.e. the `sourceId` less than `destId`)

However the TriangleCount algorithm does not verify that this condition holds and indeed even the unit tests exploits this functionality:

{code:scala}
val triangles = Array(0L -> 1L, 1L -> 2L, 2L -> 0L) ++
        Array(0L -> -1L, -1L -> -2L, -2L -> 0L)
      val rawEdges = sc.parallelize(triangles, 2)
      val graph = Graph.fromEdgeTuples(rawEdges, true).cache()
      val triangleCount = graph.triangleCount()
      val verts = triangleCount.vertices
      verts.collect().foreach { case (vid, count) =>
        if (vid == 0) {
          assert(count === 4)  // <-- Should be 2
        } else {
          assert(count === 2) // <-- Should be 1
        }
      }
{code}


",,apachespark,glenn.strycker@gmail.com,jegonzal,ovidiumarcu,robineast,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 05:37:34 UTC 2016,,,,,,,,,,"0|i20d53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/14 22:30;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2495;;;","23/Jan/15 09:28;apachespark;User 'Leolh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4176;;;","26/Jun/15 15:11;robineast;What is the status of this issue? A user on the mailing list just ran into to this issue. It looks like PR-2495 should fix the issue. Is there a version that is being targeted for the fix?;;;","27/Jun/15 05:38;srowen;[~robineast] looks like https://github.com/apache/spark/pull/2495 just never got merged for some reason. Dust it off and ping jegonzal and ankurdave for review (again);;;","18/Feb/16 10:05;ovidiumarcu;Can someone look over this issue?;;;","18/Feb/16 10:09;srowen;[~ovidiumarcu] what are you expecting here? you should try to revive the PR as I mentioned above if you're interested. I don't think anyone else is working on GraphX though.;;;","18/Feb/16 12:28;ovidiumarcu;I see interesting issues on GraphX, nobody working on, maybe low priority. too bad.;;;","18/Feb/16 13:48;robineast;I did ask if the PR could be revived but never followed up on it. If I get a moment I'll try and submit the PR myself however have been a little busy on other GraphX things.

By the way there is a workaround to the issue which is to make sure your edges are in the canonical direction before calling triangleCount.;;;","21/Feb/16 11:55;apachespark;User 'insidedctm' has created a pull request for this issue:
https://github.com/apache/spark/pull/11290;;;","22/Feb/16 05:37;ovidiumarcu;Is it possible to apply this fix to a 1.5 version?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException in GraphX custom serializers when sort-based shuffle spills,SPARK-3649,12743355,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,22/Sep/14 21:51,11/Nov/14 03:32,14/Jul/23 06:26,11/Nov/14 03:32,1.2.0,,,,,,,1.2.0,,,,,,GraphX,,,,1,,,,,,"As [reported|http://apache-spark-user-list.1001560.n3.nabble.com/java-lang-ClassCastException-java-lang-Long-cannot-be-cast-to-scala-Tuple2-td13926.html#a14501] on the mailing list, GraphX throws

{code}
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.Tuple2
        at org.apache.spark.graphx.impl.RoutingTableMessageSerializer$$anon$1$$anon$2.writeObject(Serializers.scala:39) 
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:195) 
        at org.apache.spark.util.collection.ExternalSorter.spillToMergeableFile(ExternalSorter.scala:329)
{code}

when sort-based shuffle attempts to spill to disk. This is because GraphX defines custom serializers for shuffling pair RDDs that assume Spark will always serialize the entire pair object rather than breaking it up into its components. However, the spill code path in sort-based shuffle [violates this assumption|https://github.com/apache/spark/blob/f9d6220c792b779be385f3022d146911a22c2130/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala#L329].

GraphX uses the custom serializers to compress vertex ID keys using variable-length integer encoding. However, since the serializer can no longer rely on the key and value being serialized and deserialized together, performing such encoding would require writing a tag byte. Therefore it may be better to simply remove the custom serializers.",,ankurd,apachespark,LiuZeshan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 04:45:25 UTC 2014,,,,,,,,,,"0|i20d3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/14 04:45;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2503;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shaded Guava patch causes access issues with package private classes,SPARK-3647,12743329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,vanzin,vanzin,22/Sep/14 20:50,11/Nov/14 08:11,14/Jul/23 06:26,23/Sep/14 20:42,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"The patch that introduced shading to Guava (SPARK-2848) tried to maintain backwards compatibility in the Java API by not relocating the ""Optional"" class. That causes problems when that class references package private members in the Absent and Present classes, which are now in a different package:

{noformat}
Exception in thread ""main"" java.lang.IllegalAccessError: tried to access class org.spark-project.guava.common.base.Present from class com.google.common.base.Optional
	at com.google.common.base.Optional.of(Optional.java:86)
	at org.apache.spark.api.java.JavaUtils$.optionToOptional(JavaUtils.scala:25)
	at org.apache.spark.api.java.JavaSparkContext.getSparkHome(JavaSparkContext.scala:542)
{noformat}",,aash,apachespark,donnchadh,hammer,pwendell,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 08:11:42 UTC 2014,,,,,,,,,,"0|i20cyf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 20:51;vanzin;There are two options I see here:

- extend the hack to also not relocate the affected classes (Absent and Present should  be enough)
- fork some code from Guava and modify it to avoid the issue.

I'll go on a limb and say the first option is easier.;;;","22/Sep/14 22:40;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2496;;;","23/Sep/14 20:42;pwendell;Fixed by Marcelo in this patch:

https://github.com/apache/spark/pull/2496;;;","11/Nov/14 07:45;hammer;Should this issue have a Fix version set?;;;","11/Nov/14 08:11;aash;Based on poking at the git repo below I'm marking with a fix version of 1.2.0 (the next release on branch-1.2)

{noformat}
aash@aash-mbp ~/git/spark$ git log origin/master | grep SPARK-3647
    [SPARK-3647] Add more exceptions to Guava relocation.
    Closes #2496 from vanzin/SPARK-3647 and squashes the following commits:
    84f58d7 [Marcelo Vanzin] [SPARK-3647] Add more exceptions to Guava relocation.
aash@aash-mbp ~/git/spark$ git log origin/branch-1.0 | grep SPARK-3647
aash@aash-mbp ~/git/spark$ git log origin/branch-1.1 | grep SPARK-3647
aash@aash-mbp ~/git/spark$ git log origin/branch-1.2 | grep SPARK-3647
    [SPARK-3647] Add more exceptions to Guava relocation.
    Closes #2496 from vanzin/SPARK-3647 and squashes the following commits:
    84f58d7 [Marcelo Vanzin] [SPARK-3647] Add more exceptions to Guava relocation.
aash@aash-mbp ~/git/spark$
{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make caching using SQL commands eager by default, with the option of being lazy",SPARK-3645,12743297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,marmbrus,marmbrus,22/Sep/14 19:06,06/Oct/14 00:54,14/Jul/23 06:26,06/Oct/14 00:54,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 00:54:01 UTC 2014,,,,,,,,,,"0|i20crb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"24/Sep/14 00:50;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2513;;;","06/Oct/14 00:54;marmbrus;Issue resolved by pull request 2513
[https://github.com/apache/spark/pull/2513];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly populate SparkPlan.currentContext,SPARK-3641,12743222,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,yhuai,yhuai,22/Sep/14 15:57,02/Dec/14 20:16,14/Jul/23 06:26,03/Oct/14 19:36,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"After creating a new SQLContext, we need to populate SparkPlan.currentContext before we create any SparkPlan. Right now, only SQLContext.createSchemaRDD populate SparkPlan.currentContext. SQLContext.applySchema is missing this call and we can have NPE as described in http://qnalist.com/questions/5162981/spark-sql-1-1-0-npe-when-join-two-cached-table.",,kmalik,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 20:16:41 UTC 2014,,,,,,,,,,"0|i20can:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 16:05;yhuai;[~marmbrus] Can we populate SparkPlan.currentContext in the constructor of SQLContext instead of populate it every time before using ExistingRDD?;;;","22/Sep/14 18:00;marmbrus;The idea here is to be able to support more than one SQL context, so I think we will always need populate this field before constructing physical operators.  To avoid bugs like this, it would be good to limit the number of places where physical plans are constructed.  Right now its kind of a hack that we use SparkLogicalPlan as a connector and manually create the physical ExistingRDD operator.  If we instead had a true logical concept for ExistingRDDs then this bug would not have occurred....;;;","22/Sep/14 18:20;yhuai;Sounds good. Let me fix it.;;;","22/Sep/14 20:23;marmbrus;Hey [~yhuai] have you started on this yet?  I think the addition of a logical plan for existing RDD is going to conflict with some work on caching that I'm doing.;;;","22/Sep/14 20:30;yhuai;No, I have not started. I can start after your caching stuff is in.;;;","02/Dec/14 19:25;kmalik;Hi all,

Is this expected to be fixed with Spark 1.2.0 release ?
Any work around that I can use till then ? A simple join on 2 cached tables is a pretty regular use case. Can I do something to avoid the NPE for that?

Regards,

Kapil;;;","02/Dec/14 20:16;marmbrus;This has been fixed for a while and will be in Spark 1.2 and there are nearly final RCs already available.  Running any other spark query that does not use applySchema in the same thread should also work around the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis examples set master as local,SPARK-3639,12743148,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,aniket,aniket,aniket,22/Sep/14 11:06,04/Nov/14 18:28,14/Jul/23 06:26,26/Sep/14 16:51,1.0.2,1.1.0,,,,,,1.1.1,1.2.0,,,,,DStreams,Examples,,,0,examples,,,,,Kinesis examples set master as local thus not allowing the example to be tested on a cluster,,aniket,apachespark,cfregly,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 04 18:28:27 UTC 2014,,,,,,,,,,"0|i20buf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"22/Sep/14 11:16;aniket;If the community agrees this is an issue, I can submit a PR for this ;;;","24/Sep/14 12:03;farrellee;seems reasonable to me;;;","24/Sep/14 23:00;joshrosen;This sounds reasonable to me; feel free to open a PR.  If you look at most of the other Spark examples, they only set the appName when creating the SparkContext and leave the master unspecified in order to allow it to be set when passing the script to {{spark-submit}}.;;;","25/Sep/14 14:05;apachespark;User 'aniketbhatnagar' has created a pull request for this issue:
https://github.com/apache/spark/pull/2536;;;","01/Nov/14 17:24;cfregly;great catch, aniket!

for some background context, i was trying to make the sample easier to run out of the box.  i overlooked the spark-submit scenario, unfortunately.  thanks for fixing this.

few things:
1) does the Streaming Kinesis Guide (docs/streaming-kinesis-integration.md) need updating with your change?  specifically, the Running the Example section?  i don't think so, but something to double-check.

2) i noticed you put a comment in the scaladoc about needing +1 workers/threads than receivers, perhaps we should reword this to say 

  (number of kinesis shards+1) workers/threads are needed

because the number of receivers is determined by the number of shards in the kinesis stream.  might tighten up the message a bit.  

3)  should we throw an error if the number of workers/threads is not sufficient?  nobody likes an error message, but might be helpful here.  this is the basis of https://issues.apache.org/jira/browse/SPARK-2475, btw.  might want to keep an eye on that jira.

thanks again, man.  great catch.

-chris;;;","04/Nov/14 18:28;apachespark;User 'aniketbhatnagar' has created a pull request for this issue:
https://github.com/apache/spark/pull/3092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commons HTTP client dependency conflict in extras/kinesis-asl module,SPARK-3638,12743145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,aniket,aniket,22/Sep/14 11:00,01/Apr/15 18:46,14/Jul/23 06:26,02/Oct/14 01:31,1.1.0,,,,,,,1.1.1,1.2.0,,,,,DStreams,Examples,,,0,dependencies,,,,,"Followed instructions as mentioned @ https://github.com/apache/spark/blob/master/docs/streaming-kinesis-integration.md and when running the example, I get the following error:

{code}
Caused by: java.lang.NoSuchMethodError: org.apache.http.impl.conn.DefaultClientConnectionOperator.<init>(Lorg/apache/http/conn/scheme/SchemeRegistry;Lorg/apache/http/conn/DnsResolver;)V
        at org.apache.http.impl.conn.PoolingClientConnectionManager.createConnectionOperator(PoolingClientConnectionManager.java:140)
        at org.apache.http.impl.conn.PoolingClientConnectionManager.<init>(PoolingClientConnectionManager.java:114)
        at org.apache.http.impl.conn.PoolingClientConnectionManager.<init>(PoolingClientConnectionManager.java:99)
        at com.amazonaws.http.ConnectionManagerFactory.createPoolingClientConnManager(ConnectionManagerFactory.java:29)
        at com.amazonaws.http.HttpClientFactory.createHttpClient(HttpClientFactory.java:97)
        at com.amazonaws.http.AmazonHttpClient.<init>(AmazonHttpClient.java:181)
        at com.amazonaws.AmazonWebServiceClient.<init>(AmazonWebServiceClient.java:119)
        at com.amazonaws.AmazonWebServiceClient.<init>(AmazonWebServiceClient.java:103)
        at com.amazonaws.services.kinesis.AmazonKinesisClient.<init>(AmazonKinesisClient.java:136)
        at com.amazonaws.services.kinesis.AmazonKinesisClient.<init>(AmazonKinesisClient.java:117)
        at com.amazonaws.services.kinesis.AmazonKinesisAsyncClient.<init>(AmazonKinesisAsyncClient.java:132)
{code}

I believe this is due to the dependency conflict as described @ http://mail-archives.apache.org/mod_mbox/spark-dev/201409.mbox/%3CCAJOb8btdXks-7-spJJ5jMNw0XsnrjwDpCQqtjht1hUn6j4zb_g@mail.gmail.com%3E

",,aniket,apachespark,ashrafuzzaman,cfregly,cnstar9988,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6599,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 09:43:43 UTC 2015,,,,,,,,,,"0|i20btr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/14 11:17;apachespark;User 'aniketbhatnagar' has created a pull request for this issue:
https://github.com/apache/spark/pull/2489;;;","25/Sep/14 13:55;apachespark;User 'aniketbhatnagar' has created a pull request for this issue:
https://github.com/apache/spark/pull/2535;;;","02/Oct/14 01:31;joshrosen;Issue resolved by pull request 2535
[https://github.com/apache/spark/pull/2535];;;","02/Dec/14 05:28;ashrafuzzaman;I have faced this issue in spark 1.1.0 and now updated spark version to 1.1.1 But still facing the same issue.

I can see in the spark 1.1.1 hadoop 2.4 in the spark-assembly-1.1.1-hadoop2.4.0.jar the class HttpPatch is not there which was introduced in 4.2
http://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/client/methods/HttpPatch

So I think there might be an issue in the pre built package there.;;;","02/Dec/14 06:56;ashrafuzzaman;I have build with 1.1.1 tag and used the compiled binary. And now it works. So definitely there is an issue with the pre build spark 1.1.1 hadoop 2.4 distribution.;;;","02/Dec/14 07:00;aniket;[~ashrafuzzaman] did you build using using kinesis-asl profile as described in https://github.com/apache/spark/blob/master/docs/streaming-kinesis-integration.md (mvn -Pkinesis-asl -DskipTests clean package)? None of the pre build spark distributions have the profile enabled. I just pulled down 1.1.1 source code and did a build as mentioned in the Kinesis integration documentation and I can see the class the in the built assembly.;;;","02/Dec/14 08:30;ashrafuzzaman;[~aniket] Yes you are right. I did not know that to use Kinesis I have to build the spark from source. I thought I can run that on pre build packages.;;;","03/Dec/14 07:37;aniket;Yes. You may want to open another JIRA ticket for having kinesis pre build packages in spark download page.;;;","16/Feb/15 08:21;cnstar9988;>>I can see in the spark 1.1.1 hadoop 2.4 in the spark-assembly-1.1.1-hadoop2.4.0.jar the class HttpPatch is not there which was introduced in 4.2
I see spark-1.2.1-bin-hadoop2.4.tgz\spark-1.2.1-bin-hadoop2.4\lib\spark-assembly-1.2.1-hadoop2.4.0.jar\org\apache\http\version.properties
It indicates that officical package only httpclient 4.1.2.

I think there has some way to make httpclient 4.2.3 or greate version.

;;;","16/Feb/15 08:25;cnstar9988;https://github.com/apache/spark/pull/2489/files ( <commons.httpclient.version>4.2</commons.httpclient.version>)
https://github.com/apache/spark/pull/2535/files (<commons.httpclient.version>4.2.6</commons.httpclient.version>)

It indicates that officical package only package with httpclient 4.1.2, above changes has no effects..

How to reopen this bug, thanks.;;;","16/Feb/15 08:28;aniket;Did you build spark with kinesis-asl profile? The standard distribution does not have this profile and therefore you would have to roll your won as described in https://github.com/apache/spark/blob/master/docs/streaming-kinesis-integration.md (mvn -Pkinesis-asl -DskipTests clean package). ;;;","16/Feb/15 09:24;cnstar9988;Oh, It was introduced in kinesis-asl profile only.
I think httpclient 4.1.2 is too old,  standard distribution may conflict with other ""httpclient required"" user app.
now I build spark with kinesis-asl profile, it's ok with httpclient 4.2.6, thanks.

mvn dependency:tree

;;;","16/Feb/15 09:43;aniket;Ya.. you may want to open a separate JIRA ticket for upgrading the dependency.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Find Strongly Connected Components with Graphx has a small bug,SPARK-3635,12743107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,odedz,odedz,22/Sep/14 07:50,26/Nov/14 04:22,14/Jul/23 06:26,30/Sep/14 01:07,1.1.0,,,,,,,1.1.1,1.2.0,,,,,GraphX,,,,0,,,,,,"The strongly connected components function (spark / graphx / src / main / scala / org / apache / spark / graphx / lib / StronglyConnectedComponents.scala) has a typo in the condition on line 78.
I think the condition should be ""if (e.srcAttr._1 < e.dstAttr._1)"" instead of ""if (e.srcId < e.dstId)""","VMWare, Centos 6.5",ankurd,apachespark,odedz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 01:07:52 UTC 2014,,,,,,,,,,"0|i20blz:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"22/Sep/14 08:05;apachespark;User 'odedz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2486;;;","30/Sep/14 01:07;ankurd;Issue resolved by pull request 2486
[https://github.com/apache/spark/pull/2486];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetches failure observed after SPARK-2711,SPARK-3633,12743081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,nravi,nravi,22/Sep/14 03:29,17/May/20 18:21,14/Jul/23 06:26,21/Nov/14 09:45,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Block Manager,Spark Core,,,7,,,,,,"Running a variant of PageRank on a 6-node cluster with a 30Gb input dataset. Recently upgraded to Spark 1.1. The workload fails with the following error message(s):

{code}
14/09/19 12:10:38 WARN TaskSetManager: Lost task 51.0 in stage 2.1 (TID 552, c1705.halxg.cloudera.com): FetchFailed(BlockManagerId(1, c1706.halxg.cloudera.com, 49612, 0), shuffleId=3, mapId=75, reduceId=120)

14/09/19 12:10:38 INFO DAGScheduler: Resubmitting failed stages
{code}

In order to identify the problem, I carried out change set analysis. As I go back in time, the error message changes to:

{code}
14/09/21 12:56:54 WARN TaskSetManager: Lost task 35.0 in stage 3.0 (TID 519, c1706.halxg.cloudera.com): java.io.FileNotFoundException: /var/lib/jenkins/workspace/tmp/spark-local-20140921123257-68ee/1c/temp_3a1ade13-b48a-437a-a466-673995304034 (Too many open files)
        java.io.FileOutputStream.open(Native Method)
        java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:117)
        org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:185)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:197)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:145)
        org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
        org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:51)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

All the way until Aug 4th. Turns out the problem changeset is 4fde28c. ",,aash,andrewor14,arahuja,capricornius,copris,darabos,darrenlee,erwaman,hammer,hector.yee,jerryshao,joshrosen,larryxiao,lianhuiwang,matei,nravi,pwendell,qiaohaijun,rafal.kwasny,sarutak,sb58,stephen,tgraves,vanzin,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2711,SPARK-4393,SPARK-4452,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 04 19:41:24 UTC 2015,,,,,,,,,,"0|i20bgf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Sep/14 07:14;pwendell;[~nravi] if you are trying to debug this, try and look at what file handles are open by the executor process using lsof. This can usually help narrow down bugs of this nature.;;;","24/Sep/14 00:14;nravi;[~pwendell]  I see a large number of open files of the form shuffle* (e.g., shuffle_3_101_23) as expected. Trying higher values of ulimit to see if/when this issue goes away (that would only help understand the extent of the problem). Since we have narrowed the problem down to a specific commit + know what the issue is, could someone more familiar with that code try and see where the bug is? Unless creation of a large number of temp shuffle files is a known side-effect of the commit. In which case, it may need to be rethought at a design level? 

Here is the latest ulimit settings on the cluster (rebooted):

core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 385910
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1000000
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 10240
cpu time               (seconds, -t) unlimited
max user processes              (-u) 32768
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
;;;","24/Sep/14 00:24;aash;Hi [~nravi] you can also check the limits on an individual process (sometimes they're different from system limits) on linux with {{cat /proc/$PID/limits}};;;","24/Sep/14 00:54;nravi;Looks ok (still waiting on the run to finish/fail):

Limit                     Soft Limit           Hard Limit           Units     
Max cpu time              unlimited            unlimited            seconds   
Max file size             unlimited            unlimited            bytes     
Max data size             unlimited            unlimited            bytes     
Max stack size            10485760             unlimited            bytes     
Max core file size        0                    unlimited            bytes     
Max resident set          unlimited            unlimited            bytes     
Max processes             65536                65536                processes 
Max open files            1000000              1000000              files     
Max locked memory         unlimited            unlimited            bytes     
Max address space         unlimited            unlimited            bytes     
Max file locks            unlimited            unlimited            locks     
Max pending signals       385911               385911               signals   
Max msgqueue size         819200               819200               bytes     
Max nice priority         0                    0                    
Max realtime priority     0                    0                    
Max realtime timeout      unlimited            unlimited            us        ;;;","24/Sep/14 01:26;nravi;Unsurprisingly, the run goes through. It would be good to at least document this if we don't have bandwidth to fully fix the issue.;;;","24/Sep/14 01:32;aash;You're getting exceptions but they're not fatal to the application?;;;","24/Sep/14 01:40;nravi;With higher values of ulimit and timeout values, the exceptions go away (both fetch failures and too-many-files-open) and the app runs to completion. There is a 15% perf regression though. I see 15% perf regression in another benchmark after switching to 1.1.;;;","25/Sep/14 16:19;arahuja;Which timeout values were increased to work around this?  We have been seeing many more errors with FetchFailed(BlockManagerId(21,

And I also see a 

java.io.IOException: Failed to list files for dir: /data/09/mapred/local/yarn/nm/usercache/ahujaa01/appcache/application_1403901413406_1926/spark-local-20140925115858-c4a7
        at org.apache.spark.util.Utils$.listFilesSafely(Utils.scala:673)
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:685)
exception with that failure;;;","25/Sep/14 16:34;zzhan;Increasing timeout does not help my case either. I still keep getting fetch error.;;;","25/Sep/14 17:19;arahuja;Also, which timeout setting was useful: spark.akka.timeout or spark.core.connection.ack.wait.timeout.  Using GC logging I see this both when there are many Full GC and even on smaller datasets when there are not.  It much more frequent on the former.;;;","25/Sep/14 17:41;nravi;Increasing the value of spark.core.connection.ack.wait.timeout (600) worked in my case ;;;","01/Oct/14 20:11;capricornius;I tried increasing timeout for this property spark.core.connection.ack.wait.timeout. I saw less fetch failures due to ack timeout but they still exist.

I also tried relaxing the following properties but none of them seems to help.

spark.core.connection.handler.threads.*
spark.core.connection.io.threads.*
spark.core.connection.connect.threads.*

My job only runs < 500 parallel reduce tasks and I don't see much GC activity on sender/receiver executor JVM when the timeout happens.

;;;","02/Oct/14 04:45;nravi;For a different workload (variant of TeraSort), I see fetch failures in the standalone mode but not with YARN (with identical ulimit and timeout values). Wondering why this might be.
;;;","02/Oct/14 22:54;vanzin;Hey [~pwendell] [~matei], is anyone activelly looking at this issue?;;;","04/Oct/14 09:09;nravi;Quick update: reverting 4fde28c gets rid of the fetch failures and recovers significant perf regressions in two workloads. Thanks [~vanzin].;;;","06/Oct/14 22:18;matei;I'm curious, why do you think this is caused by SPARK-2711? That will cause fewer files to be created for intermediate spills, if anything. It seems this might be due to changes in the shuffle or communication code.;;;","06/Oct/14 22:21;vanzin;[~matei], as Nishkam mentions above, we reverted that change in our internal, 1.1-based branch and all the observed issues (exceptions during fetch and performance) were fixed.;;;","06/Oct/14 22:37;matei;In that case though, the problem might be that these maps are allocating more memory without that patch, and exceeding the spark.shuffle.memoryFraction, which would lead to other bugs. That is also consistent with it running faster. It would be good to investigate why exactly this happened (e.g. was this job just at the edge of exceeding its ulimit). It sounds like PageRank has exactly the problem described in SPARK-2711 (you have reduce tasks that are also writing map output files for the next stage, so they contain two ExternalAppendOnlyMaps, so the old version of ExternalAppendOnlyMap does not properly account for memory). That's going to lead to trouble in jobs that are also doing caching.;;;","06/Oct/14 22:41;matei;BTW one other possibility is that ExternalAppendOnlyMap might not be properly closing files, leading to more open file handles when it spills more. You can determine which files are open in a running process using `lsof` and compare among the two versions. ;;;","08/Oct/14 17:53;capricornius;Looks like we have addressed fetch failure caused by ""Too many files open"". Anyone has more insight on the timeout thing?

The timeout happened during the transfer of BufferAckMessage between the sender and receiver. To shed more light on this issue, I turned on DEBUG level logging and it kind of give the trace of life cycle of this event.

* On sender host, sending of the message seems healthy.
{noformat}
4/09/25 19:59:48 DEBUG ConnectionManager: Before Sending [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)] connectionid: sender_host_60072_260
14/09/25 19:59:48 DEBUG ConnectionManager: Sending [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 DEBUG SendingConnection: Added [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to outbox for sending to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 DEBUG SendingConnection: Starting to send [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 TRACE SendingConnection: Sending chunk from [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)]
14/09/25 19:59:48 DEBUG SendingConnection: Finished sending [BufferAckMessage(aid = 582, id = 1503, size = 9601)] to [ConnectionManagerId(receiver_host,52315)] in 22 ms
{noformat}

* On receiver host, receiving of the message seems stalled for 8 minutes (14/09/25 19:59:48, 14/09/25 20:07:14). And timeout exception was thrown in between.
{noformat}
14/09/25 19:59:48 DEBUG ReceivingConnection: Starting to receive [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 19:59:48 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 19:59:48 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 19:59:48 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 19:59:48 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 TRACE ReceivingConnection: Receiving chunk of [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 DEBUG ReceivingConnection: Finished receiving [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)] in 445535 ms
14/09/25 20:07:14 DEBUG ConnectionManager: Received [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
14/09/25 20:07:14 DEBUG ConnectionManager: Handling [BufferAckMessage(aid = 582, id = 1503, size = 9601)] from [ConnectionManagerId(sender_host,60072)]
{noformat}
;;;","20/Oct/14 09:26;jerryshao;From my test, I think this problem might be existed for a while, previously we do not have timeout ack mechanism, so we might ignore this problem, after this commit (https://github.com/apache/spark/commit/bd3ce2ffb8964abb4d59918ebb2c230fe4614aa2) is merged,  the timeout checking mechanism will raise the exception as mentioned before.

My previous assumption is that there might be some thread contention or lock issue in NioBlockTransferService after refactoring, but after I roll back to branch 1.1 without refactoring connection module, the problem still exists. So I guess there might be a problem in connection module,  we neglected it until this patch is merged.;;;","12/Nov/14 13:40;copris;FWIW I get this as well, with a very straightforward job and setup.

Spark 1.1.0, executors configured to 2GB, storage.fraction=0.2, shuffle.spill=true

50GB dataset on ext4, spread over 7000 files, hence the coalescing below

The jobs is only doing: input.coalesce(72, false).groupBy(key).count

The groupBy is successful then I get the dreaded fetch error on count stage (oddly enough), but it seems to me that's when it does the actual shuffling for groupBy ?

EDIT: This might be due to Full GC on the executors during the shuffle block transfer phase. What's interesting is that it doesn't go OOM and the same amount is collected every time. (Old gen is 1.5 GB)

2014-11-12T07:17:06.899-0800: 477.697: [Full GC [PSYoungGen: 248320K->0K(466432K)] [ParOldGen: 1355469K->1301675K(1398272K)] 1603789K->1301675K(1864704K) [PSPermGen: 39031K->39031K(39424K)], 0.6565240 secs] [Times: user=3.35 sys=0.00, real=0.66 secs] 
2014-11-12T07:17:07.751-0800: 478.549: [Full GC [PSYoungGen: 248320K->0K(466432K)] [ParOldGen: 1301681K->1268312K(1398272K)] 1550001K->1268312K(1864704K) [PSPermGen: 39031K->39031K(39424K)], 0.5821160 secs] [Times: user=3.16 sys=0.00, real=0.58 secs] 
2014-11-12T07:17:08.495-0800: 479.294: [Full GC [PSYoungGen: 248320K->0K(466432K)] [ParOldGen: 1268314K->1300497K(1398272K)] 1516634K->1300497K(1864704K) [PSPermGen: 39031K->39031K(39424K)], 0.6400670 secs] [Times: user=4.07 sys=0.01, real=0.64 secs]

EDIT2: Changing to G1 collector actually causes it to go OOM. 
This must be related somehow to the number of shuffle files and hence perhaps open buffers as lowering the number of reducers from 72 to 10 runs without issues (note I'm using consolidated shuffle files). 

14/11/12 07:30:53 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Connection manager future execution context-2,5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.spark.storage.BlockMessage.set(BlockMessage.scala:94)
	at org.apache.spark.storage.BlockMessage$.fromByteBuffer(BlockMessage.scala:176)
	at org.apache.spark.storage.BlockMessageArray.set(BlockMessageArray.scala:63)
	at org.apache.spark.storage.BlockMessageArray$.fromBufferMessage(BlockMessageArray.scala:109)
	at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator$$anonfun$sendRequest$2.apply(BlockFetcherIterator.scala:124)
	at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator$$anonfun$sendRequest$2.apply(BlockFetcherIterator.scala:121)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)


;;;","12/Nov/14 17:47;copris;This looks like a memory leak in ConnectionManager where responses (BufferMessage) are retained by the TimerTask waiting for ACK even after the Future completes with Success, please see the reference chain from a heap dump below

Possibly related to https://github.com/apache/spark/commit/76fa0eaf515fd6771cdd69422b1259485debcae5 

+------------------------------------------------------+--------------+----------------------+-------------------------+
|                        Class                         |   Objects    |     Shallow Size     |      Retained Size      |
+------------------------------------------------------+--------------+----------------------+-------------------------+
|  java.util.TaskQueue                                 |    1    0 %  |           24    0 %  |     885,048,168  100 %  |
|  java.util.TimerTask[]                               |    1    0 %  |        2,064    0 %  |     885,048,144   99 %  |
|  org.apache.spark.network.ConnectionManager$$anon$5  |  286    5 %  |       13,728    0 %  |  ~  885,046,080   99 %  |
|  org.apache.spark.network.BufferMessage              |  572   10 %  |       36,608    0 %  |  ~  885,018,624   99 %  |
|  scala.concurrent.impl.Promise$DefaultPromise        |  286    5 %  |        4,576    0 %  |  ~  884,968,288   99 %  |
|  scala.util.Success                                  |  286    5 %  |        4,576    0 %  |  ~  884,963,712   99 %  |
|  scala.collection.mutable.ArrayBuffer                |  572   10 %  |       13,728    0 %  |  ~  884,915,768   99 %  |
|  java.lang.Object[]                                  |  572   10 %  |       45,760    0 %  |  ~  884,902,040   99 %  |
|  java.nio.HeapByteBuffer                             |  286    5 %  |       13,728    0 %  |  ~  884,856,280   99 %  |
|  byte[]                                              |  286    5 %  |  884,842,552   99 %  |  ~  884,842,552   99 %  |
|  java.net.InetSocketAddress                          |  572   10 %  |        9,152    0 %  |       ~  66,248    0 %  |
|  java.net.InetSocketAddress$InetSocketAddressHolder  |  572   10 %  |       13,728    0 %  |       ~  57,096    0 %  |
|  java.net.Inet4Address                               |  286    5 %  |        6,864    0 %  |       ~  43,368    0 %  |
|  java.net.InetAddress$InetAddressHolder              |  286    5 %  |        6,864    0 %  |       ~  36,504    0 %  |
|  java.lang.String                                    |  285    5 %  |        6,840    0 %  |       ~  29,640    0 %  |
|  char[]                                              |  285    5 %  |       22,800    0 %  |       ~  22,800    0 %  |
|  java.lang.Object                                    |  286    5 %  |        4,576    0 %  |        ~  4,576    0 %  |
+------------------------------------------------------+--------------+----------------------+-------------------------+

Generated by YourKit Java Profiler 2014 build 14110 12-Nov-2014 17:44:32
;;;","12/Nov/14 18:16;copris;At first sight (haven't tested this) the problem is in the code below. The TimerTask is cancelled on Success but this doesn't actually remove it from the Timer TaskQueue since the TimerThread doesn't actually remove cancelled tasks until they're actually scheduled to run, which in this case is by default 60 secs ack timeout.

A quick fix would be to call Timer.purge() after task cancel below, or better yet change to a better Timer like the HashedWheel one from Netty 

{code:title=|borderStyle=solid}

    val status = new MessageStatus(message, connectionManagerId, s => {
      timeoutTask.cancel()
      s.ackMessage match {
        case None => // Indicates a failure where we either never sent or never got ACK'd
          promise.failure(new IOException(""sendMessageReliably failed without being ACK'd""))
        case Some(ackMessage) =>
          if (ackMessage.hasError) {
            promise.failure(
              new IOException(""sendMessageReliably failed with ACK that signalled a remote error""))
          } else {
            promise.success(ackMessage)
          }
      }
    })
{code};;;","13/Nov/14 18:11;stephen;FWIW we've seen this issue several times on jobs that have moved to Spark 1.1.;;;","13/Nov/14 23:46;joshrosen;This seems like a very serious issue, so I've upgraded it to a 1.2 blocker.  I'm going to begin investigating the timeout issue now.;;;","14/Nov/14 00:52;joshrosen;I've opened SPARK-4393 to address the memory leak in the ConnectionManager timers and submitted a pull request.;;;","14/Nov/14 19:51;sarutak;Oh... I'ts my bad, sorry for that...
Thanks [~onetoinfinity@yahoo.com] for narrowing the root cause and thanks [~joshrosen] fixing that.;;;","17/Nov/14 23:31;andrewor14;Hey [~nravi] [~arahuja] were you using sort-based or hash-based shuffle when you hit this?;;;","17/Nov/14 23:35;vanzin;Nishkam was using hash-based shuffle (default for 1.1).;;;","17/Nov/14 23:35;arahuja;[~andrewor14] We were using Hash-Based shuffle when we were running into this due to the Snappy+Kryo issue with sort based shuffle;;;","19/Nov/14 02:28;andrewor14;I have filed SPARK-4480 as an immediate fix. The longer term solution is described in SPARK-4452 and will likely be targeted for 1.3, though I imagine that the fix for SPARK-4480 is sufficient for most cases.;;;","20/Nov/14 18:44;hector.yee;I'm still seeing a similar error in spark 1.2 rc2

14/11/20 18:41:12 WARN TaskSetManager: Lost task 4.1 in stage 1.0 (TID 907, i-8cb72661.inst.aws.airbnb.com): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:386)
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:383)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:382)
        at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178)
        at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
        at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
;;;","20/Nov/14 19:04;hector.yee;I think it may be a different bug.. looked at the failed executor and it looks like something is closing the connection causing fetches to fail

14/11/20 18:53:43 INFO TransportClientFactory: Found inactive connection to i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773, closing it.
14/11/20 18:53:43 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks 
java.io.IOException: Failed to connect to i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:141)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:87)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:148)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:288)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at com.airbnb.common.ml.training.LinearRankerTrainer$$anonfun$7.apply(LinearRankerTrainer.scala:246)
	at com.airbnb.common.ml.training.LinearRankerTrainer$$anonfun$7.apply(LinearRankerTrainer.scala:235)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more;;;","21/Nov/14 09:45;nravi;Fixed by PR for SPARK-4480. ;;;","22/Nov/14 01:21;pwendell;[~nravi] resolved this because his original issue was solved. However, subsequent comments have identified other issues, so if there are still users facing out-standing issues related to this, please feel free to open new JIRA's and link back to this one.;;;","23/Nov/14 21:02;stephen;Is there a Spark 1.1.1/1.2 snapshot/RC in a Maven repo somewhere to try out the fixes for this? (SPARK-3493 and Spark-4480)? Or is 1.1.1 close enough to release that I should just sit tight?;;;","24/Nov/14 00:10;matei;[~stephen] you can try the 1.1.1 RC in http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-1-1-RC2-td9439.html, which includes a Maven staging repo that you can just add as a repo in a build.;;;","24/Nov/14 03:22;stephen;Hi Matei; cool, thanks for the 1.1.1 link. In my head that had happened before these bugs got fixed, but I maybe I was thinking of rc1. Thanks!;;;","24/Nov/14 22:24;stephen;I just tried a job on 1.1.1-rc2 and am still getting this issue:

Could not get block(s) from ConnectionManagerId(ip-10-13-192-156.ec2.internal,47193)
java.io.IOException: sendMessageReliably failed because ack was not received within 60 sec

The cluster was still running; when I logged into that host (the -156.ec2.internal that did not respond to the ack), it was just fine, and *also* getting ""failed because ack not received"" messages, at basically the same time (within ~2 seconds of the other host).

I checked another host, and same thing; AFAICT it looks like ~3 hosts of out the 20 all had problems responding to acks within the 60s timeout, for ~1-2 minutes.

I'm going to up the ack connection timeout to 600, per an earlier comment, and try again.
;;;","05/Dec/14 19:56;joshrosen;[~stephen],

Do you know if the hosts that failed to ACK experienced long GC pauses?  Did bumping up the connection timeout help?  I'd like to try to figure out whether this is still an issue.;;;","05/Dec/14 21:43;stephen;Hi Josh,

Yes, it was GC issues; however, we're still kind of tracking down. Turns out the job that is failing is a real PITA, and is a month-to-date report, and started failing even on 0.9.2 at the end of last month.

We were able to get it to run on 0.9.2 with 8 i2.2xlarge machines (with 1.6gb SSDs) because we have a stage with ~1700 tasks, where the average shuffle write is ~400mb, but two of the tasks have huge schew, and shuffle writes of 15gb and 125gb (!).

Since we got the rerun to work on the i2.2xlarges on 0.9.2, we have not been able to try the same data/job on 1.1.1/1.2, but it's on my list of things to do.

(Obviously we're going to want to solve this underlying schew issue as well, but, in theory :-), it should just lead to really slow jobs, vs. OOMEs/etc.);;;","04/Jan/15 19:41;stephen;Sorry for the latest reply on this, but our skew was a join with a key of 15k rows one side, and 165k rows on the other, which resulted in .join trying to create a list with ~2.5 billion entries when joined together (this was before the recent ""use an iterator"" fix to .join).

So, yes, this pounded the GC which then showed up as fetch failures to the rest of our nodes. Apologies for the false negative. It is odd that somehow 0.9.2 powered through this, which is why I thought the job failing in 1.1/1.2 was a regression. We were probably just getting lucky with having barely enough memory/something (even though using the same machine sizes).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager can run out of receive threads with authentication on,SPARK-3632,12743073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,tgraves,22/Sep/14 03:12,25/Mar/15 19:22,14/Jul/23 06:26,25/Mar/15 16:34,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,If you turn authentication on and you are using a lot of executors. There is a chance that all the of the threads in the handleMessageExecutor could be waiting to send a message because they are blocked waiting on authentication to happen. This can cause a temporary deadlock until the connection times out.,,andrewor14,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 19:22:45 UTC 2015,,,,,,,,,,"0|i20ben:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 03:25;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2484;;;","26/Nov/14 03:00;andrewor14;This is not fully resolved; we still need to back port it to branch-1.1;;;","18/Mar/15 20:48;tgraves;[~andrewor14] At this point doesn't seem like we need to port back to branch1, you ok if we close this?;;;","25/Mar/15 16:34;srowen;Same, resolving this as there looks like no burning need to back port back to 1.1 at this stage.;;;","25/Mar/15 19:22;andrewor14;Ok, sounds good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't apply accumulator updates multiple times for tasks in result stages,SPARK-3628,12743058,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,codingcat,matei,matei,21/Sep/14 22:38,25/Mar/15 16:33,14/Jul/23 06:26,25/Mar/15 16:33,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In previous versions of Spark, accumulator updates only got applied once for accumulators that are only used in actions (i.e. result stages), letting you use them to deterministically compute a result. Unfortunately, this got broken in some recent refactorings.

This is related to https://issues.apache.org/jira/browse/SPARK-732, but that issue is about applying the same semantics to intermediate stages too, which is more work and may not be what we want for debugging.",,apachespark,codingcat,farrellee,matei,pwendell,sandyr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-732,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 16:33:18 UTC 2015,,,,,,,,,,"0|i20bbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/14 22:41;matei;BTW the problem is that this used to be guarded against in the TaskSetManager (see https://github.com/apache/spark/blob/branch-0.6/core/src/main/scala/spark/scheduler/cluster/TaskSetManager.scala#L254 or  https://github.com/apache/spark/blob/branch-0.8/core/src/main/scala/org/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L436), and that went away at some point.;;;","24/Sep/14 18:42;codingcat;https://github.com/apache/spark/pull/2524;;;","24/Sep/14 18:45;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/2524;;;","24/Nov/14 05:11;pwendell;I took a quick look at the current patch and i'm re-assigning the target version to 1.2.1. From what I can tell this involves nontrivial changes to the DAGScheduler. That's too critical of a component to modify substantially without significant testing. Let's try to get a fix into master and then put it into 1.2.1. down the road.;;;","24/Nov/14 13:21;codingcat;hmmm....OK

but for this case, shall I submit individual patches for 0.9.x, 1.0.x, because there are some merge conflicts to apply the patch directly ?;;;","27/Nov/14 01:00;matei;FYI I merged this into 1.2.0, since the patch is now quite a bit smaller. We should decide whether we want to back port it to branch-1.1, so I'll leave it open for that reason. I don't think there's much point backporting it further because the issue is somewhat rare, but we can do it if people ask for it.;;;","25/Mar/15 16:33;srowen;At this stage, calling it Fixed on the grounds that there's been no clear need to back-port further, and given the recent discussion about how far to back-port.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark on yarn reports success even though job fails,SPARK-3627,12743046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,tgraves,21/Sep/14 21:16,16/Oct/19 04:46,14/Jul/23 06:26,07/Oct/14 14:52,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"I was running a wordcount and saving the output to hdfs.  If the output directory already exists, yarn reports success even though the job fails since it requires the output directory to not be there.",,andrewor14,apachespark,arahuja,brett_s_r,pederpansen,praveenseluka,tgraves,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4647,SPARK-3293,,,,,SPARK-29465,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 21:58:21 UTC 2014,,,,,,,,,,"0|i20b8n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 17:11;tgraves;We could make this a separate issue, but I've also seen it report failure when it actually succeeded.  In that case I believe it did an sc.stop() and System.exit(0). 

;;;","22/Sep/14 17:51;tgraves;this might be the same as SPARK-3293;;;","26/Sep/14 21:29;tgraves;I'm going to go through all the exit and status reporting of yarn and clean it up. 

Here is what I plan on handling:

In all cases make sure stagingDirectory cleaned up, useful messages in log file, RM status correct, and exit code non-zero on failure, user thread stopped 

Things to check and expected results. Assume yarn configured to retries Applications 3 times on failure.

cluster mode:
  - spark context not initialized 100 seconds - not registered yet so just fail after 3 retries 

client mode:
  - spark driver doesn't respond in 100 seconds - failed after 3 retires

Both modes:  
  - issues registering AM -  retries 3 and FAIL 
  - issues allocating resources - retries 3 and FAIL
  - max exceptions during allocate loop in reporter thread - retries 3 and fail
  - max executor failures - FAIL after 3 retries
  
cluster UserClass:
  - system.exit(0) - success
  - clean exit main - success
  - system.exit(1)  - fail, 3 tries
  - system.exit(-1) - fail, 3 tries
  - throws exception - fail 3 tries

client UserClass:  (fixing client mode isn't handled here as I think its best just to change it to unmanaged AM)
  - system.exit(0) - succeeds
  - clean exit main -succeeds
  - system.exit(1)  - succeeds
  - system.exit(-1) - succeeds
  - throws exception - succeeds;;;","29/Sep/14 15:20;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2577;;;","30/Sep/14 18:53;tgraves;[~andrewor]  Why was the affects version changed on this?  As far as I'm aware this is a result of the refactoring done in spark 1.2 so this does not exist in spark 1.1.;;;","30/Sep/14 21:14;andrewor14;Did this problem not exist in 1.1? I vaguely remember seeing something similar on the mailing list even before my refactor.;;;","30/Sep/14 21:25;tgraves;As far as I am aware the common cases worked in 1.1. I didn't do an extensive test run like I did with the pr here though so its possible some were broken.     

Note there has always been the case where yarn-client mode doesn't report the correct failure case because the AM doesn't know what happened on the client side.  That isn't fixed by this jira either.

I guess I propose we file a separate jira for that if there are known cases or if we just want to go do a full test run.  The code is quite different now too so would require a separate pr anyway.;;;","30/Sep/14 21:58;andrewor14;Ok, sounds good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store analyzed plans for temp tables,SPARK-3618,12742987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,20/Sep/14 23:36,03/Nov/14 22:27,14/Jul/23 06:26,03/Nov/14 22:27,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Right now we store unanalyzed logical plans for temporary tables.  However this means that changes to session state (e.g., the current database) could result in tables becoming inaccessible.",,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 22:27:38 UTC 2014,,,,,,,,,,"0|i20avr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Nov/14 22:27;marmbrus;This was done as part of the caching overhaul.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka test should not hard code Zookeeper port,SPARK-3615,12742984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jerryshao,pwendell,pwendell,20/Sep/14 22:57,24/Nov/14 20:36,14/Jul/23 06:26,25/Sep/14 00:19,,,,,,,,1.2.0,,,,,,DStreams,,,,0,,,,,,"This is causing failures in our master build if port 2181 is contented. Instead of binding to a static port we should re-factor this such that it opens a socket on port 0 and then reads back the port. So we can never have contention.

{code}
sbt.ForkMain$ForkError: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:95)
	at org.apache.spark.streaming.kafka.KafkaTestUtils$EmbeddedZookeeper.<init>(KafkaStreamSuite.scala:200)
	at org.apache.spark.streaming.kafka.KafkaStreamSuite.beforeFunction(KafkaStreamSuite.scala:62)
	at org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.setUp(JavaKafkaStreamSuite.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:136)
	at com.novocode.junit.JUnitRunner.run(JUnitRunner.java:90)
	at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:223)
	at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:236)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,jerryshao,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1022,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 25 00:19:21 UTC 2014,,,,,,,,,,"0|i20av3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 02:55;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/2483;;;","22/Sep/14 03:05;jerryshao;Hi Patrick, I've submit a PR to fix this issue, mind taking a look at the PR? Thanks a lot.;;;","25/Sep/14 00:19;pwendell;https://github.com/apache/spark/pull/2483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor shouldn't quit if heartbeat message fails to reach the driver,SPARK-3612,12742934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,rxin,rxin,20/Sep/14 06:51,26/Feb/15 00:00,14/Jul/23 06:26,23/Sep/14 20:45,,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"The thread started by Executor.startDriverHeartbeater can actually terminate the whole executor if AkkaUtils.askWithReply[HeartbeatResponse] throws an exception. 

I don't think we should quit the executor this way. At the very least, we would want to log a more meaningful exception then simply
{code}
14/09/20 06:38:12 WARN AkkaUtils: Error sending message in 1 attempts
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:176)
        at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:379)
14/09/20 06:38:45 WARN AkkaUtils: Error sending message in 2 attempts
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:176)
        at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:379)
14/09/20 06:39:18 WARN AkkaUtils: Error sending message in 3 attempts
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:176)
        at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:379)
14/09/20 06:39:21 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Driver Heartbeater,5,main]
org.apache.spark.SparkException: Error sending message [message = Heartbeat(281,[Lscala.Tuple2;@4d9294db,BlockManagerId(281, ip-172-31-7-55.eu-west-1.compute.internal, 52303))]
        at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:190)
        at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:379)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:176)
        ... 1 more

{code}",,apachespark,hbogert,pwendell,rxin,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 20:45:14 UTC 2014,,,,,,,,,,"0|i20agf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"20/Sep/14 06:51;rxin;[~andrewor14] [~sandyryza] any comment on this? I think you guys worked on this code.;;;","20/Sep/14 14:37;sandyr;Yeah, we should catch this.  Will post a patch.;;;","22/Sep/14 08:10;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/2487;;;","23/Sep/14 20:45;pwendell;https://github.com/apache/spark/pull/2487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server log name should not be based on user input,SPARK-3610,12742884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sk,sk,19/Sep/14 23:16,19/May/15 04:37,14/Jul/23 06:26,20/Nov/14 07:25,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Right now we use the user-defined application name when creating the logging file for the history server. We should use some type of GUID generated from inside of Spark instead of allowing user input here. It can cause errors if users provide characters that are not valid in filesystem paths.

Original bug report:
{quote}
The default log files for the Mllib examples use a rather long naming convention that includes special characters like parentheses and comma.For e.g. one of my log files is named ""binaryclassifier-with-params(input.txt,100,1.0,svm,l2,0.1)-1410566770032"".

When I click on the program on the history server page (at port 18080), to view the detailed application logs, the history server crashes and I need to restart it. I am using Spark 1.1 on a mesos cluster.

I renamed the  log file by removing the special characters and  then it loads up correctly. I am not sure which program is creating the log files. Can it be changed so that the default log file naming convention does not include  special characters? 
{quote}",,andrewor14,apachespark,joshrosen,mengxr,sarutak,sk,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3377,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 04:37:39 UTC 2015,,,,,,,,,,"0|i20a5j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 20:04;andrewor14;Hi all, I don't have the time to fix this, but this is where we generate the name for these event log files:

https://github.com/apache/spark/blob/56dae30ca70489a62686cb245728b09b2179bb5a/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala#L61

I think we should try to keep the name of the application so the user can still associate with logs are with which application, and coming up with a random GUID makes this difficult. Maybe instead we should just escape more characters (there are only so many).;;;","23/Sep/14 02:03;sk;Today I found that the same issue occurs with Graphx application logs as well (basiclly it includes parentheses and commas in the log file name), and the history server gets messed up and needs to be restarted every time. 

thanks;;;","24/Sep/14 00:02;sk;I made the following change to EventLoggingListener.scala, L57 and this took care of the problem (basically just removed parentheses and commas). So I am now able to access the app details on the history server. Thanks.

private val name = appName.replaceAll(""[ :/]"", ""-"").replaceAll(""[${}'\""]"", ""_"")
    .replaceAll(""[(),]"","""")
    .toLowerCase + ""-"" + System.currentTimeMillis
  val logDir = Utils.resolveURI(logBaseDir) + ""/"" + name.stripSuffix(""/"")
;;;","25/Sep/14 02:34;sarutak;Hi [~SK], I'm trying to resolve similar issue and I think I can resolve this issue using Application ID.
See https://github.com/apache/spark/pull/2432
;;;","20/Nov/14 07:25;joshrosen;This was fixed by [~sarutak]'s patch for SPARK-3377.;;;","19/May/15 04:29;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2432;;;","19/May/15 04:37;sarutak;Please ignore the last comment written by the bot.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sizeInBytes statistics to Limit operator,SPARK-3609,12742881,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,19/Sep/14 22:51,20/Sep/14 23:31,14/Jul/23 06:26,20/Sep/14 23:31,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"The {{sizeInBytes}} statistics of a {{LIMIT}} operator can be estimated fairly precisely when all output attributes are of native data types, all native data types except {{StringType}} have fixed size. For {{StringType}}, we can use a relatively large (say 4K) default size.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 20 23:31:08 UTC 2014,,,,,,,,,,"0|i20a4v:",9223372036854775807,,,,,,,,,,,,,,1.1.1,,,,,,,,,,,"19/Sep/14 22:55;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2468;;;","20/Sep/14 23:31;marmbrus;Issue resolved by pull request 2468
[https://github.com/apache/spark/pull/2468];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark EC2 Script does not correctly break when AWS tagging succeeds.,SPARK-3608,12742868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vidaha,vidaha,vidaha,19/Sep/14 22:17,20/Sep/14 08:26,14/Jul/23 06:26,20/Sep/14 08:26,1.1.0,,,,,,,1.2.0,,,,,,EC2,,,,0,,,,,,Spark EC2 script will tag 5 times and not break out correctly if things succeed.,,apachespark,vidaha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 19 22:25:51 UTC 2014,,,,,,,,,,"0|i20a1z:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Sep/14 22:25;apachespark;User 'vidaha' has created a pull request for this issue:
https://github.com/apache/spark/pull/2466;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager threads.max configs on the thread pools don't work,SPARK-3607,12742864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilganeli,tgraves,tgraves,19/Sep/14 22:08,23/Jan/15 10:48,14/Jul/23 06:26,18/Dec/14 20:53,1.1.0,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"In the ConnectionManager we have a bunch of thread pools. They have settings for the maximum number of threads for each Threadpool (like spark.core.connection.handler.threads.max). 

Those configs don't work because its using a unbounded queue. From the threadpoolexecutor javadoc page: no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.)

luckily this doesn't matter to much as you can work around it by just increasing the minimum like spark.core.connection.handler.threads.min. 

These configs aren't documented either so its more of an internal thing when someone is reading the code.
",,apachespark,ilganeli,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5375,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 10 17:37:53 UTC 2014,,,,,,,,,,"0|i20a13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/14 17:37;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/3664;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-on-Yarn AmIpFilter does not work with Yarn HA.,SPARK-3606,12742846,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,19/Sep/14 21:14,17/Oct/14 07:53,14/Jul/23 06:26,17/Oct/14 07:53,1.1.0,,,,,,,1.1.1,1.2.0,,,,,YARN,,,,0,,,,,,"The current IP filter only considers one of the RMs in an HA setup. If the active RM is not the configured one, you get a ""connection refused"" error when clicking on the Spark AM links in the RM UI.

Similar to YARN-1811, but for Spark.",,apachespark,tgraves,vanzin,wh831019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 00:21:13 UTC 2014,,,,,,,,,,"0|i209x3:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"19/Sep/14 23:50;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2469;;;","23/Sep/14 00:21;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2497;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in SchemaRDD JavaDoc,SPARK-3605,12742788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,sandyr,sandyr,19/Sep/14 18:16,19/Sep/14 22:35,14/Jul/23 06:26,19/Sep/14 22:35,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"""Examples are loading data from Parquet files by using by using the""",,apachespark,marmbrus,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 19 22:35:02 UTC 2014,,,,,,,,,,"0|i209kv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/14 18:25;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/2460;;;","19/Sep/14 22:35;marmbrus;Issue resolved by pull request 2460
[https://github.com/apache/spark/pull/2460];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cast to timestamp should be the same as hive,SPARK-3598,12742636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,19/Sep/14 02:46,13/Oct/14 07:53,14/Jul/23 06:26,13/Oct/14 07:53,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"select cast(1000 as timestamp) from src limit 1;
should return 1970-01-01 00:00:01

also, current implementation has bug when the time is before 1970-01-01 00:00:00",,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 07:53:32 UTC 2014,,,,,,,,,,"0|i208nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/14 17:15;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2458;;;","23/Sep/14 18:46;marmbrus;Issue resolved by pull request 2458
[https://github.com/apache/spark/pull/2458];;;","13/Oct/14 07:53;adrian-wang;reopen to change assignee...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MesosSchedulerBackend does not implement `killTask`,SPARK-3597,12742633,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brenden,brenden,brenden,19/Sep/14 02:02,05/Oct/14 16:50,14/Jul/23 06:26,05/Oct/14 16:50,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Mesos,,,,0,,,,,,"The MesosSchedulerBackend class does not implement `killTask`, and therefore results in exceptions like this:

14/09/19 01:52:53 ERROR TaskSetManager: Task 238 in stage 1.0 failed 4 times; aborting job
14/09/19 01:52:53 INFO TaskSchedulerImpl: Cancelling stage 1
14/09/19 01:52:53 INFO DAGScheduler: Could not cancel tasks for stage 1
java.lang.UnsupportedOperationException
	at org.apache.spark.scheduler.SchedulerBackend$class.killTask(SchedulerBackend.scala:32)
	at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend.killTask(MesosSchedulerBackend.scala:41)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:194)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:192)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:192)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:192)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:185)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:185)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1211)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1197)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1197)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1197)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",,andrewor14,brenden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 05 16:50:42 UTC 2014,,,,,,,,,,"0|i208n3:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"05/Oct/14 16:50;andrewor14;Fixed by https://github.com/apache/spark/pull/2453;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
applySchema to an RDD of Row,SPARK-3592,12742527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,18/Sep/14 17:33,19/Sep/14 22:33,14/Jul/23 06:26,19/Sep/14 22:33,,,,,,,,1.2.0,,,,,,PySpark,SQL,,,0,,,,,,"Right now, we can not appy schema to a RDD of Row, this should be a Bug,

{code}
>>> srdd = sqlCtx.jsonRDD(sc.parallelize([""""""{""a"":2}""""""]))
>>> sqlCtx.applySchema(srdd.map(lambda x:x), srdd.schema())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/daviesliu/work/spark/python/pyspark/sql.py"", line 1121,
in applySchema
    _verify_type(row, schema)
  File ""/Users/daviesliu/work/spark/python/pyspark/sql.py"", line 736,
in _verify_type
    % (dataType, type(obj)))
TypeError: StructType(List(StructField(a,IntegerType,true))) can not
accept abject in type <class 'pyspark.sql.Row'>
{code}",,apachespark,davies,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 19 22:33:58 UTC 2014,,,,,,,,,,"0|i2080n:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"18/Sep/14 18:30;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2448;;;","19/Sep/14 22:33;marmbrus;Issue resolved by pull request 2448
[https://github.com/apache/spark/pull/2448];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sbin/slaves doesn't work when we use password authentication for SSH,SPARK-3584,12742398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,18/Sep/14 09:15,25/Sep/14 23:49,14/Jul/23 06:26,25/Sep/14 23:49,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In sbin/slaves, ssh command run in the background but if we use password authentication, background ssh command doesn't work so sbin/slaves doesn't work.

Also I suggest improvement for sbin/slaves.
In current implementation, slaves file is trucked by Git but it can be edited by user so we prepare slaves.template instead of slaves.

Default slaves file has one entry, localhost, so we should use localhost as a default host list.
I modified sbin/slaves to choose localhost as a default host list.",,apachespark,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 25 23:49:57 UTC 2014,,,,,,,,,,"0|i2077j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"18/Sep/14 09:20;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2444;;;","25/Sep/14 23:49;pwendell;Resolved by:
https://github.com/apache/spark/pull/2444;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL having issue with existing Hive UDFs which take Map as a parameter,SPARK-3582,12742370,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,Saurabh Santhosh,Saurabh Santhosh,18/Sep/14 05:55,29/Sep/14 09:01,14/Jul/23 06:26,23/Sep/14 18:49,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"I have a UDF with the following evaluate method :
public Text evaluate(Text argument, Map<Text, Text> params)

And when i tried invoking this UDF, i was getting the following error.

scala.MatchError: interface java.util.Map (of class java.lang.Class)
at org.apache.spark.sql.hive.HiveInspectors$class.javaClassToDataType(HiveInspectors.scala:35)
at org.apache.spark.sql.hive.HiveFunctionRegistry.javaClassToDataType(hiveUdfs.scala:37)

had a look at HiveInspectors.scala and was not able to see any resolver for java.util.Map ",,adrian-wang,apachespark,ravipesala,Saurabh Santhosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 29 09:01:15 UTC 2014,,,,,,,,,,"0|i2071b:",9223372036854775807,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,"18/Sep/14 05:59;Saurabh Santhosh;When i changed the parameter to Object it works.
and the funny thing is that when i print the class of the runtime instance it shows 'java.util.HashMap';;;","23/Sep/14 07:35;adrian-wang;Hi Saurabh, I have created a PR( https://github.com/apache/spark/pull/2506 ) for this issue. Maybe you can have a try:);;;","23/Sep/14 07:35;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2506;;;","29/Sep/14 09:01;Saurabh Santhosh;Issue resolved by Pull request :
https://github.com/apache/spark/pull/2506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jekyll doc generation is different across environments,SPARK-3579,12742319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,17/Sep/14 23:54,18/Sep/14 17:30,14/Jul/23 06:26,18/Sep/14 17:30,,,,,,,,1.2.0,,,,,,Documentation,,,,0,,,,,,"This can result in a lot of false changes when someone alters something with the docs. It is relevant to the both the Spark website (maintained in a separate subversion repo) and the Spark docs.

There are at least two issues here. One is that the HTML character escaping can be different in certain cases. Another is that the highlighting output seems a bit different depending on (I think) what version of pygments is used.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 17:30:46 UTC 2014,,,,,,,,,,"0|i206qv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"18/Sep/14 05:00;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/2443;;;","18/Sep/14 17:30;pwendell;Issue resolved by pull request 2443
[https://github.com/apache/spark/pull/2443];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphGenerators.sampleLogNormal sometimes returns too-large result,SPARK-3578,12742312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ankurd,ankurd,ankurd,17/Sep/14 23:11,22/Sep/14 20:52,14/Jul/23 06:26,22/Sep/14 20:52,1.2.0,,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,"GraphGenerators.sampleLogNormal is supposed to return an integer strictly less than maxVal. However, it violates this guarantee. It generates its return value as follows:

{code}
var X: Double = maxVal

while (X >= maxVal) {
  val Z = rand.nextGaussian()
  X = math.exp(mu + sigma*Z)
}
math.round(X.toFloat)
{code}

When X is sampled to be close to (but less than) maxVal, then it will pass the while loop condition, but the rounded result will be equal to maxVal, which will fail the test.

For example, if maxVal is 5 and X is 4.9, then X < maxVal, but math.round(X.toFloat) is 5.

A solution is to round X down instead of to the nearest integer.",,ankurd,apachespark,jegonzal,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 22 20:52:03 UTC 2014,,,,,,,,,,"0|i206pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 23:20;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2439;;;","18/Sep/14 04:42;pwendell;@ankurdave, could you tag stuff as GraphX component on JIRA? Thanks!;;;","18/Sep/14 07:29;ankurd;[~pwendell] Sorry, I forgot to do that this time.;;;","22/Sep/14 20:52;jegonzal;Resolved by https://github.com/apache/spark/pull/2439;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add task metric to report spill time,SPARK-3577,12742308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sitalkedia@gmail.com,kayousterhout,kayousterhout,17/Sep/14 22:59,29/Jun/17 06:27,14/Jul/23 06:26,29/Jun/17 06:26,1.1.0,,,,,,,2.3.0,,,,,,Shuffle,Spark Core,,,4,,,,,,"The {{ExternalSorter}} passes its own {{ShuffleWriteMetrics}} into {{ExternalSorter}}.  The write time recorded in those metrics is never used.  We should probably add task metrics to report this spill time, since for shuffles, this would have previously been reported as part of shuffle write time (with the original hash-based sorter).",,apachespark,cloud_fan,csun,dougb,dreamworks007,hammer,hongyu.bi,igor.berman,jey,joshrosen,kayousterhout,lucacanali,mammothcm,roczei,rxin,sandyr,sitalkedia@gmail.com,tgraves,thubregtsen,tzachz,vish741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7413,,,,,,SPARK-3172,,,,,,,,,,,,,,,,"10/Oct/16 18:14;dreamworks007;spill_size.jpg;https://issues.apache.org/jira/secure/attachment/12832515/spill_size.jpg",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 29 06:26:56 UTC 2017,,,,,,,,,,"0|i206of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/14 00:08;sandyr;On it;;;","18/Sep/14 01:17;sandyr;Have you noticed the incorrect metrics reported or is this just something you noticed in the code?  My understanding is that my patch maintained the existing behavior of not including the time it spent spilling bytes for ExternalSorter in the shuffle write time.  Are you saying we should in fact include the time we spent spilling in the shuffle write time?
;;;","18/Sep/14 01:24;kayousterhout;I noticed it in the code when I was trying to fix a different problem.  I
thought in the old code, the disk writer grabbed the shuffle metrics object
from the TaskContext, so the time would be included?


;;;","18/Sep/14 01:33;sandyr;In the old code, the ShuffleWriteMetrics didn't get passed into the disk writer.  The disk writer would track the bytes it wrote itself and how long it took to write them and these would be used to create the shuffle metrics.  ExternalSorter didn't interact with ShuffleWriteMetrics at all.  With the change, ExternalSorter creates a ShuffleWriteMetrics because it's now what's used to find how many bytes were written from the disk writer.  It might make sense to rename ShuffleWriteMetrics to something like WriteMetrics or DiskWriteMetrics, though I think this was discussed and rejected in the initial patch.;;;","18/Sep/14 01:46;kayousterhout;Yeah sorry you're totally right -- sorry for looking at this too quickly!!  I wonder if we should just add a new metric in task metrics for the time spent spilling data -- since as you said, it seems maybe non-intuitive to report this as part of the shuffle write metrics.  I've updated the JIRA.  Thanks for looking at this!!;;;","21/Sep/14 17:41;sandyr;No problem.  Yeah, I agree that a spill time metric would be useful.;;;","23/Sep/14 06:05;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/2504;;;","09/Jun/15 03:09;mammothcm;Why have not the metric been added? I think this is rather important, for that it may affect the results of the research work on this :  https://kayousterhout.github.io/trace-analysis/;;;","15/Jul/15 20:13;thubregtsen;I could also use this metric;;;","11/Aug/16 20:54;tzachz;Does this mean that currently, spill time will be displayed as part of the *Scheduler Delay*? 
Scheduler Delay is calculated pretty much as ""everything that isn't specifically measured"" (see [StagePage.getSchedulerDelay|https://github.com/apache/spark/blob/v2.0.0/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala#L770]), so I'm wondering if indeed it might include  spill time if it's not included anywhere else. 

If so - this might explain long Scheduler Delay values which would be hard to make sense of otherwise (which I think is what I'm seeing...).

Thanks;;;","11/Aug/16 21:00;kayousterhout;I believe spill time will currently be displayed as part of the task runtime, but not as part of scheduler delay.

The scheduler delay is calculated by looking at the difference between two values:

(1) The time that the task was running on the executor
(2) The time from when the scheduler sent information about the task to the executor (so the executor could run the task) until the scheduler received a message that the task completed.

Scheduler delay is (2) - (1).  Usually when it's high, it's because of queueing delays in the scheduler that are either delaying the task getting sent to the executor (e.g., because the scheduler has a long queue of other tasks that need to be launched, or because tasks are large so take a while to send over the network) or that are delaying the task completion message getting back to the scheduler (which can happen when the rate of task launch is high -- greater than 1K or so task launches / second).;;;","10/Oct/16 17:35;dreamworks007;Hi [~kayousterhout], 

Just want to make sure that this JIRA is still relevant, right ?  Is there any changes to the requirement ?

I am currently working on this one, so just want to make sure.

Thanks !;;;","10/Oct/16 17:39;rxin;This is still relevant.;;;","10/Oct/16 17:41;rxin;Actually instead of tracking spill time, it's more important to first report spill data size.
;;;","10/Oct/16 18:16;dreamworks007;[~rxin] I find that the spill size metrics is already added  in https://github.com/apache/spark/commit/bb8098f203e61111faddf2e1a04b03d62037e6c7#diff-1bd3dc38f6306e0a822f93d62c32b1d0, and I have confirm in the UI. (please refer to the attachment of this JIRA - https://issues.apache.org/jira/secure/attachment/12832515/spill_size.jpg)

Also, we noticed that it's wield that the spill size is somehow not reported in the reducer , but reported in the mapper.

Back to the previous question, for the spill time, if it's still relevant to add, then I plan to work on it if there is no objections. ;;;","11/Oct/16 04:59;rxin;[~dreamworks007] can you take a look at the problem here? https://github.com/apache/spark/pull/15347
;;;","29/Mar/17 18:38;sitalkedia@gmail.com;I am making a change to report correct spill data size on disk. ;;;","29/Mar/17 18:47;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/17471;;;","29/Jun/17 06:26;cloud_fan;Issue resolved by pull request 17471
[https://github.com/apache/spark/pull/17471];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide script for creating the Spark AMI from scratch,SPARK-3576,12742304,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,17/Sep/14 22:53,25/Sep/14 06:31,14/Jul/23 06:26,25/Sep/14 06:31,,,,,,,,,,,,,,EC2,,,,0,,,,,,,,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 25 06:31:23 UTC 2014,,,,,,,,,,"0|i206nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/14 06:31;pwendell;This was fixed in spark-ec2 itself;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Schema is ignored when using convertMetastoreParquet,SPARK-3575,12742301,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,17/Sep/14 22:40,05/Feb/15 23:30,14/Jul/23 06:26,05/Feb/15 23:30,,,,,,,,1.3.0,,,,,,SQL,,,,2,,,,,,This can cause problems when for example one of the columns is defined as TINYINT.  A class cast exception will be thrown since the parquet table scan produces INTs while the rest of the execution is expecting bytes.,,apachespark,LabOctoCat,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 23:30:52 UTC 2015,,,,,,,,,,"0|i206n3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"25/Nov/14 03:47;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3441;;;","02/Feb/15 19:06;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4308;;;","05/Feb/15 23:30;marmbrus;Issue resolved by pull request 4308
[https://github.com/apache/spark/pull/4308];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle finish time always reported as -1,SPARK-3574,12742267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,kayousterhout,kayousterhout,17/Sep/14 20:20,20/Sep/14 23:03,14/Jul/23 06:26,20/Sep/14 23:03,1.1.0,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"shuffleFinishTime is always reported as -1.  I think the way we fix this should be to set the shuffleFinishTime in each ShuffleWriteMetrics as the shuffles finish, but when aggregating the metrics, only report shuffleFinishTime as something other than -1 when *all* of the shuffles have completed.

[~sandyr], it looks like this was introduced in your recent patch to incrementally report metrics.  Any chance you can fix this?",,apachespark,kayousterhout,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 20 23:03:46 UTC 2014,,,,,,,,,,"0|i206fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/14 00:08;sandyr;On it;;;","18/Sep/14 00:10;kayousterhout;Thanks Sandy!!;;;","18/Sep/14 01:10;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/2440;;;","20/Sep/14 23:03;pwendell;Fixed by:
https://github.com/apache/spark/pull/2440;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark standalone cluster mode doesn't work.,SPARK-3571,12742260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,17/Sep/14 20:05,17/Sep/14 23:24,14/Jul/23 06:26,17/Sep/14 23:24,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Recent changes of Master.scala causes Spark standalone cluster mode not working.

I think, the loop in Master#schedule never assign worker for driver.

{code}
    for (driver <- waitingDrivers.toList) { // iterate over a copy of waitingDrivers
      // We assign workers to each waiting driver in a round-robin fashion. For each driver, we
      // start from the last worker that was assigned a driver, and continue onwards until we have
      // explored all alive workers.
      curPos = (curPos + 1) % aliveWorkerNum
      val startPos = curPos
      var launched = false
      while (curPos != startPos && !launched) {
        val worker = shuffledAliveWorkers(curPos)
        if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {
          launchDriver(worker, driver)
          waitingDrivers -= driver
          launched = true
        }
        curPos = (curPos + 1) % aliveWorkerNum
      }

{code}",,apachespark,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 17 23:24:09 UTC 2014,,,,,,,,,,"0|i206e7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 20:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2436;;;","17/Sep/14 23:24;pwendell;Issue resolved by pull request 2436
[https://github.com/apache/spark/pull/2436];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle write time does not include time to open shuffle files,SPARK-3570,12742258,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,17/Sep/14 19:57,24/Mar/15 23:29,14/Jul/23 06:26,24/Mar/15 23:29,0.9.2,1.0.2,1.1.0,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,"Currently, the reported shuffle write time does not include time to open the shuffle files.  This time can be very significant when the disk is highly utilized and many shuffle files exist on the machine (I'm not sure how severe this is in 1.0 onward -- since shuffle files are automatically deleted, this may be less of an issue because there are fewer old files sitting around).  In experiments I did, in extreme cases, adding the time to open files can increase the shuffle write time from 5ms (of a 2 second task) to 1 second.  We should fix this for better performance debugging.

Thanks [~shivaram] for helping to diagnose this problem.  cc [~pwendell]",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 20:01;kayousterhout;3a_1410854905_0_job_log_waterfall.pdf;https://issues.apache.org/jira/secure/attachment/12669493/3a_1410854905_0_job_log_waterfall.pdf","17/Sep/14 20:03;kayousterhout;3a_1410957857_0_job_log_waterfall.pdf;https://issues.apache.org/jira/secure/attachment/12669494/3a_1410957857_0_job_log_waterfall.pdf",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 20:54:17 UTC 2015,,,,,,,,,,"0|i206dr:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,"17/Sep/14 20:01;kayousterhout;In case anyone is extra curious about this...here are two plots of the same job, with the fixed logging (that includes file open time) in the first job.  You can see that fixing this metric can be the difference between mysterious stragglers tasks and stragglers that are clearly due to disk activity.;;;","12/Feb/15 03:11;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4550;;;","12/Feb/15 20:54;kayousterhout;There are a bunch of times when files are opened and written to that are not currently logged, so I did some investigation of this to figure out when the time may be significant and is therefore worth logging.  I did this on a 5-machine cluster using ext3 (which exacerbates disk access issues, making it easy to see when times may be long) running query 3a of the big data benchmark (which struggles with disk access because of the many shuffle files).  Here's what I found:

SortShuffleWriter.write, call to shuffleBlockManager.getDataFile: this just opens 1 file, and typically takes about 100us, so not worth adding logging

SortShuffleWriter.write, call to shuffleBlockManager.getIndexFile: this writes a single index file and typically took about 0.1ms (as high as 1ms). Also doesn't seem worth logging.

ExternalSorter.spillToPartitionFiles, creating the disk writers for each partition: because this creates one file for each partition, the time to create all of the files adds up: this took 75-100ms

ExternalSorter.writePartitionedFile, copying the data from the partitioned files to a single file: because this reads and writes all of the shuffle data, it can be long; ~13ms on the workload I looked at.

ExternalSorter.writePartitionedFile, time to call blockManager.getDiskWriter on line 748: getDiskWriter *CAN* take a long time because of the call to file.length(), which may hit disk. However, in this case, each call takes 20us or less (and this is likely noisy -- getting small to measure). To totally speculate, I'd guess that because this is called many times on the same file, as opposed to different files, and the file is actively being written to, the length is cached in memory by the OS.

To summarize, this all leads to the intuitive conclusion that we only need to long when we're writing lots of data (e.g., when copying all of the shuffle data to a single file) or when we're opening a lot of files.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
appId field in SparkDeploySchedulerBackend should be volatile,SPARK-3567,12742100,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,17/Sep/14 10:24,29/Sep/14 06:38,14/Jul/23 06:26,29/Sep/14 06:38,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"appId field in SparkDeploySchedulerBackend is set by AppClient.ClientActor#receiveWithLogging and appId is referred via SparkDeploySchedulerBackend#applicationId.

A thread which runs AppClient.ClientActor and a thread invoking SparkDeploySchedulerBackend#applicationId can be another threads so appId should be volatile.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 17 10:25:31 UTC 2014,,,,,,,,,,"0|i205gf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Sep/14 10:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2428;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make code consistent with document,SPARK-3565,12742074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,17/Sep/14 08:22,18/Sep/14 05:00,14/Jul/23 06:26,18/Sep/14 05:00,,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"The configuration item represent ""Default number of retries in binding to a port"" in code is ""spark.ports.maxRetries"" while ""spark.port.maxRetries"" in document configuration.md. We need to make them consistent.
 
In org.apache.spark.util.Utils.scala:
  /**
   * Default number of retries in binding to a port.
   */
  val portMaxRetries: Int = {
    if (sys.props.contains(""spark.testing"")) {
      // Set a higher number of retries for tests...
      sys.props.get(""spark.port.maxRetries"").map(_.toInt).getOrElse(100)
    } else {
      Option(SparkEnv.get)
        .flatMap(_.conf.getOption(""spark.port.maxRetries""))
        .map(_.toInt)
        .getOrElse(16)
    }
  }

In configuration.md:
<tr>
  <td><code>spark.port.maxRetries</code></td>
  <td>16</td>
  <td>
    Maximum number of retries when binding to a port before giving up.
  </td>
</tr>",,apachespark,pwendell,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 05:00:20 UTC 2014,,,,,,,,,,"0|i205an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 08:30;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/2427;;;","17/Sep/14 22:14;pwendell;Please use ""Spark Core"" component and not ""core"";;;","18/Sep/14 05:00;pwendell;Fixed by: https://github.com/apache/spark/pull/2427;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In yarn-cluster mode, the same jars are distributed through multiple mechanisms.",SPARK-3560,12742009,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mshen,sandyr,sandyr,17/Sep/14 00:38,27/Jun/16 22:08,14/Jul/23 06:26,18/Sep/14 23:08,1.1.0,,,,,,,1.1.1,1.2.0,,,,,YARN,,,,0,,,,,,"In yarn-cluster mode, jars given to spark-submit's --jars argument should be distributed to executors through the distributed cache, not through fetching.

Currently, Spark tries to distribute the jars both ways, which can cause executor errors related to trying to overwrite symlinks without write permissions.

It looks like this was introduced by SPARK-2260, which sets spark.jars in yarn-cluster mode.  Setting spark.jars is necessary for standalone cluster deploy mode, but harmful for yarn cluster deploy mode.",,andrewor14,apachespark,kostas,mshen,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2260,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 23:08:29 UTC 2014,,,,,,,,,,"0|i204w7:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"17/Sep/14 01:27;andrewor14;[~sandyr] So is the fix simply to not set `spark.jars` for yarn-cluster mode?;;;","17/Sep/14 07:13;sandyr;Right.  I believe Min from LinkedIn who discovered the issue is planning to submit a patch.;;;","18/Sep/14 19:00;apachespark;User 'Victsm' has created a pull request for this issue:
https://github.com/apache/spark/pull/2449;;;","18/Sep/14 23:00;andrewor14;Fixed by https://github.com/apache/spark/pull/2449;;;","18/Sep/14 23:08;andrewor14;Reopening just to reassign. Closing right afterwards, please disregard.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
appendReadColumnIDs and appendReadColumnNames introduce unnecessary columns in the lists of needed column ids and column names stored in hiveConf,SPARK-3559,12741989,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gvramana,yhuai,yhuai,16/Sep/14 23:21,13/Oct/14 20:45,14/Jul/23 06:26,13/Oct/14 20:45,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Because we are using the same hiveConf and we are currently using ColumnProjectionUtils.appendReadColumnIDs ColumnProjectionUtils.appendReadColumnNames to append needed column ids and names for a table, lists of needed column ids and names can have unnecessary columns.

Also, for a join operation, TableScanOperators for both tables are sharing the same hiveConf and they may need to set table-specific properties.",,apachespark,gvramana,marmbrus,ravipesala,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3823,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 20:45:44 UTC 2014,,,,,,,,,,"0|i204rr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"08/Oct/14 08:15;gvramana;As same hiveConf is used across queries columns get appended, and cannot be controlled to send only required columns.
HiveConf can be cloned at TableScanOperator and configure required properties.
deserializers are expecting this property to be set in HiveConf but not in table-specific properties.;;;","08/Oct/14 14:25;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/2713;;;","13/Oct/14 20:45;marmbrus;Issue resolved by pull request 2713
[https://github.com/apache/spark/pull/2713];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UI port contention suite flakey,SPARK-3555,12741952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,pwendell,pwendell,16/Sep/14 21:27,16/Sep/14 23:04,14/Jul/23 06:26,16/Sep/14 23:04,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"This has been failing in our master build since this afternoon. Not sure exactly what is causing it, but one clear improvement would be to bind to a random port and then read back the port. This is a strict improvement so we should just do this and see if it fixes the test.

{code}
UISuite:
- jetty selects different port under contention *** FAILED ***
  4040 equaled 4040 (UISuite.scala:127)
{code}",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-901,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 16 23:04:06 UTC 2014,,,,,,,,,,"0|i204jj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"16/Sep/14 21:35;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2418;;;","16/Sep/14 23:04;pwendell;Issue resolved by pull request 2418
[https://github.com/apache/spark/pull/2418];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputStream of ManagedBuffer is not closed and causes running out of file descriptor,SPARK-3546,12741765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sarutak,sarutak,16/Sep/14 12:19,26/Nov/14 11:38,14/Jul/23 06:26,16/Sep/14 19:41,1.2.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"If application has lots of shuffle blocks, resource leak (running out of file descriptor) is occurred.

Following text is file descriptors of an Executor which has lots of blocks
{code}
・・・
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9980 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/13/shuffle_0_340_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9981 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/37/shuffle_0_355_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9982 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/10/shuffle_0_370_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9983 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/0c/shuffle_0_385_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9984 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/0e/shuffle_0_390_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9985 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/1d/shuffle_0_405_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9986 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/36/shuffle_0_420_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9987 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/1b/shuffle_0_425_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9988 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/0b/shuffle_0_430_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9989 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/0d/shuffle_0_450_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:11 2014 999 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/28/shuffle_1_630_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9990 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/29/shuffle_0_465_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9991 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/14/shuffle_0_495_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9992 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/3c/shuffle_0_525_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9993 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/2a/shuffle_0_530_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9994 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/05/shuffle_0_535_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9995 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/15/shuffle_0_540_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9996 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/2c/shuffle_0_550_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9997 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/13/shuffle_0_560_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:28 2014 9998 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/12/shuffle_0_570_0.data
lr-x------ 1 yarn yarn 64  9月 16 20:27 2014 9999 -> /hadoop/yarn/local/usercache/kou/appcache/application_1410858801629_0012/spark-local-20140916200509-7444/2f/shuffle_0_580_0.data
{code}

This is caused by not closing InputStream generated by ManagedBuffer.",,andrewor14,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 11:38:00 UTC 2014,,,,,,,,,,"0|i203fr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"16/Sep/14 12:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2408;;;","26/Nov/14 03:03;andrewor14;Hey [~sarutak] do we want this for branch-1.1? If so we should re-open this.;;;","26/Nov/14 11:38;sarutak;[~andrewor14] I don't think we need this for branch-1.1 because ManagedBuffer is not implemented in the branch.
I'll remove branch-1.1 from the target version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write TaskContext in Java and expose it through a static accessor,SPARK-3543,12741672,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,prashant,pwendell,pwendell,16/Sep/14 04:49,03/Feb/15 06:17,14/Jul/23 06:26,27/Sep/14 04:30,,,,,,,,1.2.0,,,,,,Spark Core,,,,1,,,,,,"Right now we have these xWithContext methods and it's a bit awkward (for instance, we don't support accessing taskContext from a normal map or filter operation). I'd propose the following

1. Re-write TaskContext in Java - it's a simple class. It can still refer to the scala version of TaskMetrics.
2. Have a static method `TaskContext.get()` which will return the current in-scope TaskContext. Under the hood this uses a thread local variable similar to SparkEnv that the Executor sets.
3. Deprecate all of the existing xWithContext methods.",,apachespark,chengxiang li,pwendell,rxin,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2895,,,,,,,,SPARK-5549,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 28 05:45:31 UTC 2014,,,,,,,,,,"0|i202un:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"16/Sep/14 04:53;rxin;FYI you can define the current Scala closure in Java too
{code}
public TaskContext addTaskCompletionListener(scala.Function1<TaskContext, scala.Unit> f) {
  // do whatever
  return this;
 }
{code}
;;;","16/Sep/14 06:02;chengxiang li;I think this would solve SPARK-2895 as well, glad to have it.;;;","17/Sep/14 08:20;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/2425;;;","27/Sep/14 04:45;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2557;;;","28/Sep/14 05:45;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2560;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Task description ""apply at Option.scala:120""; no user code involved",SPARK-3539,12741624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,jsalvatier,jsalvatier,15/Sep/14 23:35,26/Feb/15 00:44,14/Jul/23 06:26,26/Feb/15 00:44,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"In Spark 1.1, I'm seeing tasks with callbacks that don't involve my code at all!
I'd seen something like this before in 1.0.0, but the behavior seems to be back

apply at Option.scala:120
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
scala.Option.getOrElse(Option.scala:120)
org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
org.apache.spark.rdd.FilteredRDD.getPartitions(FilteredRDD.scala:29)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
scala.Option.getOrElse(Option.scala:120)
org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
scala.Option.getOrElse(Option.scala:120)
org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
org.apache.spark.rdd.FilteredRDD.getPartitions(FilteredRDD.scala:29)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
scala.Option.getOrElse(Option.scala:120)
org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
",,jsalvatier,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1280,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 00:44:31 UTC 2015,,,,,,,,,,"0|i202jz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/14 17:20;pwendell;could you describe in more detail what you mean here? What do you mean by callbacks?;;;","16/Sep/14 18:09;jsalvatier;Sorry, in the Spark UI, a task appears with the description ""apply at Option.scala:120"" and the stack trace posted above if you click ""details"".;;;","26/Feb/15 00:44;srowen;I'm all but certain this was fixed by SPARK-1853 and https://github.com/apache/spark/commit/729952a5efce755387c76cdf29280ee6f49fdb72 

The problem here is that the ""scala.Option..."" line is parsed as user code but it isn't of course. It should keep skipping these frames. User code is higher up. That was part of the fix that was made, to skip things starting with ""scala"".;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT on empty parquet table throws exception,SPARK-3536,12741604,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,marmbrus,marmbrus,15/Sep/14 21:35,23/Sep/14 18:52,14/Jul/23 06:26,23/Sep/14 18:52,,,,,,,,1.2.0,,,,,,SQL,,,,0,starter,,,,,"Reported by [~matei].  Reproduce as follows:
{code}
scala> case class Data(i: Int)
defined class Data

scala> createParquetFile[Data](""testParquet"")
scala> parquetFile(""testParquet"").count()
14/09/15 14:34:17 WARN scheduler.DAGScheduler: Creating new stage failed due to exception - job: 0
java.lang.NullPointerException
	at org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getSplits(ParquetTableOperations.scala:438)
	at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:344)
	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
{code}",,apachespark,isaias.barroso,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 23 18:52:26 UTC 2014,,,,,,,,,,"0|i202fr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"19/Sep/14 07:39;ravipesala;It return null metadata from parquet if querying on empty parquet file while calculating splits.So we should add null check and returns the empty splits solves the issue.;;;","19/Sep/14 09:31;isaias.barroso;Ravindra Pesala, I've fixed it on a BRANCH and ran the Test Suite for parquet, if someone hasn't started to fix, I can submit a PR. If ok please assign me to this Issue.;;;","19/Sep/14 10:03;ravipesala;[~isaias.barroso] I have submitted the PR  4 hours ago,but I am not sure why it is not yet linked it to jira.;;;","19/Sep/14 17:15;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2456;;;","23/Sep/14 18:52;marmbrus;Issue resolved by pull request 2456
[https://github.com/apache/spark/pull/2456];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Mesos not correctly setting heap overhead,SPARK-3535,12741598,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brenden,brenden,brenden,15/Sep/14 21:12,20/May/15 08:25,14/Jul/23 06:26,03/Oct/14 19:59,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Mesos,,,,1,,,,,,"Spark on Mesos does account for any memory overhead.  The result is that tasks are OOM killed nearly 95% of the time.

Like with the Hadoop on Mesos project, Spark should set aside 15-25% of the executor memory for JVM overhead.

For example, see: https://github.com/mesos/hadoop/blob/master/src/main/java/org/apache/hadoop/mapred/ResourcePolicy.java#L55-L63",,aash,brenden,deric,tstclair,vinodkone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2445,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 19 01:26:46 UTC 2014,,,,,,,,,,"0|i202ef:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"15/Sep/14 21:20;tstclair;Are you seeing this under fine grained, course grained, or both.  ;;;","15/Sep/14 21:31;brenden;I'm seeing this in fine grained mode. I confirmed that Spark sets the heap size to equal to the task memory.;;;","15/Sep/14 21:37;brenden;I wrote a patch:

https://github.com/apache/spark/pull/2401

I'm not very familiar with Spark internals or conventions, so feel free to rip this apart.;;;","15/Sep/14 22:21;aash;Why does the task need extra memory if the heap size equals the available task memory?  Filesystem cache?;;;","15/Sep/14 22:26;brenden;The JVM heap size does not include the stack size, GC overhead, or anything malloc'd by other libraries linked into the JVM.;;;","16/Sep/14 07:05;deric;I'm having the same issue in course grained mode - {{spark.mesos.coarse=true}} (in fine grained it's less frequent) and with cgroups isolation enabled. Without the isolation it might be harder to reproduce the bug.;;;","16/Sep/14 22:01;brenden;I've updated the patch to include the same change for coarse mode, and also updated the documentation.;;;","18/Sep/14 23:58;brenden;After some even further digging, I noticed the following error in the Mesos slave log:

  E0918 23:13:48.726176 130743 slave.cpp:2204] Failed to update resources for container ae051668-6cb8-4252-8b6e-cfbac38e7e5c of executor 20140813-050807-3852091146-5050-1861-231 running task 3 on status update for terminal task, destroying container:Collect failed: No cpus resource given

I'll update my patch accordingly.;;;","19/Sep/14 01:26;vinodkone;This can happen if the spark executor doesn't use any cpus (or memory) and there are no tasks running on it. Note that in the next release of Mesos, such an executor is not allowed to launch. https://issues.apache.org/jira/browse/MESOS-1807;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid running MLlib and Streaming tests when testing SQL PRs,SPARK-3534,12741555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,nchammas,marmbrus,marmbrus,15/Sep/14 18:38,18/Sep/14 03:55,14/Jul/23 06:26,17/Sep/14 19:45,,,,,,,,1.2.0,,,,,,Project Infra,SQL,,,0,,,,,,"We are bumping up against the 120 minute time limit for tests pretty regularly now.  Since we have decreased the number of shuffle partitions and up-ed the parallelism I don't think there is much low hanging fruit to speed up the SQL tests. (The tests that are listed as taking 2-3 minutes are actually 100s of tests that I think are valuable).  Instead I propose we avoid running tests that we don't need to.

This will have the added benefit of eliminating failures in SQL due to flaky streaming tests.

Note that this won't fix the full builds that are run for every commit.  There I think we just just up the test timeout.

cc: [~joshrosen] [~pwendell]",,apachespark,joshrosen,marmbrus,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1455,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 03:55:25 UTC 2014,,,,,,,,,,"0|i2025b:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Sep/14 18:42;joshrosen;Looks like this has been proposed before: SPARK-1455;;;","15/Sep/14 20:06;marmbrus;Ah, yeah.  Good catch Josh.  I'll settle for a simpler version where we only fix SQL (since that is where the pain is being felt now).  Bonus points for implementing the full solution :);;;","16/Sep/14 02:43;nchammas;[~marmbrus] - If you just want to run all the SQL tests, what Maven or {{sbt}} command would you run?;;;","16/Sep/14 02:46;marmbrus;sbt/sbt -Phive catalyst/test sql/test hive/test

;;;","17/Sep/14 00:15;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2420;;;","17/Sep/14 21:00;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2437;;;","18/Sep/14 03:55;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark On FreeBSD. Snappy used by torrent broadcast fails to load native libs.,SPARK-3532,12741446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,prashant,prashant,15/Sep/14 10:51,30/Sep/14 09:06,14/Jul/23 06:26,30/Sep/14 09:06,,,,,,,,,,,,,,,,,,0,,,,,,"While trying out spark on freebsd, this seemed like first blocker. 

Workaround: In conf/spark-defaults.conf, Set ""spark.broadcast.compress 	false""
Even better workaround is set:
spark.io.compression.codec 	lzf
",,hsn,prashant,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 15 12:01:44 UTC 2014,,,,,,,,,,"0|i201hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/14 12:01;hsn;you need to grab snappy native library from _snappy-java_ freebsd port, its not included in maven central JAR.

{quote}
(hsn@sanatana:pts/8):~% pkg info -l snappyjava
snappyjava-1.0.4.1_1:
        /usr/local/lib/libsnappyjava.so
        /usr/local/share/java/classes/snappy-java.jar
{quote};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select null from table would throw a MatchError,SPARK-3531,12741435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,adrian-wang,adrian-wang,15/Sep/14 09:18,26/Sep/14 19:05,14/Jul/23 06:26,26/Sep/14 19:05,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"""select null from src limit 1"" will lead to a scala.MatchError",,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 26 19:05:01 UTC 2014,,,,,,,,,,"0|i201fj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/14 09:25;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2396;;;","26/Sep/14 19:05;marmbrus;Issue resolved by pull request 2396
[https://github.com/apache/spark/pull/2396];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strip the physical plan message margin,SPARK-3527,12741415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,chenghao,chenghao,chenghao,15/Sep/14 07:48,16/Sep/14 18:21,14/Jul/23 06:26,16/Sep/14 18:21,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 15 07:50:25 UTC 2014,,,,,,,,,,"0|i201b3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/14 07:50;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2392;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark RDDs are missing the distinct(n) method,SPARK-3519,12741266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,farrellee,nchammas,nchammas,13/Sep/14 16:55,16/Sep/14 18:40,14/Jul/23 06:26,16/Sep/14 18:40,1.1.0,,,,,,,1.2.0,,,,,,PySpark,Spark Core,,,0,,,,,,"{{distinct()}} works but {{distinct(N)}} doesn't.

{code}
>>> sc.parallelize([1,1,2]).distinct()
PythonRDD[15] at RDD at PythonRDD.scala:43
>>> sc.parallelize([1,1,2]).distinct(2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: distinct() takes exactly 1 argument (2 given)
{code}

The PySpark docs only call out [the {{distinct()}} signature|http://spark.apache.org/docs/1.1.0/api/python/pyspark.rdd.RDD-class.html#distinct], but the programming guide [includes the {{distinct(N)}} signature|http://spark.apache.org/docs/1.1.0/programming-guide.html#transformations] as well.

{quote}
{noformat}
distinct([numTasks]))	Return a new dataset that contains the distinct elements of the source dataset.
{noformat}
{quote}",,apachespark,joshrosen,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3500,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 16 18:40:10 UTC 2014,,,,,,,,,,"0|i200d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/14 16:58;nchammas;[~joshrosen] & [~davies]: Here is a ticket for the missing {{distinct(N)}} method. I marked it as a bug since the programming guide says it should exist.;;;","13/Sep/14 21:25;apachespark;User 'mattf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2383;;;","16/Sep/14 18:40;joshrosen;Issue resolved by pull request 2383
[https://github.com/apache/spark/pull/2383];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove useless statement in JsonProtocol,SPARK-3518,12741249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,13/Sep/14 08:40,15/Sep/14 23:12,14/Jul/23 06:26,15/Sep/14 23:12,1.2.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"In org.apache.spark.util.JsonProtocol#taskInfoToJson, a variable named ""accumUpdateMap"" is created as follows.

{code}
val accumUpdateMap = taskInfo.accumulables
{code}

But accumUpdateMap is never used and there is 2nd invocation of ""taskInfo.accumlables"" as follows.

{code}
(""Accumulables"" -> JArray(taskInfo.accumulables.map(accumulableInfoToJson).toList))
{code}",,apachespark,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 15 23:12:32 UTC 2014,,,,,,,,,,"0|i2009j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/14 09:00;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2380;;;","15/Sep/14 23:12;pwendell;Fixed by:
https://github.com/apache/spark/pull/2380;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetMetastoreSuite fails when executed together with other suites under Maven,SPARK-3515,12741209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,13/Sep/14 00:17,14/Sep/14 03:05,14/Jul/23 06:26,14/Sep/14 03:05,1.1.1,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Reproduction step:
{code}
mvn -Phive,hadoop-2.4 -DwildcardSuites=org.apache.spark.sql.parquet.ParquetMetastoreSuite,org.apache.spark.sql.hive.StatisticsSuite -pl core,sql/catalyst,sql/core,sql/hive test
{code}
Maven instantiates all discovered test suite object at first, and then starts executing all test cases. {{ParquetMetastoreSuite}} sets up several temporary tables in constructor, but these tables are deleted immediately since {{StatisticsSuite}}'s constructor calls {{TestHiveContext.reset()}}.

To fix this issue, we shouldn't put this kind of side effect in constructor, but in {{beforeAll}}.",,apachespark,joshrosen,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3481,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 13 00:54:08 UTC 2014,,,,,,,,,,"0|i2000n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/14 00:30;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2375;;;","13/Sep/14 00:54;lian cheng;The bug SPARK-3481 fixed actually covered up the bug mentioned in this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.1.0-SNAPSHOT in docs for 1.1.0 under docs/latest,SPARK-3506,12741024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,pwendell,jlaskowski,jlaskowski,12/Sep/14 11:19,13/Oct/14 15:53,14/Jul/23 06:26,13/Oct/14 15:53,1.1.0,,,,,,,1.1.1,,,,,,Documentation,,,,0,,,,,,"In https://spark.apache.org/docs/latest/ there are references to 1.1.0-SNAPSHOT:

* This documentation is for Spark version 1.1.0-SNAPSHOT.
* For the Scala API, Spark 1.1.0-SNAPSHOT uses Scala 2.10.

It should be version 1.1.0 since that's the latest released version and the header tells so, too.",,jlaskowski,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 15:53:37 UTC 2014,,,,,,,,,,"0|i1zyvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/14 15:00;srowen;Yeah, I imagine that can be touched up right now. For the future, I imagine the issue was just that the site was built from the branch before the release plugin upped the version and created the artifacts? So the site might be better built from the final released source artifact.

I imagine it's a release-process doc change but don't know whether that lives.;;;","15/Sep/14 23:31;pwendell;Yeah the issue is some people wanted to get doc changes in after the release was posted, I didn't realize the version had been updated though. I'll re-build the docs. Thanks for the heads up.;;;","13/Oct/14 15:53;srowen;Looks like the site has been updated, and I see no SNAPSHOT on the page.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive SimpleUDF will create duplicated type cast which cause exception in constant folding,SPARK-3501,12740955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,12/Sep/14 05:04,19/Sep/14 22:29,14/Jul/23 06:26,19/Sep/14 22:29,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"When do the query like:
select datediff(cast(value as timestamp), cast('2002-03-21 00:00:00' as timestamp)) from src;

SparkSQL will raise exception:
{panel}
[info] - Cast Timestamp to Timestamp in UDF *** FAILED ***
[info]   scala.MatchError: TimestampType (of class org.apache.spark.sql.catalyst.types.TimestampType$)
[info]   at org.apache.spark.sql.catalyst.expressions.Cast.castToTimestamp(Cast.scala:77)
[info]   at org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:251)
[info]   at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
[info]   at org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
[info]   at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$5$$anonfun$applyOrElse$2.applyOrElse(Optimizer.scala:217)
[info]   at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$5$$anonfun$applyOrElse$2.applyOrElse(Optimizer.scala:210)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$2.apply(TreeNode.scala:180)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
{panel}",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 19 22:29:57 UTC 2014,,,,,,,,,,"0|i1zygn:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Sep/14 05:20;chenghao;https://github.com/apache/spark/pull/2368;;;","12/Sep/14 05:20;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2368;;;","19/Sep/14 22:29;marmbrus;Issue resolved by pull request 2368
[https://github.com/apache/spark/pull/2368];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
coalesce() and repartition() of SchemaRDD is broken,SPARK-3500,12740950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,12/Sep/14 04:55,13/Sep/14 16:56,14/Jul/23 06:26,13/Sep/14 02:29,1.1.0,,,,,,,1.1.1,1.2.0,,,,,PySpark,SQL,,,0,,,,,,"{code}
>>> sqlCtx.jsonRDD(sc.parallelize(['{""foo"":""bar""}', '{""foo"":""baz""}'])).coalesce(1)
Py4JError: An error occurred while calling o94.coalesce. Trace:
py4j.Py4JException: Method coalesce([class java.lang.Integer, class java.lang.Boolean]) does not exist
{code}

repartition() is also missing too.",,apachespark,davies,joshrosen,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3519,,SPARK-2797,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 13 02:29:15 UTC 2014,,,,,,,,,,"0|i1zyfj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Sep/14 06:30;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2369;;;","12/Sep/14 16:02;nchammas;[~davies] - Shouldn't the target version for this bugfix be 1.1.1?;;;","12/Sep/14 16:16;pwendell;If it's just a missing feature we tend to be conservative and only put it in the new minor release. [~davies] - is there code that people can use in 1.1 that works around this (i.e if they add a conversion themselves)? If there is a workaround for 1.1 I think we should just publish that and target this for 1.2. ;;;","12/Sep/14 16:24;davies;I think it's a bug, there is a workaround for it:

{code}
srdd2 = SchemaRDD(srdd._jschema_rdd.coalesce(N, False, None), sqlCtx)
{code} ;;;","12/Sep/14 17:07;nchammas;Hmm, you _could_ perhaps consider this a missing feature, though since all base RDD operations should also be valid SchemaRDD operations (right?), it definitely feels like a bug. And it's not just for SchemaRDDs created by jsonRDD (as noted in the title).

It looks like {{repartition}} is missing, too.

{code}
from pyspark.sql import SQLContext
from pyspark.sql import Row
sqlContext = SQLContext(sc)

a = sc.parallelize([Row(field1=1, field2=""row1"")])
sqlContext.inferSchema(a).coalesce(1)  # Method coalesce does not exist
sqlContext.inferSchema(a).repartition(1)  # Method repartition does not exist
{code};;;","12/Sep/14 17:08;nchammas;Btw, this seems like the same type of problem reported in [SPARK-2797].;;;","12/Sep/14 18:22;davies;repartition() and distinct(N) are also missing too.;;;","12/Sep/14 18:54;nchammas;[~davies] - PySpark doesn't seem to support {{distinct(N)}} on even a regular RDD. Should it?

{code}
>>> sc.parallelize([1,2,3]).distinct(2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: distinct() takes exactly 1 argument (2 given)
{code}

This sounds like it's a separate issue. Maybe it should be tracked in a separate JIRA issue?

Also, could we edit the title of this JIRA issue to read something like ""SchemaRDDs are missing these methods: ...""? The problem is not limited to SchemaRDDs created by jsonRDD().;;;","13/Sep/14 01:02;joshrosen;This feels like a bug, not a missing feature, since SchemaRDD instances have a public method that always throws an exception when called.  It seems fair to include this fix in 1.1.1.;;;","13/Sep/14 02:29;joshrosen;Issue resolved by pull request 2369
[https://github.com/apache/spark/pull/2369];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Block replication can by mistake choose driver BlockManager as a peer for replication,SPARK-3496,12740931,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,12/Sep/14 01:41,11/Nov/14 02:37,14/Jul/23 06:26,02/Oct/14 20:50,1.0.2,1.1.0,,,,,,1.1.1,1.2.0,,,,,Block Manager,DStreams,Spark Core,,0,,,,,,"When selecting peer block managers for replicating a block, the driver block manager can also get chosen accidentally. This is because BlockManagerMasterActor did not filter out the driver block manager.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 22:32:55 UTC 2014,,,,,,,,,,"0|i1zybb:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"10/Nov/14 22:32;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Block replication fails continuously when the replication target node is dead,SPARK-3495,12740926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,12/Sep/14 01:01,11/Nov/14 02:37,14/Jul/23 06:26,02/Oct/14 20:50,1.0.2,1.1.0,,,,,,1.1.1,1.2.0,,,,,Block Manager,DStreams,Spark Core,,0,,,,,,"If a block manager (say, A) wants to replicate a block and the node chosen for replication (say, B) is dead, then the attempt to send the block to B fails. However, this continues to fail indefinitely. Even if the driver learns about the demise of the B, A continues to try replicating to B and failing miserably. 

The reason behind this bug is that A initially fetches a list of peers from the driver (when B was active), but never updates it after B is dead. This affects Spark Streaming as its receiver uses block replication.

",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3498,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 22:32:54 UTC 2014,,,,,,,,,,"0|i1zya7:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"12/Sep/14 01:15;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2366;;;","10/Nov/14 22:32;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecisionTree overflow error in calculating maxMemoryUsage,SPARK-3494,12740892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,11/Sep/14 22:40,12/Sep/14 08:39,14/Jul/23 06:26,12/Sep/14 08:38,1.1.0,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"maxMemoryUsage can easily overflow.  It needs to use long ints, and also check for overflows afterwards.",,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 12 08:38:58 UTC 2014,,,,,,,,,,"0|i1zy33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/14 08:38;mengxr;https://github.com/apache/spark/pull/2341;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alleviate port collisions during tests,SPARK-3490,12740820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,11/Sep/14 16:42,16/May/15 11:56,14/Jul/23 06:26,16/May/15 11:56,1.1.0,,,,,,,0.9.3,1.1.1,1.2.0,,,,Spark Core,,,,0,,,,,,"A few tests, notably SparkSubmitSuite and DriverSuite, have been failing intermittently because we open too many ephemeral ports and occasionally can't bind to new ones.

We should minimize the use of ports during tests where possible. A great candidate is the SparkUI, which is not needed for most tests.",,andrewor14,apachespark,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1047,,,,,SPARK-3431,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 11:56:35 UTC 2015,,,,,,,,,,"0|i1zxnb:",9223372036854775807,,,,,,,,,,,,,,0.9.3,1.0.3,1.1.1,1.2.0,,,,,,,,"12/Sep/14 00:24;andrewor14;Fixed in https://github.com/apache/spark/pull/2363;;;","12/Sep/14 18:05;andrewor14;This still needs to be back ported into branch-1.1;;;","16/Sep/14 18:35;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2415;;;","17/Sep/14 01:23;pwendell;Issue resolved by pull request 2415
[https://github.com/apache/spark/pull/2415];;;","17/Sep/14 01:24;andrewor14;Backported. Closing this.;;;","08/Jan/15 23:38;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3959;;;","09/Jan/15 00:01;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/3961;;;","09/Jan/15 00:03;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2363;;;","09/Jan/15 00:59;andrewor14;Reopening this for branches 0.9 and 1.0;;;","03/Mar/15 09:51;srowen;[~andrewor14] is this one not back-portable to 1.0? I saw you closed your PR.;;;","16/May/15 11:56;srowen;I think it's not likely there would be another 0.9 or 1.0 branch release now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn ClientBase.validateArgs memory checks wrong,SPARK-3476,12740597,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,tgraves,tgraves,10/Sep/14 20:13,26/Sep/14 18:51,14/Jul/23 06:26,26/Sep/14 18:51,1.2.0,,,,,,,,,,,,,YARN,,,,0,,,,,,"The yarn ClientBase.validateArgs  memory checks are no longer valid.  It used to be that the overhead was taken out of what the user specified, now we add it on top of what the user specifies.   We can probably just remove these. 

(args.amMemory <= memoryOverhead) -> (""Error: AM memory size must be"" +
        ""greater than: "" + memoryOverhead),
      (args.executorMemory <= memoryOverhead) -> (""Error: Executor memory size"" +
        ""must be greater than: "" + memoryOverhead.toString)
",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 25 00:45:44 UTC 2014,,,,,,,,,,"0|i1zwbj:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"10/Sep/14 20:13;tgraves;note https://github.com/apache/spark/pull/2253 fixes up the am memory calculations.;;;","25/Sep/14 00:45;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2528;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task metrics are not aggregated correctly in local mode,SPARK-3465,12740243,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,09/Sep/14 21:49,12/Sep/14 21:30,14/Jul/23 06:26,12/Sep/14 21:30,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"In local mode, after onExecutorMetricsUpdate(), t.taskMetrics will be the same object with that in TaskContext (because there is no serialization for MetricsUpdate in local mode), then all the upcoming changes in metrics will be lost, because updateAggregateMetrics() only counts the difference in these two. 

This bug was introduced in https://issues.apache.org/jira/browse/SPARK-2099, cc [~sandyr]]",,apachespark,davies,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 10 01:49:38 UTC 2014,,,,,,,,,,"0|i1zuq7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"10/Sep/14 01:49;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2338;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnAllocator can lose container requests to RM,SPARK-3456,12740155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,tgraves,09/Sep/14 15:06,13/Sep/14 01:31,14/Jul/23 06:26,13/Sep/14 01:31,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"I haven't actually tested this yet, but I believe that spark on yarn can lose container requests to the RM.  The reason is that we ask for the total number upfront (say x) but then we don't ask for anymore unless some are missing and if we do then we could erase the original request.

For example

- ask for 3 containers
- 1 is allocated
- ask for 0 containers since asked for 3 originally (2 left)
- the 1 allocated dies
- We now ask for 1 since its missing, this will override whatever is on the yarn side (in this case 2).

Then we lose the 2 more we need.",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 12 19:40:40 UTC 2014,,,,,,,,,,"0|i1zu6v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"09/Sep/14 15:15;tgraves;Note this is only a problem on yarn alpha because in stable we use the AMRMClient interface which actually does an add.  ;;;","12/Sep/14 19:40;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2373;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
**HotFix** Unit test failed due to can not resolve the attribute references,SPARK-3455,12740114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,chenghao,chenghao,09/Sep/14 11:34,13/Sep/14 06:03,14/Jul/23 06:26,13/Sep/14 06:03,,,,,,,,,,,,,,SQL,,,,0,,,,,,"The test case ""SPARK-3349 partitioning after limit"" failed, the exception as :
{panel}
23:10:04.117 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 274.0 failed 1 times; aborting job
[info] - SPARK-3349 partitioning after limit *** FAILED ***
[info]   Exception thrown while executing query:
[info]   == Parsed Logical Plan ==
[info]   Project [*]
[info]    Join Inner, Some(('subset1.n = 'lowerCaseData.n))
[info]     UnresolvedRelation None, lowerCaseData, None
[info]     UnresolvedRelation None, subset1, None
[info]   
[info]   == Analyzed Logical Plan ==
[info]   Project [n#605,l#606,n#12]
[info]    Join Inner, Some((n#12 = n#605))
[info]     SparkLogicalPlan (ExistingRdd [n#605,l#606], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219)
[info]     Limit 2
[info]      Sort [n#12 DESC]
[info]       Distinct 
[info]        Project [n#12]
[info]         SparkLogicalPlan (ExistingRdd [n#607,l#608], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219)
[info]   
[info]   == Optimized Logical Plan ==
[info]   Project [n#605,l#606,n#12]
[info]    Join Inner, Some((n#12 = n#605))
[info]     SparkLogicalPlan (ExistingRdd [n#605,l#606], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219)
[info]     Limit 2
[info]      Sort [n#12 DESC]
[info]       Distinct 
[info]        Project [n#12]
[info]         SparkLogicalPlan (ExistingRdd [n#607,l#608], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219)
[info]   
[info]   == Physical Plan ==
[info]   Project [n#605,l#606,n#12]
[info]    ShuffledHashJoin [n#605], [n#12], BuildRight
[info]     Exchange (HashPartitioning [n#605], 10)
[info]      ExistingRdd [n#605,l#606], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219
[info]     Exchange (HashPartitioning [n#12], 10)
[info]      TakeOrdered 2, [n#12 DESC]
[info]       Distinct false
[info]        Exchange (HashPartitioning [n#12], 10)
[info]         Distinct true
[info]          Project [n#12]
[info]           ExistingRdd [n#607,l#608], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219
[info]   
[info]   Code Generation: false
[info]   == RDD ==
[info]   == Exception ==
[info]   org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[info]   Exchange (HashPartitioning [n#12], 10)
[info]    TakeOrdered 2, [n#12 DESC]
[info]     Distinct false
[info]      Exchange (HashPartitioning [n#12], 10)
[info]       Distinct true
[info]        Project [n#12]
[info]         ExistingRdd [n#607,l#608], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219
[info]   
[info]   org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[info]   Exchange (HashPartitioning [n#12], 10)
[info]    TakeOrdered 2, [n#12 DESC]
[info]     Distinct false
[info]      Exchange (HashPartitioning [n#12], 10)
[info]       Distinct true
[info]        Project [n#12]
[info]         ExistingRdd [n#607,l#608], MapPartitionsRDD[13] at mapPartitions at basicOperators.scala:219
[info]   
[info]   	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
[info]   	at org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:44)
[info]   	at org.apache.spark.sql.execution.ShuffledHashJoin.execute(joins.scala:354)
[info]   	at org.apache.spark.sql.execution.Project.execute(basicOperators.scala:42)
[info]   	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:85)
[info]   	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:438)
[info]   	at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:40)
[info]   	at org.apache.spark.sql.SQLQuerySuite$$anonfun$31.apply$mcV$sp(SQLQuerySuite.scala:369)
[info]   	at org.apache.spark.sql.SQLQuerySuite$$anonfun$31.apply(SQLQuerySuite.scala:362)
[info]   	at org.apache.spark.sql.SQLQuerySuite$$anonfun$31.apply(SQLQuerySuite.scala:362)
[info]   	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   	at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   	at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:158)
[info]   	at org.scalatest.Suite$class.withFixture(Suite.scala:1121)
[info]   	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1559)
[info]   	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:155)
[info]   	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
[info]   	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:167)
[info]   	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:167)
[info]   	at org.scalatest.FunSuite.runTest(FunSuite.scala:1559)
[info]   	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:200)
[info]   	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:200)
[info]   	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   	at scala.collection.immutable.List.foreach(List.scala:318)
[info]   	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:200)
[info]   	at org.scalatest.FunSuite.runTests(FunSuite.scala:1559)
[info]   	at org.scalatest.Suite$class.run(Suite.scala:1423)
[info]   	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1559)
[info]   	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:204)
[info]   	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:204)
[info]   	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:204)
[info]   	at org.apache.spark.sql.SQLQuerySuite.org$scalatest$BeforeAndAfterAll$$super$run(SQLQuerySuite.scala:29)
[info]   	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
[info]   	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
[info]   	at org.apache.spark.sql.SQLQuerySuite.run(SQLQuerySuite.scala:29)
[info]   	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:444)
[info]   	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:651)
[info]   	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   	at java.lang.Thread.run(Thread.java:745)
[info]   Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 274.0 failed 1 times, most recent failure: Lost task 0.0 in stage 274.0 (TID 911, localhost): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: n#12
[info]           org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
[info]           org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:47)
[info]           org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:46)
[info]           org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
[info]           org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
[info]           org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:46)
[info]           org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection$$anonfun$$init$$2.apply(Projection.scala:52)
[info]           org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection$$anonfun$$init$$2.apply(Projection.scala:52)
[info]           scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]           scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]           scala.collection.immutable.List.foreach(List.scala:318)
[info]           scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
[info]           scala.collection.AbstractTraversable.map(Traversable.scala:105)
[info]           org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.<init>(Projection.scala:52)
[info]           org.apache.spark.sql.execution.SparkPlan$$anonfun$newMutableProjection$1.apply(SparkPlan.scala:106)
[info]           org.apache.spark.sql.execution.SparkPlan$$anonfun$newMutableProjection$1.apply(SparkPlan.scala:106)
[info]           org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:43)
[info]           org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
[info]           org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
[info]           org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
[info]           org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
[info]           org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[info]           org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[info]           org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
[info]           org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[info]           org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[info]           org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
[info]           org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[info]           org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[info]           org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
[info]           org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
[info]           org.apache.spark.scheduler.Task.run(Task.scala:54)
[info]           org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
[info]           java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]           java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]           java.lang.Thread.run(Thread.java:745)
[info]   Driver stacktrace:
[info]   	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
[info]   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
[info]   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
[info]   	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[info]   	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
[info]   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
[info]   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
[info]   	at scala.Option.foreach(Option.scala:236)
[info]   	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
[info]   	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
[info]   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
[info]   	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
[info]   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
[info]   	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
[info]   	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
[info]   	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
[info]   	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
[info]   	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
[info]   	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) (QueryTest.scala:42)
{panel}",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 09 11:48:08 UTC 2014,,,,,,,,,,"0|i1ztxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/14 11:48;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2334;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven build should skip publishing artifacts people shouldn't depend on,SPARK-3452,12740072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,prashant,pwendell,pwendell,09/Sep/14 06:46,20/Jan/15 23:41,14/Jul/23 06:26,15/Sep/14 04:19,1.0.0,1.1.0,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"I think it's easy to do this by just adding a skip configuration somewhere. We shouldn't be publishing repl, yarn, assembly, tools, repl-bin, or examples.",,aniket,dfallside,dmcwhorter,peng,pwendell,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5144,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 23:41:31 UTC 2015,,,,,,,,,,"0|i1ztof:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Sep/14 04:19;pwendell;Fixed by:
https://github.com/apache/spark/pull/2329;;;","22/Dec/14 21:51;peng;IMHO REPL should be kept being published, one of my project extends its API to initialize some third-party components upon launching.
This should be made an 'official' API to encourage more platform integrate with it.;;;","06/Jan/15 06:27;aniket;I would like this to be revisited. The issue I am facing is that while people may not dependent on some modules during compile time but they may dependent on them during runtime. For example, I am building a spark server that lets users submit spark jobs using convenient REST endpoints. This used to work great even in yarn-client mode. However, once I migrate to 1.2.0, this breaks because I can no longer add dependency of my spark server to spark-yarn module which is used while submitting jobs to YARN cluster.;;;","06/Jan/15 09:35;srowen;[~aniket] I think that's a little different. You may find the Spark YARN API you want now in spark-network-yarn.;;;","06/Jan/15 14:26;aniket;Ok.. I'll test this out by adding dependency to spark-network-yarn and see how it goes. Fingers crossed!;;;","07/Jan/15 09:56;aniket;Here is the exception I am getting while triggering a job that contains SparkContext having master as yarn-client. A quick look at 1.2.0 source code suggests I should depend on spark-yarn module which I can't as it is not longer published. Do you want me to log a separate defect for this and submit appropriate pull request? 

2015-01-07 14:39:22,799 [pool-10-thread-13] [info] o.a.s.s.MemoryStore - MemoryS
tore started with capacity 731.7 MB
Exception in thread ""pool-10-thread-13"" java.lang.ExceptionInInitializerError
        at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
        at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
        at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
        at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
        at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
        at com.myimpl.Server:23)
        at scala.util.Success$$anonfun$map$1.apply(Try.scala:236)
        at scala.util.Try$.apply(Try.scala:191)
        at scala.util.Success.map(Try.scala:236)
        at com.myimpl.FutureTry$$anonfun$1.apply(FutureTry.scala:23)
        at com.myimpl.FutureTry$$anonfun$1.apply(FutureTry.scala:23)
        at scala.util.Success$$anonfun$map$1.apply(Try.scala:236)
        at scala.util.Try$.apply(Try.scala:191)
        at scala.util.Success.map(Try.scala:236)
        at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
        at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Unable to load YARN support
        at org.apache.spark.deploy.SparkHadoopUtil$.liftedTree1$1(SparkHadoopUtil.scala:199)
        at org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:194)
        at org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
        ... 27 more
Caused by: java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.YarnSparkHadoopUtil
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:190)
        at org.apache.spark.deploy.SparkHadoopUtil$.liftedTree1$1(SparkHadoopUtil.scala:195)
        ... 29 more
;;;","08/Jan/15 10:22;aniket;I have opened another defect - SPARK-5144 suggesting yarn-client to be published.;;;","20/Jan/15 23:41;dmcwhorter;Same problem here -- if spark-yarn is not available, what is the correct way to submit yarn jobs programatically?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpecificMutableRow.update doesn't check for null,SPARK-3448,12740031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,09/Sep/14 00:47,10/Sep/14 01:40,14/Jul/23 06:26,10/Sep/14 01:40,1.1.0,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,"{code}
  test(""SpecificMutableRow.update with null"") {
    val row = new SpecificMutableRow(Seq(IntegerType))
    row(0) = null
    assert(row.isNullAt(0))
  }
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 09 01:47:07 UTC 2014,,,,,,,,,,"0|i1ztfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/14 01:47;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2325;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kryo NPE when serializing JListWrapper,SPARK-3447,12740003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,08/Sep/14 23:00,18/Sep/14 14:33,14/Jul/23 06:26,11/Sep/14 04:00,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Repro (provided by [~davies]):
{code}
from pyspark.sql import SQLContext; SQLContext(sc).inferSchema(sc.parallelize([{""a"": [3]}]))._jschema_rdd.collect()
{code}

{code}
14/09/05 21:59:47 ERROR TaskResultGetter: Exception while getting task result
com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException
Serialization trace:
underlying (scala.collection.convert.Wrappers$JListWrapper)
values (org.apache.spark.sql.catalyst.expressions.GenericRow)
at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)
at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
at org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:514)
at org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:355)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1276)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:701)
Caused by: java.lang.NullPointerException
at scala.collection.convert.Wrappers$MutableBufferWrapper.add(Wrappers.scala:80)
at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
... 23 more
{code}",,apachespark,glenn.strycker@gmail.com,marmbrus,mohan.gadm,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 14:33:35 UTC 2014,,,,,,,,,,"0|i1zt9j:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"08/Sep/14 23:47;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2323;;;","18/Sep/14 12:51;mohan.gadm;I am also facing the same issue with spark streaming API, kryo serializer and Avro Objects.
I have observed this behavior with output operations like print, collect etc. also observed that if the avro object is simple, no problem. but if the avro objects are complex with unions/Arrays, gives the exception.

find the stack trace below:
ERROR scheduler.JobScheduler: Error running job streaming job 1411043845000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException
Serialization trace:
value (xyz.Datum)
data (xyz.ResourceMessage)
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


below is the avro message.
{""version"": ""01"", ""sequence"": ""00001"", ""resource"": ""sensor-001"", ""controller"": ""002"", ""controllerTimestamp"": ""1411038710358"", ""data"": {""value"": [{""name"": ""Temperature"", ""value"": ""30""}, {""name"": ""Speed"", ""value"": ""60""}, {""name"": ""Location"", ""value"": [""+401213.1"", ""-0750015.1""]}, {""name"": ""Timestamp"", ""value"": ""2014-09-09T08:15:25-05:00""}]}}

message is been successfully decoded in decoder, but throws exception for output operation.;;;","18/Sep/14 13:03;yhuai;[~mohan.gadm] From the trace, seems the NPE was caused by 
{code}
value (com.globallogic.goliath.model.Datum)
data (com.globallogic.goliath.model.ResourceMessage)
{code}
Can you check these two classes (I can not find them online)? 
Seems the avro message was decoded and then you stored the message as a com.globallogic.goliath.model.ResourceMessage. In a ResourceMessage, an array was repreented by Datum, which caused the problem.;;;","18/Sep/14 13:12;mohan.gadm;sorry for the mistake, those are the project specific classes. and those are the classes generated by Avro itself from an avro avdl file and available in driver jar and also worker nodes.;;;","18/Sep/14 13:21;mohan.gadm;record KeyValueObject {
		union{boolean, int, long, float, double, bytes, string} name; //can be of any one of them mentioned in union.
		union {boolean, int, long, float, double, bytes, string, array<union{boolean, int, long, float, double, bytes, string, KeyValueObject}>, KeyValueObject} value;
	}
	record Datum {
		union {boolean, int, long, float, double, bytes, string, array<union{boolean, int, long, float, double, bytes, string, KeyValueObject}>, KeyValueObject} value;
	}
	record ResourceMessage {
		string version;
		string sequence;
		string resource;
		string controller;
		string controllerTimestamp;
		union {Datum, array<Datum>} data;
	};;;","18/Sep/14 13:32;yhuai;Did you register your classes (please check https://spark.apache.org/docs/latest/tuning.html)? Also, if you have further question about it, please send it to our user list (user@spark.apache.org) since this issue is not related to this JIRA.;;;","18/Sep/14 14:33;mohan.gadm;Thanks for the suggestion.
Yes i have registered my classes with kryo.
Created a topic in spark user list for the same. at the below link:
http://apache-spark-user-list.1001560.n3.nabble.com/Kryo-fails-with-avro-having-Arrays-and-unions-but-succeeds-with-simple-avro-tc14549.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 and CLI should retrieve Hive result set schema,SPARK-3440,12739941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,lian cheng,lian cheng,08/Sep/14 18:48,15/Sep/15 20:54,14/Jul/23 06:26,15/Sep/15 20:54,1.0.2,1.1.0,,,,,,,,,,,,SQL,,,,0,,,,,,"When executing Hive native queries/commands with {{HiveContext.runHive}}, Spark SQL only calls {{Driver.getResults}} and returns a {{Seq\[String\]}}. The schema of the result set is not retrieved, and thus not possible to split the row string into proper columns and assign column names to them. For example, currently all {{NativeCommand}} only returns a single column named {{result}}.

For existing Hive applications that rely on result set schemas, this breaks compatibility.",,adrian-wang,gvramana,lian cheng,marmbrus,ravipesala,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 20:54:47 UTC 2015,,,,,,,,,,"0|i1zsw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/15 06:10;adrian-wang;It seems Hive will only have a schema of null val for native command like CREATE TABLE and DROP TABLE.;;;","27/May/15 19:33;yhuai;[~lian cheng] I am removing the target version of it. Feel free to re-target it.;;;","15/Sep/15 20:54;marmbrus;I think that in general most native commands are actually implemented by Spark SQL directly now.  Please reopen if there is other stuff to be done here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mima false-positives with @DeveloperAPI and @Experimental annotations,SPARK-3433,12739806,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,prashant,joshrosen,joshrosen,08/Sep/14 01:49,13/Jan/15 00:44,14/Jul/23 06:26,12/Dec/14 00:46,1.1.0,1.2.0,,,,,,1.1.2,1.2.0,,,,,Build,,,,0,,,,,,"In https://github.com/apache/spark/pull/2315, I found two cases where {{@DeveloperAPI}} and {{@Experimental}} annotations didn't prevent false-positive warnings from Mima.  To reproduce this problem, run dev/mima as of https://github.com/JoshRosen/spark/commit/ec90e21947b615d4ef94a3a54cfd646924ccaf7c.  The spurious warnings are listed at the top of https://gist.github.com/JoshRosen/5d8df835516dc367389d.",,apachespark,joshrosen,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 00:44:22 UTC 2015,,,,,,,,,,"0|i1zs33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/14 00:46;srowen;The PR was https://github.com/apache/spark/pull/2285 and it was merged, looks like for 1.2: https://github.com/apache/spark/commit/ecf0c02935815f0d4018c0e30ec4c784e60a5db0;;;","13/Jan/15 00:44;joshrosen;I've backported this to {{branch-1.1}} in order to fix a MiMa false-positive in that branch.;;;","13/Jan/15 00:44;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/2285;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't include the empty string """" as a defaultAclUser",SPARK-3429,12739751,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aash,aash,aash,07/Sep/14 06:34,12/Sep/14 00:29,14/Jul/23 06:26,12/Sep/14 00:29,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"The empty string is included as a default ACL user for reads and admin permissions.  I'm unsure if it's a security bug, but we should exclude this """" user as it's an obvious bug.",,aash,andrewor14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 12 00:29:25 UTC 2014,,,,,,,,,,"0|i1zrrb:",9223372036854775807,,,,,tgraves,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"12/Sep/14 00:29;andrewor14;https://github.com/apache/spark/pull/2286;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort-based shuffle compression behavior is inconsistent,SPARK-3426,12739711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,andrewor14,andrewor14,06/Sep/14 18:09,22/Oct/14 22:12,14/Jul/23 06:26,22/Oct/14 22:12,1.1.0,1.2.0,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"We have the following configs:

{code}
spark.shuffle.compress
spark.shuffle.spill.compress
{code}

When these two diverge, sort-based shuffle fails with a compression exception under certain workloads. This is because in sort-based shuffle we serve the index file (using spark.shuffle.spill.compress) as a normal shuffle file (using spark.shuffle.compress). It was unfortunate in retrospect that these two configs were exposed so we can't easily remove them.

Here is how this can be reproduced. Set the following in your spark-defaults.conf:
{code}
spark.master                  local-cluster[1,1,512]
spark.shuffle.spill.compress  false
spark.shuffle.compress        true
spark.shuffle.manager         sort
spark.shuffle.memoryFraction  0.001
{code}
Then run the following in spark-shell:
{code}
sc.parallelize(0 until 100000).map(i => (i/4, i)).groupByKey().collect()
{code}

This leads to compression errors, such as the following:

{code}
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 8, joshs-mbp): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
[info]         org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
[info]         org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
[info]         org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)
[info]         org.xerial.snappy.Snappy.uncompress(Snappy.java:480)
[info]         org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:127)
[info]         org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
[info]         org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58)
[info]         org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
[info]         org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1090)
[info]         org.apache.spark.storage.BlockManager.getLocalShuffleFromDisk(BlockManager.scala:350)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$fetchLocalBlocks$1$$anonfun$apply$4.apply(ShuffleBlockFetcherIterator.scala:196)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$fetchLocalBlocks$1$$anonfun$apply$4.apply(ShuffleBlockFetcherIterator.scala:196)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:243)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
[info]         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
[info]         org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
[info]         org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
[info]         org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:129)
[info]         org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
[info]         org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
[info]         org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
[info]         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[info]         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[info]         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
[info]         org.apache.spark.scheduler.Task.run(Task.scala:56)
[info]         org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
[info]         java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]         java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]         java.lang.Thread.run(Thread.java:745)
{code}

Similarly, with

{code}
spark.shuffle.spill.compress  true
spark.shuffle.compress        false
{code}

we see

{code}
info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 8, joshs-mbp): java.io.StreamCorruptedException: invalid stream header: 82534E41
[info]         java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:804)
[info]         java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
[info]         org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:57)
[info]         org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:57)
[info]         org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:95)
[info]         org.apache.spark.storage.BlockManager.getLocalShuffleFromDisk(BlockManager.scala:355)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$fetchLocalBlocks$1$$anonfun$apply$4.apply(ShuffleBlockFetcherIterator.scala:197)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$fetchLocalBlocks$1$$anonfun$apply$4.apply(ShuffleBlockFetcherIterator.scala:197)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:244)
[info]         org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
[info]         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
[info]         org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
[info]         org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
[info]         org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:129)
[info]         org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
[info]         org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
[info]         org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
[info]         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
[info]         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
[info]         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
[info]         org.apache.spark.scheduler.Task.run(Task.scala:56)
[info]         org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
[info]         java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]         java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]         java.lang.Thread.run(Thread.java:745)
{code}",,aash,andrewor14,apachespark,jerryshao,joshrosen,lukas.nalezenec,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3367,,SPARK-3630,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 22 22:12:29 UTC 2014,,,,,,,,,,"0|i1zrif:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"14/Oct/14 02:23;jerryshao;Hi [~andrewor14], are you going to fix this issue, what is plan to fix this: remove ""spark.shuffle.spill.compress"" to keep the consistency or just modify the code in ExternalSorter to fit this inconsistency? 

We've also met this issue and seems two solutions are both not so good, what is your opinion?

Thanks a lot.;;;","14/Oct/14 02:35;jerryshao;Sorry about that, I just saw the PR (https://github.com/apache/spark/pull/2247) discuss about this.;;;","21/Oct/14 23:48;joshrosen;I've edited this issue to list the actual exception that's produced.  This may explain another one of the distinct Snappy errors that we've seen.  I'll start working on a fix now.;;;","21/Oct/14 23:58;joshrosen;Based on the discussion in that PR, it sounds folks would rather fix the underlying bug rather than changing / ignoring configurations.  I'll look into a small, targeted fix for this.;;;","22/Oct/14 07:00;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2890;;;","22/Oct/14 22:12;joshrosen;Fixed in 1.1.1. and 1.2.0 by my PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaAPISuite.getHadoopInputSplits isn't used anywhere,SPARK-3422,12739592,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sandyr,sandyr,sandyr,05/Sep/14 20:45,25/Sep/14 07:34,14/Jul/23 06:26,25/Sep/14 07:34,1.0.2,,,,,,,,,,,,,Spark Core,,,,0,,,,,,,,apachespark,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 08 23:47:21 UTC 2014,,,,,,,,,,"0|i1zqsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/14 23:47;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/2324;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StructField.toString should quote the name field to allow arbitrary character as struct field name,SPARK-3421,12739569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,05/Sep/14 19:04,10/Oct/14 07:34,14/Jul/23 06:26,10/Oct/14 07:34,1.0.2,,,,,,,,,,,,,SQL,,,,0,,,,,,"The original use case is something like this:
{code}
// JSON snippet with ""illegal"" characters in field names
val json =
  """"""{ ""a(b)"": { ""c(d)"": ""hello"" } }"""""" ::
  """"""{ ""a(b)"": { ""c(d)"": ""world"" } }"""""" ::
  Nil

val jsonSchemaRdd = sqlContext.jsonRDD(sparkContext.makeRDD(json))
jsonSchemaRdd.saveAsParquetFile(""/tmp/file.parquet"")

java.lang.Exception: java.lang.RuntimeException: Unsupported dataType: StructType(ArrayBuffer(StructField(a(b),StructType(ArrayBuffer(StructField(c(d),StringType,true))),true))), [1.37] failure: `,' expected but `(' found
{code}
The reason is that, the {{DataType}} parser only allows {{\[a-zA-Z0-9_\]*}} as struct field name.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3713,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 05 20:40:24 UTC 2014,,,,,,,,,,"0|i1zqnb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Sep/14 20:40;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2291;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of old-style classes in pyspark,SPARK-3417,12739500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,mrocklin,mrocklin,05/Sep/14 15:25,08/Sep/14 22:46,14/Jul/23 06:26,08/Sep/14 22:46,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"pyspark seems to use old-style classes

class Foo:

These are relatively ancient and should be replaced by

class Foo(object):

Many newer libraries depend on this change.",,apachespark,mrocklin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 05 15:40:31 UTC 2014,,,,,,,,,,"0|i1zq8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/14 15:40;apachespark;User 'mrocklin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using sys.stderr in pyspark results in error,SPARK-3415,12739470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,wardviaene,wardviaene,05/Sep/14 13:10,08/Sep/14 01:54,14/Jul/23 06:26,08/Sep/14 01:54,1.0.2,1.1.0,,,,,,1.2.0,,,,,,PySpark,,,,0,python,,,,,"Using sys.stderr in pyspark results in: 
  File ""/home/spark-1.1/dist/python/pyspark/cloudpickle.py"", line 660, in save_file
    from ..transport.adapter import SerializingAdapter
ValueError: Attempted relative import beyond toplevel package

Code to reproduce (copy paste the code in pyspark):

import sys
  
class TestClass(object):
    def __init__(self, out = sys.stderr):
        self.out = out
    def getOne(self):
        return 'one'
  
    
def f():
    print type(t)
    return 'ok'
    
  
t = TestClass()
a = [ 1 , 2, 3, 4, 5 ]
b = sc.parallelize(a)
b.map(lambda x: f()).first()
",,joshrosen,wardviaene,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 08 01:54:51 UTC 2014,,,,,,,,,,"0|i1zq1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/14 01:54;joshrosen;Issue resolved by pull request 2287
[https://github.com/apache/spark/pull/2287];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case insensitivity breaks when unresolved relation contains attributes with uppercase letters in their names,SPARK-3414,12739436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,lian cheng,lian cheng,05/Sep/14 09:13,20/Sep/14 23:41,14/Jul/23 06:26,20/Sep/14 23:41,1.0.2,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Paste the following snippet to {{spark-shell}} (need Hive support) to reproduce this issue:
{code}
import org.apache.spark.sql.hive.HiveContext

val hiveContext = new HiveContext(sc)
import hiveContext._

case class LogEntry(filename: String, message: String)
case class LogFile(name: String)

sc.makeRDD(Seq.empty[LogEntry]).registerTempTable(""rawLogs"")
sc.makeRDD(Seq.empty[LogFile]).registerTempTable(""logFiles"")

val srdd = sql(
  """"""
    SELECT name, message
    FROM rawLogs
    JOIN (
      SELECT name
      FROM logFiles
    ) files
    ON rawLogs.filename = files.name
  """""")

srdd.registerTempTable(""boom"")
sql(""select * from boom"")
{code}
Exception thrown:
{code}
SchemaRDD[7] at RDD at SchemaRDD.scala:103
== Query Plan ==
== Physical Plan ==
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: *, tree:
Project [*]
 LowerCaseSchema
  Subquery boom
   Project ['name,'message]
    Join Inner, Some(('rawLogs.filename = name#2))
     LowerCaseSchema
      Subquery rawlogs
       SparkLogicalPlan (ExistingRdd [filename#0,message#1], MapPartitionsRDD[1] at mapPartitions at basicOperators.scala:208)
     Subquery files
      Project [name#2]
       LowerCaseSchema
        Subquery logfiles
         SparkLogicalPlan (ExistingRdd [name#2], MapPartitionsRDD[4] at mapPartitions at basicOperators.scala:208)
{code}
Notice that {{rawLogs}} in the join operator is not lowercased.

The reason is that, during analysis phase, the {{CaseInsensitiveAttributeReferences}} batch is only executed before the {{Resolution}} batch. And when {{srdd}} is registered as temporary table {{boom}}, its original (unanalyzed) logical plan is stored into the catalog:
{code}
Join Inner, Some(('rawLogs.filename = 'files.name))
 UnresolvedRelation None, rawLogs, None
 Subquery files
  Project ['name]
   UnresolvedRelation None, logFiles, None
{code}
notice that attributes referenced in the join operator (esp. {{rawLogs}}) is not lowercased yet.

And then, when {{select * from boom}} is been analyzed, its input logical plan is:
{code}
Project [*]
 UnresolvedRelation None, boom, None
{code}
here the unresolved relation points to the unanalyzed logical plan of {{srdd}} above, which is later discovered by rule {{ResolveRelations}}, thus not touched by {{CaseInsensitiveAttributeReferences}} at all, and {{rawLogs.filename}} is thus not lowercased:
{code}
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations ===
 Project [*]                            Project [*]
! UnresolvedRelation None, boom, None    LowerCaseSchema
!                                         Subquery boom
!                                          Project ['name,'message]
!                                           Join Inner, Some(('rawLogs.filename = 'files.name))
!                                            LowerCaseSchema
!                                             Subquery rawlogs
!                                              SparkLogicalPlan (ExistingRdd [filename#0,message#1], MapPartitionsRDD[1] at mapPartitions at basicOperators.scala:208)
!                                            Subquery files
!                                             Project ['name]
!                                              LowerCaseSchema
!                                               Subquery logfiles
!                                                SparkLogicalPlan (ExistingRdd [name#2], MapPartitionsRDD[4] at mapPartitions at basicOperators.scala:208)
{code}

A reasonable fix for this could be always register analyzed logical plan to the catalog when registering temporary tables.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2063,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 20 23:41:30 UTC 2014,,,,,,,,,,"0|i1zpuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/14 20:40;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2293;;;","13/Sep/14 19:20;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2382;;;","20/Sep/14 23:41;marmbrus;Issue resolved by pull request 2382
[https://github.com/apache/spark/pull/2382];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Missing Types for Row API,SPARK-3412,12739340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,05/Sep/14 07:53,13/Oct/14 07:47,14/Jul/23 06:26,09/Oct/14 22:01,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,chenghao,joshrosen,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 22:01:34 UTC 2014,,,,,,,,,,"0|i1zprj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/14 09:40;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2284;;;","10/Sep/14 01:47;marmbrus;Instead of adding methods for each datatype the consensus on the PR seems to be that we can just add the following method to the base Row class:

{code}
def getAs[T](i: Int): T = apply(i).asInstanceOf[T]
{code};;;","25/Sep/14 02:15;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2529;;;","07/Oct/14 06:40;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2689;;;","08/Oct/14 01:09;joshrosen;Issue resolved by pull request 2689
[https://github.com/apache/spark/pull/2689];;;","08/Oct/14 16:59;joshrosen;It looks like that other PR accidentally mentioned this JIRA, so the merge script automatically closed this one.  I'm going to re-open this.  Sorry about that.;;;","09/Oct/14 22:01;marmbrus;Issue resolved by pull request 2529
[https://github.com/apache/spark/pull/2529];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit operator doesn't work with sort based shuffle,SPARK-3408,12739307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,05/Sep/14 02:18,08/Sep/14 01:43,14/Jul/23 06:26,08/Sep/14 01:43,1.1.0,,,,,,,1.1.1,1.2.0,,,,,SQL,,,,0,,,,,,,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 05 04:35:26 UTC 2014,,,,,,,,,,"0|i1zpkf:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"05/Sep/14 04:35;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2281;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python persist API does not have a default storage level,SPARK-3406,12739290,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,holden,holden,05/Sep/14 00:27,06/Sep/14 21:50,14/Jul/23 06:26,06/Sep/14 21:50,,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,PySpark's persist method on RDD's does not have a default storage level. This is different than the Scala API which defaults to in memory caching. This is minor.,,holden,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 06 21:50:16 UTC 2014,,,,,,,,,,"0|i1zpgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/14 21:50;joshrosen;Fixed by Holden in https://github.com/apache/spark/pull/2280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkSubmitSuite fails with ""spark-submit exits with code 1""",SPARK-3404,12739168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,srowen,srowen,04/Sep/14 16:50,09/Sep/14 21:15,14/Jul/23 06:26,09/Sep/14 21:15,1.0.2,1.1.0,,,,,,1.1.1,1.2.0,,,,,Build,,,,0,,,,,,"Maven-based Jenkins builds have been failing for over a month. For example:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/

It's SparkSubmitSuite that fails. For example:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/541/hadoop.version=2.0.0-mr1-cdh4.1.2,label=centos/consoleFull

{code}
SparkSubmitSuite
...
- launch simple application with spark-submit *** FAILED ***
  org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.SimpleApplicationTest, --name, testApp, --master, local, file:/tmp/1409815981504-0/testJar-1409815981505.jar) exited with code 1
  at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:837)
  at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
  at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply$mcV$sp(SparkSubmitSuite.scala:291)
  at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
  at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
  at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
  at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  ...
- spark submit includes jars passed in through --jar *** FAILED ***
  org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.JarCreationTest, --name, testApp, --master, local-cluster[2,1,512], --jars, file:/tmp/1409815984960-0/testJar-1409815985029.jar,file:/tmp/1409815985030-0/testJar-1409815985087.jar, file:/tmp/1409815984959-0/testJar-1409815984959.jar) exited with code 1
  at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:837)
  at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
  at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply$mcV$sp(SparkSubmitSuite.scala:305)
  at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
  at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
  at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
  at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  ...
{code}

SBT builds don't fail, so it is likely to be due to some difference in how the tests are run rather than a problem with test or core project.

This is related to http://issues.apache.org/jira/browse/SPARK-3330 but the cause identified in that JIRA is, at least, not the only cause. (Although, it wouldn't hurt to be doubly-sure this is not an issue by changing the Jenkins config to invoke {{mvn clean && mvn ... package}} {{mvn ... clean package}}.)

This JIRA tracks investigation into a different cause. Right now I have some further information but not a PR yet.

Part of the issue is that there is no clue in the log about why {{spark-submit}} exited with status 1. See https://github.com/apache/spark/pull/2108/files and https://issues.apache.org/jira/browse/SPARK-3193 for a change that would at least print stdout to the log too.

The SparkSubmit program exits with 1 when the main class it is supposed to run is not found (https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L322) This is for example SimpleApplicationTest (https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala#L339)

The test actually submits an empty JAR not containing this class. It relies on {{spark-submit}} finding the class within the compiled test-classes of the Spark project. However it does seem to be compiled and present even with Maven.

If modified to print stdout and stderr, and dump the actual command, I see an empty stdout, and only the command to stderr:

{code}
Spark Command: /Library/Java/JavaVirtualMachines/jdk1.8.0_20.jdk/Contents/Home/bin/java -cp null::/Users/srowen/Documents/spark/conf:/Users/srowen/Documents/spark/assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop1.0.4.jar:/Users/srowen/Documents/spark/core/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/repl/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/mllib/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/bagel/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/graphx/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/streaming/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/sql/catalyst/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/sql/core/target/scala-2.10/test-classes:/Users/srowen/Documents/spark/sql/hive/target/scala-2.10/test-classes:/Users/srowen/Documents/Cloudera/bottou/hadoop-conf/ -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.deploy.JarCreationTest --name testApp --master local-cluster[2,1,512] --jars file:/var/folders/vl/nbmbr36j0692ch5r98b5cn040000gn/T/1409845282367-0/testJar-1409845282404.jar,file:/var/folders/vl/nbmbr36j0692ch5r98b5cn040000gn/T/1409845282405-0/testJar-1409845282436.jar file:/var/folders/vl/nbmbr36j0692ch5r98b5cn040000gn/T/1409845282366-0/testJar-1409845282366.jar
{code}

Strangely, while tests fail under {{mvn test}}, they *pass* if I run just {{SparkSubmitSuite}} in Maven with {{mvn -DwildcardSuites=org.apache.spark.deploy.SparkSubmitSuite test -rf :spark-core_2.10}}

It does seem to do with the classpath that gets picked up by {{spark-submit}} varying in these different scenarios.

I verified that the test suite and Jenkins set {{SPARK_TESTING=1}}, since that affects access to {{test-classes}} on the classpath in {{spark-submit}}.

I'm still investigating but posting this to track the issue, which is kind of bothersome since it means Jenkins isn't catching (other) build problems from Maven. And to see if anyone has bright ideas.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2232,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 09 21:15:01 UTC 2014,,,,,,,,,,"0|i1zouv:",9223372036854775807,,,,,,,,,,,,,,1.1.1,,,,,,,,,,,"04/Sep/14 17:48;andrewor14;Thanks for looking into this Sean. Does this happen all the time or only once in a while? We have observed the same tests failing on our Jenkins, which runs the test through sbt. The behavior is consistent with running it through maven. If we run it through 'sbt test-only SparkSubmitSuite' then it always passes, but if we run 'sbt test' then sometimes it fails.

This has also been failing for a while for sbt. Very roughly I remember we began seeing it after https://github.com/apache/spark/pull/1777 went in. Though I have gone down that path to debug any possibilities of port collision to no avail. A related test failure is in DriverSuite, which also calls `Utils.executeAndGetOutput`. Have you seen that failing in maven?

I will keep investigating it in parallel for sbt, though I suspect the root cause is the same. Let me know if you find anything.;;;","04/Sep/14 17:50;andrewor14;I have updated the title to reflect this.;;;","04/Sep/14 17:52;srowen;It's 100% repeatable in Maven for me locally, which seems to be Jenkins' experience too. I don't see the same problem with SBT (/dev/run-tests) locally, although I can't say I run that regularly.

I could rewrite the SparkSubmitSuite to submit a JAR file that actually contains the class it's trying to invoke. Maybe that's smarter? the problem here seems to be the vagaries of what the run-time classpath is during an SBT vs Maven test. Would anyone second that?

Separately it would probably not hurt to get in that change that logs stdout / stderr from the Utils method.;;;","09/Sep/14 06:47;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2328;;;","09/Sep/14 21:15;srowen;Tests are now failing due to HiveQL test problems, but you can see they have passed SparkSubmitSuite:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/

I think this one's resolved now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong usage of tee command in python/run-tests,SPARK-3401,12739030,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,04/Sep/14 06:48,06/Sep/14 23:12,14/Jul/23 06:26,06/Sep/14 23:12,1.1.0,,,,,,,1.1.1,,,,,,PySpark,,,,0,,,,,,"In python/run-test, tee command is used with -a option to append ""unit-tests.log"" for logging but the usage is wrong.
In current implementation, the output of tee command is redirected to unit-tests.log like ""tee -a > unit-tests.log"".
tee command is not needed to redirect its output.

This issue affects invalid truncate of unit-tests.log.",,apachespark,farrellee,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 06 23:12:08 UTC 2014,,,,,,,,,,"0|i1zo0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/14 06:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2272;;;","06/Sep/14 23:12;farrellee;nice catch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX unit tests fail nondeterministically,SPARK-3400,12739029,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ankurd,ankurd,ankurd,04/Sep/14 06:43,22/Sep/14 01:28,14/Jul/23 06:26,04/Sep/14 06:50,1.1.0,,,,,,,1.1.1,,,,,,GraphX,,,,0,,,,,,"GraphX unit tests have been failing since the fix to SPARK-2823 was merged: https://github.com/apache/spark/commit/9b225ac3072de522b40b46aba6df1f1c231f13ef. Failures have appeared as Snappy parsing errors and shuffle FileNotFoundExceptions. A local test showed that these failures occurred in about 3/10 test runs.

Reverting the mentioned commit seems to solve the problem. Since this is blocking everyone else, I'm submitting a hotfix to do that, and we can diagnose the problem in more detail afterwards.",,aash,ankurd,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3630,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 22 01:28:42 UTC 2014,,,,,,,,,,"0|i1zo0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/14 06:50;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2271;;;","04/Sep/14 06:50;ankurd;Issue resolved by pull request 2271
[https://github.com/apache/spark/pull/2271];;;","16/Sep/14 17:35;aash;[~ankurd] is there another ticket to track the root cause diagnosis?  I'm seeing a similar error with a custom registrator (not GraphX's) and would like to follow along.;;;","16/Sep/14 20:05;ankurd;[~aash] There isn't another ticket yet. Would you mind creating one?;;;","22/Sep/14 01:28;aash;Filed as SPARK-3630;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test for PySpark should ignore HADOOP_CONF_DIR and YARN_CONF_DIR,SPARK-3399,12739024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,04/Sep/14 06:33,05/Sep/14 18:07,14/Jul/23 06:26,05/Sep/14 18:07,,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"Some tests for PySpark make temporary files on /tmp of local file system but if environment variable HADOOP_CONF_DIR or YARN_CONF_DIR is set in spark-env.sh, tests expects temporary files are on FileSystem configured in core-site.xml even though actual files are on local file system.

I think, we should ignore HADOOP_CONF_DIR and YARN_CONF_DIR.
If we need those variables in some tests, we should set those variables in such tests specially.",,apachespark,davies,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 05 18:07:10 UTC 2014,,,,,,,,,,"0|i1znzb:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"04/Sep/14 06:40;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2270;;;","05/Sep/14 16:46;davies;Could you give an example to show the problem?;;;","05/Sep/14 17:06;sarutak;Some test for pyspark, for instance rdd.py, use NamedTemporaryFile for creating input data.
NamedTemporaryFile creates temporary file on /tmp on local filesystem.
rdd.py is kicked by pyspark script in python/run-tests.
If we set environment variables HADOOP_CONF_DIR or YARN_CONF_DIR in spark-env.sh before testing, pyspark command load values from those variables.
After loading those value, Spark expects input data is on the filesystem configured by the environment variables.
;;;","05/Sep/14 17:26;davies;Thanks for the explain. I still can not reproduce the problem by putting  HADOOP_CONF_DIR and YARN_CONF_DIR into conf/spark-env.sh, run-tests can run successfully. Did I miss something?;;;","05/Sep/14 17:31;sarutak;Ah, did you set fs.defaultFs to like hdfs:// in core-site.xml on the path  configured by HADOOP_CONF_DIR or YARN_CONF_DIR?;;;","05/Sep/14 17:38;davies;Given fs.defaultFs as hdfs://,  saveAsTextFile() will save the files into HDFS, but other parts of code assume that the files are saved in local filesystem, then test cases failed. Do I understand correctly?

Then this patch make sense, thanks to your work!;;;","05/Sep/14 17:48;sarutak;Yes I meant like what you mentioned.;;;","05/Sep/14 18:07;joshrosen;Issue resolved by pull request 2270
[https://github.com/apache/spark/pull/2270];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] DSL uses incorrect attribute ids after a distinct(),SPARK-3395,12739000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ekhliang,ekhliang,ekhliang,04/Sep/14 02:03,22/Jan/15 07:06,14/Jul/23 06:26,10/Sep/14 06:49,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"In the following example, 

val rdd = ... // two columns: {key, value}
val derivedRDD = rdd.distinct().limit(1)

sql(""explain select * from rdd inner join derivedRDD on rdd.key = derivedRDD.key"")

The inner join executes incorrectly since the two keys end up with the same attribute id after analysis.",,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 02:10:27 UTC 2014,,,,,,,,,,"0|i1zntz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"04/Sep/14 02:10;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/2266;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TakeOrdered crashes when limit is 0,SPARK-3394,12738999,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ekhliang,ekhliang,ekhliang,04/Sep/14 01:54,09/Sep/14 02:30,14/Jul/23 06:26,08/Sep/14 00:58,,,,,,,,1.0.3,1.1.1,1.2.0,,,,Spark Core,SQL,,,0,,,,,,,,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 02:00:28 UTC 2014,,,,,,,,,,"0|i1zntr:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"04/Sep/14 01:57;ekhliang;https://github.com/apache/spark/pull/2264;;;","04/Sep/14 02:00;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/2264;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Set command always get <undefined> for key ""mapred.reduce.tasks""",SPARK-3392,12738993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,chenghao,chenghao,chenghao,04/Sep/14 01:41,05/Sep/14 02:17,14/Jul/23 06:26,05/Sep/14 02:17,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"This is a tiny fix for getting the value of ""mapred.reduce.tasks"", which make more sense for the hive user.",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 01:45:28 UTC 2014,,,,,,,,,,"0|i1znsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/14 01:45;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2261;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sqlContext.jsonRDD fails on a complex structure of JSON array and JSON object nesting,SPARK-3390,12738986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,vidaha,vidaha,04/Sep/14 00:48,11/Sep/14 22:25,14/Jul/23 06:26,11/Sep/14 22:25,1.0.2,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"I found a valid JSON string, but which Spark SQL fails to correctly parse:

Try running these lines in a spark-shell to reproduce:

{code:borderStyle=solid}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val badJson = ""{\""foo\"": [[{\""bar\"": 0}]]}""
val rdd = sc.parallelize(badJson :: Nil)
sqlContext.jsonRDD(rdd).count()
{code}

I've tried running these lines on the 1.0.2 release as well latest Spark1.1 release candidate, and I get this stack trace:

{panel}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2.0:3 failed 1 times, most recent failure: Exception failure in TID 7 on host localhost: scala.MatchError: StructType(List()) (of class org.apache.spark.sql.catalyst.types.StructType)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:333)
        org.apache.spark.sql.json.JsonRDD$$anonfun$enforceCorrectType$1.apply(JsonRDD.scala:335)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        scala.collection.AbstractTraversable.map(Traversable.scala:105)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:335)
        org.apache.spark.sql.json.JsonRDD$$anonfun$enforceCorrectType$1.apply(JsonRDD.scala:335)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        scala.collection.AbstractTraversable.map(Traversable.scala:105)
        org.apache.spark.sql.json.JsonRDD$.enforceCorrectType(JsonRDD.scala:335)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1$$anonfun$apply$12.apply(JsonRDD.scala:365)
        scala.Option.map(Option.scala:145)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:364)
        org.apache.spark.sql.json.JsonRDD$$anonfun$org$apache$spark$sql$json$JsonRDD$$asRow$1.apply(JsonRDD.scala:349)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        org.apache.spark.sql.json.JsonRDD$.org$apache$spark$sql$json$JsonRDD$$asRow(JsonRDD.scala:349)
        org.apache.spark.sql.json.JsonRDD$$anonfun$createLogicalPlan$1.apply(JsonRDD.scala:51)
        org.apache.spark.sql.json.JsonRDD$$anonfun$createLogicalPlan$1.apply(JsonRDD.scala:51)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
....
{panel}",,apachespark,guoxu1231,vidaha,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 11 20:20:24 UTC 2014,,,,,,,,,,"0|i1znqv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"05/Sep/14 00:53;yhuai;Oh, I see the problem. I am out of town this week. Will fix it next week.;;;","11/Sep/14 20:20;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/2364;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Replace the word ""SparkSQL"" with right word ""Spark SQL""",SPARK-3378,12738841,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,sarutak,sarutak,03/Sep/14 16:50,04/Sep/14 22:07,14/Jul/23 06:26,04/Sep/14 22:07,1.1.0,,,,,,,1.2.0,,,,,,Documentation,,,,0,,,,,,"In programming-guide.md, there are 2 ""SparkSQL"". We should use ""Spark SQL"" instead.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 16:55:25 UTC 2014,,,,,,,,,,"0|i1zmvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 16:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2251;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metrics can be accidentally aggregated against our intention,SPARK-3377,12738825,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,sarutak,sarutak,03/Sep/14 15:30,11/Dec/14 21:06,14/Jul/23 06:26,10/Nov/14 21:16,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"I'm using codahale base MetricsSystem of Spark with JMX or Graphite, and I saw following 2 problems.

(1) When applications which have same spark.app.name run on cluster at the same time, some metrics names are mixed. For instance, if 2+ application is running on the cluster at the same time, each application emits the same named metric like  ""SparkPi.DAGScheduler.stage.failedStages"" and Graphite cannot distinguish the metrics is for which application.

(2) When 2+ executors run on the same machine, JVM metrics of each executors are mixed. For instance, 2+ executors running on the same node can emit the same named metric ""jvm.memory"" and Graphite cannot distinguish the metrics is from which application.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2127,,,,,,SPARK-2127,SPARK-3610,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 17 18:10:31 UTC 2014,,,,,,,,,,"0|i1zmsf:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Sep/14 15:35;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2250;;;","17/Sep/14 18:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2432;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark on yarn container allocation issues,SPARK-3375,12738819,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,tgraves,03/Sep/14 15:13,05/Sep/14 14:56,14/Jul/23 06:26,05/Sep/14 14:56,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,It looks like if yarn doesn't get the containers immediately it stops asking for them and the yarn application hangs with never getting any executors.  This was introduced by https://github.com/apache/spark/pull/2169,,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3187,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 15:20:25 UTC 2014,,,,,,,,,,"0|i1zmr3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Sep/14 19:10;tgraves;It appears there is a race condition in which if you ask for the containers to soon yarn won't pick them up.  If I remove the call to     allocator.allocateResources() in the registerAM and let it wait until the report thread asks then it works.  I need to investigate more on the YARN side.
;;;","03/Sep/14 20:12;tgraves;Ok so the real issue here is that we are sending the number of containers as 0 after we send the original one of X.  on the yarn side this clears out the original request.  I would expect this to be affect 2.x also. I think the original 0.23 code kept just sending max-running and now we are sending max-pending-running.

For a ping we should just send empty asks.;;;","04/Sep/14 15:20;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2275;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLlib doesn't pass maven build / checkstyle due to multi-byte character contained in Gradient.scala,SPARK-3372,12738767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,03/Sep/14 09:45,04/Sep/14 03:48,14/Jul/23 06:26,04/Sep/14 03:47,1.1.0,,,,,,,1.2.0,,,,,,MLlib,,,,0,,,,,,"Gradient.scala includes 2 UTF-8 hyphens.
Caused by this, mvn package falied on Windows8 because it cannot pass checkstyle.",Windows8 / Java 7 / Maven,apachespark,mengxr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 03:47:34 UTC 2014,,,,,,,,,,"0|i1zmfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 09:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2248;;;","04/Sep/14 03:47;mengxr;Issue resolved by pull request 2248
[https://github.com/apache/spark/pull/2248];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL: Renaming a function expression with group by gives error,SPARK-3371,12738760,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ravi.pesala,pllee,pllee,03/Sep/14 09:24,13/Oct/14 19:11,14/Jul/23 06:26,02/Oct/14 06:54,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val rdd = sc.parallelize(List(""""""{""foo"":""bar""}""""""))
sqlContext.jsonRDD(rdd).registerAsTable(""t1"")
sqlContext.registerFunction(""len"", (s: String) => s.length)
sqlContext.sql(""select len(foo) as a, count(1) from t1 group by len(foo)"").collect()
{code}

running above code in spark-shell gives the following error

{noformat}
14/09/03 17:20:13 ERROR Executor: Exception in task 2.0 in stage 3.0 (TID 214)
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: foo#0
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:43)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$2.apply(TreeNode.scala:201)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:199)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:212)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:168)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
{noformat}

remove ""as a"" in the query causes no error",,apachespark,kayfeng,marmbrus,pllee,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 02 06:54:23 UTC 2014,,,,,,,,,,"0|i1zmdz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"23/Sep/14 07:43;ravipesala;It seems like an issue, By default SQl parser creates the aliases to the functions in grouping expressions with generated alias names. So if user gives the alias names to the functions inside projection then it does not match the generated alias name of grouping expression. 

I am working on this issue, will create the PR today.;;;","23/Sep/14 19:03;ravipesala;https://github.com/apache/spark/pull/2511;;;","23/Sep/14 19:05;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2511;;;","02/Oct/14 06:54;marmbrus;Issue resolved by pull request 2511
[https://github.com/apache/spark/pull/2511];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to save Lists to Parquet,SPARK-3365,12738722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,03/Sep/14 06:12,13/Feb/15 06:19,14/Jul/23 06:26,13/Feb/15 06:19,1.1.0,,,,,,,1.3.0,,,,,,SQL,,,,1,,,,,,"Reproduction, same works if type is Seq. (props to [~chrisgrier] for finding this)
{code}
scala> case class Test(x: List[String])
defined class Test

scala> sparkContext.parallelize(Test(List()) :: Nil).saveAsParquetFile(""bug"")
23:09:51.807 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArithmeticException: / by zero
	at parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:99)
	at parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:92)
	at parquet.hadoop.ParquetRecordWriter.<init>(ParquetRecordWriter.java:64)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:282)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:300)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}
",,apachespark,boyork,capricornius,cenyuhai,gmedasani,josephkb,lian cheng,marmbrus,tianyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 06:19:33 UTC 2015,,,,,,,,,,"0|i1zm5b:",9223372036854775807,,,,,,,,,,,,,,1.2.0,1.3.0,,,,,,,,,,"05/Feb/15 07:11;josephkb;I found this as follows:
{code}
import org.apache.spark.sql._
val data = sc.parallelize(Seq((List.empty[Double], 5)))
val sql = new SQLContext(sc)
import sql._
val df = data.toDataFrame(""l"", ""a"")
df.saveAsParquetFile(""blah"")
{code}

Here's the error:
{code}
java.lang.IllegalStateException: Cannot build an empty group
	at parquet.Preconditions.checkState(Preconditions.java:58)
	at parquet.schema.Types$GroupBuilder.build(Types.java:536)
	at parquet.schema.Types$GroupBuilder.build(Types.java:408)
	at parquet.schema.Types$Builder.named(Types.java:210)
	at parquet.format.converter.ParquetMetadataConverter.buildChildren(ParquetMetadataConverter.java:623)
	at parquet.format.converter.ParquetMetadataConverter.fromParquetSchema(ParquetMetadataConverter.java:584)
	at parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:526)
	at parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:520)
	at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:426)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$3.apply(ParquetTypes.scala:461)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$3.apply(ParquetTypes.scala:461)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.readMetaData(ParquetTypes.scala:461)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.readSchemaFromFile(ParquetTypes.scala:481)
	at org.apache.spark.sql.parquet.ParquetRelation.<init>(ParquetRelation.scala:65)
	at org.apache.spark.sql.parquet.ParquetRelation$$anon$1.<init>(ParquetRelation.scala:170)
	at org.apache.spark.sql.parquet.ParquetRelation$.createEmpty(ParquetRelation.scala:170)
	at org.apache.spark.sql.parquet.ParquetRelation$.create(ParquetRelation.scala:147)
	at org.apache.spark.sql.execution.SparkStrategies$ParquetOperations$.apply(SparkStrategies.scala:211)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:545)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:543)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:549)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:549)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:552)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:552)
	at org.apache.spark.sql.DataFrameImpl.saveAsParquetFile(DataFrameImpl.scala:305)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:32)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:36)
	at $iwC$$iwC$$iwC.<init>(<console>:38)
	at $iwC$$iwC.<init>(<console>:40)
	at $iwC.<init>(<console>:42)
	at <init>(<console>:44)
	at .<init>(<console>:48)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:854)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:899)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:811)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:654)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:662)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:667)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:994)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:942)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:942)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:942)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1039)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:403)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:77)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
;;;","12/Feb/15 08:01;tianyi;The reason is Spark generated wrong schema for type {{List}} in {{ScalaReflection.scala}}
for example:

the generated schema for type {{Seq\[String\]}} is:
{code}
{""name"":""x"",""type"":{""type"":""array"",""elementType"":""string"",""containsNull"":true},""nullable"":true,""metadata"":{}}
{code}

the generated schema for type {{List\[String\]}} is:
{code}
{""name"":""x"",""type"":{""type"":""struct"",""fields"":[]},""nullable"":true,""metadata"":{}}
{code}

The related code is [here|https://github.com/apache/spark/blob/500dc2b4b3136029457e708859fe27da93b1f9e8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L110] 


The order of resolution is:
# UserCustomType
# Option\[\_\]
# Product
# Array\[Byte\]
# Array\[\_\]
# Seq\[\_\]
# Map\[\_, _\]
# String
# Timestamp
# java.sql.Date
# BigDecimal
# java.math.BigDecimal
# Decimal
# java.lang.Integer
# ...

I think the {{List}} type should belong to {{Seq\[\_\]}} pattern, so we should move {{Product}} behind {{Seq\[\_\]}}.

May I open a PR for this issue?;;;","12/Feb/15 10:21;lian cheng;Hey [~tianyi], please open a PR for this. However, I'd suggest adding a {{List[_]}} clause before {{Product}}, rather than moving the latter. Thanks!;;;","13/Feb/15 02:52;apachespark;User 'tianyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/4581;;;","13/Feb/15 03:03;lian cheng;Actually, after rethinking about this, your solution should be OK.;;;","13/Feb/15 06:19;lian cheng;Issue resolved by pull request 4581
[https://github.com/apache/spark/pull/4581];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zip equal-length but unequally-partition,SPARK-3364,12738709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kallsu,kallsu,03/Sep/14 05:01,10/Sep/14 07:12,14/Jul/23 06:26,10/Sep/14 07:12,1.0.2,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"ZippedRDD losts some elements after zipping RDDs with equal numbers of partitions but unequal numbers of elements in their each partitions.
This can happen when a user creates RDD by sc.textFile(path,partitionNumbers) with physically unbalanced HDFS file.

{noformat}
var x = sc.parallelize(1 to 9,3)
var y = sc.parallelize(Array(1,1,1,1,1,2,2,3,3),3).keyBy(i=>i)
var z = y.partitionBy(new RangePartitioner(3,y))

expected
x.zip(y).count()
9
x.zip(y).collect()
Array[(Int, (Int, Int))] = Array((1,(1,1)), (2,(1,1)), (3,(1,1)), (4,(1,1)), (5,(1,1)), (6,(2,2)), (7,(2,2)), (8,(3,3)), (9,(3,3)))

unexpected
x.zip(z).count()
7
x.zip(z).collect()
Array[(Int, (Int, Int))] = Array((1,(1,1)), (2,(1,1)), (3,(1,1)), (4,(2,2)), (5,(2,2)), (7,(3,3)), (8,(3,3)))
{noformat}",,gq,kallsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 06:02:00 UTC 2014,,,,,,,,,,"0|i1zm2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 06:02;gq;This bug has been fixed in 1.1.0 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] Type Coercion should support every type to have null value,SPARK-3363,12738697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,03/Sep/14 04:26,10/Sep/14 17:49,14/Jul/23 06:26,10/Sep/14 17:49,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Current implementation only support numeric(ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DecimalType) and boolean",,adrian-wang,apachespark,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 05:20:29 UTC 2014,,,,,,,,,,"0|i1zlzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 05:20;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2246;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] bug in CaseWhen resolve,SPARK-3362,12738688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,03/Sep/14 03:15,10/Sep/14 17:47,14/Jul/23 06:26,10/Sep/14 17:47,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"select case x when 0 then null else y/x end from t;
will lead to an match error in toHiveString() when output, because spark would consider the output is always NullType, which is not right.",,adrian-wang,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 04:15:33 UTC 2014,,,,,,,,,,"0|i1zlxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 04:15;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2245;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`sbt/sbt unidoc` doesn't work with Java 8,SPARK-3359,12738665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,mengxr,mengxr,03/Sep/14 01:04,12/Dec/22 18:10,14/Jul/23 06:26,29/Nov/16 09:42,1.1.0,,,,,,,2.1.0,,,,,,Documentation,,,,2,,,,,,"It seems that Java 8 is stricter on JavaDoc. I got many error messages like

{code}
[error] /Users/meng/src/spark-mengxr/core/target/java/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.java:2: error: modifier private not allowed here
[error] private abstract interface SparkHadoopMapRedUtil {
[error]                  ^
{code}

This is minor because we can always use Java 6/7 to generate the doc.",,apachespark,holden,mengxr,michalsenkyr,neelesh77,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12047,,,,,SPARK-18692,,,,,,,,,,,,,,"07/Dec/16 23:16;michalsenkyr;errors.txt;https://issues.apache.org/jira/secure/attachment/12842238/errors.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 08 00:04:05 UTC 2016,,,,,,,,,,"0|i1zlsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/14 21:18;srowen;Yeah I noticed this. The problem is that {{sbt-unidoc}} uses {{genjavadoc}}, and it looks like it generates invalid Java like the snippet you quote (top-level classes can't be private). That's almost all of the extra warnings.

It seems to have been fixed in {{genjavadoc}} 0.8:
https://github.com/typesafehub/genjavadoc/blob/v0.8/plugin/src/main/scala/com/typesafe/genjavadoc/AST.scala#L107

I can see how to update the plugin in the Maven build, but not yet in the SBT build. If someone who gets SBT can explain how to set {{unidocGenjavadocVersion}} to ""0.8"" in the {{genjavadocSettings}} that is inherited in {{project/SparkBuild.scala}}, I bet that would fix it.

https://github.com/sbt/sbt-unidoc/blob/master/src/main/scala/sbtunidoc/Plugin.scala#L22;;;","21/Oct/14 11:52;prashant;I guess this will happen along with scala 2.11 upgrade. Since the old version does not work with scala 2.11. ;;;","21/Oct/14 16:19;srowen;Sure, but it can be fixed right now too. I tried to figure out how to set plugin properties in SBT and failed, although, I'm sure it's not a bit hard to someone who knows how it works.;;;","22/Oct/14 16:03;holden;I think I've got a fix for it, I'll send a PR :);;;","23/Oct/14 10:47;srowen;[~holdenk_amp]'s PR https://github.com/apache/spark/pull/2893 indeed fixes the immediate problem, thank you.

I find that the generated Java source code's generated javadoc still doesn't parse as valid javadoc in Java 8. Some are directly fixable and I'll send another PR with those touch-ups.

Some are just differences between scaladoc and javadoc that the plugin doesn't quite handle:

@example / @note tags -- unrecognized, probably delete-able without much effect, but seems like something the plugin could just ignore and pass on the text as-is in generated Java code.

@group tag -- unrecognized, and may have more value to keep?

@param tag on class arguments -- end up as @param tags on the Java class which are not valid (well they are for generic types). Should ideally end up as constructor javadoc.

[[link]] syntax for URLs and methods is not the same and the direct translation to javadoc doesn't work.

I will request some changes in the plugin to maybe pass on these. But bottom line is this still won't work to make javadoc from Scala source code in Java 8. Do we need that? maybe find a way to just javadoc the Java source.;;;","23/Oct/14 15:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2909;;;","23/Oct/14 16:01;srowen;I inquired about these with the plugin project: https://github.com/typesafehub/genjavadoc/issues/43 https://github.com/typesafehub/genjavadoc/issues/44;;;","25/Jan/15 01:21;srowen;I spent more time on this tonight, mostly looking at the {{genjavadoc}} code, and I don't think this can be made to work, not without both touching up hundreds of scaladoc comments, and overhauling {{genjavadoc}}. The rough translation it does works for javadoc 7, but not nearly for the stricter javadoc 8. It wouldn't be a matter of small fixes.

Realistically I'd suggest using javadoc 7, or, altering the doc generation to produce javadoc and scaladoc separately rather than try to get unidoc to work.

In the meantime I can submit a PR with a number of small fixes that at least resolve more javadoc 8 errors.;;;","25/Jan/15 01:34;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4193;;;","15/Jul/16 11:25;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14221;;;","17/Nov/16 13:53;gurwls223;Hi [~srowen], It seems I met this issue while trying to build Java API documentation which I eventually made it with a lower version. I just wonder if it might be better to note this in https://github.com/apache/spark/tree/master/docs to prevent the duplicated efforts I made. ;;;","19/Nov/16 17:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15939;;;","20/Nov/16 09:53;srowen;Current state: lots of PRs over time have removed most errors/warnings, but still some remain. Getting closer.;;;","24/Nov/16 04:21;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15999;;;","25/Nov/16 17:41;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16013;;;","29/Nov/16 09:42;srowen;Issue resolved by pull request 16013
[https://github.com/apache/spark/pull/16013];;;","07/Dec/16 23:16;michalsenkyr;Javadoc errors as of 2016-12-08 - see attachment;;;","07/Dec/16 23:18;srowen;How are you building?;;;","07/Dec/16 23:20;gurwls223;Yup, I noticed roughtly 6 errors intdocued in a PR related with {{<}} and {{>}}. Other errors do not break the build.;;;","07/Dec/16 23:26;michalsenkyr;Exactly how it's stated in the readme:
cd docs
jekyll build;;;","07/Dec/16 23:32;gurwls223;I believe you ran {{jekyll build}} and {{grep error}}. If you see the logs, it says actually only 6(ish) errors about < and > IIRC, which I guess was missed in the attched log file.

I was not submiting a PR because it might be too minor (including java lint break).

I can fix both in a single minor PR if it seems worth.;;;","07/Dec/16 23:39;michalsenkyr;Yes, I did do a grep on `[error]`. I am trying to build the docs now with those `>` replaced by `&amp;gt;` to see if it completes.

I believe it would be worth it to fix. As a newcomer to the Spark community, I had no idea it was Java 8 related.;;;","07/Dec/16 23:47;michalsenkyr;The build passed. I can put it into a PR if you'd like.;;;","08/Dec/16 00:04;apachespark;User 'michalsenkyr' has created a pull request for this issue:
https://github.com/apache/spark/pull/16201;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark worker fork()ing performance regression in m3.* / PVM instances,SPARK-3358,12738660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joshrosen,joshrosen,03/Sep/14 00:37,13/Dec/14 07:33,14/Jul/23 06:26,13/Dec/14 07:33,1.1.0,,,,,,,1.1.0,1.2.0,,,,,PySpark,,,,0,,,,,,"SPARK-2764 (and some followup commits) simplified PySpark's worker process structure by removing an intermediate pool of processes forked by daemon.py.  Previously, daemon.py forked a fixed-size pool of processes that shared a socket and handled worker launch requests from Java.  After my patch, this intermediate pool was removed and launch requests are handled directly in daemon.py.

Unfortunately, this seems to have increased PySpark task launch latency when running on m3* class instances in EC2.  Most of this difference can be attributed to m3 instances' more expensive fork() system calls.  I tried the following microbenchmark on m3.xlarge and r3.xlarge instances:

{code}
import os

for x in range(1000):
  if os.fork() == 0:
    exit()
{code}

On the r3.xlarge instance:

{code}
real	0m0.761s
user	0m0.008s
sys	0m0.144s
{code}

And on m3.xlarge:

{code}
real    0m1.699s
user    0m0.012s
sys     0m1.008s
{code}

I think this is due to HVM vs PVM EC2 instances using different virtualization technologies with different fork costs.

It may be the case that this performance difference only appears in certain microbenchmarks and is masked by other performance improvements in PySpark, such as improvements to large group-bys.  I'm in the process of re-running spark-perf benchmarks on m3 instances in order to confirm whether this impacts more realistic jobs.",m3.* instances on EC2,apachespark,davies,joshrosen,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3333,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 13 07:32:32 UTC 2014,,,,,,,,,,"0|i1zlrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 00:57;joshrosen;Credit where it's due: Davies pointed out the potential for this problem in the original PR: https://github.com/apache/spark/pull/1680#issuecomment-50721351

The Redis team did their own benchmarking on this (http://redislabs.com/blog/testing-fork-time-on-awsxen-infrastructure (or https://web.archive.org/web/20140529181436/http://redislabs.com/blog/testing-fork-time-on-awsxen-infrastructure, since their site may be down / slow right now)).

Based on those results, and updated numbers at http://redislabs.com/blog/benchmarking-the-new-aws-m3-instances-with-redis, it looks like HVM AMIs don't have this problem.  I'm going to try running a similar microbenchmark on m3.xlarge with the spark-ec2 HVM AMI to see if that improves performance.  If so, we should consider changing from PVM to HVM for those instance types.;;;","03/Sep/14 00:59;joshrosen;Update: that same microbenchmark that I posted above runs really fast in a m3.xlarge instance when using the HVM AMI:

{code}
real	0m0.713s
user	0m0.020s
sys	0m0.108s
{code};;;","03/Sep/14 01:31;nchammas;Josh, do you think this is related to the r3 vs. m3 performance difference you [reported in SPARK-3333|https://issues.apache.org/jira/browse/SPARK-3333?focusedCommentId=14118695&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14118695]?;;;","03/Sep/14 01:32;joshrosen;Yes, I meant to link the two issues.;;;","03/Sep/14 03:32;pwendell;There was actually a patch a couple months ago that changed the default instance type to PVM for these from HVM. I didn't backport that particular change to 1.0 to be conservative (turned out to be a good call!) in case somehow this caused a regression.

https://github.com/apache/spark/pull/1156

I'll just revert that part of the patch in master and 1.1 as well.;;;","03/Sep/14 03:35;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/2244;;;","03/Sep/14 03:42;nchammas;Nit: Isn't it [PV and not PVM|http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html]?;;;","03/Sep/14 19:13;davies;Changing from PV to HVM may cause performance regression for real workload, because PVM is slower than PV generally. [1]

In long term, we should re-use the Python worker to reduce the fork() overhead.

[1] http://palakonda.org/2012/10/30/aws-virtualization-hvm-vs-paravirtualization/;;;","03/Sep/14 20:07;joshrosen;Agreed.  Long term, I think it would be better to address the causes behind why we need to fork so many processes.;;;","04/Sep/14 01:15;davies;I had created an PR to reuse Python worker:  https://github.com/apache/spark/pull/2259 ;;;","12/Dec/14 00:27;srowen;Is this then resolved by one of https://github.com/apache/spark/pull/2244 or https://github.com/apache/spark/pull/2259 ?;;;","13/Dec/14 07:32;joshrosen;[~srowen],

Yeah, I think it's okay to mark this as fixed for now.  We can always re-open if people complain that it's causing them problems.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect partitioning after LIMIT operator,SPARK-3349,12738483,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,02/Sep/14 18:21,09/Sep/14 02:29,14/Jul/23 06:26,08/Sep/14 23:15,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Reproduced by the following example:
{code}
import org.apache.spark.sql.catalyst.plans.Inner
import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
import org.apache.spark.sql.catalyst.expressions._

val series = sql(""select distinct year from sales order by year asc limit 10"")
val results = sql(""select * from sales"")
series.registerTempTable(""series"")
results.registerTempTable(""results"")

sql(""select * from results inner join series where results.year = series.year"").count

-----------

java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
	at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:56)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:79)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:80)
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:191)
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:189)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:189)
	at org.apache.spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:298)
	at org.apache.spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:310)
	at org.apache.spark.scheduler.DAGScheduler.newStage(DAGScheduler.scala:246)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:723)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1333)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 01:55:54 UTC 2014,,,,,,,,,,"0|i1zl5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/14 01:55;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/2262;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on yarn alpha doesn't compile due to SPARK-2889,SPARK-3347,12738448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,tgraves,tgraves,02/Sep/14 15:26,02/Sep/14 18:34,14/Jul/23 06:26,02/Sep/14 18:34,1.2.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"Spark on yarn alpha doesn't compile after SPARK-2889 was merged in.

[ERROR] /home/tgraves/tgravescs-spark/yarn/alpha/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:43: not found: value SparkHadoopUtil",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2889,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 02 17:15:38 UTC 2014,,,,,,,,,,"0|i1zkyv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"02/Sep/14 17:15;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do correct parameters for ShuffleFileGroup,SPARK-3345,12738375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,02/Sep/14 10:30,10/Sep/14 07:16,14/Jul/23 06:26,10/Sep/14 07:16,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"In the method newFileGroup of class FileShuffleBlockManager, the parameters for creating new ShuffleFileGroup object is in wrong order.

Wrong: new ShuffleFileGroup(fileId, shuffleId, files)
Corrent: new ShuffleFileGroup(shuffleId, fileId, files)

Because in current codes, the parameters shuffleId and fileId are not used. So it doesn't cause problem now. However it should be corrected for readability and avoid future problem.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 02 10:35:26 UTC 2014,,,,,,,,,,"0|i1zkjb:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"02/Sep/14 10:35;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/2235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
m3 instances don't get local SSDs,SPARK-3342,12738346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,darabos,matei,matei,02/Sep/14 05:12,26/Oct/15 17:54,14/Jul/23 06:26,02/Sep/14 05:20,1.0.2,,,,,,,1.1.0,,,,,,EC2,,,,0,,,,,,"As discussed on https://github.com/apache/spark/pull/2081, these instances ignore the block device mapping on the AMI and require ephemeral drives to be added programmatically when launching them.",,matei,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 17:54:20 UTC 2015,,,,,,,,,,"0|i1zkd3:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Sep/14 05:13;matei;In particular see http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html:

""For M3 instances, you must specify instance store volumes in the block device mapping for the instance. When you launch an M3 instance, we ignore any instance store volumes specified in the block device mapping for the AMI."";;;","26/Oct/15 17:54;nchammas;FWIW, that statement on M3 instances is [no longer there|http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html], so we should be able to drop [this logic|https://github.com/apache/spark/blob/07ced43424447699e47106de9ca2fa714756bdeb/ec2/spark_ec2.py#L588-L595] in spark-ec2. cc [~shivaram];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The dataType of Sqrt expression should be DoubleType.,SPARK-3341,12738342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,02/Sep/14 03:55,03/Sep/14 03:41,14/Jul/23 06:26,03/Sep/14 03:32,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 03:41:45 UTC 2014,,,,,,,,,,"0|i1zkc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/14 04:00;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2233;;;","03/Sep/14 03:41;ueshin;Hi, this issue's Fix Version should be the same as SPARK-2813, 1.1.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for skipping json lines that fail to parse,SPARK-3339,12738328,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,marmbrus,marmbrus,01/Sep/14 22:51,09/Oct/14 21:57,14/Jul/23 06:26,09/Oct/14 21:57,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,When dealing with large datasets there is alway some data that fails to parse.  Would be nice to handle this instead of throwing an exception requiring the user to filter it out manually.,,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 21:57:46 UTC 2014,,,,,,,,,,"0|i1zk93:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"06/Oct/14 17:55;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/2680;;;","09/Oct/14 21:57;marmbrus;Issue resolved by pull request 2680
[https://github.com/apache/spark/pull/2680];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect user setting of spark.submit.pyFiles,SPARK-3338,12738327,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,01/Sep/14 22:50,30/Oct/14 22:29,14/Jul/23 06:26,30/Oct/14 22:29,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"We currently override any setting of spark.submit.pyFiles. Even though this is not documented, we should still respect this if the user explicitly sets this in his/her default properties file.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 19:48:55 UTC 2014,,,,,,,,,,"0|i1zk8v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"02/Sep/14 00:16;andrewor14;https://github.com/apache/spark/pull/2232;;;","07/Oct/14 19:48;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2232;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Spark SQL] In pyspark, cannot use broadcast variables in UDF ",SPARK-3335,12738218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,kayfeng,kayfeng,01/Sep/14 08:21,04/Sep/14 02:10,14/Jul/23 06:26,04/Sep/14 02:10,1.1.0,,,,,,,1.2.0,,,,,,PySpark,SQL,,,0,,,,,,"Running pyspark on a spark cluster with standalone master, spark sql cannot use broadcast variables in UDF. But we can use broadcast variable in spark in scala.

For example,
bar={""a"":""aa"", ""b"":""bb"", ""c"":""abc""}
foo=sc.broadcast(bar)
sqlContext.registerFunction(""MYUDF"", lambda x: foo.value[x] if x else '').
q= sqlContext.sql('SELECT MYUDF(c)  FROM foobar')
out = q.collect()

Got the following exception:
Py4JJavaError: An error occurred while calling o169.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 51.0 failed 4 times, most recent failure: Lost task 4.3 in stage 51.0 (TID 13040, ip-10-33-9-144.us-west-2.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/root/spark/python/pyspark/worker.py"", line 75, in main
    command = pickleSer._read_with_length(infile)
  File ""/root/spark/python/pyspark/serializers.py"", line 150, in _read_with_length
    return self.loads(obj)
  File ""/root/spark/python/pyspark/broadcast.py"", line 41, in _from_id
    raise Exception(""Broadcast variable '%s' not loaded!"" % bid)
Exception: (Exception(""Broadcast variable '21' not loaded!"",), <function _from_id at 0x35042a8>, (21L,))

        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)
        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:87)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

 ",,apachespark,kayfeng,pllee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 02:00:28 UTC 2014,,,,,,,,,,"0|i1zjl3:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Sep/14 02:00;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2243;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large number of partitions causes OOM,SPARK-3333,12738156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,nchammas,nchammas,31/Aug/14 21:04,18/Sep/14 05:46,14/Jul/23 06:26,18/Sep/14 05:46,1.1.0,,,,,,,,,,,,,PySpark,,,,0,,,,,,"Here’s a repro for PySpark:

{code}
a = sc.parallelize([""Nick"", ""John"", ""Bob""])
a = a.repartition(24000)
a.keyBy(lambda x: len(x)).reduceByKey(lambda x,y: x + y).take(1)
{code}

This code runs fine on 1.0.2. It returns the following result in just over a minute:

{code}
[(4, 'NickJohn')]
{code}

However, when I try this with 1.1.0-rc3 on an identically-sized cluster, it runs for a very, very long time (at least > 45 min) and then fails with {{java.lang.OutOfMemoryError: Java heap space}}.

Here is a stack trace taken from a run on 1.1.0-rc2:

{code}
>>> a = sc.parallelize([""Nick"", ""John"", ""Bob""])
>>> a = a.repartition(24000)
>>> a.keyBy(lambda x: len(x)).reduceByKey(lambda x,y: x + y).take(1)
14/08/29 21:53:40 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(0, ip-10-138-29-167.ec2.internal, 46252, 0) with no recent heart beats: 175143ms exceeds 45000ms
14/08/29 21:53:50 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(10, ip-10-138-18-106.ec2.internal, 33711, 0) with no recent heart beats: 175359ms exceeds 45000ms
14/08/29 21:54:02 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(19, ip-10-139-36-207.ec2.internal, 52208, 0) with no recent heart beats: 173061ms exceeds 45000ms
14/08/29 21:54:13 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(5, ip-10-73-142-70.ec2.internal, 56162, 0) with no recent heart beats: 176816ms exceeds 45000ms
14/08/29 21:54:22 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(7, ip-10-236-145-200.ec2.internal, 40959, 0) with no recent heart beats: 182241ms exceeds 45000ms
14/08/29 21:54:40 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(4, ip-10-139-1-195.ec2.internal, 49221, 0) with no recent heart beats: 178406ms exceeds 45000ms
14/08/29 21:54:41 ERROR Utils: Uncaught exception in thread Result resolver thread-3
java.lang.OutOfMemoryError: Java heap space
    at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:296)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.read(DefaultArraySerializers.java:35)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.read(DefaultArraySerializers.java:18)
    at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
    at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
    at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
    at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
    at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
    at org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:514)
    at org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:355)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Exception in thread ""Result resolver thread-3"" 14/08/29 21:56:26 ERROR SendingConnection: Exception while reading SendingConnection to ConnectionManagerId(ip-10-73-142-223.ec2.internal,54014)
java.nio.channels.ClosedChannelException
    at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:252)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:295)
    at org.apache.spark.network.SendingConnection.read(Connection.scala:390)
    at org.apache.spark.network.ConnectionManager$$anon$7.run(ConnectionManager.scala:199)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
java.lang.OutOfMemoryError: Java heap space
    at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:296)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.read(DefaultArraySerializers.java:35)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.read(DefaultArraySerializers.java:18)
    at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
    at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
    at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
    at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
    at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
    at org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:514)
    at org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:355)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
14/08/29 21:54:43 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(6, ip-10-137-1-139.ec2.internal, 42539, 0) with no recent heart beats: 183978ms exceeds 45000ms
14/08/29 21:57:42 ERROR ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(ip-10-138-9-33.ec2.internal,41924) not found
14/08/29 21:57:51 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(11, ip-10-236-181-116.ec2.internal, 46847, 0) with no recent heart beats: 178629ms exceeds 45000ms
14/08/29 21:57:43 ERROR ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(ip-10-137-1-139.ec2.internal,42539) not found
14/08/29 21:57:54 ERROR SendingConnection: Exception while reading SendingConnection to ConnectionManagerId(ip-10-141-136-168.ec2.internal,42960)
java.nio.channels.ClosedChannelException
    at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:252)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:295)
    at org.apache.spark.network.SendingConnection.read(Connection.scala:390)
    at org.apache.spark.network.ConnectionManager$$anon$7.run(ConnectionManager.scala:199)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
{code}","EC2; 1 master; 1 slave; {{m3.xlarge}} instances",davies,ilikerps,joshrosen,matei,nchammas,prashant,pwendell,stephen,SuYan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3358,,,,,,,,,,,,,,,,"03/Sep/14 01:16;nchammas;nick-1.0.2.driver.log.zip;https://issues.apache.org/jira/secure/attachment/12666105/nick-1.0.2.driver.log.zip","03/Sep/14 01:16;nchammas;nick-1.1.0-rc3.driver.log.zip;https://issues.apache.org/jira/secure/attachment/12666104/nick-1.1.0-rc3.driver.log.zip","02/Sep/14 21:57;joshrosen;spark-3333-logs.zip;https://issues.apache.org/jira/secure/attachment/12666000/spark-3333-logs.zip",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 05:46:01 UTC 2014,,,,,,,,,,"0|i1zj7j:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"31/Aug/14 21:12;nchammas;Note: I have not yet confirmed that 1.1.0-rc3 yields the exact same stack trace as the one provided above (which is for 1.1.0-rc2), though I expect them to be the same. I _can_ confirm that it takes a very, very long time to run, as it is running right now on rc3 and has been for about 45 minutes. Since I have to be offline for a bit, I thought I'd report this issue ASAP with the rc2 stack trace and update it later with a stack trace from rc3.;;;","31/Aug/14 21:23;pwendell;Hey [~nchammas] - I don't think anything relevant to this issue has changed between RC2 and RC3, so the RC2 trace is probably sufficient.;;;","31/Aug/14 22:18;joshrosen;Even on my laptop, I notice a huge speed difference between 1.0.2 and 1.1.0 here.  I haven't hit the OOM yet, though.;;;","31/Aug/14 22:36;matei;The slowdown might be partly due to adding external spilling in Python, but it's weird that this would crash the driver.;;;","31/Aug/14 23:25;davies;[~matei] I think this is not related to external spilling in Python, because the dataset is too small that it will not trigger spilling in Python. 

Also, this slowdown can be reproduced in Scala, such as:

sc.parallelize(1 to 10)
rdd.repartition(24000).keyBy(x=>x).reduceByKey(_+_).collect()

The second stage ( 24000 tasks) will take 105 mins (not finished yet). 

The CPU usage of JVM is 250%, memory is about 655M, it may trigger OOM somewhere. 

PS: I am running on master.
;;;","31/Aug/14 23:31;joshrosen;I agree with Davies; I think this is a more general Spark issue, perhaps related to {{repartition()}}.

I just tried testing this locally with commit eff9714e1c88e39e28317358ca9ec87677f121dc, which is the commit immediately prior to [14174abd421318e71c16edd24224fd5094bdfed4|https://github.com/apache/spark/commit/14174abd421318e71c16edd24224fd5094bdfed4], Davies' patch that adds hash-based disk spilling aggregation to PySpark, and I still saw the same slowdown there.

;;;","01/Sep/14 00:07;matei;I see, that makes sense.;;;","01/Sep/14 00:27;joshrosen;Working on doing some manual bisecting to find the patch that introduced the slowdown.  It's still slow as early as 8d338f64c4eda45d22ae33f61ef7928011cc2846.;;;","01/Sep/14 01:45;joshrosen;Looks like the issue was introduced somewhere between 273afcb and 62d4a0f.;;;","01/Sep/14 02:01;nchammas;For the record, I got the OOM in [my original report|http://apache-spark-user-list.1001560.n3.nabble.com/PySpark-large-of-partitions-causes-OOM-td13155.html] (which I duplicated here in this JIRA) in a relatively short amount of time (less than 30 min) on a 1.1.0-rc2 EC2 cluster with 20 {{m1.xlarge}} slaves. Perhaps one of y'all can replicate the OOM with that kind of environment.;;;","01/Sep/14 02:40;joshrosen;I'll resume work on this later tonight, but just wanted to note that things run fast as recently as commit 5ad5e34 and slow down as long ago as 6587ef7.;;;","01/Sep/14 04:38;joshrosen;It looks like https://github.com/apache/spark/pull/1138 may be the culprit, since this job runs quickly immediately prior to that commit.;;;","01/Sep/14 04:46;davies;@joserosen This should not be the culprit, it just show the bad things up in PySpark. Before it, the default partitions of reduceByKey() could be something much smaller, such as 4.

The root cause should be inside Scala, you should use the Scala one to test it.;;;","01/Sep/14 04:49;joshrosen;Good point.  I guess ""culprit"" was the wrong word, but that commit helps to narrow down the problem.;;;","01/Sep/14 06:06;joshrosen;[~shivaram] and I discussed this; we have a few ideas about what might be happening.

I tried running {{sc.parallelize(1 to 10).repartition(24000).keyBy(x=>x).reduceByKey(_+_).collect()}} in 1.0.2 and observed similarly slow speed to what I saw in the current 1.1.0 release candidate.  When I modified my job to use fewer reducers ({{reduceByKey(_+_, 4)}}, then the job completed quickly.  You can see similar behavior in Python by explicitly specifying a smaller number of reducers.

I think the issue here is that the overhead of sending and processing task completions is proportional to O(numReducers).  Specifically, the uncompressed size of ShuffleMapTask results is roughly O(numReducers), and there's a O(numReducers) processing cost for task completions within DAGScheduler (since mapOutputLocations is O(numReducers)).

This normally isn't a problem, but it can impact performance for jobs with large numbers of extremely small map tasks (like this job, where nearly all of the map tasks are effectively no-ops).  For larger tasks, this cost should be masked by larger overheads (such as task processing time).

I'm not sure where the OOM is coming from, but the slow performance that you're observing here is probably due to the new default number of reducers (https://github.com/apache/spark/pull/1138 exposed this in Python by changing it's defaults to match Scala Spark).

As a result, I'm not sure that this is a regression from 1.0.2, since it behaves similarly for Scala jobs.  I think we already do some compression of the task results and there are probably other improvements that we can make to lower these overheads, but I think we should postpone that to 1.2.0.;;;","01/Sep/14 22:51;pwendell;[~nchammas]. I think the default number of reducers could be the culprit. If you look at the actual job run in Spark 1.0.2, how many reducers are there in the shuffle? What happens, if you run the code in Spark 1.1.0 and specify that number of reducers that matches the number chose in 1.0.2... then is the performance the same?

If this is the case we should probably document this clearly in the release notes, since changing this default could have major implications for those relying on the default behavior.;;;","02/Sep/14 03:11;nchammas;It looks like the default number of  reducers does indeed explain most of the performance difference here. But there is still a significant difference even after controlling this variable. 

I have 2 identical EC2 clusters as described in this JIRA issue, one on 1.0.2 and one on 1.1.0-rc3. This time I ran the following PySpark code:

{code}
a = sc.parallelize([""Nick"", ""John"", ""Bob""])
a = a.repartition(24000)
a.keyBy(lambda x: len(x)).reduceByKey(lambda x,y: x + y, sc.defaultParallelism).take(1)
{code}

Here are the runtimes for 3 runs on each cluster:
||1.0.2||1.1.0-rc3||
| 95s | 343s |
| 89s | 336s |
| 95s | 334s |

So manually setting the number of reducers to a smaller number does help a lot, but there is still a 3-4x performance slowdown.

Can anyone else replicate this result?;;;","02/Sep/14 05:18;ilikerps;Anyone have a jmap on the driver during this time? Perhaps also a GC log? To be clear, a ""jmap -histo:live"" and a ""jmap -histo"" should be sufficient over a full heap dump. I wonder if something changed with the MapOutputTracker, since that state grows with O(M * R), which is extremely high in this situation.;;;","02/Sep/14 05:27;joshrosen;Still investigating.  I tried this on my laptop by running the following script through spark-submit:

{code}
from pyspark import SparkContext

sc = SparkContext(appName=""test"")
a = sc.parallelize([""Nick"", ""John"", ""Bob""])
a = a.repartition(10000)
parallelism = 4
a.keyBy(lambda x: len(x)).reduceByKey(lambda x,y: x + y, parallelism).take(1)
{code}

With spark-1.0.2-bin-hadoop1, this ran in ~46 seconds, while branch-1.1 took ~30 seconds.  Spark 1.0.2 seemed to experience one long pause that might have been due to GC, but I'll have to measure that.  Both ran to completion without crashing.  I'll see what happens if I bump up the number of partitions.;;;","02/Sep/14 18:15;pwendell;Hey [~nchammas] unfortunately we couldn't reproduce this at a smaller scale.

The reproduction here is a slightly pathological case (24,000 empty tasks) so it's not totally clear to me that this would affect any actual workload. Also by default I think Spark launches with a very small amount of driver memory, so it might be that there is GC happening due to an increase in the amount of memory required for tasks, the web UI, or other meta-data and that's why it's slower. It would be good to log GC data by setting spark.driver.extraJavaOptions in to -XX:+printGCDetails in spark-defaults.conf and see if you launch this with something more sane (a 10GB heap, e.g.) does it still show a difference.

I'll make a call soon about whether to let this block the release. If we can't narrow it down in time, it's not worth holding a bunch of other features for this... we can fix issues in a patch release shortly if we find any.;;;","02/Sep/14 19:35;pwendell;Hey [~nchammas] one other thing. If you go back to the original job that was causing this issue, can you see any regression? This benchmark is pathological enough that it would be good to see if there is actually an issue in Spark.;;;","02/Sep/14 20:36;joshrosen;I was unable to reproduce this on a cluster with two *r3*.xlarge instances (trying on *m3* shortly) using the following script:

{code}
from pyspark import SparkContext

sc = SparkContext(appName=""test"")
a = sc.parallelize([""Nick"", ""John"", ""Bob""])
a = a.repartition(24000)
parallelism = sc.defaultParallelism
a.keyBy(lambda x: len(x)).reduceByKey(lambda x,y: x + y, parallelism).take(1)
{code}

In both 1.0.2 and 1.1.0-rc3, the job ran in ~1 minute 10 seconds (with essentially no timing difference between the two versions).  I'm going to try this again on m3 instances and double-check both configurations.;;;","02/Sep/14 21:00;nchammas;{quote}
If we can't narrow it down in time, it's not worth holding a bunch of other features for this... we can fix issues in a patch release shortly if we find any.
{quote}

Sounds fine to me. I'm trying to retrace how I originally stumbled on this but am currently tied up with other stuff. I'll report back when I can. It looks like the issue is limited to some specific setup which Josh seems to be slowly honing in on, which is great.

Just FYI Josh, if it matters, I've been running the little benchmarks reported here via in the shell, not using spark-submit.;;;","02/Sep/14 21:21;joshrosen;Tried this on a m3.xlarge cluster (1 master, 1 worker) using the same script from my previous comment.  On this cluster, 1.1.0 was noticeably slower.

In v1.0.2-rc1, it took ~220 seconds.
In v1.1.0-rc3, it took ~360 seconds.

Going to run again and keep the logs to see if I can spot where the extra time is coming from.;;;","02/Sep/14 21:57;joshrosen;I've attached some logs from my most recent run, showing a ~100 second difference between the two releases.;;;","02/Sep/14 22:09;joshrosen;360/220 is approximately 1.6.  Using Splunk, I computed the average task execution time from these logs.  It looks like 1.1.0 took around 60ms per task, while 1.0.2 only took 37ms, and 60/37 is also about 1.6 (the standard deviations appear to be in the same ballpark, too)..  It seems that the tasks themselves are running slightly slower on 1.1.0, at least according to these logs.;;;","03/Sep/14 01:13;nchammas;Just to double check my results, I re-ran my earlier test with the same setup as in Josh's most recent report. 

Setup:
* 2 clusters, each with 1 master and 1 slave
* all instances are {{m3.xlarge}}
* one cluster is on 1.0.2 and the other is on 1.1.0-rc3
* AWS region is {{us-east-1c}}

Here are my results for running the following script in the PySpark shell, restarting the shell between runs.

Script:
{code}
a = sc.parallelize([""Nick"", ""John"", ""Bob""])
a = a.repartition(24000)
a.keyBy(lambda x: len(x)).reduceByKey(lambda x,y: x + y, sc.defaultParallelism).take(1)
{code}

Results (3 runs on each cluster):
|| 1.0.2 || 1.1.0-rc3 ||
| 62s | 356s |
| 63s | 358s |
| 63s | 365s |

The speed slowdown here is roughly 356/62 ~= 5.7.

So the results I'm seeing here are similar to the ones I got in my last test. I don't know why Josh's test didn't show as drastic a performance difference. I'll attach the driver logs for the record. I'll also try on some {{r3.xlarge}} clusters, like Josh did, for comparison.

As has been discussed, I agree this doesn't look like a blocking issue for the 1.1.0 release, but I do think there is a real issue here (though perhaps not an important one). I have not been testing on the 20-node cluster that originally yielded the OOM reported here, but will do so later this week if I have time.;;;","03/Sep/14 01:16;nchammas;Here are the driver logs from a test run on the {{m3.xlarge}} clusters.;;;","03/Sep/14 01:52;nchammas;So I've repeated the tests with the exact same setup described in my previous comment, except this time the instances are {{r3.xlarge}}. 

Here are my results:

|| 1.0.2 || 1.1.0-rc3 ||
| 74s | 66s |
| 69s | 67s |
| 72s | 67s |

So this agrees with Josh's earlier results and suggests this may be something specific to {{m3}} PV instances. Josh is exploring that in more detail in [SPARK-3358].

In conclusion, once you control the number of reducers, there should be no performance degradation (with the m3/PV caveats Josh is investigating).

Perhaps for this JIRA issue all we need is some documentation of the change in the default number of reducers in PySpark from 1.0 to 1.1 and we're good to go?;;;","18/Sep/14 05:46;pwendell;This was documented in the release upgrade notes, so I think we're all set.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tagging is not atomic with launching instances on EC2,SPARK-3332,12738155,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,douglaz,douglaz,31/Aug/14 20:52,10/Mar/15 03:15,14/Jul/23 06:26,26/Nov/14 00:08,,,,,,,,1.1.1,1.2.0,,,,,EC2,,,,0,,,,,,"The implementation for SPARK-2333 changed the machine membership mechanism from security groups to tags.

This is a fundamentally flawed strategy as there aren't guarantees at all the machines will have a tag (even with a retry mechanism).

For instance, if the script is killed after launching the instances but before setting the tags the machines will be ""invisible"" to a destroy command, leaving a unmanageable cluster behind.

The initial proposal is to go back to the previous behavior for all cases but when the new flag (--security-group-prefix) is used.

Also it's worthwhile to mention that SPARK-3180 introduced the --additional-security-group flag which is a reasonable solution to SPARK-2333 (but isn't a full replacement to all use cases of --security-group-prefix).",,apachespark,douglaz,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4509,,SPARK-4983,SPARK-2333,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 01 00:47:32 UTC 2014,,,,,,,,,,"0|i1zj7b:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"31/Aug/14 21:00;apachespark;User 'douglaz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2223;;;","31/Aug/14 21:50;pwendell;I changed the title slightly - I think the underlying problem here is that tagging is not atomic with launching instances. Removing the use of tagging entirely is one potential solution for this. Another is that we just print better errors if tagging does not succeed and explain there might be orphaned instances.

The reason why SPARK-2333 was added is that the current approach can lead to too many security groups, so we can't just revert this without any cost.

In the mean time I'd like to revert SPARK-2333 in branch-1.1 so we can defer the design decision to the 1.2 release timeframe. Thanks [~douglaz] for highlighting this issue.;;;","31/Aug/14 22:00;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2225;;;","01/Sep/14 00:47;douglaz;[~pwendell], yes this is a good reword and I agree to revert it for now. You mentioned the two potential solutions but I think a good compromise is the one implemented by the PR which keeps the flag allowing reuse of the same security group but also allowing to match the machines by the security group in the other cases.

Perhaps more messages could be added when the flag has been used and the tagging failed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEP8 tests fail because they check unzipped py4j code,SPARK-3331,12738145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,srowen,srowen,31/Aug/14 17:16,02/Sep/14 17:30,14/Jul/23 06:26,02/Sep/14 17:30,1.0.2,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"PEP8 tests run on files under ""./python"", but unzipped py4j code is found at ""./python/build/py4j"". Py4J code fails style checks and can fail ./dev/run-tests if this code is present locally.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 02 17:30:45 UTC 2014,,,,,,,,,,"0|i1zj5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/14 17:30;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2222;;;","02/Sep/14 17:30;joshrosen;Issue resolved by pull request 2222
[https://github.com/apache/spark/pull/2222];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveQuerySuite SET tests depend on map orderings,SPARK-3329,12738143,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,willbenton,willbenton,willbenton,31/Aug/14 17:08,09/Sep/14 02:30,14/Jul/23 06:26,09/Sep/14 02:30,1.0.2,1.1.0,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,The SET tests in HiveQuerySuite that return multiple values depend on the ordering in which map pairs are returned from Hive and can fail spuriously if this changes due to environment or library changes.,,apachespark,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 31 17:10:25 UTC 2014,,,,,,,,,,"0|i1zj4v:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"31/Aug/14 17:10;apachespark;User 'willb' has created a pull request for this issue:
https://github.com/apache/spark/pull/2220;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./make-distribution.sh --with-tachyon build is broken,SPARK-3328,12738053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prudhvije,elijah.epifanov@gmail.com,elijah.epifanov@gmail.com,31/Aug/14 10:21,03/Sep/14 00:38,14/Jul/23 06:26,03/Sep/14 00:37,1.1.0,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"cp: tachyon-0.5.0/target/tachyon-0.5.0-jar-with-dependencies.jar: No such file or directory
",,apachespark,elijah.epifanov@gmail.com,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 00:37:13 UTC 2014,,,,,,,,,,"0|i1zj1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/14 12:10;apachespark;User 'prudhvije' has created a pull request for this issue:
https://github.com/apache/spark/pull/2228;;;","03/Sep/14 00:37;pwendell;Issue resolved by pull request 2228
[https://github.com/apache/spark/pull/2228];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn website's Tracking UI links to the Standby RM,SPARK-3323,12737980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,30/Aug/14 06:23,19/May/15 06:51,14/Jul/23 06:26,19/May/15 06:51,1.1.0,,,,,,,,,,,,,YARN,,,,0,,,,,,"Running a big application, sometimes will occur this situation:
When clicking the Tracking UI of the running application, it links to the Standby RM.
With Info as fellow:
This is standby RM.Redirecting to the current active RM: ""some address""
But actually the address of this website is the same with the ""some address""
",,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3742,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-08-30 06:23:43.0,,,,,,,,,,"0|i1zilr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager logs an error when the application ends,SPARK-3322,12737974,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,30/Aug/14 05:43,31/Oct/14 23:10,14/Jul/23 06:26,31/Oct/14 23:10,1.1.0,,,,,,,,,,,,,Spark Core,,,,1,,,,,,"Athough it does not influence the result, it always would log an error from ConnectionManager.
Sometimes only log ""ConnectionManagerId(vm2,40992) not found"" and sometimes it also will log ""CancelledKeyException""
The log Info as fellow:
14/08/29 16:54:53 ERROR ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(vm2,40992) not found
14/08/29 16:54:53 INFO ConnectionManager: key already cancelled ? sun.nio.ch.SelectionKeyImpl@457245f9
java.nio.channels.CancelledKeyException
        at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:386)
        at org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:139)",,prashant,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 23:10:02 UTC 2014,,,,,,,,,,"0|i1zikf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/14 07:27;prashant;I remember seeing this discussion somewhere else as well, probably on a pull request. It seems the PR is not linked to this issue somehow. Would be great if you could do it. ;;;","01/Sep/14 10:52;prashant;So I felt, SPARK-3171 is related. I am not sure I fully understand the intentions, probably giving a bit more information on what this issue is about will be helpful. However if you think, this is almost or same issue as the SPARK-3171. Feel free to close it.;;;","31/Oct/14 23:10;scwf;yes, to close this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batched in-memory column buffer building doesn't work for SchemaRDDs with empty partitions,SPARK-3320,12737950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,30/Aug/14 00:20,31/Aug/14 22:57,14/Jul/23 06:26,31/Aug/14 22:57,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Empty partition iterator is not properly handled in [#1880|https://github.com/apache/spark/pull/1880/files#diff-b47dac3d98014877d5879f5cf37ab0d1R115], and throws exception when accessing empty partition of the target SchemaRDD",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 30 00:25:46 UTC 2014,,,,,,,,,,"0|i1zif3:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Aug/14 00:25;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resolve spark.jars, spark.files, and spark.submit.pyFiles etc.",SPARK-3319,12737948,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,30/Aug/14 00:07,30/Oct/14 22:29,14/Jul/23 06:26,30/Oct/14 22:29,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"We already do this for --jars, --files, and --py-files etc. For consistency, we should do the same for the corresponding spark configs as well in case the user sets them in spark-defaults.conf instead.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3620,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 01 23:10:28 UTC 2014,,,,,,,,,,"0|i1zien:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"01/Sep/14 23:10;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2232;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkFiles.get doesn't work in local mode,SPARK-3311,12737879,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,holden,holden,29/Aug/14 19:01,29/Aug/14 22:18,14/Jul/23 06:26,29/Aug/14 22:18,,,,,,,,,,,,,,Spark Core,,,,0,,,,,,In local mode SparkFiles.get doesn't work. This leads to having to special case code for local mode which is not great.,,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 22:18:30 UTC 2014,,,,,,,,,,"0|i1zhzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/14 21:48;holden;Note: this works if you add a file under ./ but not otherwise.;;;","29/Aug/14 22:18;holden;turns out the comment was just out of date, I'll fix the comment in another PR so documentation matches the functionality.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to read JSON Arrays as tables,SPARK-3308,12737862,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,marmbrus,marmbrus,29/Aug/14 17:49,12/Dec/22 18:10,14/Jul/23 06:26,16/Sep/14 18:40,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,Right now we can only read json where each object is on its own line.  It would be nice to be able to read top level json arrays where each element maps to a row.,,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13764,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 17 01:12:17 UTC 2016,,,,,,,,,,"0|i1zhw7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Sep/14 19:55;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/2400;;;","16/Mar/16 04:47;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/11752;;;","17/Mar/16 01:12;gurwls223;I removed the PR link, https://github.com/apache/spark/pull/11752 because a new JIRA was linked for that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix doc string of SparkContext.broadcast(),SPARK-3307,12737858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,29/Aug/14 17:41,29/Aug/14 18:48,14/Jul/23 06:26,29/Aug/14 18:48,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"There is no parameter called `keep`, remove the doc.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 18:48:14 UTC 2014,,,,,,,,,,"0|i1zhvb:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"29/Aug/14 17:50;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2202;;;","29/Aug/14 18:48;joshrosen;Issue resolved by pull request 2202
[https://github.com/apache/spark/pull/2202];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationMaster's Finish status is wrong when uncaught exception is thrown from ReporterThread,SPARK-3304,12737815,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,29/Aug/14 15:13,23/Sep/14 16:42,14/Jul/23 06:26,23/Sep/14 16:42,1.1.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"It's rare case but even though uncaught exception is thrown from Reporter thread, allocating containers, finish status is marked as SUCCEEDED.

In addition, in YARN Cluster mode, if we don't notice Reporter thread is dead, we waits long time until timeout.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 16:21:43 UTC 2014,,,,,,,,,,"0|i1zhlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/14 15:16;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2198;;;","03/Sep/14 16:21;sarutak;Actually, I didn't see exception causing Reporter thread dead but unexpected exception like OOM or any other exceptions due to communication error with RM during resource negotiation caused by temporary traffic jam can be thrown, I think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContextSchedulerCreationSuite test failed when mesos native lib is set,SPARK-3303,12737812,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,scwf,scwf,scwf,29/Aug/14 15:04,04/Sep/14 01:40,14/Jul/23 06:26,04/Sep/14 01:40,1.0.2,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"run test with the master branch with this command 
sbt/sbt -Phive ""test-only org.apache.spark.SparkContextSchedulerCreationSuite""

get this error:
[info] SparkContextSchedulerCreationSuite:
[info] - bad-master
[info] - local
[info] - local-*
[info] - local-n
[info] - local--n-failures
[info] - local-n-failures
[info] - bad-local-n
[info] - bad-local-n-failures
[info] - local-default-parallelism
[info] - simr
[info] - local-cluster
[info] - yarn-cluster
[info] - yarn-standalone
[info] - yarn-client
[info] - mesos fine-grained
[info] - mesos coarse-grained ** FAILED ***
[info] Executor Spark home `spark.mesos.executor.home` is not set!",,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 15:16:11 UTC 2014,,,,,,,,,,"0|i1zhl3:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"29/Aug/14 15:16;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2199;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The wrong version information in SparkContext,SPARK-3302,12737759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,29/Aug/14 09:42,04/Sep/14 01:43,14/Jul/23 06:26,04/Sep/14 01:43,1.1.0,1.2.0,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,,,apachespark,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3301,SPARK-3273,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 01:43:11 UTC 2014,,,,,,,,,,"0|i1zh9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/14 09:45;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2197;;;","29/Aug/14 11:07;srowen;This duplicates at least one of https://issues.apache.org/jira/browse/SPARK-2697 or https://issues.apache.org/jira/browse/SPARK-3273;;;","04/Sep/14 01:43;gq;Issue resolved by pull request 2255
[https://github.com/apache/spark/pull/2255];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The spark version in the welcome message of pyspark is not correct,SPARK-3301,12737755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,29/Aug/14 09:33,06/Sep/14 22:09,14/Jul/23 06:26,06/Sep/14 22:09,,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,,,apachespark,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3273,,SPARK-3302,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 14:52:00 UTC 2014,,,,,,,,,,"0|i1zh8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/14 09:40;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2196;;;","29/Aug/14 11:07;srowen;This duplicates at least one of https://issues.apache.org/jira/browse/SPARK-2697 or https://issues.apache.org/jira/browse/SPARK-3273;;;","29/Aug/14 14:52;gq;I do not think this is a duplicate.But we may be able to re-implement it. reads the version information from the same place.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Spark SQL][UI] SchemaRDD toString with many columns messes up Storage tab display,SPARK-3297,12737742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,falaki,velvia,velvia,29/Aug/14 07:32,07/Oct/14 18:48,14/Jul/23 06:26,07/Oct/14 18:48,1.0.2,,,,,,,1.1.1,1.2.0,,,,,SQL,Web UI,,,0,newbie,,,,,"When a SchemaRDD with many columns (for example, 57 columns in this example) is cached using sqlContext.cacheTable, the Storage tab of the driver Web UI display gets messed up, because the long string of the SchemaRDD causes the first column to be much much wider than the others, and in fact much wider than the width of the browser.  It would be nice to have the first column be restricted to, say, 50% of the width of the browser window, with some minimum.

For example this is the SchemaRDD text for my table:

        RDD Storage Info for ExistingRdd [ActionGeo_ADM1Code#198,ActionGeo_CountryCode#199,ActionGeo_FeatureID#200,ActionGeo_FullName#201,ActionGeo_Lat#202,ActionGeo_Long#203,ActionGeo_Type#204,Actor1Code#205,Actor1CountryCode#206,Actor1EthnicCode#207,Actor1Geo_ADM1Code#208,Actor1Geo_CountryCode#209,Actor1Geo_FeatureID#210,Actor1Geo_FullName#211,Actor1Geo_Lat#212,Actor1Geo_Long#213,Actor1Geo_Type#214,Actor1KnownGroupCode#215,Actor1Name#216,Actor1Religion1Code#217,Actor1Religion2Code#218,Actor1Type1Code#219,Actor1Type2Code#220,Actor1Type3Code#221,Actor2Code#222,Actor2CountryCode#223,Actor2EthnicCode#224,Actor2Geo_ADM1Code#225,Actor2Geo_CountryCode#226,Actor2Geo_FeatureID#227,Actor2Geo_FullName#228,Actor2Geo_Lat#229,Actor2Geo_Long#230,Actor2Geo_Type#231,Actor2KnownGroupCode#232,Actor2Name#233,Actor2Religion1Code#234,Actor2Religion2Code#235,Actor2Type1Code#236,Actor2Type2Code#237,Actor2Type3Code#238,AvgTone#239,DATEADDED#240,Day#241,EventBaseCode#242,EventCode#243,EventId#244,EventRootCode#245,FractionDate#246,GoldsteinScale#247,IsRootEvent#248,MonthYear#249,NumArticles#250,NumMentions#251,NumSources#252,QuadClass#253,Year#254], MappedRDD[200]

I would personally love to fix the toString method to not necessarily print every column, but to cut it off after a while.  This would aid the printout in the Spark Shell as well.  For example:

[ActionGeo_ADM1Code#198,ActionGeo_CountryCode#199,ActionGeo_FeatureID#200,ActionGeo_FullName#201,ActionGeo_Lat#202 .... and 52 more columns]",,joshrosen,velvia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3827,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 18:48:03 UTC 2014,,,,,,,,,,"0|i1zh67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/14 18:48;joshrosen;Fixed by https://github.com/apache/spark/pull/2687;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-example should be run-example in head notation of DenseKMeans and SparseNaiveBayes,SPARK-3296,12737741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,29/Aug/14 07:12,30/Aug/14 00:38,14/Jul/23 06:26,30/Aug/14 00:37,1.0.2,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,,,apachespark,mengxr,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 30 00:37:51 UTC 2014,,,,,,,,,,"0|i1zh5z:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"29/Aug/14 07:15;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2193;;;","30/Aug/14 00:37;mengxr;Issue resolved by pull request 2193
[https://github.com/apache/spark/pull/2193];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TestcaseName in createQueryTest should not contain "":""",SPARK-3291,12737637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chouqin,chouqin,29/Aug/14 02:37,29/Aug/14 22:38,14/Jul/23 06:26,29/Aug/14 22:38,,,,,,,,1.1.1,,,,,,SQL,,,,0,,,,,,""":"" is not allowed to appear in a file name of Windows system. If file name contains "":"", this file can't be checked out in a Windows system and developers using Windows must be careful to not commit the deletion of such files, Which is very inconvenient. ",,apachespark,chouqin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 03:36:07 UTC 2014,,,,,,,,,,"0|i1zgzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/14 03:36;apachespark;User 'chouqin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No unpersist callls in SVDPlusPlus,SPARK-3290,12737619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,Darlwen,Darlwen,29/Aug/14 00:31,14/Feb/15 04:15,14/Jul/23 06:26,14/Feb/15 04:15,1.0.2,,,,,,,1.3.0,,,,,,GraphX,,,,0,,,,,,"The implementation of SVDPlusPlus will cache graph produced by each iteration and do not unpersist them, so as iteration goes on, more and more useless graph will be cached and out of memory happens.",,ankurd,apachespark,Darlwen,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 14 04:15:55 UTC 2015,,,,,,,,,,"0|i1zgvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/15 23:16;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4234;;;","14/Feb/15 04:15;ankurd;Issue resolved by pull request 4234
https://github.com/apache/spark/pull/4234;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid job failures due to rescheduling of failing tasks on buggy machines,SPARK-3289,12737581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joshrosen,joshrosen,28/Aug/14 22:21,11/May/19 22:10,14/Jul/23 06:26,11/May/19 22:10,,,,,,,,,,,,,,Spark Core,,,,3,,,,,,"Some users have reported issues where a task fails due to an environment / configuration issue on some machine, then the task is reattempted _on that same buggy machine_ until the entire job failures because that single task has failed too many times.

To guard against this, maybe we should add some randomization in how we reschedule failed tasks.",,besil,joshrosen,markhamstra,tewfikzeghmi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2425,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 11 22:10:39 UTC 2019,,,,,,,,,,"0|i1zgn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 23:51;markhamstra;https://github.com/apache/spark/pull/1360;;;","09/Jul/15 10:12;besil;We are experimenting this issue running Spark 1.4 on Mesos cluster.
When a Mesos slave fails and there is a Spark executor running tasks in it, tasks are rescheduled on the same executor, which is no more reachable.

Mesos LOST tasks are not rescheduled by the Spark framework (see https://mail-archives.apache.org/mod_mbox/mesos-user/201310.mbox/%3CCAAkWvAxPRRNRCdLAZcybnmk1_9eLyhEOdAf8urf8ssrLBAcx8g@mail.gmail.com%3E).

;;;","11/May/19 22:10;joshrosen;As part of a cleanup of old tickets filed by me, I'm resolving this as ""Fixed"" because I believe that our current executor blacklisting tools provide sufficient options to support this use-case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot view ApplicationMaster UI when Yarn’s url scheme is https,SPARK-3286,12737552,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,benoyantony,benoyantony,28/Aug/14 20:01,10/Sep/14 17:02,14/Jul/23 06:26,10/Sep/14 17:02,1.0.2,,,,,,,1.2.0,,,,,,Web UI,YARN,,,0,,,,,,"The spark Application Master starts its web UI at http://<host-name>:port.
When Spark ApplicationMaster registers its URL with Resource Manager , the URL does not contain URI scheme.
If the URL scheme is absent, Resource Manager’s web app proxy will use the HTTP Policy of the Resource Manager.(YARN-1553)
If the HTTP Policy of the Resource Manager is https, then web app proxy  will try to access https://<host-name>:port.
This will result in error.
",,apachespark,benoyantony,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 22:03;benoyantony;SPARK-3286-branch-1-0.patch;https://issues.apache.org/jira/secure/attachment/12665101/SPARK-3286-branch-1-0.patch","28/Aug/14 21:57;benoyantony;SPARK-3286.patch;https://issues.apache.org/jira/secure/attachment/12665098/SPARK-3286.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 17:30:29 UTC 2014,,,,,,,,,,"0|i1zggn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 20:03;benoyantony;In the attached patch, the Application Master URL registered by the spark Application Master, will contain the scheme.(http);;;","28/Aug/14 21:57;benoyantony;Attaching the patch for the master;;;","28/Aug/14 23:51;benoyantony;I'll submit a git pull request.;;;","29/Aug/14 20:45;apachespark;User 'benoyantony' has created a pull request for this issue:
https://github.com/apache/spark/pull/2206;;;","04/Sep/14 17:30;apachespark;User 'benoyantony' has created a pull request for this issue:
https://github.com/apache/spark/pull/2276;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Receivers sometimes do not get spread out to multiple nodes,SPARK-3283,12737343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,28/Aug/14 09:14,12/Sep/15 01:04,14/Jul/23 06:26,12/Sep/15 01:04,,,,,,,,1.5.0,,,,,,DStreams,,,,1,,,,,,"The probable reason this happens is because the JobGenerator and JobScheduler start generating jobs with tasks. When the ReceiverTracker submits the task containing receivers, the tasks get assigned according to empty slots, which may be instantaneously available on one node, instead of all the nodes. 

The original behavior was that the jobs started only after the receivers are started, thus ensuring that all the slots are free and the receivers are spread evenly across all the nodes. ",,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8882,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-08-28 09:14:25.0,,,,,,,,,,"0|i1zflz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove useless field variable in ApplicationMaster,SPARK-3279,12737328,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,28/Aug/14 07:42,29/Aug/14 05:53,14/Jul/23 06:26,29/Aug/14 05:53,1.1.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"ApplicationMaster no longer use ""ALLOCATE_HEARTBEAT_INTERVAL"".
Let's remove it.",,apachespark,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 05:53:20 UTC 2014,,,,,,,,,,"0|i1zfin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 07:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2177;;;","29/Aug/14 05:53;joshrosen;Issue resolved by pull request 2177
[https://github.com/apache/spark/pull/2177];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LZ4 compression cause the the ExternalSort exception,SPARK-3277,12737317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,carlmartin,carlmartin,28/Aug/14 06:27,05/Nov/14 10:45,14/Jul/23 06:26,29/Aug/14 00:06,1.0.2,1.1.0,,,,,,1.1.0,,,,,,Shuffle,Spark Core,,,0,,,,,,"I tested the LZ4 compression,and it come up with such problem.(with wordcount)
Also I tested the snappy and LZF,and they were OK.
At last I set the  ""spark.shuffle.spill"" as false to avoid such exeception, but once open this ""switch"", this error would come.
It seems that if num of the[ words is few, wordcount will go through,but if it is a complex text ,this problem will show
Exeception Info as follow:
{code}
java.lang.AssertionError: assertion failed
        at scala.Predef$.assert(Predef.scala:165)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.<init>(ExternalAppendOnlyMap.scala:416)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:235)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:150)
        at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
        at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:55)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}
",,andrewor14,apachespark,carlmartin,joshrosen,matei,mridulm80,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 17:36;mridulm80;test_lz4_bug.patch;https://issues.apache.org/jira/secure/attachment/12665024/test_lz4_bug.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 00:06:29 UTC 2014,,,,,,,,,,"0|i1zfg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 13:26;mridulm80;This looks like unrelated changes pushed to BlockObjectWriter as part of introduction of ShuffleWriteMetrics.
I had introducing checks and also documented that we must not infer size based on position of stream after flush - since close can write data to the streams (and one flush can result in more data getting generated which need not be flushed to streams).

Apparently this logic was modified subsequently causing this bug.
Solution would be to revert changes to update shuffleBytesWritten before close of stream.
It must be done after close and based on file.length;;;","28/Aug/14 14:24;carlmartin;Sorry,I can not understand it clearly since I'm not familiar with the code of this class.
Can you point the line number of the code where it goes wrong or make a pr to fix this problem ;;;","28/Aug/14 17:34;mridulm80;[~matei] Attaching a patch which reproduces the bug consistently.
I suspect the issue is more serious than what I detailed above - spill to disk seems completely broken if I understood the assertion message correctly.
Unfortunately, this is based on a few minutes of free time I could grab - so a more principled debugging session is definitely warranted !

;;;","28/Aug/14 17:36;mridulm80;Attached patch is against master, though I noticed similar changes in 1.1 also : but not yet verified.;;;","28/Aug/14 17:39;mridulm80;[~hzw] did you notice this against 1.0.2 ?
I did not think the changes for consolidated shuffle were backported to that branch, [~mateiz] can comment more though.;;;","28/Aug/14 17:43;andrewor14;This assert was added after 1.0.2. I'm assuming what you mean is that you're running a commit on master after 1.0.2 is released.;;;","28/Aug/14 19:58;matei;Thanks Mridul -- I think Andrew and Patrick have figured this out.;;;","28/Aug/14 22:24;mridulm80;Sounds great, thx !
I suspect it is because for lzo we configure it to write block on flush (partial if insufficient data to fill block); but for lz4, either such config does not exist or we dont use that.
Resulting in flush becoming noop in case the data in current block is insufficientto cause a compressed block to be created - while close will force patial block to be written out.

Which is why the asserion lists all sizes as 0
;;;","28/Aug/14 23:00;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2187;;;","29/Aug/14 00:06;pwendell;Fixed by https://github.com/apache/spark/pull/2187

Thanks to everyone who helped isolate and debug this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Socket receiver can not recover when the socket server restarted ,SPARK-3275,12737302,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jhu,jhu,28/Aug/14 04:35,06/Mar/15 05:44,14/Jul/23 06:26,06/Mar/15 05:44,1.0.2,,,,,,,1.1.0,,,,,,DStreams,,,,0,failover,,,,,"To reproduce this issue:
1. create a application with a socket dstream
2. start the socket server and start the application
3. restart the socket server
4. the socket dstream will fail to reconnect (it will close the connection after a successful connect)

The main issue should be the status in SocketReceiver and ReceiverSupervisor is incorrect after the reconnect:
In SocketReceiver ::receive() the while loop will never be entered after reconnect since the isStopped will returns true:
     val iterator = bytesToObjects(socket.getInputStream())
      while(!isStopped && iterator.hasNext) {
        store(iterator.next)
      }
      logInfo(""Stopped receiving"")
      restart(""Retrying connecting to "" + host + "":"" + port)

That is caused by the status flag ""receiverState"" in ReceiverSupervisor will be set to Stopped when the connection losses, but it is reset after the call of Receiver start method:

def startReceiver(): Unit = synchronized {
    try {
      logInfo(""Starting receiver"")
      receiver.onStart()
      logInfo(""Called receiver onStart"")
      onReceiverStart()
      receiverState = Started
    } catch {
      case t: Throwable =>
        stop(""Error starting receiver "" + streamId, Some(t))
    }
  }",,jhu,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 05:44:53 UTC 2015,,,,,,,,,,"0|i1zfcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/14 09:01;prashant;AFAIK, from my conversation with TD once. Spark Streaming has a listener interface which lets you listen to events of receiver dying. So you can then manually restart/start another receiver somewhere. ;;;","06/Mar/15 05:44;jhu;Checked in 1.1.0, do not find similar issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should read the version information from the same place.,SPARK-3273,12737296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,28/Aug/14 03:59,06/Sep/14 22:08,14/Jul/23 06:26,06/Sep/14 22:08,,,,,,,,1.2.0,,,,,,Spark Shell,,,,0,,,,,,,,apachespark,gq,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3301,SPARK-3302,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 06 22:08:59 UTC 2014,,,,,,,,,,"0|i1zfbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 04:02;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2175;;;","06/Sep/14 22:08;joshrosen;Issue resolved by pull request 2175
[https://github.com/apache/spark/pull/2175];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQLOperationManager.getNextRowSet OOMs when a large maxRows is set,SPARK-3269,12737271,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,28/Aug/14 00:26,29/Aug/14 22:36,14/Jul/23 06:26,29/Aug/14 22:36,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"{{SparkSQLOperationManager.getNextRowSet}} allocates an {{ArrayBuffer[Row]}} as large as {{maxRows}}, which can lead to OOM if {{maxRows}} is large, even if the actual size of the row set is much smaller.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 28 00:57:42 UTC 2014,,,,,,,,,,"0|i1zf5z:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"28/Aug/14 00:57;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2171;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DoubleType should support modulus ,SPARK-3268,12737255,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gvramana,chrisgrier,chrisgrier,27/Aug/14 23:23,23/Sep/14 19:23,14/Jul/23 06:26,23/Sep/14 19:23,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Using the modulus operator (%) on Doubles throws and exception. 

eg: 

SELECT 1388632775.0 % 60 from tablename LIMIT 1

Throws: 
java.lang.Exception: Type DoubleType does not support numeric operations",,apachespark,chrisgrier,gvramana,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 19 17:15:33 UTC 2014,,,,,,,,,,"0|i1zf2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/14 10:19;gvramana;Problem:
double, decimal and float are identified under fractional type hierarchy
as per Scala, % (rem) function is implemented under integral type hierarchy.
Currently there is no mechanism exists to allow fractional type using integral operations.

Solution:
Scala provides overridden classes like DoubleAsIfIntegral, FloatAsIfIntegral, BigDecimalAsIfIntegral to allow these types to work with integral operators.
So we can add asIntegral to FractionalType  to support calling Integral related functions.
i2 function can call functions on asIntegral to execute the same.

Implemented the same, writing test cases is pending, will submit the patch for the same.;;;","19/Sep/14 17:15;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/2457;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaDoubleRDD doesn't contain max(),SPARK-3266,12737240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,ameyc,ameyc,27/Aug/14 22:13,07/Jun/15 22:27,14/Jul/23 06:26,17/Mar/15 16:23,1.0.1,1.0.2,1.1.0,1.2.0,,,,1.2.2,1.3.1,1.4.0,,,,Java API,,,,3,,,,,,"While I can compile my code, I see:

Caused by: java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaDoubleRDD.max(Ljava/util/Comparator;)Ljava/lang/Double;

When I try to execute my Spark code. Stepping into the JavaDoubleRDD class, I don't notice max()
although it is clearly listed in the documentation.",,ameyc,apachespark,ArnaudL,aseigneurin,jlewandowski,joshrosen,lanzaa,pwendell,trecloux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1834,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 20:50;lanzaa;spark-repro-3266.tar.gz;https://issues.apache.org/jira/secure/attachment/12665070/spark-repro-3266.tar.gz",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 22:29:04 UTC 2015,,,,,,,,,,"0|i1zezb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 12:34;srowen;The method is declared in the superclass, JavaRDDLike: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala#L538

You are running a different version of Spark than you are compiling with, and the runtime version is perhaps too old to contain this method. This is not a Spark issue.;;;","28/Aug/14 20:50;lanzaa;I have attached a simple java project which reproduces the issue. [^spark-repro-3266.tar.gz]

{code}
> tar xvzf spark-repro-3266.tar.gz
...
> cd spark-repro-3266
> mvn clean package
> /path/to/spark-1.0.2-bin-hadoop2/bin/spark-submit --class SimpleApp target/testcase-4-1.0.jar
...
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaDoubleRDD.max(Ljava/util/Comparator;)Ljava/lang/Double;
        at SimpleApp.main(SimpleApp.java:17)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code};;;","28/Aug/14 21:18;joshrosen;Thanks for the reproduction!  I tried it myself and see the same issue.

If I replace 

{code}
        JavaDoubleRDD javaDoubleRDD = sc.parallelizeDoubles(numbers);
{code}

with 

{code}
        JavaRDDLike<Double, ?> javaDoubleRDD = sc.parallelizeDoubles(numbers);
{code}

then it seems to work.  I'll take a closer look using {{javap}} to see if I can figure out why this is happening.;;;","28/Aug/14 21:34;lanzaa;So there is no method:
{code}
org.apache.spark.api.java.JavaDoubleRDD.max(Ljava/util/Comparator;)Ljava/lang/Double;
{code}
but there is a method:
{code}
org.apache.spark.api.java.JavaDoubleRDD.max(Ljava/util/Comparator;)Ljava/lang/Object;
{code}

I've heard that the return type is part of the type signature in java bytecode, so the two are different. (one returns a Double, the other an Object)

This looks a bit like a scala type erasure related issue. The spark/scala code generated for JavaRDDLike includes a max method that returns an object. In JavaDoubleRDD the type is bounded to Double, so java code which calls max on JavaDoubleRDD expects a method returning Double. Since the code for max is implemented in the JavaRDDLike trait, the java code doesn't seem to inherit it correctly when types are involved.

I tested making JavaRDDLike an abstract class instead of a trait. It was able to compile and run correctly. However it is not compatible with 1.0.2.;;;","28/Aug/14 21:36;srowen;(Mea culpa! The example shows this is a legitimate question. I'll be quiet now.);;;","28/Aug/14 21:46;ameyc;No worries, I initially assumed my runtime env was old too until i rechecked.;;;","28/Aug/14 21:46;joshrosen;JavaRDDLike probably should be an abstract class.  I think the current trait implementation was a holdover from an earlier prototype that attempted to achieve higher code reuse for operations like map() and filter().

I added a test case to JavaAPISuite that reproduces this issue on master, too.

The simplest solution is probably to make JavaRDDLike into a trait.  I think we can do this while maintaining source compatibility.  A less invasive but messier solution would be to just copy the implementation of max() and min() into each Java*RDD class and remove it from the trait.  ;;;","28/Aug/14 22:05;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2186;;;","29/Aug/14 00:50;pwendell;[~joshrosen] is there a solution here that preserves binary compatibility? That's been our goal at this point and we've maintained it by and large except for a few very minor mandatory Scala 2.11 upgrades.;;;","29/Aug/14 05:07;joshrosen;[~pwendell] I think the JavaRDDLike trait compiles down to a Java interface (named JavaRDDLike) and an abstract class named JavaRDDLike$class that contains the implementations of the trait's members.  After this change, I think JavaRDDLike would compile into a Java abstract base class with the same name and we wouldn't have a separate interface.

My concern here is that it's going to be a _huge_ pain to find and fix all of the possible issues that could be caused by this being a trait instead of an abstract base class.  Having it be a trait was a mistake that we should have caught and fixed earlier.;;;","29/Aug/14 05:37;lanzaa;Broken for JavaDoubleRDD: fold, reduce, min, max
{code}
java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaDoubleRDD.fold(Ljava/lang/Double;Lorg/apache/spark/api/java/function/Function2;)Ljava/lang/Double;
java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaDoubleRDD.reduce(Lorg/apache/spark/api/java/function/Function2;)Ljava/lang/Double;
java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaDoubleRDD.min(Ljava/util/Comparator;)Ljava/lang/Double;
java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaDoubleRDD.max(Ljava/util/Comparator;)Ljava/lang/Double;
{code}

The ""first"" method would also be affected, but it seems that [~joshrosen] fixed that when he first implemented JavaDoubleRDD.

Also, I would bet JavaPairRDD and JavaSchemaRDD have similar issues.;;;","13/Oct/14 20:09;joshrosen;I believe the erasure to {{Object}} is due to a Scala compiler bug; I've opened https://issues.scala-lang.org/browse/SI-8905 to let the Scala team know about this.  In the meantime, though, it seems that moving the implementation of these methods into the JavaRDD classes should fix things.  I'll work on putting together a patch based on that workaround.;;;","26/Oct/14 22:52;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2951;;;","26/Oct/14 23:12;joshrosen;I've opened a new pull request which tries to work around the Scala issue by moving the implementations of these methods from the Java*Like traits into abstract base classes that inherit from those traits (essentially making the traits act as interfaces).  This breaks binary compatibility from Scala's point of view, since the fact that a trait contains a default implementation of a method is part of its API contract (it affects implementors of that trait).  I don't think there's any legitimate reason for someone to have extended JavaRDDLike from their own code, so we shouldn't have to worry about this.

From a simplicity perspective, I prefer the approach from my first PR of simply converting JavaRDDLike into an abstract class.  This would cause problems for Java API users who were invoking methods through the interface, though.  I can't imagine that most users would have done this, but maybe it's important to not break compatibility.  On the other hand, the current API is functionally broken as long as it's throwing NoSuchMethodErrors.

The one approach that doesn't break _any_ binary compatibility would be to just keep the default implementations of methods in JavaRDDLike then copy-paste the ones affected by the bugs into the individual JavaRDD classes.  This is a mess, but I can do it if necessary.;;;","26/Oct/14 23:33;pwendell;I think it sort of depends how many people use JavaRDDLike and how they use it. In my mind it wasn't intended to be used by user applications, but probably some do because there isn't really a way to write functions that pass RDD's around and deal with both Pair RDD's and normal ones in Java. [~matei], what do you think of this vis-a-vis compatibility?;;;","20/Nov/14 07:09;joshrosen;I'm unassigning myself from this since I'm no longer actively working on it (although I really want to fix this as soon as I have time).

Copying a [comment from one of my pull requests|https://github.com/apache/spark/pull/2951#issuecomment-63769092]:

{quote}
@viper-kun No, I'm not actively working on this. A pull request here would be very welcome, since this is an annoying bug. If you're planning to work on this, make sure to include the extra test cases that I added to JavaAPISuite; these tests should be useful regardless of what approach you taking to fixing these bugs.

From a binary compatibility standpoint, it's important to keep the Java*Like interfaces since there's some code in the wild that uses these interfaces to abstract over the different implementations. Removing default implementations from traits technically breaks compatibility for anyone who might have extended those traits, but I don't think that should be a huge concern / likely problem.

If you want to avoid copying / moving everything around, then I think it would be sufficient to just identify the methods that are affected by this compiler bug and copy only those methods to each subclass. We could maintain 100% binary compatibility with this approach, even for the obscure case where someone extended a trait, and it might make it easier to backport the fix to maintenance branches, but it also seems sort of risk-prone because someone might add new default implementations in the trait.

For the sake of keeping the discussion in one place, let's chat about alternative designs on the JIRA ticket. I'll unassign myself from it and copy this comment over there.
{quote};;;","14/Mar/15 21:46;srowen;Related note to self for the future: the histogram() function here returns Pair, which is an alias for Tuple2, but one that's deprecated in Scala 2.11. This shouldn't be Pair and maybe shouldn't even be Tuple2 in the Java API.;;;","16/Mar/15 17:59;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5050;;;","17/Mar/15 16:23;joshrosen;Issue resolved by pull request 5050
[https://github.com/apache/spark/pull/5050];;;","17/Mar/15 16:26;joshrosen;After many months, I've finally fixed this issue thanks to a good workaround suggested by Jason Zaugg: https://issues.scala-lang.org/browse/SI-8905?focusedCommentId=72096&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-72096.

I've merged this fix into 1.4.0 and 1.3.1.  If there's any demand to backport this to other maintenance branches, please let me know, but I suspect that merging into the next release should be sufficient for most users (users running legacy versions of Spark probably have their own workarounds in place already).;;;","22/Mar/15 10:04;jlewandowski;[~joshrosen] it would be awesome to include this fix into 1.2.x. We've recently added cross compilation for Scala 2.10 and Scala 2.11 of our Spark Cassandra Connector. It turned out that we were unable to subclass {{JavaPairRDD}} in Java when Scala 2.11 compiler was used. The compilation error said:
{noformat}
[error] /Users/jlewandowski/Projects/OpenSource/spark-cassandra-connector/spark-cassandra-connector-java/src/main/java/com/datastax/spark/connector/japi/rdd/CassandraJavaPairRDD.java:32: error: min(Comparator) in JavaPairRDD cannot implement min(Comparator<T>) in JavaRDDLike
[error] public class CassandraJavaPairRDD<K, V> extends JavaPairRDD<K, V> {
[error]        ^
[error]   return type Object is not compatible with Tuple2<K,V>
[error]   where T,K,V are type-variables:
[error]     T extends Object declared in interface JavaRDDLike
[error]     K extends Object declared in class CassandraJavaPairRDD
[error]     V extends Object declared in class CassandraJavaPairRDD
[error] 1 error
[error] (spark-cassandra-connector-java/compile:compile) javac returned nonzero exit code
[error] Total time: 3 s, completed Mar 22, 2015 8:46:28 AM
{noformat}

Which seems to be tightly related to this issue.
;;;","24/Mar/15 20:57;joshrosen;[~jlewandowski] I have now merged this into `branch-1.2` (1.2.2) as well.;;;","25/Mar/15 22:29;jlewandowski;Thanks a lot [~joshrosen], really appreciate that ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow users to set executor Spark home in Mesos,SPARK-3264,12737215,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,27/Aug/14 20:44,28/Aug/14 18:06,14/Jul/23 06:26,28/Aug/14 18:06,1.0.2,,,,,,,1.1.0,,,,,,Mesos,,,,0,,,,,,"There is an existing way to do this, through ""spark.home"". However, this is neither documented nor intuitive. I propose that we add a more specific config ""spark.mesos.executor.home"" for this purpose, and fallback to the existing settings if this is not set.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 20:48:08 UTC 2014,,,,,,,,,,"0|i1zetr:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"27/Aug/14 20:48;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2166;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PR #720 broke GraphGenerator.logNormal,SPARK-3263,12737213,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,rnowling,rnowling,27/Aug/14 20:25,08/Sep/14 01:11,14/Jul/23 06:26,03/Sep/14 21:17,,,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,"PR #720 made multiple changes to GraphGenerator.logNormalGraph including:

* Replacing the call to functions for generating random vertices and edges with in-line implementations with different equations
* Hard-coding of RNG seeds so that method now generates the same graph for a given number of vertices, edges, mu, and sigma -- user is not able to override seed or specify that seed should be randomly generated.
* Backwards-incompatible change to logNormalGraph signature with introduction of new required parameter.
* Failed to update scala docs and programming guide for API changes

I also see that PR #720 added a Synthetic Benchmark in the examples.

Based on reading the Pregel paper, I believe the in-line functions are incorrect.  I proposed to:

* Removing the in-line calls
* Adding a seed for deterministic behavior (when desired)
* Keeping the number of partitions parameter.
* Updating the synthetic benchmark example",,ankurd,apachespark,rnowling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 21:17:47 UTC 2014,,,,,,,,,,"0|i1zetb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/14 21:13;apachespark;User 'rnowling' has created a pull request for this issue:
https://github.com/apache/spark/pull/2168;;;","03/Sep/14 21:17;ankurd;Issue resolved by pull request 2168
[https://github.com/apache/spark/pull/2168];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn - pass acls along with executor launch,SPARK-3260,12737196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,27/Aug/14 18:52,05/Sep/14 14:55,14/Jul/23 06:26,05/Sep/14 14:55,1.1.0,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"In https://github.com/apache/spark/pull/1196 I added passing the spark view and modify acls into yarn.  Unfortunately we are only passing them into the application master and I missed passing them in when we launch individual containers (executors). 

We need to modify the ExecutorRunnable.startContainer to set the acls in the ContainerLaunchContext.",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 28 21:45:42 UTC 2014,,,,,,,,,,"0|i1zepz:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"28/Aug/14 21:45;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2185;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User data should be given to the master,SPARK-3259,12737186,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,douglaz,douglaz,douglaz,27/Aug/14 18:33,27/Aug/14 19:43,14/Jul/23 06:26,27/Aug/14 19:43,,,,,,,,1.1.0,,,,,,EC2,,,,0,,,,,,This is something SPARK-2246 missed. The master also should receive the given user data.,,apachespark,douglaz,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 19:43:38 UTC 2014,,,,,,,,,,"0|i1zenr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/14 18:37;apachespark;User 'douglaz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2162;;;","27/Aug/14 19:43;pwendell;Issue resolved by pull request 2162
[https://github.com/apache/spark/pull/2162];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.driver.* system properties are outdated if --driver-* options exist,SPARK-3243,12737003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,27/Aug/14 03:49,27/Aug/14 21:47,14/Jul/23 06:26,27/Aug/14 21:47,1.0.2,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"If we set ""spark.driver.extraClassPath"" AND ""--driver-class-path"", then the latter correctly overrides the former. However! The value of the system property ""spark.driver.extraClassPath"" remains the former, which is not actually used.

That is just an example. Of course this also affects java options, library path and memory etc.",,andrewor14,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 21:47:17 UTC 2014,,,,,,,,,,"0|i1zdjb:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"27/Aug/14 04:22;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2154;;;","27/Aug/14 21:47;pwendell;Issue resolved by pull request 2154
[https://github.com/apache/spark/pull/2154];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.0.2 ec2 scripts creates clusters with Spark 1.0.1 installed by default,SPARK-3242,12736999,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,27/Aug/14 03:16,09/Feb/15 19:51,14/Jul/23 06:26,09/Feb/15 19:51,1.0.2,,,,,,,,,,,,,EC2,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 19:51:45 UTC 2015,,,,,,,,,,"0|i1zdif:",9223372036854775807,,,,,,,,,,,,,,1.0.3,,,,,,,,,,,"27/Aug/14 03:17;tdas;Current workaround is to create a explicitly specify Spark version as 1.0.2 in the spark-ec2 script. ;;;","08/Feb/15 13:57;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4458;;;","09/Feb/15 19:51;srowen;Resolved by https://github.com/apache/spark/commit/444ccdd80ec5df249978d8498b4fc501cc3429d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commas/spaces/dashes are not escaped properly when transferring schema information to parquet readers,SPARK-3238,12736984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,marmbrus,marmbrus,27/Aug/14 00:46,03/Nov/14 22:21,14/Jul/23 06:26,03/Nov/14 22:21,1.1.0,,,,,,,,,,,,,SQL,,,,0,,,,,,,,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 03 22:21:25 UTC 2014,,,,,,,,,,"0|i1zdfb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"03/Nov/14 22:21;marmbrus;I think this was fixed by the conversion to JSON for serializing schema;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push down of predicates with UDFS into parquet scan can result in serialization errors,SPARK-3237,12736983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,27/Aug/14 00:37,27/Aug/14 08:01,14/Jul/23 06:26,27/Aug/14 08:01,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Two things: we should be using the closure cleaner on UDFS.  Also we should not be using kryo to serialize expression trees.,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 01:47:44 UTC 2014,,,,,,,,,,"0|i1zdf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/14 01:47;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2153;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading Parquet tables from Metastore mangles location,SPARK-3236,12736970,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ilikerps,ilikerps,ilikerps,27/Aug/14 00:07,27/Aug/14 22:06,14/Jul/23 06:26,27/Aug/14 22:06,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Currently we do ""relation.hiveQlTable.getDataLocation.getPath"", which returns the path-part of the URI (e.g., s3n://my-bucket/my-path => /my-path). We should do ""relation.hiveQlTable.getDataLocation.toString"" instead, as a URI's toString returns a faithful representation of the full URI, which can later be passed into a Hadoop Path.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 00:12:34 UTC 2014,,,,,,,,,,"0|i1zdc7:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"27/Aug/14 00:12;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/2150;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In-Memory relation has a bad default size.,SPARK-3235,12736884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,26/Aug/14 20:03,27/Aug/14 22:14,14/Jul/23 06:26,27/Aug/14 22:14,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,All operators must have a default size that prevents automatic broadcasting otherwise we can easily OOM the driver.,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 26 20:37:30 UTC 2014,,,,,,,,,,"0|i1zctb:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"26/Aug/14 20:37;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2147;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK_HADOOP_VERSION and SPARK_HIVE depend on deprecated make-distribution.sh command line options,SPARK-3234,12736873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,26/Aug/14 19:15,29/Aug/14 22:31,14/Jul/23 06:26,29/Aug/14 22:31,1.0.2,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"In {{make-distribution.sh}}, {{SPARK_HADOOP_VERSION}} is determined by {{\-\-hadoop}}, and {{SPARK_HIVE}} is determined by {{\-\-with-hive}}, but these two command line options are deprecated by `-Dhadoop.version`/`-Phadoop-??` and `-Phive` respectively. The result is that:

# we may end up with something like {{spark-1.1.0-SNAPSHOT-bin-.tgz}} unless {{\-\-name}} is specified explicitly
# Datanucleus jars will not be included in the distribution folder/tarball unless we set {{SPARK_HIVE=true}} explicitly

A possible fix is to check {{hadoop.version}} and {{project.activeProfiles}} with {{mvn help:evaluate}}, similar to what we do now to determine {{VERSION}}.",,apachespark,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 22:31:12 UTC 2014,,,,,,,,,,"0|i1zcqv:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"29/Aug/14 22:15;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2208;;;","29/Aug/14 22:31;pwendell;Issue resolved by pull request 2208
[https://github.com/apache/spark/pull/2208];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Executor never stop its SparnEnv, BlockManager, ConnectionManager etc.",SPARK-3233,12736820,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sarutak,sarutak,26/Aug/14 15:20,04/Sep/14 01:42,14/Jul/23 06:26,04/Sep/14 01:42,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Executor never stop its SparnEnv. Because of this,  following component in Executor never stop until JVM is shutdown.

* HttpFileServer
* MapOutputTracker
* ShuffleManager
* BlockManager
* MetricSystem
* ActorSystem",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 26 15:32:28 UTC 2014,,,,,,,,,,"0|i1zcfb:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"26/Aug/14 15:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2138;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport SPARK-3006 into branch-1.0,SPARK-3232,12736809,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,sarutak,sarutak,26/Aug/14 14:07,12/Oct/14 03:51,14/Jul/23 06:26,12/Oct/14 03:51,1.0.2,,,,,,,1.0.2,,,,,,Spark Core,Windows,,,0,,,,,,"As well as SPARK-3216, we need to backport SPARK-3006 into branch-1.0.",Windows,apachespark,huitseeker,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 12:22:47 UTC 2014,,,,,,,,,,"0|i1zccv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/14 14:12;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2136;;;","01/Oct/14 12:22;huitseeker;This is closed by https://github.com/apache/spark/commit/8dd7690e2b4c3269d2777d3e208903bf596d1509;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select on a table in parquet format containing smallint as a field type does not work,SPARK-3231,12736764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,arov,chiragaggarwal,chiragaggarwal,26/Aug/14 10:12,16/Sep/15 08:02,14/Jul/23 06:26,15/Sep/15 20:56,1.1.0,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"A table is created through hive. This table has a field of type smallint. The format of the table is parquet.
select on this table works perfectly on hive shell.
But, when the select is run on this table from spark-sql, then the query fails.

Steps to reproduce the issue:
--------------------------------------
hive> create table abct (a smallint, b int) row format delimited fields terminated by '|' stored as textfile;
A text file is stored in hdfs for this table.

hive> create table abc (a smallint, b int) stored as parquet; 
hive> insert overwrite table abc select * from abct;
hive> select * from abc;
2	1
2	2
2	3

spark-sql> select * from abc;
10:08:46 ERROR CliDriver: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 33.0 (TID 2340) had a not serializable result: org.apache.hadoop.io.IntWritable
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1158)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1147)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1146)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1146)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:685)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:685)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:685)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1364)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)



But, if the type of this table is now changed to int, then spark-sql gives the correct results.

hive> alter table abc change a a int;    
spark-sql> select * from abc;

2	1
2	2
2	3","The table is created through Hive-0.13.
SparkSql 1.1 is used.",arov,chiragaggarwal,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 05 16:02:09 UTC 2015,,,,,,,,,,"0|i1zc2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/15 16:02;arov;This is no longer an issue in master. I just verified that it works correctly. If you can upgrade to a later version of Spark and try this operation again, it would be helpful to know in what version this was fixed in case someone is interested in backporting the fixes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFs that return structs result in ClassCastException,SPARK-3230,12736762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,26/Aug/14 09:53,28/Aug/14 07:20,14/Jul/23 06:26,28/Aug/14 07:20,,,,,,,,,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 26 09:57:25 UTC 2014,,,,,,,,,,"0|i1zc2f:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"26/Aug/14 09:57;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2133;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in script,SPARK-3225,12736709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,26/Aug/14 05:42,27/Aug/14 00:32,14/Jul/23 06:26,27/Aug/14 00:32,,,,,,,,1.2.0,,,,,,Deploy,,,,0,,,,,,use_conf_dir => user_conf_dir in load-spark-env.sh.,,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-08-26 05:42:40.0,,,,,,,,,,"0|i1zbqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchFailed stages could show up multiple times in failed stages in web ui,SPARK-3224,12736708,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rxin,rxin,rxin,26/Aug/14 05:38,11/Dec/14 22:57,14/Jul/23 06:26,27/Aug/14 05:13,,,,,,,,1.1.0,,,,,,Web UI,,,,0,,,,,,"Today I saw a job in which a reduce stage failed and showed up a lot of times in the failed stages. I think the reason is that the DAGScheduler stage complete (with failure) event multiple times in the case of FetchFailed. 

",,apachespark,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2947,,,SPARK-2947,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 05:13:53 UTC 2014,,,,,,,,,,"0|i1zbqf:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"26/Aug/14 05:42;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2127;;;","27/Aug/14 05:13;pwendell;Issue resolved by pull request 2127
[https://github.com/apache/spark/pull/2127];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
runAsSparkUser cannot change HDFS write permission properly in mesos cluster mode,SPARK-3223,12736707,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jongyoul,jongyoul,jongyoul,26/Aug/14 05:26,05/Nov/14 23:58,14/Jul/23 06:26,05/Nov/14 23:58,1.0.2,,,,,,,1.1.1,1.2.0,,,,,Input/Output,Mesos,,,0,,,,,,"While running mesos with --no-switch_user option, HDFS account name is different from driver and executor. It makes a permission error at last stage. Executor's id is mesos' user id and driver's id is who runs spark-submit. So, moving output from _temporary/path/to/output/part-xxxx to /output/path/part-xxxx fails because of permission error. The solution for this is only setting SPARK_USER to HADOOP_USER_NAME when MesosExecutorBackend calls runAsSparkUser. HADOOP_USER_NAME is used when FileSystem get user.",,apachespark,jongyoul,joshrosen,tgraves,tstclair,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 08:39:15 UTC 2014,,,,,,,,,,"0|i1zbq7:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"26/Aug/14 05:42;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/2126;;;","16/Sep/14 18:56;tstclair;As [~tnachen] mentioned in the PR, specifying the user for the framework should resolve the issue. ;;;","31/Oct/14 08:39;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/3034;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shaded Guava jar doesn't play well with Maven build when SPARK_PREPEND_CLASSES is set,SPARK-3217,12736661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,lian cheng,lian cheng,25/Aug/14 23:12,01/Nov/14 14:02,14/Jul/23 06:26,12/Sep/14 21:55,1.2.0,,,,,,,1.2.0,,,,,,Build,,,,0,,,,,,"PR [#1813|https://github.com/apache/spark/pull/1813] shaded Guava jar file and moved Guava classes to package {{org.spark-project.guava}} when Spark is built by Maven. But if developers set the environment variable {{SPARK_PREPEND_CLASSES}} to {{true}}, commands like {{bin/spark-shell}} throws {{ClassNotFoundException}}:
{code}
# Set the env var
$ export SPARK_PREPEND_CLASSES=true

# Build Spark with Maven
$ mvn clean package -Phive,hadoop-2.3 -Dhadoop.version=2.3.0 -DskipTests
...

# Then spark-shell complains
$ ./bin/spark-shell
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/google/common/util/concurrent/ThreadFactoryBuilder
        at org.apache.spark.util.Utils$.<init>(Utils.scala:636)
        at org.apache.spark.util.Utils$.<clinit>(Utils.scala)
        at org.apache.spark.repl.SparkILoop.<init>(SparkILoop.scala:134)
        at org.apache.spark.repl.SparkILoop.<init>(SparkILoop.scala:65)
        at org.apache.spark.repl.Main$.main(Main.scala:30)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:317)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:73)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: com.google.common.util.concurrent.ThreadFactoryBuilder
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 13 more

# Check the assembly jar file
$ jar tf assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop2.3.0.jar | grep -i ThreadFactoryBuilder
org/spark-project/guava/common/util/concurrent/ThreadFactoryBuilder$1.class
org/spark-project/guava/common/util/concurrent/ThreadFactoryBuilder.class
{code}
SBT build is fine since we don't shade Guava with SBT right now.",,apachespark,donnchadh,hammer,lian cheng,pwendell,qiaohaijun,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2848,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 01 14:02:19 UTC 2014,,,,,,,,,,"0|i1zbg7:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"26/Aug/14 00:14;vanzin;Just did a ""git clean -dfx"" on master and rebuilt using maven. This works fine for me.

Did you by any chance do one of the following:
- forget to ""clean"" after pulling that change
- mix sbt and mvn built artifacts in the same build
- set SPARK_PREPEND_CLASSES

I can see any of those causing this issue. I think only the last one is something we need to worry about; we now need to figure out a way to add the guava jar to the classpath when using that option.;;;","26/Aug/14 01:36;lian cheng;[~vanzin] Thanks, I did set {{SPARK_PREPEND_CLASSES}}. Will change the title and description of this issue after verifying it.;;;","26/Aug/14 18:47;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2141;;;","26/Aug/14 18:53;lian cheng;[~vanzin] Verified locally, everything is OK after unsetting {{SPARK_PREPEND_CLASSES}}, updated issue title/description and changed priority to Major. Thanks for the tips!;;;","12/Sep/14 21:55;pwendell;Fixed by https://github.com/apache/spark/pull/2141/;;;","01/Nov/14 14:02;qiaohaijun;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-shell is broken for branch-1.0,SPARK-3216,12736658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor14,25/Aug/14 23:08,03/Sep/14 20:16,14/Jul/23 06:26,03/Sep/14 20:05,,,,,,,,1.0.3,,,,,,Spark Core,,,,0,,,,,,"This fails when EC2 tries to clone the most recent version of Spark from branch-1.0. This does not actually affect any released distributions, and so I did not set the affected/fix/target versions. I marked this a blocker because this is completely broken, but it is technically not ""blocking"" anything.

This was caused by https://github.com/apache/spark/pull/1831, which broke spark-shell. The follow-up fix in https://github.com/apache/spark/pull/1825 was only merged into branch-1.1 and master, but not branch-1.0.",,andrewor14,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 20:04:07 UTC 2014,,,,,,,,,,"0|i1zbfj:",9223372036854775807,,,,,,,,,,,,,,1.0.3,,,,,,,,,,,"25/Aug/14 23:12;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2122;;;","26/Aug/14 14:12;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2136;;;","03/Sep/14 20:04;pwendell;[~andrewor14] for this you should Mark it as a blocker for target version 1.0.3 and also put fix version 1.0.3 when it is merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the clarity of caching semantics,SPARK-3212,12736605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,25/Aug/14 19:48,03/Oct/14 19:34,14/Jul/23 06:26,03/Oct/14 19:34,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Right now there are a bunch of different ways to cache tables in Spark SQL. For example:
 - tweets.cache()
 - sql(""SELECT * FROM tweets"").cache()
 - table(""tweets"").cache()
 - tweets.cache().registerTempTable(tweets)
 - sql(""CACHE TABLE tweets"")
 - cacheTable(""tweets"")

Each of the above commands has subtly different semantics, leading to a very confusing user experience.  Ideally, we would stop doing caching based on simple tables names and instead have a phase of optimization that does intelligent matching of query plans with available cached data.",,apachespark,cfregly,jonathak,marmbrus,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3641,,,,,,,,SPARK-2189,SPARK-3298,SPARK-1379,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 03 19:34:46 UTC 2014,,,,,,,,,,"0|i1zb47:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"10/Sep/14 02:29;marmbrus;[~matei] also points out that we should make sure to uncache cached RDDs when the base table is dropped.;;;","23/Sep/14 03:00;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2501;;;","03/Oct/14 19:34;marmbrus;Issue resolved by pull request 2501
[https://github.com/apache/spark/pull/2501];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.take() is OOM-prone when there are empty partitions,SPARK-3211,12736597,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aash,aash,aash,25/Aug/14 19:16,07/Sep/14 06:07,14/Jul/23 06:26,06/Sep/14 01:52,1.0.2,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"Filed on dev@ on 22 August by [~pnepywoda]:

{quote}
On line 777
https://github.com/apache/spark/commit/42571d30d0d518e69eecf468075e4c5a823a2ae8#diff-1d55e54678eff2076263f2fe36150c17R771
the logic for take() reads ALL partitions if the first one (or first k) are
empty. This has actually lead to OOMs when we had many partitions
(thousands) and unfortunately the first one was empty.

Wouldn't a better implementation strategy be

numPartsToTry = partsScanned * 2

instead of

numPartsToTry = totalParts - 1

(this doubling is similar to most memory allocation strategies)

Thanks!
- Paul
{quote}",,aash,apachespark,jdanbrown,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 07 06:07:06 UTC 2014,,,,,,,,,,"0|i1zb2n:",9223372036854775807,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"25/Aug/14 19:22;apachespark;User 'ash211' has created a pull request for this issue:
https://github.com/apache/spark/pull/2117;;;","07/Sep/14 06:07;aash;This was merged into branch-1.1 and develop;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Parquet SerDe returns null columns,SPARK-3208,12736579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,marmbrus,marmbrus,25/Aug/14 18:30,15/Sep/15 20:57,14/Jul/23 06:26,15/Sep/15 20:57,1.1.0,,,,,,,,,,,,,SQL,,,,0,,,,,,"There is a workaround, which is to set 'spark.sql.hive.convertMetastoreParquet=true'.  However, it would still be good to figure out what is going on here.

Relatedly the following also doesn't work (the results are corrupted).

{code}

sql(""CREATE TABLE test (a int, b string) ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat' OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'"")

case class MyClass(a: Int, b: String)
val rows = sc.parallelize(Seq(MyClass(1, ""x""), MyClass(2, ""y"")))
rows.insertInto(""test"")

sql(""select * from test"").collect()
a	b
1	eA==
2	eQ==
{code}",,avignon,marmbrus,ravipesala,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 20:57:37 UTC 2015,,,,,,,,,,"0|i1zayn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/15 20:57;marmbrus;Convert metastore is now the default, so I'll close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in PageRank values,SPARK-3206,12736555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,pfontana3w2,pfontana3w2,25/Aug/14 17:13,12/Nov/14 08:48,14/Jul/23 06:26,12/Nov/14 08:48,1.0.2,,,,,,,1.2.0,,,,,,GraphX,,,,0,,,,,,"I have found a small example where the PageRank values using run and runUntilConvergence differ quite a bit.

I am running the Pagerank module on the following graph:

Edge Table:

|| Node1 || Node2 ||
|1 | 2 |
|1 |	3|
|3 |	2|
|3 |	4|
|5 |	3|
|6 |	7|
|7 |	8|
|8 |	9|
|9 |	7|

Node Table (note the extra node):

|| NodeID  || NodeName  ||
|a |	1|
|b |	2|
|c |	3|
|d |	4|
|e |	5|
|f |	6|
|g |	7|
|h |	8|
|i |	9|
|j.longaddress.com |	10|

with a default resetProb of 0.15.
When I compute the pageRank with runUntilConvergence, running 

{{val ranks = PageRank.runUntilConvergence(graph,0.0001).vertices}}

I get the ranks
(4,0.29503124999999997)
(1,0.15)
(6,0.15)
(3,0.34124999999999994)
(7,1.3299054047985106)
(9,1.2381240056453071)
(8,1.2803346052504254)
(10,0.15)
(5,0.15)
(2,0.35878124999999994)

However, when I run page Rank with the run() method, running  

{{val ranksI = PageRank.run(graph,100).vertices}} 

I get the page ranks

(4,0.29503124999999997)
(1,0.15)
(6,0.15)
(3,0.34124999999999994)
(7,0.9999999387662847)
(9,0.9999999256447741)
(8,0.9999999256447741)
(10,0.15)
(5,0.15)
(2,0.29503124999999997)

These are quite different, leading me to suspect that one of the PageRank methods is incorrect. I have examined the source, but I do not know what the correct fix is, or which set of values is correct.",UNIX with Hadoop,ankurd,pfontana3w2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 08:47:12 UTC 2014,,,,,,,,,,"0|i1zatb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/14 21:46;ankurd;I think the `run` method is incorrect due to a bug in Pregel. I wrote a standalone (non-Pregel) version of PageRank that provides the functionality of `run` without using Pregel: https://github.com/ankurdave/spark/blob/low-level-PageRank/graphx/src/main/scala/org/apache/spark/graphx/impl/GraphImpl.scala#L274

I'd like to run that and check the results against these ones when I get a chance.;;;","12/Nov/14 08:47;ankurd;I just tested this with the standalone version of PageRank that was introduced in SPARK-3427, and it seems to be fixed, so I'm closing this.

{code}
scala> val e = sc.parallelize(List(
  (1, 2), (1, 3), (3, 2), (3, 4), (5, 3), (6, 7), (7, 8), (8, 9), (9, 7)))

scala> val g = Graph.fromEdgeTuples(e.map(kv => (kv._1.toLong, kv._2.toLong)), 0)

scala> g.pageRank(0.0001).vertices.collect.foreach(println)
(8,1.2808550959634413)
(1,0.15)
(9,1.2387268204156412)
(2,0.35878124999999994)
(3,0.34124999999999994)
(4,0.29503124999999997)
(5,0.15)
(6,0.15)
(7,1.330417786200011)

scala> g.staticPageRank(100).vertices.collect.foreach(println)
(8,1.2803346052504254)
(1,0.15)
(9,1.2381240056453071)
(2,0.35878124999999994)
(3,0.34124999999999994)
(4,0.29503124999999997)
(5,0.15)
(6,0.15)
(7,1.3299054047985106)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimizer removes required attributes with capital letters when using case insensitive resolution.,SPARK-3194,12736366,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,24/Aug/14 20:19,27/Aug/14 02:48,14/Jul/23 06:26,27/Aug/14 02:48,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 24 20:37:16 UTC 2014,,,,,,,,,,"0|i1za1b:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"24/Aug/14 20:37;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2109;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some scripts have 2 space indentation but other scripts have 4 space indentation.,SPARK-3192,12736242,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,23/Aug/14 09:25,24/Aug/14 16:44,14/Jul/23 06:26,24/Aug/14 16:44,1.1.0,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"spark-sql, pyspark and spark-thriftserver.sh have 2 space indentation but some parts of spark-shell has 4 space indentation and some other part of spark-shell has 2 space indentation.",,apachespark,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 24 16:44:21 UTC 2014,,,,,,,,,,"0|i1z9ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/14 09:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2104;;;","24/Aug/14 16:44;pwendell;Issue resolved by pull request 2104
[https://github.com/apache/spark/pull/2104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creation of large graph(> 2.15 B nodes) seems to be broken:possible overflow somewhere ,SPARK-3190,12736210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ankurd,npanj,npanj,23/Aug/14 00:29,02/May/16 06:20,14/Jul/23 06:26,04/Aug/15 06:08,1.0.3,,,,,,,1.0.3,1.1.0,1.2.0,1.3.2,1.4.2,1.5.0,GraphX,,,,0,,,,,,"While creating a graph with 6B nodes and 12B edges, I noticed that 'numVertices' api returns incorrect result; 'numEdges' reports correct number. For few times(with different dataset > 2.5B nodes) I have also notices that numVertices is returned as -ive number; so I suspect that there is some overflow (may be we are using Int for some field?).

Here is some details of experiments  I have done so far: 
1. Input: numNodes=6101995593 ; noEdges=12163784626
   Graph returns: numVertices=1807028297 ;  numEdges=12163784626

2. Input : numNodes=2157586441 ; noEdges=2747322705
   Graph Returns: numVertices=-2137380855 ;  numEdges=2747322705

3. Input: numNodes=1725060105 ; noEdges=204176821
   Graph: numVertices=1725060105 ;  numEdges=2041768213

You can find the code to generate this bug here: 

https://gist.github.com/npanj/92e949d86d08715bf4bf

Note: Nodes are labeled are 1...6B .














 ",Standalone mode running on EC2 . Using latest code from master branch upto commit #db56f2df1b8027171da1b8d2571d1f2ef1e103b6 .,ankurd,apachespark,joshrosen,liyuance,npanj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10228,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 02 06:20:04 UTC 2016,,,,,,,,,,"0|i1z97b:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.2,1.5.0,,,,,,,,,"23/Aug/14 23:31;ankurd;I haven't tried to reproduce this yet, but counting the vertices occurs in [VertexRDD.scala:110|https://github.com/apache/spark/blob/3519b5e8e55b4530d7f7c0bcab254f863dbfa814/graphx/src/main/scala/org/apache/spark/graphx/VertexRDD.scala#L110], which sums up an Int from each partition and only promotes it to a Long when returning the result. Therefore it should fix the problem to change line 111 to
{code}
partitionsRDD.map(_.size.toLong).reduce(_ + _)
{code};;;","24/Aug/14 01:12;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2106;;;","25/Aug/14 06:17;npanj;Thanks Ankur for patch. I can confirm that this pull request fixed the issue.;;;","28/Aug/14 22:18;joshrosen;Issue resolved by pull request 2106
[https://github.com/apache/spark/pull/2106];;;","04/Aug/15 05:33;ankurd;Regression due to https://github.com/apache/spark/commit/a5ef58113667ff73562ce6db381cff96a0b354b0#diff-dded6985c865d7623d83ec5a860e1bd4R75;;;","04/Aug/15 05:39;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/7923;;;","02/May/16 05:55;liyuance;The PR{2106，7923} can not fix the problem completely, as  the number of vertices in one of partition exceed Integer.MAX_VALUE also can repreduce this Bug。 The fundamental cause of this problem is the variable “size” is defined as type Int  in class VertexPartitionBase.;;;","02/May/16 06:20;apachespark;User 'liyuance' has created a pull request for this issue:
https://github.com/apache/spark/pull/12835;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
setting SPARK_WORKER_MEMORY to a value without a label (m or g) sets the worker memory limit to zero,SPARK-3178,12735914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bbejeck,rustyrazorblade,rustyrazorblade,21/Aug/14 21:46,14/Oct/14 19:12,14/Jul/23 06:26,14/Oct/14 19:12,,,,,,,,1.2.0,,,,,,,,,,1,starter,,,,,This should either default to m or just completely fail.  Starting a worker with zero memory isn't very helpful.,osx,apachespark,bcantoni,helena_e,joshrosen,rustyrazorblade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 19:12:54 UTC 2014,,,,,,,,,,"0|i1z7en:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/14 01:22;helena_e;+1 it doesn't look like the input data is validated to fail fast if mb/g is not noted;;;","01/Sep/14 12:00;apachespark;User 'bbejeck' has created a pull request for this issue:
https://github.com/apache/spark/pull/2227;;;","07/Sep/14 02:43;apachespark;User 'bbejeck' has created a pull request for this issue:
https://github.com/apache/spark/pull/2309;;;","14/Oct/14 19:12;joshrosen;Issue resolved by pull request 2309
[https://github.com/apache/spark/pull/2309];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn-alpha ClientBaseSuite Unit test failed,SPARK-3177,12735908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,chesterxgchen,chesterxgchen,21/Aug/14 21:24,17/Sep/14 15:27,14/Jul/23 06:26,17/Sep/14 15:27,1.1.1,,,,,,,1.2.0,,,,,,YARN,,,,0,test,,,,,"Yarn-alpha ClientBaseSuite Unit test failed due to differences of MRJobConfig API between yarn-stable and yarn-alpha. 

The class field
MRJobConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH 

in yarn-alpha 
returns String Array

in yarn 
returns String 

the method will works for yarn-stable but will fail as it try to cast String Array to String. 

val knownDefMRAppCP: Seq[String] =
     getFieldValue[String, Seq[String]](classOf[MRJobConfig],
""DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"",
                                         Seq[String]())(a => a.split("",""))
",,apachespark,chesterxgchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 18:55:27 UTC 2014,,,,,,,,,,"0|i1z7db:",9223372036854775807,,,,,,,,,,,,,,1.1.1,,,,,,,,,,,"21/Aug/14 21:27;chesterxgchen;This issue should exists on master branch as well. It has been over there for a while. ;;;","25/Aug/14 01:07;apachespark;User 'chesterxgchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2111;;;","29/Aug/14 18:55;apachespark;User 'chesterxgchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp support in the parser,SPARK-3173,12735829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,byFcz,byFcz,21/Aug/14 17:04,29/Aug/14 22:39,14/Jul/23 06:26,29/Aug/14 22:39,1.0.2,1.1.0,,,,,,1.1.1,,,,,,SQL,,,,0,,,,,,"If you have a table with TIMESTAMP column, that column can't be used in WHERE clause properly - it is not evaluated properly.

F.e., SELECT * FROM a WHERE timestamp='2014-08-21 00:00:00.0', would return nothing even if there would be a row with such a timestamp. The literal is not interpreted into a timestamp.

The workaround SELECT * FROM a WHERE timestamp=CAST('2014-08-21 00:00:00.0' AS TIMESTAMP) fails, because the parser does not allow anything but STRING in the CAST dataType expression.",,byFcz,chutium,chuxi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 25 09:22:09 UTC 2014,,,,,,,,,,"0|i1z6wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/14 09:22;chutium;vote on this ticket, it seems should link to this PR https://github.com/apache/spark/pull/2084;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug Fix in Storage UI,SPARK-3170,12735653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,uncleGen,uncleGen,21/Aug/14 04:34,27/Aug/14 17:33,14/Jul/23 06:26,27/Aug/14 17:33,1.0.0,1.0.2,,,,,,,,,,,,Spark Core,,,,0,,,,,,"current compeleted stage only need to remove its own partitions that are no longer cached. Currently, ""Storage"" in Spark UI may lost some rdds which are cached actually.",,apachespark,pwendell,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 26 07:07:22 UTC 2014,,,,,,,,,,"0|i1z67z:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,,"21/Aug/14 04:47;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2076;;;","23/Aug/14 05:24;pwendell;I think this is actually fairly important to fix if possible.;;;","26/Aug/14 07:07;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2131;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make-distribution.sh failed,SPARK-3169,12735648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,gq,gq,21/Aug/14 03:32,23/Aug/14 04:35,14/Jul/23 06:26,23/Aug/14 04:35,,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"{code}./make-distribution.sh -Pyarn -Phadoop-2.3 -Phive-thriftserver -Phive -Dhadoop.version=2.3.0 
{code}
 =>
{noformat}
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: scala.reflect.internal.Types$TypeError: bad symbolic reference. A signature in TestSuiteBase.class refers to term dstream
in package org.apache.spark.streaming which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling TestSuiteBase.class.

{noformat}",,apachespark,gq,pwendell,syn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 23 04:35:14 UTC 2014,,,,,,,,,,"0|i1z66v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/14 03:47;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2075;;;","21/Aug/14 10:20;srowen;Same as https://issues.apache.org/jira/browse/SPARK-2798 ? it's resolving similar problems in the Flume build.;;;","23/Aug/14 02:17;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2101;;;","23/Aug/14 04:35;pwendell;Issue resolved by pull request 2101
[https://github.com/apache/spark/pull/2101];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port recent spark-submit changes to windows,SPARK-3167,12735609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,pwendell,pwendell,21/Aug/14 00:21,05/Nov/14 10:45,14/Jul/23 06:26,27/Aug/14 06:06,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,,,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 06:07:30 UTC 2014,,,,,,,,,,"0|i1z5y7:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"26/Aug/14 06:37;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2129;;;","27/Aug/14 06:06;pwendell;Issue resolved by pull request 2156
[https://github.com/apache/spark/pull/2156];;;","27/Aug/14 06:07;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2156;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make FlumePollingInputDStream shutdown cleaner,SPARK-3154,12735512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hshreedharan,hshreedharan,20/Aug/14 19:32,08/Dec/14 08:13,14/Jul/23 06:26,27/Aug/14 09:41,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,Currently lot of errors get thrown from Avro IPC layer when the dstream or sink is shutdown. This jira is to clean it up,,apachespark,hshreedharan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 08 08:13:30 UTC 2014,,,,,,,,,,"0|i1z5cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 19:37;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/2065;;;","08/Dec/14 08:13;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DiskStore attempts to map any size BlockId without checking MappedByteBuffer limit,SPARK-3151,12735492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,eyalfa,damonab,damonab,20/Aug/14 18:41,17/Aug/17 01:23,14/Jul/23 06:26,17/Aug/17 01:22,1.0.2,,,,,,,2.3.0,,,,,,Block Manager,Spark Core,,,2,,,,,,"[DiskStore|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/DiskStore.scala] attempts to memory map the block file in {{def getBytes}}.  If the file is larger than 2GB (Integer.MAX_VALUE) as specified by [FileChannel.map|http://docs.oracle.com/javase/7/docs/api/java/nio/channels/FileChannel.html#map%28java.nio.channels.FileChannel.MapMode,%20long,%20long%29], then the memory map fails.

{code}
Some(channel.map(MapMode.READ_ONLY, segment.offset, segment.length)) # line 104
{code}",IBM 64-bit JVM PPC64,apachespark,cloud_fan,damonab,eyalfa,glenn.strycker@gmail.com,holden,jxiang,mathieude,mylesbaker,rdub,smolav,taroleo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1476,,SPARK-6190,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 01:22:48 UTC 2017,,,,,,,,,,"0|i1z587:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/15 21:44;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/4857;;;","05/Jul/17 08:06;eyalfa;I just hit this with spark 2.1 when processing a disk persisted RDD
while the root cause for this is probably data skew, this seems like a severe limitation on spark.
this specific failure is a bit surprising as spark already knows the block size on disk at this point (it maps the entire file from offset 0 to file size), so it should easily be possible to split this into several blocks, after all the code is using ChunkedByteBuffer.
a better approach would be lazily loading the blocks as deserialization progresses , and an even better solution (proposed by [~matei] in [comment|https://issues.apache.org/jira/browse/SPARK-1476?focusedCommentId=13967947&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13967947] for #1476) would be splitting these block in higher levels of spark (a mapper that produces multiple blocks, cached RDD partition that consists of several blocks...)

I can see the attached PR is closed, is there an expected/in-progress fix for this?


2017-07-05 07:04:51,368 UTC	WARN 	task-result-getter-0	org.apache.spark.scheduler.TaskSetManager	 Lost task 131.0 in stage 14.0 (TID 2228, 172.20.1.137, executor 1): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869)
	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103)
	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:465)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:701)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748);;;","08/Aug/17 13:49;eyalfa;created PR 18855: https://github.com/apache/spark/pull/18855;;;","17/Aug/17 01:22;cloud_fan;Issue resolved by pull request 18855
[https://github.com/apache/spark/pull/18855];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in Spark recovery after simultaneous fall of master and driver,SPARK-3150,12735475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tanyatik,tanyatik,20/Aug/14 18:08,28/Aug/14 17:38,14/Jul/23 06:26,28/Aug/14 17:38,1.0.2,,,,,,,1.0.3,1.1.1,,,,,Spark Core,,,,0,,,,,,"The issue happens when Spark is run standalone on a cluster.

When master and driver fall simultaneously on one node in a cluster, master tries to recover its state and restart spark driver.
While restarting driver, it falls with NPE exception (stacktrace is below).
After falling, it restarts and tries to recover its state and restart Spark driver again. It happens over and over in an infinite cycle.

Namely, Spark tries to read DriverInfo state from zookeeper, but after reading it happens to be null in DriverInfo.worker.

Stacktrace (on version 1.0.0, but reproduceable on version 1.0.2, too)

2014-08-14 21:44:59,519] ERROR  (akka.actor.OneForOneStrategy)
java.lang.NullPointerException
        at org.apache.spark.deploy.master.Master$$anonfun$completeRecovery$5.apply(Master.scala:448)
        at org.apache.spark.deploy.master.Master$$anonfun$completeRecovery$5.apply(Master.scala:448)
        at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)
        at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
        at scala.collection.TraversableLike$class.filter(TraversableLike.scala:263)
        at scala.collection.AbstractTraversable.filter(Traversable.scala:105)
        at org.apache.spark.deploy.master.Master.completeRecovery(Master.scala:448)
        at org.apache.spark.deploy.master.Master$$anonfun$receive$1.applyOrElse(Master.scala:376)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

How to reproduce: kill all Spark processes when running Spark standalone on a cluster on some cluster node, where driver runs (kill driver, master and worker simultaneously).", Linux 3.2.0-23-generic x86_64,apachespark,tanyatik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 18:27:11 UTC 2014,,,,,,,,,,"0|i1z54f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 18:27;apachespark;User 'tanyatik' has created a pull request for this issue:
https://github.com/apache/spark/pull/2062;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connection establishment information is not enough.,SPARK-3149,12735449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,sarutak,sarutak,20/Aug/14 16:55,20/Aug/14 21:05,14/Jul/23 06:26,20/Aug/14 21:05,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"When establishing connection, we get following log.

{code}
14/08/18 19:47:40 INFO network.ConnectionManager: Accepted connection from [spark-slave00/10.37.129.4]
14/08/18 19:47:40 INFO network.ConnectionManager: Accepted connection from [spark-slave00/10.37.129.4]
14/08/18 19:47:40 INFO network.SendingConnection: Initiating connection to [spark-slave00/10.37.129.4:48746]
14/08/18 19:47:40 INFO network.SendingConnection: Initiating connection to [spark-slave00/10.37.129.4:49558]
{code}

As you see, when connecting to another host, we have get remote address and port, but when accepting from another host, we have only address.",,apachespark,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 21:05:36 UTC 2014,,,,,,,,,,"0|i1z4z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 17:07;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2060;;;","20/Aug/14 21:05;joshrosen;Issue resolved by pull request 2060
[https://github.com/apache/spark/pull/2060];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"No need to set ""spark.local.dir"" in ExecutorLauncher",SPARK-3144,12735249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,carlmartin,carlmartin,20/Aug/14 02:29,20/Aug/14 17:23,14/Jul/23 06:26,20/Aug/14 17:23,1.0.2,,,,,,,1.1.0,,,,,,YARN,,,,0,,,,,,"ExecutorLauncher only will be lanched in Yarn-Client mode as a simple Application Master.
In this  process, it would not create the spark.local.dir since Spark driver or executor will not be launched in it.
So it may delete such code.
",,apachespark,carlmartin,gq,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 17:23:31 UTC 2014,,,,,,,,,,"0|i1z2wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 02:47;apachespark;User 'hzw19900416' has created a pull request for this issue:
https://github.com/apache/spark/pull/2050;;;","20/Aug/14 08:11;gq;[~carlmartin]
 [PR 2002|https://github.com/apache/spark/pull/2002] has to fix this issue.
;;;","20/Aug/14 17:23;joshrosen;This was fixed in my PR https://github.com/apache/spark/pull/2002 for [SPARK-2974] [SPARK-2975].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sortByKey() break take(),SPARK-3141,12735216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,19/Aug/14 23:41,20/Aug/14 05:44,14/Jul/23 06:26,20/Aug/14 05:44,1.1.0,,,,,,,1.1.0,,,,,,PySpark,,,,0,,,,,,"https://github.com/apache/spark/pull/1898/files#r16449470

I think there might be two unintended side effects of this change. This code used to work in pyspark:

sc.parallelize([5,3,4,2,1]).map(lambda x: (x,x)).sortByKey().take(1)
Now it failswith the error:

File ""<...>/spark/python/pyspark/rdd.py"", line 1023, in takeUpToNumLeft
    yield next(iterator)
TypeError: list object is not an iterator
Changing mapFunc and sort back to generators rather than regular functions fixes that problem.

After making that change, there is a second side effect due to the removal of flatMap where the above code returns the following unexpected result due to the default partitioning scheme:

[[(1, 1), (2, 2)]]
Removing sortByKey, e.g.:

sc.parallelize([5,3,4,2,1]).map(lambda x: (x,x)).take(1)
returns the expected result [(5, 5)]. Restoring the call to flatMap resolves this as well.

",,apachespark,davies,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 05:44:16 UTC 2014,,,,,,,,,,"0|i1z307:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"20/Aug/14 00:01;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2045;;;","20/Aug/14 05:44;pwendell;Issue resolved by pull request 2045
[https://github.com/apache/spark/pull/2045];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark start-up throws confusing exception,SPARK-3140,12735204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,19/Aug/14 22:29,05/Nov/14 10:45,14/Jul/23 06:26,25/Aug/14 19:01,1.0.2,,,,,,,1.1.0,,,,,,PySpark,,,,0,,,,,,"Currently we read the pyspark port through stdout of the spark-submit subprocess. However, if there is stdout interference, e.g. spark-submit echoes something unexpected to stdout, we print the following:

{code}
Exception: Launching GatewayServer failed! (Warning: unexpected output detected.)
{code}

This condition is fine. However, we actually throw the same exception if there is *no* output from the subprocess as well. This is very confusing because it implies that the subprocess is outputting something (possibly whitespace, which is not visible) when it's actually not.",,andrewor14,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2313,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 22:17:16 UTC 2014,,,,,,,,,,"0|i1z2mf:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"20/Aug/14 17:21;joshrosen;Is it the gateway server JVM -> PySpark driver communication that's getting messed up (the step where the Python driver's Java child process launches with some ephemeral port and communicates that port number back to the Python driver)?  Wouldn't that imply that the GatewayServer is has some extra logging to stdout that's being printed before it writes the port number?
;;;","20/Aug/14 18:13;andrewor14;Yes, normally it implies exactly that. What I mean is even when there is no stdout printed at all (i.e. even the port is not printed) it still throws this exception because it tries to read the empty string as an int. For the longest time I was looking for where we print out some whitespace in the code, but really it there was simply no stdout.;;;","20/Aug/14 22:17;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/2067;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Akka timeouts from ContextCleaner when cleaning shuffles,SPARK-3139,12735201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,joshrosen,joshrosen,19/Aug/14 22:19,27/Aug/14 07:18,14/Jul/23 06:26,27/Aug/14 07:18,1.1.0,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"When running spark-perf tests on EC2, I have a job that's consistently logging the following Akka exceptions:

{code}
4/08/19 22:07:12 ERROR spark.ContextCleaner: Error cleaning shuffle 0
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
  at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
  at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
  at scala.concurrent.Await$.result(package.scala:107)
  at org.apache.spark.storage.BlockManagerMaster.removeShuffle(BlockManagerMaster.scala:118)
  at org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:159)
  at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(ContextCleaner.scala:131)
  at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(ContextCleaner.scala:124)
  at scala.Option.foreach(Option.scala:236)
  at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:124)
  at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:120)
  at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:120)
  at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1252)
  at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:119)
  at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
{code}

and

{code}
14/08/19 22:07:12 ERROR storage.BlockManagerMaster: Failed to remove shuffle 0
akka.pattern.AskTimeoutException: Timed out
  at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
  at akka.actor.Scheduler$$anon$11.run(Scheduler.scala:118)
  at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
  at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
  at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:455)
  at akka.actor.LightArrayRevolverScheduler$$anon$12.executeBucket$1(Scheduler.scala:407)
  at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:411)
  at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
  at java.lang.Thread.run(Thread.java:745)
{code}

This doesn't seem to prevent the job from completing successfully, but it's serious issue because it means that resources aren't being cleaned up.  The test script, ScalaAggByKeyInt, runs each test 10 times, and I see the same error after each test, so this seems deterministically reproducible.

I'll look at the executor logs to see if I can find more info there.","10 r3.2xlarge tests on EC2, running the scala-agg-by-key-int spark-perf test against master commit d7e80c2597d4a9cae2e0cb35a86f7889323f4cbb.",andrewor14,apachespark,glenn.strycker@gmail.com,huasanyelao,joshrosen,pwendell,rdub,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3015,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 27 07:18:11 UTC 2014,,,,,,,,,,"0|i1z2mv:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 22:21;andrewor14;These are both caused by akka timing out because of context cleaner;;;","19/Aug/14 22:28;joshrosen;I used pssh + grep to search through the application logs on the workers and I couldn't find any ERRORs or Exceptions (I'm sure that I was searching the right log directories, since other searches return matches).;;;","20/Aug/14 09:02;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/2056;;;","26/Aug/14 18:52;tdas;The error is happening because we made the ContextCleaner block on each deletion event (rdd, broadcast, or shuffle) and shuffles can take a long time to be deleted. Given that the RC for 1.1 is imminent, its better to make a narrower change in the context cleaner - not wait for shuffle cleanups to complete. ;;;","26/Aug/14 18:57;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/2143;;;","27/Aug/14 07:18;pwendell;Issue resolved by pull request 2143
[https://github.com/apache/spark/pull/2143];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sqlContext.parquetFile should be able to take a single file as parameter,SPARK-3138,12735198,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chutium,chutium,19/Aug/14 22:09,27/Aug/14 20:13,14/Jul/23 06:26,27/Aug/14 20:13,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"http://apache-spark-user-list.1001560.n3.nabble.com/sqlContext-parquetFile-path-fails-if-path-is-a-file-but-succeeds-if-a-directory-tp12345.html

to reproduce this issue in spark-shell
{code:java}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext._
import org.apache.hadoop.fs.{FileSystem, Path}

case class TestRDDEntry(key: Int, value: String)

val path = ""/tmp/parquet_test""
sc.parallelize((1 to 100)).map(i => TestRDDEntry(i, s""val_$i"")).coalesce(1).saveAsParquetFile(path)

val fsPath = new Path(path)
val fs: FileSystem = fsPath.getFileSystem(sc.hadoopConfiguration)
val children = fs.listStatus(fsPath).filter(_.getPath.getName.endsWith("".parquet""))

val readFile = sqlContext.parquetFile(path + ""/"" + children(0).getPath.getName)
{code}

it throws exception:

{code}
java.lang.IllegalArgumentException: Expected file:/tmp/parquet_test/part-r-1.parquet for be a directory with Parquet files/metadata
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.readMetaData(ParquetTypes.scala:374)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.readSchemaFromFile(ParquetTypes.scala:414)
        at org.apache.spark.sql.parquet.ParquetRelation.<init>(ParquetRelation.scala:66)
{code}",,apachespark,chutium,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 19 22:41:48 UTC 2014,,,,,,,,,,"0|i1z2n3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/14 22:29;chutium;be careful if someone is working on SPARK-2551, make sure the new change passes test case {code}test(""Read a parquet file instead of a directory""){code};;;","19/Aug/14 22:32;apachespark;User 'chutium' has created a pull request for this issue:
https://github.com/apache/spark/pull/2044;;;","19/Aug/14 22:41;chutium;after this PR, we can pass the full path of a parquet file instead of the path of directory to sqlContext.parquetFile

given a path of a non parquet file, will throw a RuntimeException:

{code}
scala> val readFile = sqlContext.parquetFile(""/tmp/people.txt"")
java.lang.RuntimeException: file:/tmp/people.txt is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [44, 50, 52, 10]
        at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:288)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$3.apply(ParquetTypes.scala:391)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$3.apply(ParquetTypes.scala:391)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not allow negative values in naive Bayes,SPARK-3130,12735102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,19/Aug/14 17:08,20/Aug/14 04:02,14/Jul/23 06:26,20/Aug/14 04:02,1.1.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,because NB treats feature values as term frequencies.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 04:02:47 UTC 2014,,,,,,,,,,"0|i1z1yf:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 17:27;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/2038;;;","20/Aug/14 04:02;mengxr;Issue resolved by pull request 2038
[https://github.com/apache/spark/pull/2038];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modifying Spark SQL related scripts should trigger Spark SQL test suites,SPARK-3127,12735085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,19/Aug/14 15:59,20/Aug/14 20:20,14/Jul/23 06:26,20/Aug/14 20:20,1.0.1,1.0.2,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Currently only modifying files under {{sql/}} triggers execution of Spark SQL test suites, {{bin/spark-sql}} and {{sbin/start-thriftserver.sh}} are not included. This is an indirect cause of SPARK-3126.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 19 16:07:04 UTC 2014,,,,,,,,,,"0|i1z20n:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 16:07;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveThriftServer2Suite hangs,SPARK-3126,12735083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,lian cheng,lian cheng,19/Aug/14 15:55,20/Aug/14 20:03,14/Jul/23 06:26,20/Aug/14 20:03,,,,,,,,1.0.1,1.0.2,,,,,SQL,,,,0,,,,,,"[PR #1851|https://github.com/apache/spark/pull/1851] modified {{sbin/start-thriftserver.sh}}, added proper quotation and removed {{eval}}, but {{HiveThriftServer2Suite}}, which invokes {{sbin/start-thriftserver.sh}}, was not updated accordingly. The JDBC URL command line option shouldn't be quoted after removing {{eval}} from the script, otherwise, the following wrong command will be issued (notice the unclosed double quote):
{code}
../../sbin/start-thriftserver.sh ... --hiveconf javax.jdo.option.ConnectionURL=""xxx ...
{code}
This makes test cases hang until time out.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 19 16:07:03 UTC 2014,,,,,,,,,,"0|i1z1nj:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 16:07;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive thriftserver test suite failure,SPARK-3125,12735080,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,19/Aug/14 15:45,20/Aug/14 20:19,14/Jul/23 06:26,20/Aug/14 20:19,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"hive thriftserver test suite failure
1 CliSuite:
[info] - simple commands *** FAILED ***
[info]   java.lang.AssertionError: assertion failed: Didn't find ""OK"" in the output:
[info]   at scala.Predef$.assert(Predef.scala:179)
[info]   at org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70)
[info]   at org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:26)
[info]   at org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62)
[info]   at org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:26)
[info]   at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply$mcV$sp(CliSuite.scala:54)
[info]   at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:52)
[info]   at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:52)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   ...

2.HiveThriftServer2Suite

- test query execution against a Hive Thrift server *** FAILED ***
[info]   java.sql.SQLException: Could not open connection to jdbc:hive2://localhost:41419/: java.net.ConnectException: Connection refused
[info]   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146)
[info]   at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123)
[info]   at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:571)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:215)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:152)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:155)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:113)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:110)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:110)
[info]   ...
[info]   Cause: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
[info]   at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
[info]   at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
[info]   at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
[info]   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
[info]   at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123)
[info]   at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:571)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:215)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:152)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:155)
[info]   ...
[info]   Cause: java.net.ConnectException: Connection refused
[info]   at java.net.PlainSocketImpl.socketConnect(Native Method)
[info]   at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
[info]   at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
[info]   at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
[info]   at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
[info]   at java.net.Socket.connect(Socket.java:579)
[info]   at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
[info]   at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
[info]   at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
[info]   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
[info]   ...
[info] - SPARK-3004 regression: result set containing NULL *** FAILED ***
[info]   java.sql.SQLException: Could not open connection to jdbc:hive2://localhost:41419/: java.net.ConnectException: Connection refused
[info]   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146)
[info]   at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123)
[info]   at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:571)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:215)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:152)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:155)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$2.apply$mcV$sp(HiveThriftServer2Suite.scala:135)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$2.apply(HiveThriftServer2Suite.scala:132)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$2.apply(HiveThriftServer2Suite.scala:132)
[info]   ...
[info]   Cause: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
[info]   at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
[info]   at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
[info]   at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
[info]   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
[info]   at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123)
[info]   at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:571)
[info]   at java.sql.DriverManager.getConnection(DriverManager.java:215)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:152)
[info]   at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:155)
[info]   ...
[info]   Cause: java.net.ConnectException: Connection refused
[info]   at java.net.PlainSocketImpl.socketConnect(Native Method)
[info]   at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
[info]   at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
[info]   at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
[info]   at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
[info]   at java.net.Socket.connect(Socket.java:579)
[info]   at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
[info]   at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
[info]   at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
[info]   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
[info]   ...
[info] ScalaTest
",,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 19 16:31:23 UTC 2014,,,,,,,,,,"0|i1z1zj:",9223372036854775807,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 16:31;scwf;for clisuite i print the error info, as follows:
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

Logging initialized using configuration in jar:file:/home/wf/code/spark/assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop2.3.0.jar!/hive-log4j.properties
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:301)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:271)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:38)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd$lzycompute(HiveContext.scala:359)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd(HiveContext.scala:359)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:103)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:98)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:291)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:314)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:73)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jar version conflict in the assembly package,SPARK-3124,12735078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,chenghao,chenghao,19/Aug/14 15:34,12/Dec/14 00:23,14/Jul/23 06:26,12/Dec/14 00:23,,,,,,,,,,,,,,SQL,,,,0,,,,,,"Both netty-3.2.2.Final.jar and netty-3.6.6.Final.jar are flatten into the assembly package, however, the class(NioWorker) signature difference leads to the failure in launching sparksql CLI/ThriftServer.",,apachespark,chenghao,chutium,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 00:23:21 UTC 2014,,,,,,,,,,"0|i1z1nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/14 15:40;gq;What's your command?
{{./make-distribution.sh  -Pyarn -Phadoop-2.3 -Phive-thriftserver -Phive -Dhadoop.version=2.3.0}}  should be no problem.;;;","19/Aug/14 15:41;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/2035;;;","19/Aug/14 15:43;chenghao;Can you try ""bin/spark-sql"" after make distribution?;;;","19/Aug/14 15:54;gq;
We should modify the file sql/hive-thriftserver/pom.xml
{code:xml}
    <dependency>
      <groupId>org.spark-project.hive</groupId>
      <artifactId>hive-cli</artifactId>
      <version>${hive.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.jboss.netty</groupId>
          <artifactId>netty</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
{code};;;","19/Aug/14 15:57;chenghao;Yes, actually I did in the PR.;;;","20/Aug/14 18:18;pwendell;Hey [~chenghao] for cases like this, can you actually show where the conflict comes from by running dependency analysis? The current patch you have adds random exclusions to hadoop client and zookeeper. I think the proposal [~gq] has seems like a better idea.

One other thing, I ran the thriftserver just fine with Hadoop 2.3 as recently as yesterday, so I'm a little confused what exact errors you are seeing.;;;","21/Aug/14 01:07;chenghao;Oh, sorry, I should put more logs.

Here is what I did with the latest master:
{panel}
sbt/sbt -Phive -Phive-thriftserver -Dhadoop.version=2.0.0-mr1-cdh4.3.0 assembly
bin/spark-sql --hiveconf hive.root.logger=INFO,console

Spark assembly has been built with Hive, including Datanucleus jars on classpath
log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
14/08/21 08:04:44 WARN common.LogUtils: hive-site.xml not found on CLASSPATH

Logging initialized using configuration in jar:file:/home/hcheng/git/catalyst/assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.3.0.jar!/hive-log4j.properties
14/08/21 08:04:44 INFO SessionState:
Logging initialized using configuration in jar:file:/home/hcheng/git/catalyst/assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.3.0.jar!/hive-log4j.properties
14/08/21 08:04:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/08/21 08:04:44 WARN util.Utils: Your hostname, haocheng-desktop resolves to a loopback address: 127.0.0.1; using 10.239.34.117 instead (on interface eth0)
14/08/21 08:04:44 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
14/08/21 08:04:44 INFO spark.SecurityManager: Changing view acls to: hcheng,
14/08/21 08:04:44 INFO spark.SecurityManager: Changing modify acls to: hcheng,
14/08/21 08:04:44 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hcheng, ); users with modify permissions: Set(hcheng, )
14/08/21 08:04:45 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/08/21 08:04:45 INFO Remoting: Starting remoting
14/08/21 08:04:45 ERROR actor.ActorSystemImpl: Uncaught fatal error from thread [spark-akka.actor.default-dispatcher-4] shutting down ActorSystem [spark]
java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function
        at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282)
        at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:78)
        at scala.util.Try$.apply(Try.scala:161)
        at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:73)
        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84)
        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84)
        at scala.util.Success.flatMap(Try.scala:200)
        at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:84)
        at akka.remote.EndpointManager$$anonfun$8.apply(Remoting.scala:618)
        at akka.remote.EndpointManager$$anonfun$8.apply(Remoting.scala:610)
        at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
        at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens(Remoting.scala:610)
        at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse(Remoting.scala:450)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/08/21 08:04:45 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
...
{panel}

And I noticed that there are 2 netty jars (netty-3.2.2.Final.jar & netty3.6.6.Final.jar) under the folder lib_managed/bundle. After applying the [#PR2035|https://github.com/apache/spark/pull/2035], the netty-3.2.2.Final.jar will be evicted, and everything works fine. 

I also tried to assembly like 
{panel}
sbt/sbt -Phive -Phive-thriftserver -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 clean assembly
{panel}
The netty-3.2.2.Final.jar also was evicted, seems the maven dependency exclusion rule has been well configured for hadoop 2.3.;;;","23/Aug/14 22:41;chutium;Hi all, i got same error:
{noformat}
sbt/sbt -Dhadoop.version=1.0.3-mapr-3.0.3 -Phive assembly
bin/spark-shell
{noformat}
then error kommt
{noformat}
14/08/23 23:29:46 INFO SecurityManager: Changing view acls to: client09,
14/08/23 23:29:46 INFO SecurityManager: Changing modify acls to: client09,
14/08/23 23:29:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(client09, ); users with modify permissions: Set(client09, )
14/08/23 23:29:50 INFO Slf4jLogger: Slf4jLogger started
14/08/23 23:29:50 INFO Remoting: Starting remoting
14/08/23 23:29:50 ERROR ActorSystemImpl: Uncaught fatal error from thread [spark-akka.actor.default-dispatcher-2] shutting down ActorSystem [spark]
java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function
        at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282)
        at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:78)
        at scala.util.Try$.apply(Try.scala:161)
        at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:73)
        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84)
        at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84)
        at scala.util.Success.flatMap(Try.scala:200)
        at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:84)
        at akka.remote.EndpointManager$$anonfun$8.apply(Remoting.scala:618)
        at akka.remote.EndpointManager$$anonfun$8.apply(Remoting.scala:610)
        at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
        at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens(Remoting.scala:610)
        at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse(Remoting.scala:450)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/08/23 23:29:50 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/08/23 23:29:50 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/08/23 23:29:51 INFO Remoting: Remoting shut down
14/08/23 23:29:51 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
[ERROR] [08/23/2014 23:30:00.548] [main] [Remoting] Remoting error: [Startup timed out] [
akka.remote.RemoteTransportException: Startup timed out
        at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)
        at akka.remote.Remoting.start(Remoting.scala:191)
        at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
        at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
        at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
        at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
        at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
        at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
        at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121)
        at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54)
        at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53)
        at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
        at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
        at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:56)
        at org.apache.spark.SparkEnv$.create(SparkEnv.scala:150)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:203)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:972)
        at $line3.$read$$iwC$$iwC.<init>(<console>:8)
        at $line3.$read$$iwC.<init>(<console>:14)
        at $line3.$read.<init>(<console>:16)
        at $line3.$read$.<init>(<console>:20)
        at $line3.$read$.<clinit>(<console>)
        at $line3.$eval$.<init>(<console>:7)
        at $line3.$eval$.<clinit>(<console>)
        at $line3.$eval.$print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:859)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:771)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:121)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:120)
        at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:264)
        at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:120)
        at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:931)
        at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:142)
        at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:104)
        at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:948)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:997)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:317)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:73)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at akka.remote.Remoting.start(Remoting.scala:173)
        ... 61 more
]
java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at akka.remote.Remoting.start(Remoting.scala:173)
        at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
        at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
        at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
        at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
        at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
        at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
        at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121)
        at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54)
        at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53)
        at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
        at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1442)
        at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:56)
        at org.apache.spark.SparkEnv$.create(SparkEnv.scala:150)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:203)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:972)
        at $iwC$$iwC.<init>(<console>:8)
        at $iwC.<init>(<console>:14)
        at <init>(<console>:16)
        at .<init>(<console>:20)
        at .<clinit>(<console>)
        at .<init>(<console>:7)
        at .<clinit>(<console>)
        at $print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:859)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:771)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:121)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:120)
        at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:264)
        at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:120)
        at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:931)
        at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:142)
        at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:104)
        at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:56)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:948)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:997)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:317)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:73)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Spark context available as sc.

scala> Stopping spark context.
<console>:11: error: not found: value sc
              sc.stop()
              ^
{noformat}

i merged this patch, then it works again, so i think this PR should be good.

Thanks;;;","12/Dec/14 00:23;srowen;I'm a little unclear on the outcome, but in master, running {{mvn  -Phive -Phive-thriftserver -Dhadoop.version=2.0.0-mr1-cdh4.3.0 dependency:tree}} says there is no Netty 3.2.2 anymore. So it looks fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong implementation of implicit bytesWritableConverter,SPARK-3121,12735000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,dubovsky,dubovsky,19/Aug/14 10:18,13/Oct/14 05:04,14/Jul/23 06:26,13/Oct/14 05:04,1.0.2,1.1.0,1.2.0,,,,,1.0.3,1.1.1,1.2.0,,,,Spark Core,,,,0,,,,,,"val path = ... //path to seq file with BytesWritable as type of both key and value
val file = sc.sequenceFile[Array[Byte],Array[Byte]](path)
file.take(1)(0)._1

This prints incorrect content of byte array. Actual content starts with correct one and some ""random"" bytes and zeros are appended. BytesWritable has two methods:

getBytes() - return content of all internal array which is often longer then actual value stored. It usually contains the rest of previous longer values

copyBytes() - return just begining of internal array determined by internal length property

It looks like in implicit conversion between BytesWritable and Array[byte] getBytes is used instead of correct copyBytes.
",,apachespark,dubovsky,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412940,,,Mon Oct 13 05:04:44 UTC 2014,,,,,,,,,,"0|i1z1an:",412926,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/14 14:01;apachespark;User 'james64' has created a pull request for this issue:
https://github.com/apache/spark/pull/2712;;;","08/Oct/14 14:37;dubovsky;I am working on an issue;;;","13/Oct/14 05:04;joshrosen;Issue resolved by pull request 2712
[https://github.com/apache/spark/pull/2712];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDFS broken in Spark SQL,SPARK-3114,12734903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,19/Aug/14 00:34,19/Aug/14 03:43,14/Jul/23 06:26,19/Aug/14 03:43,1.1.0,,,,,,,1.1.0,,,,,,PySpark,SQL,,,0,,,,,,"Python UDFs were inadvertently broken in SparkSQL by the PySpark broadcast-optimization commit:

{code}
**********************************************************************
File ""/Users/joshrosen/Documents/Spark/python/pyspark/sql.py"", line 975, in pyspark.sql.SQLContext.registerFunction
Failed example:
    sqlCtx.sql(""SELECT twoArgs('test', 1)"").collect()
Exception raised:
    Traceback (most recent call last):
      File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/doctest.py"", line 1253, in __run
        compileflags, 1) in test.globs
      File ""<doctest pyspark.sql.SQLContext.registerFunction[5]>"", line 1, in <module>
        sqlCtx.sql(""SELECT twoArgs('test', 1)"").collect()
      File ""/Users/joshrosen/Documents/Spark/python/pyspark/sql.py"", line 1615, in collect
        rows = RDD.collect(self)
      File ""/Users/joshrosen/Documents/Spark/python/pyspark/rdd.py"", line 725, in collect
        bytesInJava = self._jrdd.collect().iterator()
      File ""/Users/joshrosen/Documents/Spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
        self.target_id, self.name)
      File ""/Users/joshrosen/Documents/Spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
        format(target_id, '.', name), value)
    Py4JJavaError: An error occurred while calling o607.collect.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 60.0 failed 1 times, most recent failure: Lost task 0.0 in stage 60.0 (TID 141, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
      File ""pyspark/worker.py"", line 75, in main
        command = ser._read_with_length(infile)
      File ""pyspark/serializers.py"", line 150, in _read_with_length
        return self.loads(obj)
      File ""pyspark/serializers.py"", line 420, in loads
        return self.serializer.loads(zlib.decompress(obj))
    error: Error -3 while decompressing data: incorrect header check

            org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)
            org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)
            org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:87)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
            org.apache.spark.sql.SchemaRDD.compute(SchemaRDD.scala:115)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
            org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
            org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
            org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
            org.apache.spark.scheduler.Task.run(Task.scala:54)
            org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
            java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
            java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
            java.lang.Thread.run(Thread.java:745)
    Driver stacktrace:
    	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
    	at scala.Option.foreach(Option.scala:236)
    	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
    	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

The zlib compression was introduced in a recent commit for improving PySpark’s broadcast variable performance (https://github.com/apache/spark/pull/1912).  It looks like the worker is expecting to receive a zlib-compressed command, but somehow is receiving something else.  

It looks like the code that registers Python UDFs doesn’t perform this compression, leading to this issue:

{code}
        self._ssql_ctx.registerPython(name,
                                      bytearray(CloudPickleSerializer().dumps(command)),
                                      env,
                                      includes,
                                      self._sc.pythonExec,
                                      self._sc._javaAccumulator,
                                      str(returnType))
{code}

The root problem here is that the SparkSQL Python tests weren't run by Jenkins.  I think the problem is that PySpark’s SparkSQL tests are skipped unless _RUN_SQL_TESTS is true, and this is variable is only set when we detect changes to SparkSQL.  Instead, it should always be set when running the PySpark tests.",,apachespark,joshrosen,qiaohaijun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412843,,,Tue Aug 19 02:02:02 UTC 2014,,,,,,,,,,"0|i1z0pz:",412829,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 00:52;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2026;;;","19/Aug/14 02:02;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2027;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix UTF8 encoding in PySpark saveAsTextFile().,SPARK-3103,12734798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,joshrosen,joshrosen,18/Aug/14 17:26,18/Aug/14 20:59,14/Jul/23 06:26,18/Aug/14 20:59,1.0.2,1.1.0,,,,,,1.1.0,,,,,,PySpark,,,,0,starter,,,,,"This is a follow-up JIRA for https://github.com/apache/spark/pull/1914, where Ahir and Davies identified a bug in Python JsonRDD when trying to encode non-ASCII strings into unicode.

The same underlying issue affects saveAsTextFile, so we should apply the same fix there, too, and search for any other code that needs to be updated (and maybe refactor this out into a utility function).",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412738,,,Mon Aug 18 18:01:54 UTC 2014,,,,,,,,,,"0|i1z03z:",412724,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"18/Aug/14 18:01;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing volatile annotation in ApplicationMaster,SPARK-3101,12734781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,18/Aug/14 15:39,27/Sep/14 19:09,14/Jul/23 06:26,27/Sep/14 19:09,1.1.0,,,,,,,,,,,,,YARN,,,,0,,,,,,"In ApplicationMaster, a field variable 'isLastAMRetry' is used as a flag but it's not declared as volatile though it's used from multiple threads.",,apachespark,sarutak,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412721,,,Sat Sep 27 19:09:20 UTC 2014,,,,,,,,,,"0|i1z007:",412707,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/14 15:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2016;;;","22/Aug/14 21:03;vanzin;I covered this in the PR for SPARK-2933 also.;;;","27/Sep/14 19:09;srowen;This was actually subsumed by the commit for SPARK-2933;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Avoid not stopping SparkContext with YARN Client mode,SPARK-3090,12734657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,sarutak,sarutak,17/Aug/14 19:21,27/Apr/15 23:47,14/Jul/23 06:26,27/Apr/15 23:47,1.1.0,,,,,,,1.4.0,,,,,,Spark Core,YARN,,,0,,,,,,"When we use YARN Cluster mode, ApplicationMaser register a shutdown hook, stopping SparkContext.
Thanks to this, SparkContext can stop even if Application forgets to stop SparkContext itself.

But, unfortunately, YARN Client mode doesn't have such mechanism.",,apachespark,dougb,sarutak,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412597,,,Mon Apr 27 23:47:19 UTC 2015,,,,,,,,,,"0|i1yz9b:",412584,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/14 19:31;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2001;;;","22/Aug/14 21:08;vanzin;I think that if we want to add this, it would be better to do so for all modes, not just yarn-client. Basically have SparkContext itself register a shutdown hook to shut itself down, and publish the priority of the hook so that apps / backends can register hooks that run before it (see Hadoop's ShutdownHookManager for the priority thing - http://goo.gl/BQ1bjk).

That way the code in the yarn-cluster backend can be removed too.;;;","01/Sep/14 05:20;sarutak;It sound good idea that SparkContext register shutdown-hook itself.;;;","24/Apr/15 23:50;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5696;;;","27/Apr/15 23:47;srowen;Resolved by https://github.com/apache/spark/pull/5696;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix meaningless error message in ConnectionManager,SPARK-3089,12734652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,17/Aug/14 18:02,19/Aug/14 17:16,14/Jul/23 06:26,19/Aug/14 17:16,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"When ConnectionManager#removeConnection is invoked and it cannot find SendingConnection to be closed corresponding to a ConnectionManagerId, following message is logged.

{code}
logError(""Corresponding SendingConnectionManagerId not found"")
{code}

But, we cannot get which SendingConnectionManagerId is meant from the message.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412592,,,Sun Aug 17 18:06:58 UTC 2014,,,,,,,,,,"0|i1yz87:",412579,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/14 18:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2000;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChiSqTest only stores results in the first 100 columns.,SPARK-3087,12734626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,17/Aug/14 04:26,18/Aug/14 03:53,14/Jul/23 06:26,18/Aug/14 03:53,,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,due to an indexing error in the implementation.,,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412566,,,Mon Aug 18 03:53:40 UTC 2014,,,,,,,,,,"0|i1yz2f:",412553,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"18/Aug/14 03:53;mengxr;Issue resolved by pull request 1997
[https://github.com/apache/spark/pull/1997];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collect broadcasted tables in parallel in joins,SPARK-3084,12734623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,matei,17/Aug/14 01:54,12/Sep/14 08:22,14/Jul/23 06:26,18/Aug/14 17:06,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"BroadcastHashJoin has a broadcastFuture variable that tries to collect the broadcasted table in a separate thread, but this doesn't help because it's a lazy val that only gets initialized when you attempt to build the RDD. Thus queries that broadcast multiple tables would collect and broadcast them sequentially.",,apachespark,matei,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412563,,,Fri Sep 12 08:22:36 UTC 2014,,,,,,,,,,"0|i1yz1r:",412550,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"17/Aug/14 02:12;apachespark;User 'mateiz' has created a pull request for this issue:
https://github.com/apache/spark/pull/1990;;;","12/Sep/14 08:22;rxin;Note that the current fix actually launches jobs immediately in explain ... we should fix that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn.Client.logClusterResourceDetails throws NPE if requested queue doesn't exist,SPARK-3082,12734579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sandyr,sandyr,sandyr,16/Aug/14 07:51,05/Sep/14 22:24,14/Jul/23 06:26,05/Sep/14 22:24,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,,,apachespark,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412519,,,Sat Aug 16 08:06:46 UTC 2014,,,,,,,,,,"0|i1yyrz:",412506,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/14 08:06;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/1984;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make LRWithLBFGS API consistent with others,SPARK-3078,12734535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,15/Aug/14 22:17,16/Aug/14 04:05,14/Jul/23 06:26,16/Aug/14 04:05,1.1.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,Should ask users to use optimizer to set parameters.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412475,,,Sat Aug 16 04:05:07 UTC 2014,,,,,,,,,,"0|i1yyif:",412462,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Aug/14 22:21;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1973;;;","16/Aug/14 04:05;mengxr;Issue resolved by pull request 1973
[https://github.com/apache/spark/pull/1973];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChiSqTest bugs,SPARK-3077,12734529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,dorx,dorx,15/Aug/14 22:08,17/Aug/14 04:16,14/Jul/23 06:26,17/Aug/14 04:16,,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,"- promote nullHypothesis field in ChiSqTestResult to TestResult. Every test should have a null hypothesis
- Correct null hypothesis statement for independence test
- line 59 in TestResult: 0.05 -> 0.5",,apachespark,dorx,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412469,,,Sun Aug 17 04:16:50 UTC 2014,,,,,,,,,,"0|i1yyh3:",412456,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/14 04:36;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1982;;;","17/Aug/14 04:16;mengxr;Issue resolved by pull request 1982
[https://github.com/apache/spark/pull/1982];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn AM not always properly exiting after unregistering from RM,SPARK-3072,12734439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,tgraves,15/Aug/14 15:50,19/Aug/14 14:41,14/Jul/23 06:26,19/Aug/14 14:41,1.0.2,,,,,,,1.1.0,,,,,,YARN,,,,0,,,,,,"The yarn application master doesn't always exit properly after unregistering from the RM.  

One way to reproduce is to ask for large containers (> 4g) but use jdk32 so that all of them fail.",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412379,,,Mon Aug 18 21:21:54 UTC 2014,,,,,,,,,,"0|i1yxx3:",412366,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Aug/14 16:08;tgraves;Note that in yarn-cluster mode the client side does quit, but the application master is still running on the cluster.;;;","18/Aug/14 19:24;tgraves;Note that this also isn't an issue on hadoop 0.23 because there it shoots the containers after unregister.  In hadoop 2.x it doesn't shoot them as aggressively to give the application time to cleanup .;;;","18/Aug/14 21:21;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/2022;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobProgressPage could not show Fair Scheduler Pools section sometimes,SPARK-3067,12734367,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yantangzhai,yantangzhai,yantangzhai,15/Aug/14 08:55,20/Oct/14 15:31,14/Jul/23 06:26,17/Oct/14 02:30,1.1.0,,,,,,,1.2.0,,,,,,Spark Core,Web UI,,,0,,,,,,"JobProgressPage could not show Fair Scheduler Pools section sometimes.
SparkContext starts webui and then postEnvironmentUpdate. Sometimes JobProgressPage is accessed between webui starting and postEnvironmentUpdate, then the lazy val isFairScheduler will be false. The Fair Scheduler Pools section will not display any more.",,apachespark,joshrosen,yantangzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4010,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412307,,,Tue Sep 16 22:30:50 UTC 2014,,,,,,,,,,"0|i1yxgv:",412294,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Aug/14 09:11;apachespark;User 'YanTangZhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1966;;;","16/Sep/14 22:30;joshrosen;Do you think SPARK-1208 sounds related to this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Locale setting to HiveCompatibilitySuite,SPARK-3065,12734360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,luogankun,luogankun,15/Aug/14 07:24,27/Aug/14 22:09,14/Jul/23 06:26,27/Aug/14 22:09,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Run the udf_unix_timestamp of org.apache.spark.sql.hive.execution.HiveCompatibilitySuite testcase
with not ""America/Los_Angeles"" TimeZone throws： 

[info] - udf_unix_timestamp *** FAILED ***
[info]   Results do not match for udf_unix_timestamp:
[info]   SELECT
[info]     '2009 Mar 20 11:30:01 am',
[info]     unix_timestamp('2009 Mar 20 11:30:01 am', 'yyyy MMM dd h:mm:ss a')
[info]   FROM oneline
[info]   == Logical Plan ==
[info]   Project [2009 Mar 20 11:30:01 am AS c_0#25,HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp(2009 Mar 20 11:30:01 am,yyyy MMM dd h:mm:ss a) AS c_1#26L]
[info]    MetastoreRelation default, oneline, None
[info]   
[info]   == Optimized Logical Plan ==
[info]   Project [2009 Mar 20 11:30:01 am AS c_0#25,HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp(2009 Mar 20 11:30:01 am,yyyy MMM dd h:mm:ss a) AS c_1#26L]
[info]    MetastoreRelation default, oneline, None
[info]   
[info]   == Physical Plan ==
[info]   Project [2009 Mar 20 11:30:01 am AS c_0#25,HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp(2009 Mar 20 11:30:01 am,yyyy MMM dd h:mm:ss a) AS c_1#26L]
[info]    HiveTableScan [], (MetastoreRelation default, oneline, None), None
[info]   
[info]   Code Generation: false
[info]   == RDD ==
[info]   (2) MappedRDD[37] at map at HiveContext.scala:350
[info]     MapPartitionsRDD[36] at mapPartitions at basicOperators.scala:42
[info]     MapPartitionsRDD[35] at mapPartitions at TableReader.scala:112
[info]     MappedRDD[34] at map at TableReader.scala:240
[info]     HadoopRDD[33] at HadoopRDD at TableReader.scala:230
[info]   c_0    c_1
[info]   !== HIVE - 1 row(s) ==                == CATALYST - 1 row(s) ==
[info]   !2009 Mar 20 11:30:01 am       1237573801   2009 Mar 20 11:30:01 am    NULL (HiveComparisonTest.scala:367)",CentOS release 6.3 (Final),apachespark,luogankun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412300,,,Thu Aug 21 17:22:22 UTC 2014,,,,,,,,,,"0|i1yxfb:",412287,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/14 09:51;apachespark;User 'luogankun' has created a pull request for this issue:
https://github.com/apache/spark/pull/1968;;;","21/Aug/14 17:22;apachespark;User 'byF' has created a pull request for this issue:
https://github.com/apache/spark/pull/2084;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExistingRdd should convert Map to catalyst Map.,SPARK-3063,12734354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,15/Aug/14 06:47,26/Aug/14 22:05,14/Jul/23 06:26,26/Aug/14 22:05,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Currently {{ExistingRdd.convertToCatalyst}} doesn't convert {{Map}} value.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412294,,,Fri Aug 15 06:56:44 UTC 2014,,,,,,,,,,"0|i1yxdz:",412281,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/14 06:56;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1963;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShutdownHookManager is only available in Hadoop 2.x,SPARK-3062,12734349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,lian cheng,lian cheng,15/Aug/14 06:27,20/Aug/14 20:28,14/Jul/23 06:26,20/Aug/14 20:28,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"PR [#1891|https://github.com/apache/spark/pull/1891] leverages {{ShutdownHookManager}} to avoid {{IOException}} when {{EventLogging}} is enabled. But unfortunately {{ShutdownHookManager}} is only available in Hadoop 2.x. Compilation fails when building Spark with Hadoop 1.

{code}
$ ./sbt/sbt -Phive-thriftserver
...
[ERROR] /home/spark/software/source/compile/spark/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala:30: object ShutdownHookManager is not a member of package org.apache.hadoop.util
[ERROR] import org.apache.hadoop.util.ShutdownHookManager
[ERROR]        ^
[ERROR] /home/spark/software/source/compile/spark/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala:125: not found: value ShutdownHookManager
[ERROR]     ShutdownHookManager.get.addShutdownHook(
[ERROR]     ^‍
[WARNING] one warning found
[ERROR] two errors found‍
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2970,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412289,,,Fri Aug 15 17:21:58 UTC 2014,,,,,,,,,,"0|i1yxcv:",412276,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Aug/14 17:21;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1970;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven build fails in Windows OS,SPARK-3061,12734344,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,tsudukim,tsudukim,15/Aug/14 05:29,09/Sep/14 22:27,14/Jul/23 06:26,09/Sep/14 22:26,1.0.2,1.1.0,,,,,,1.1.1,1.2.0,,,,,Build,,,,0,,,,,,"Maven build fails in Windows OS with this error message.

{noformat}
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (default) on project spark-core_2.10: Command execution failed. Cannot run program ""unzip"" (in directory ""C:\path\to\gitofspark\python""): CreateProcess error=2, w肳ꂽt@ -> [Help 1]
{noformat}",Windows,andrewor14,apachespark,joshrosen,pwendell,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412284,,,Tue Sep 09 22:27:03 UTC 2014,,,,,,,,,,"0|i1yxbr:",412271,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"15/Aug/14 05:30;tsudukim;This is because pom.xml in spark core executes unzip command.

{code:xml}
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.2.1</version>
        <executions>
          <execution>
            <phase>generate-resources</phase>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>unzip</executable>
          <workingDirectory>../python</workingDirectory>
          <arguments>
            <argument>-o</argument>
            <argument>lib/py4j*.zip</argument>
            <argument>-d</argument>
            <argument>build</argument>
          </arguments>
        </configuration>
      </plugin>
{code};;;","15/Aug/14 06:01;pwendell;At this time, I'm not sure we intend to support building Spark on Windows.;;;","15/Aug/14 08:29;srowen;At least, you almost certainly need to use Cygwin on Windows. It provides unzip. So I think the OP is not even doing that.;;;","19/Aug/14 04:51;tsudukim;Thank you for your comments.
Yes it might work on Cygwin, but as you know Cygwin is not always acceptable for enterprise systems.;;;","25/Aug/14 23:50;joshrosen;Maybe we can use a Maven plugin to unzip?  http://stackoverflow.com/questions/3264064/unpack-zip-in-zip-with-maven;;;","27/Aug/14 20:32;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2165;;;","02/Sep/14 17:44;andrewor14;Here's a friendly reminder for myself to back port this into branch-1.1 after the release. Others please disregard this comment. :);;;","04/Sep/14 18:23;joshrosen;Re-assigning to Andrew, who's going to backport it.;;;","09/Sep/14 22:27;andrewor14;Ok, backported. Thanks Josh.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell.cmd doesn't accept application options in Windows OS,SPARK-3060,12734341,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsudukim,tsudukim,tsudukim,15/Aug/14 05:11,20/Dec/14 03:23,14/Jul/23 06:26,20/Dec/14 03:23,1.0.2,,,,,,,1.3.0,,,,,,Windows,,,,0,,,,,,"spark-shell.cmd accepts submit options ([SPARK-3006]).
But we have no way to specify the appliaction options with spark-shell.cmd.

This problem is only for the Windows OS.",Windows,apachespark,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3006,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412281,,,Tue Nov 18 23:35:24 UTC 2014,,,,,,,,,,"0|i1yxb3:",412268,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"18/Nov/14 23:35;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/3350;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set executor's class loader as the default serializer class loader,SPARK-3046,12734234,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,14/Aug/14 19:37,16/Aug/14 22:00,14/Jul/23 06:26,16/Aug/14 00:52,,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"This is an attempt to fix the problem outlined in SPARK-2878.
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412174,,,Fri Aug 15 17:51:51 UTC 2014,,,,,,,,,,"0|i1ywof:",412163,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Aug/14 19:41;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1948;;;","15/Aug/14 17:51;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1972;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecisionTree: isSampleValid indexing incorrect,SPARK-3041,12734120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,14/Aug/14 18:12,15/Aug/14 21:56,14/Jul/23 06:26,15/Aug/14 21:56,1.1.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,"In DecisionTree, isSampleValid treats unordered categorical features incorrectly: It treated the bins as if indexed by featured values, rather than by subsets of values/categories.
This bug is exhibited for unordered features (multi-class classification with categorical features of low arity).
Proposed fix: Index bins correctly for unordered categorical features.
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412147,,,Thu Aug 14 20:16:47 UTC 2014,,,,,,,,,,"0|i1ywif:",412136,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Aug/14 20:16;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1950;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark assembly for new hadoop API (hadoop 2) contains avro-mapred for hadoop 1 API,SPARK-3039,12734060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,bbossy,bbossy,bbossy,14/Aug/14 13:34,08/Feb/15 13:32,14/Jul/23 06:26,08/Feb/15 10:36,0.9.1,1.0.0,1.1.0,1.2.0,,,,1.3.0,,,,,,Build,Input/Output,Spark Core,,4,,,,,,"The spark assembly contains the artifact ""org.apache.avro:avro-mapred"" as a dependency of ""org.spark-project.hive:hive-serde"".

The avro-mapred package provides a hadoop FileInputFormat to read and write avro files. There are two versions of this package, distinguished by a classifier. avro-mapred for the new Hadoop API uses the classifier ""hadoop2"". avro-mapred for the old Hadoop API uses no classifier.

E.g. when reading avro files using 
{code}
sc.newAPIHadoopFile[AvroKey[SomeClass]],NullWritable,AvroKeyInputFormat[SomeClass]](""hdfs://path/to/file.avro"")
{code}

The following error occurs:
{code}
java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
        at org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyInputFormat.java:47)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.Task.run(Task.scala:51)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

This error usually is a hint that there was a mix up of the old and the new Hadoop API. As a work-around, if avro-mapred for hadoop2 is ""forced"" to appear before the version that is bundled with Spark, reading avro files works fine. 

Also, if Spark is built using avro-mapred for hadoop2, it works fine as well.



","hadoop2, hadoop-2.4.0, HDP-2.1",apachespark,bbossy,derrickburns,erwaman,medale,pwendell,theclaymethod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3965,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412088,,,Sun Feb 08 10:36:02 UTC 2015,,,,,,,,,,"0|i1yw5r:",412078,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"14/Aug/14 13:57;apachespark;User 'bbossy' has created a pull request for this issue:
https://github.com/apache/spark/pull/1945;;;","15/Aug/14 09:55;bbossy;Also need to update the README: See SPARK-3069 and https://github.com/apache/spark/pull/1945;;;","15/Sep/14 04:12;pwendell;Resolved by:
https://github.com/apache/spark/pull/1945;;;","13/Nov/14 08:23;bbossy;Needs more fixes, since although I got the build to work for hadoop-2.4, yarn and hive with the sbt build. It doesn't work with the maven build, which AFAIK is required for pySpark. Dependency management seems to be quite different in sbt and maven;;;","08/Dec/14 09:09;derrickburns;I get the same bug when attempting to save a RDD as a parquet file when using Hadoop 1.0.4;;;","08/Dec/14 14:08;bbossy;@[~derrickburns]: Can you post some more info, such as spark version/distribution used, etc..? Spark's build system has received some updates. I still have to verify this, but AFAIK, these kind of issues should not be present in future releases.

Also: have a careful look at the stack trace:
if you have {code}Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected{code} then avro-mapred for hadoop1 was found, but avro-mapred for hadoop2 was expected. However, if you have
{code}Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected{code} or something similar, it's the other way round
;;;","08/Dec/14 18:23;derrickburns;Running in local mode. Spark 1.1.1/Hadoop 1.0.4 built using maven with:

{code:xml}
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.10</artifactId>
        <version>1.1.1</version>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.10</artifactId>
        <version>1.1.1</version>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-mllib_2.10</artifactId>
        <version>1.1.1</version>
      </dependency>
{code}

The code is rather trivial:

{code}
    val sc = new SparkContext(sparkConf)
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    val tweets = sqlContext.jsonFile(path).cache()
    tweets.saveAsParquetFile(""tweets.parquet"")
{code}

Here is the stack trace:

{code}
java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected
	at org.apache.spark.sql.parquet.AppendingParquetOutputFormat.getDefaultWorkFile(ParquetTableOperations.scala:334)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:251)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:300)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/12/08 10:21:06 ERROR executor.ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected
	at org.apache.spark.sql.parquet.AppendingParquetOutputFormat.getDefaultWorkFile(ParquetTableOperations.scala:334)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:251)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:300)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

{code}
;;;","27/Dec/14 07:53;pwendell;This fix did appear in Spark 1.2.0 so I'm closing this issue.;;;","02/Feb/15 19:50;medale;For me, Spark 1.2.0 either downloading spark-1.2.0-bin-hadoop2.4.tgz or
compiling the source with 

{code}
mvn -Pyarn -Phadoop-2.4 -Phive-0.13.1 -DskipTests clean package
{code}

still had the same problem:

{noformat}
java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
	at org.apache.avro.mapreduce.AvroRecordReaderBase.initialize(AvroRecordReaderBase.java:87)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:135)

{noformat}

Starting the build with a clean .m2/repository, the repository afterwards contained:

* avro-mapred/1.7.5 (with the default jar - i.e. hadoop1)
* avro-mapred/1.7.6 with the avro-mapred-1.7.6-hadoop2.jar (the one we want). 

Seemed that sharding these two dependencies into the spark-assembly-jar resulted in the error above
at least in the downloaded hadoop2.4 spark bin and my own build.

Running the following (after doing a mvn install and by-hand copy of all the spark artifacts into my local repo for spark-repl/yarn):

{code}
 mvn -Pyarn -Phadoop-2.4 -Phive -DskipTests dependency:tree -Dincludes=org.apache.avro:avro-mapred
{code}

Showed that the culprit was in the Hive project, namely org.spark-project.hive:hive-exec's
dependency on 1.7.5.

{noformat}
Building Spark Project Hive 1.2.0
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.4:tree (default-cli) @ spark-hive_2.10 ---
[INFO] org.apache.spark:spark-hive_2.10:jar:1.2.0
[INFO] +- org.spark-project.hive:hive-exec:jar:0.13.1a:compile
[INFO] |  \- org.apache.avro:avro-mapred:jar:1.7.5:compile
[INFO] \- org.apache.avro:avro-mapred:jar:hadoop2:1.7.6:compile
[INFO]
{noformat}

Editing spark-1.2.0/sql/hive/pom.xml and excluding avro-mapred from hive-exec,
then recompile, fixed the problem and the resulting dist works well against
Avro/Hadoop2 code:

{code:xml}
    <dependency>
      <groupId>org.spark-project.hive</groupId>
      <artifactId>hive-exec</artifactId>
      <version>${hive.version}</version>
      <exclusions>
        <exclusion>
          <groupId>commons-logging</groupId>
          <artifactId>commons-logging</artifactId>
        </exclusion>
        <exclusion>
          <groupId>com.esotericsoftware.kryo</groupId>
          <artifactId>kryo</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.apache.avro</groupId>
          <artifactId>avro-mapred</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
{code}
   
Just the last exclusion added. For version 1.2.1-rc2 the change is here: https://github.com/medale/spark/compare/apache:v1.2.1-rc2...medale:avro-hadoop2-v1.2.1-rc2;;;","02/Feb/15 22:05;apachespark;User 'medale' has created a pull request for this issue:
https://github.com/apache/spark/pull/4315;;;","03/Feb/15 03:21;srowen;Sounds like there's a lingering version of this issue from the Hive dependency.;;;","04/Feb/15 11:22;bbossy;hmm.. Is there a way to test the contents of the assembly jar, or what jars get packaged? I fear that this will come up again..;;;","04/Feb/15 11:42;srowen;[~bbossy] See the good analysis and additional fix in the PR above: https://github.com/apache/spark/pull/4315  ;;;","04/Feb/15 11:49;medale;The Aapche Maven Enforcer plugin has a dependency convergence rule that could be added to ensure that dependencies/transitive dependencies don't clash. Maybe this could be added to a ""deploy"" profile to be checked (initially set fail to false until all dependency convergence errors are fixed) during release builds initially. See https://issues.apache.org/jira/browse/SPARK-5584.

For the spark-1.3.0-SNAPSHOT, looks like the fix for ""[SPARK-4048] Enhance and extend hadoop-provided profile"" introduced a new scope attribute in the Maven build:

{code}
grep -R hive.deps.scope *
assembly/pom.xml: <hive.deps.scope>provided</hive.deps.scope>
examples/pom.xml: <hive.deps.scope>provided</hive.deps.scope>
pom.xml:    <hive.deps.scope>compile</hive.deps.scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
pom.xml:        <scope>${hive.deps.scope}</scope>
{code}

avro-mapred and hive-exec are marked with that scope so neither library nor their dependencies will be included in the spark assembly jar. This means that Spark jobs that want to use Avro-based InputFormat from avro-mapred have to include their desired avro-mapred version in their jars. So this particular problem will be gone but still need to prevent this class of problems by ensuring dependency convergence.;;;","04/Feb/15 11:53;srowen;I think Spark's dependency tree is already well too complex to actually avoid conflicts. It doesn't converge, but manages to work. This is indeed a big source of complexity and problems. I'd love to whittle down the extent and number of permutations to support but this is a story for a little later.;;;","08/Feb/15 10:36;srowen;Issue resolved by pull request 4315
[https://github.com/apache/spark/pull/4315];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ArrayType containing null value support to Parquet.,SPARK-3037,12734021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ueshin,ueshin,ueshin,14/Aug/14 10:17,27/Aug/14 01:30,14/Jul/23 06:26,27/Aug/14 01:30,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Parquet support should handle {{ArrayType}} when {{containsNull}} is {{true}}.

When {{containsNull}} is {{true}}, the schema should be as follows:

{noformat}
message root {
  optional group a (LIST) {
    repeated group bag {
      optional int32 array_element;
    }
  }
}
{noformat}

FYI:
Hive's Parquet writer *always* uses this schema, and reader can read only from this schema, i.e. current Parquet support of SparkSQL is not compatible with Hive.

NOTICE:
If Hive compatiblity is top priority, we also have to use this schma regardless of {{containsNull}}, which will break backward compatibility.
But using this schema could affect performance.
",,apachespark,marmbrus,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412049,,,Thu Aug 21 09:31:03 UTC 2014,,,,,,,,,,"0|i1yvx3:",412038,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 08:52;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2032;;;","20/Aug/14 20:50;marmbrus;[~ueshin], thanks for investigating these issues.  Here are my thoughts:

 - As far as I understand, this does not result in any breaking changes (i.e. newer versions of Spark SQL are unable to read data written by older versions). However, it does add new functionality. (i.e. older versions of Spark SQL will not be able to read data written by newer versions).  Please correct me if I'm wrong here.
 - Hive compatibility is important, but I think its also important for us to do the 'right' thing and push Hive to fix their implementation if necessary.  Also, since Hive cannot read data written by us now, we are not introducing a regression.

Based on this, here is what I think is the most reasonable thing to do.  Update our code to use the ""bag"" approach when containsNull is true and just use repeated fields when containsNull is false.  Create a JIRA with Hive asking them to support reading data in both formats.  What do you think?;;;","21/Aug/14 09:31;ueshin;I agree with it. It was completely the same as what I wanted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add MapType containing null value support to Parquet.,SPARK-3036,12734020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ueshin,ueshin,ueshin,14/Aug/14 10:17,27/Aug/14 01:30,14/Jul/23 06:26,27/Aug/14 01:30,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Current Parquet schema for {{MapType}} is as follows regardless of {{valueContainsNull}}:

{noformat}
message root {
  optional group a (MAP) {
    repeated group map (MAP_KEY_VALUE) {
      required int32 key;
      required int32 value;
    }
  }
}
{noformat}

and if the map contains {{null}} value, it throws runtime exception.

To handle {{MapType}} containing {{null}} value, the schema should be as follows if {{valueContainsNull}} is {{true}}:

{noformat}
message root {
  optional group a (MAP) {
    repeated group map (MAP_KEY_VALUE) {
      required int32 key;
      optional int32 value;
    }
  }
}
{noformat}

FYI:
Hive's Parquet writer *always* uses the latter schema, but reader can read from both schema.

NOTICE:
This change will break backward compatibility when the schema is read from Parquet metadata ({{""org.apache.spark.sql.parquet.row.metadata""}}).
",,apachespark,marmbrus,rrusso2007,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2721,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412048,,,Thu Aug 21 04:10:18 UTC 2014,,,,,,,,,,"0|i1yvwv:",412037,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Aug/14 08:51;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/2032;;;","20/Aug/14 20:53;marmbrus;Can you explain more about what you mean when you say we are breaking backwards compatibility?  It seems like newer version of Spark SQL should always be able to read data written by older version as long as we support both versions.  Choosing between them when writing based on valueContainsNull seems like the best solution.

I think it is okay (though undesirable) for older versions of Spark SQL to be unable to read from data written by newer versions, as this is unavoidable as we add features.;;;","21/Aug/14 04:10;ueshin;Ah, that's right. It was my mistake.
Newer version will be able to read data written by older version.

We are referencing to DataType tree to build converter tree and the converter tree needs to be the same as the Parquet schema, so I thought the difference between them like older version's Parquet schema and DatType from metadata causes the incompatible, but the converter tree is the same regardless of ""require"" or ""optional"" of map value, i.e. valueContainsNull.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Hive] java.math.BigDecimal cannot be cast to org.apache.hadoop.hive.common.type.HiveDecimal,SPARK-3033,12734006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,pengyanhong,pengyanhong,14/Aug/14 08:52,06/Aug/15 23:42,14/Jul/23 06:26,06/Aug/15 23:42,1.0.2,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"run a complex HiveQL via yarn-cluster, got error as below:
{quote}
14/08/14 15:05:24 WARN org.apache.spark.Logging$class.logWarning(Logging.scala:70): Loss was due to java.lang.ClassCastException
java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.hadoop.hive.common.type.HiveDecimal
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.getPrimitiveJavaObject(JavaHiveDecimalObjectInspector.java:51)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getHiveDecimal(PrimitiveObjectInspectorUtils.java:1022)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$HiveDecimalConverter.convert(PrimitiveObjectInspectorConverter.java:306)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ReturnObjectInspectorResolver.convertIfNecessary(GenericUDFUtils.java:179)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.evaluate(GenericUDFIf.java:82)
	at org.apache.spark.sql.hive.HiveGenericUdf.eval(hiveUdfs.scala:276)
	at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:84)
	at org.apache.spark.sql.catalyst.expressions.MutableProjection.apply(Projection.scala:62)
	at org.apache.spark.sql.catalyst.expressions.MutableProjection.apply(Projection.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.sql.execution.BroadcastNestedLoopJoin$$anonfun$4.apply(joins.scala:309)
	at org.apache.spark.sql.execution.BroadcastNestedLoopJoin$$anonfun$4.apply(joins.scala:303)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}
",,apachespark,davies,marmbrus,pengyanhong,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412034,,,Thu Aug 06 23:42:18 UTC 2015,,,,,,,,,,"0|i1yvtr:",412023,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/14 19:37;marmbrus;Can you provide the query?;;;","21/Aug/14 02:13;pengyanhong;There are expressions In the Select clause of query as below:{quote}
        IF(
            d.lost_order_amt IS NULL ,
            0.0 ,
            d.lost_order_amt
        ) AS lost_order_amt
{quote}
;;;","22/Aug/14 06:51;pengyanhong;I changed the file {quote}
sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveUdfs.scala
{quote}
changed the method ""eval"" in the class ""HiveGenericUdf"" as below:
{code}
    while (i < children.length) {
      val idx = i
      deferedObjects(i).asInstanceOf[DeferredObjectAdapter].set(() => {
        children(idx).eval(input)
      })
      if (deferedObjects(i).get().isInstanceOf[java.math.BigDecimal] == true) {
        val decimal = deferedObjects(i).get().asInstanceOf[java.math.BigDecimal]
        deferedObjects(i).asInstanceOf[DeferredObjectAdapter].set(() => {
          new org.apache.hadoop.hive.common.`type`.HiveDecimal(decimal).asInstanceOf[EvaluatedType]
        })
      }
      i += 1
    }
{code}
also, changed the method ""wrap"" in the trait ""HiveInspectors"", add line:{code}
case b: org.apache.hadoop.hive.common.`type`.HiveDecimal => b
{code}

So this issue has been fixed.
;;;","04/Nov/14 15:21;apachespark;User 'pengyanhong' has created a pull request for this issue:
https://github.com/apache/spark/pull/3090;;;","06/Aug/15 23:30;davies;[~yhuai] Could you help to confirm this bug is still in 1.5 or not, thanks!;;;","06/Aug/15 23:42;yhuai;I tested it with hive udaf stddev on decimal columns. It works. Based on the code history, I believe it is fixed by https://issues.apache.org/jira/browse/SPARK-4573.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential bug when running sort-based shuffle with sorting using TimSort,SPARK-3032,12733995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jerryshao,jerryshao,jerryshao,14/Aug/14 07:56,17/May/20 18:30,14/Jul/23 06:26,29/Sep/14 18:26,1.1.0,,,,,,,1.1.1,1.2.0,,,,,Shuffle,Spark Core,,,1,,,,,,"When using SparkPerf's aggregate-by-key workload to test sort-based shuffle, data type for key and value is (String, String), always meet this issue:

{noformat}
java.lang.IllegalArgumentException: Comparison method violates its general contract!
        at org.apache.spark.util.collection.Sorter$SortState.mergeLo(Sorter.java:755)
        at org.apache.spark.util.collection.Sorter$SortState.mergeAt(Sorter.java:493)
        at org.apache.spark.util.collection.Sorter$SortState.mergeCollapse(Sorter.java:420)
        at org.apache.spark.util.collection.Sorter$SortState.access$200(Sorter.java:294)
        at org.apache.spark.util.collection.Sorter.sort(Sorter.java:128)
        at org.apache.spark.util.collection.SizeTrackingPairBuffer.destructiveSortedIterator(SizeTrackingPairBuffer.scala:83)
        at org.apache.spark.util.collection.ExternalSorter.spillToMergeableFile(ExternalSorter.scala:323)
        at org.apache.spark.util.collection.ExternalSorter.spill(ExternalSorter.scala:271)
        at org.apache.spark.util.collection.ExternalSorter.maybeSpill(ExternalSorter.scala:249)
        at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:220)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:85)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{noformat}

Seems the current partitionKeyComparator which use hashcode of String as key comparator break some sorting contracts. 

Also I tested using data type Int as key, this is OK to pass the test, since hashcode of Int is its self. So I think potentially partitionDiff + hashcode of String may break the sorting contracts.",,aash,apachespark,ilikerps,jerryshao,LiuZeshan,matei,mengxr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3656,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412023,,,Mon Sep 29 18:26:58 UTC 2014,,,,,,,,,,"0|i1yvrj:",412012,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"22/Sep/14 20:49;ilikerps;[~matei] any thoughts on this issue?;;;","23/Sep/14 02:59;matei;I'm not completely sure that this is because hashCode provides a partial ordering, because I believe TimSort is supposed to work on partial orderings as well. I believe the problem is an integer over flow when we subtract key1.hashCode - key2.hashCode. Can you try replacing the line that returns h1 - h2 in keyComparator with returning Integer.compare(h1, h2)? This will properly deal with overflow.

Returning h1 - h2 is definitely wrong: for example suppose that h1 = Int.MaxValue and h2 = Int.MinValue, then h1 - h2 = -1.

Please add a unit test for this case as well.;;;","23/Sep/14 03:00;matei;Yeah actually I'm sure TimSort works fine with a partial ordering, I read through the contract of Comparable. We also use it that way all the time when we only sort by partition ID.;;;","23/Sep/14 03:15;jerryshao;Hi Matei, thanks for your reply, I will try again using your comments.;;;","24/Sep/14 04:50;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/2514;;;","25/Sep/14 22:35;aash;This bug prevents people from doing further testing of sort-based shuffle on the rest of the 1.1.x series.  Is this a good candidate for a backport to 1.1.1 or a later 1.1 hotfix ?;;;","29/Sep/14 18:26;matei;Yup, this will appear in 1.1.1. I've merged it into branch-1.1 already if you want to try it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sparkEventToJson should support SparkListenerExecutorMetricsUpdate,SPARK-3028,12733971,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,rxin,rxin,14/Aug/14 06:30,15/Aug/14 18:36,14/Jul/23 06:26,15/Aug/14 18:35,,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"SparkListenerExecutorMetricsUpdate was added without updating org.apache.spark.util.JsonProtocol.sparkEventToJson.

This can crash the listener.

",,andrewor14,apachespark,pwendell,rxin,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411999,,,Fri Aug 15 18:35:49 UTC 2014,,,,,,,,,,"0|i1yvmf:",411988,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Aug/14 06:31;rxin;[~sandyr] [~sandyryza] Would you be able to do this quickly (tmr maybe)?

If not, somebody else should fix this ...

cc [~andrewor14];;;","14/Aug/14 07:16;pwendell;I think we currently do not intend to ever serialize an update event as JSON.

However, I think we should be more explicit about this in two ways.

1. In EventLoggingListener we should explicit put a no-op implementation of onExecutorMetricsUpdate with a comment - I actually thought one version of the PR had this, but maybe I'm misremembering.

2. We should also include the SparkListenerExecutorMetricsUpdate in the match block and just have a no-op there also with a comment.
;;;","14/Aug/14 07:22;rxin;Ok in that case, you suggestion makes sense;;;","14/Aug/14 16:48;sandyr;+1 to what Patrick said.  I'll post a patch along those lines.;;;","15/Aug/14 04:21;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/1961;;;","15/Aug/14 18:35;pwendell;Issue resolved by pull request 1961
[https://github.com/apache/spark/pull/1961];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a good error message if JDBC server is used but Spark is not compiled with -Pthriftserver,SPARK-3026,12733962,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,pwendell,pwendell,14/Aug/14 04:39,28/Aug/14 07:20,14/Jul/23 06:26,28/Aug/14 07:20,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Instead of giving a ClassNotFoundException we should detect this case and just tell the user to build with -Phiveserver.,,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411990,,,Thu Aug 14 13:37:23 UTC 2014,,,,,,,,,,"0|i1yvkf:",411979,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Aug/14 13:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1944;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow JDBC clients to set a fair scheduler pool,SPARK-3025,12733961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,14/Aug/14 04:38,18/Aug/14 17:52,14/Jul/23 06:26,18/Aug/14 17:52,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411989,,,Thu Aug 14 05:11:57 UTC 2014,,,,,,,,,,"0|i1yvk7:",411978,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Aug/14 05:11;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/1937;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print completed indices rather than tasks in web UI,SPARK-3020,12733922,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,13/Aug/14 23:48,14/Aug/14 01:08,14/Jul/23 06:26,14/Aug/14 01:08,,,,,,,,1.1.0,,,,,,Web UI,,,,0,,,,,,"When speculation is used, it's confusing to print the number of completed tasks, since it can exceed the number of total tasks. Instead we should just report the number of unique indices that are completed.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411950,,,Wed Aug 13 23:53:28 UTC 2014,,,,,,,,,,"0|i1yvbj:",411939,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"13/Aug/14 23:53;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/1933;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing broadcast in quick successions causes Akka timeout,SPARK-3015,12733890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor14,13/Aug/14 21:55,15/Apr/19 07:31,14/Jul/23 06:26,16/Aug/14 05:56,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"This issue is originally reported in SPARK-2916 in the context of MLLib, but we were able to reproduce it using a simple Spark shell command:

{code}
(1 to 10000).foreach { i => sc.parallelize(1 to 1000, 48).sum }
{code}

We still do not have a full understanding of the issue, but we have gleaned the following information so far. When the driver runs a GC, it attempts to clean up all the broadcast blocks that go out of scope at once. This causes the driver to send out many blocking RemoveBroadcast messages to the executors, which in turn send out blocking UpdateBlockInfo messages back to the driver. Both of these calls block until they receive the expected responses. We suspect that the high frequency at which we send these blocking messages is the cause of either dropped messages or internal deadlock somewhere.

Unfortunately, it is highly difficult to reproduce depending on the environment. We have been able to reproduce it on a 6-node cluster in us-west-2, but not in us-west-1, for instance.",Standalone EC2 Spark shell,andrewor14,apachespark,gq,huasanyelao,pwendell,rdub,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2916,,,,,,,,,,SPARK-3139,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411918,,,Wed Aug 20 08:16:08 UTC 2014,,,,,,,,,,"0|i1yv4n:",411908,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"13/Aug/14 22:42;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1931;;;","16/Aug/14 05:56;pwendell;Issue resolved by pull request 1931
[https://github.com/apache/spark/pull/1931];;;","16/Aug/14 05:57;pwendell;[~andrewor] I changed ""Affects version/s"" to 1.1.0 instead of 1.0.2 because I don't think this issue was ever seen in Spark 1.0.2. Is that correct?;;;","18/Aug/14 17:41;andrewor14;Sure, makes sense.;;;","20/Aug/14 08:16;gq;In version 1.0.1, I also found the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doctest of inferSchema in Spark SQL Python API fails,SPARK-3013,12733818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,lian cheng,lian cheng,13/Aug/14 16:31,13/Aug/14 23:53,14/Jul/23 06:26,13/Aug/14 23:53,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Doctest of `inferSchema` in `sql.py` keeps failing and makes Jenkins crazy:

{code}
File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/sql.py"", line 1021, in pyspark.sql.SQLContext.inferSchema
Failed example:
    srdd.collect()
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib64/python2.6/doctest.py"", line 1253, in __run
        compileflags, 1) in test.globs
      File ""<doctest pyspark.sql.SQLContext.inferSchema[6]>"", line 1, in <module>
        srdd.collect()
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/sql.py"", line 1613, in collect
        rows = RDD.collect(self)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/rdd.py"", line 724, in collect
        bytesInJava = self._jrdd.collect().iterator()
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
        self.target_id, self.name)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
        format(target_id, '.', name), value)
    Py4JJavaError: An error occurred while calling o399.collect.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 35.0 failed 1 times, most recent failure: Lost task 1.0 in stage 35.0 (TID 72, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to java.util.ArrayList
            net.razorvine.pickle.objects.ArrayConstructor.construct(ArrayConstructor.java:33)
            net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
            net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
            net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
            net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
            org.apache.spark.api.python.PythonRDD$$anonfun$pythonToJavaArray$1$$anonfun$apply$4.apply(PythonRDD.scala:722)
            org.apache.spark.api.python.PythonRDD$$anonfun$pythonToJavaArray$1$$anonfun$apply$4.apply(PythonRDD.scala:721)
            scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
            scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
            scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
            scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
            scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
            scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:966)
            scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)
            scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
            scala.collection.Iterator$class.foreach(Iterator.scala:727)
            scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
            scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
            scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
            scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
            scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
            scala.collection.AbstractIterator.to(Iterator.scala:1157)
            scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
            scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
            scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
            scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
            org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774)
            org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774)
            org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
            org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
            org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
            org.apache.spark.scheduler.Task.run(Task.scala:54)
            org.apache.spark.executSLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
or.Executor$TaskRunner.run(Executor.scala:199)
            java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
            java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
            java.lang.Thread.run(Thread.java:745)
    Driver stacktrace:
    	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
    	at scala.Option.foreach(Option.scala:236)
    	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
    	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

One of the failed builds can be found [here|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18437/consoleFull].",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2951,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411846,,,Wed Aug 13 19:02:24 UTC 2014,,,,,,,,,,"0|i1yup3:",411837,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"13/Aug/14 19:02;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1928;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
_temporary directory should be filtered out by sqlContext.parquetFile,SPARK-3011,12733809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joesu,joesu,13/Aug/14 15:51,15/Aug/14 00:36,14/Jul/23 06:26,14/Aug/14 18:06,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Sometimes _temporary directory is not removed after the file committed on S3. sqlContext.parquetFile will raise because it is trying to read the metadata in  _temporary .sqlContext.parquetFile should just ignore the directory.  ,,apachespark,joesu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411837,,,Thu Aug 14 20:02:00 UTC 2014,,,,,,,,,,"0|i1yun3:",411828,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/14 15:54;srowen;Duplicate, or very closely related: https://issues.apache.org/jira/browse/SPARK-2700;;;","13/Aug/14 15:57;joesu;Pull request is here: https://github.com/apache/spark/pull/1924

SPARK-2700 did not filter out temp dir. ;;;","13/Aug/14 15:57;apachespark;User 'joesu' has created a pull request for this issue:
https://github.com/apache/spark/pull/1924;;;","14/Aug/14 20:02;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1949;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationInfo doesn't get initialised after deserialisation during recovery,SPARK-3009,12733757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jlewandowski,jlewandowski,13/Aug/14 12:58,25/Nov/14 12:14,14/Jul/23 06:26,25/Nov/14 12:14,1.0.1,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"The {{readObject}} method has been removed from {{ApplicationInfo}} so that it does not initialise its transient fields properly after deserialisation. It follows throwing NPE during recovery of an application in {{MetricSystem.registerSource}}. As [~andrewor14] said, he removed {{readObject}} method by accident. 
",,apachespark,jlewandowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411785,,,Tue Nov 25 12:14:25 UTC 2014,,,,,,,,,,"0|i1yuc7:",411776,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/14 13:52;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/1922;;;","13/Aug/14 13:57;jlewandowski;[~andrewor14] could you review it please?;;;","14/Aug/14 17:42;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/1947;;;","25/Nov/14 12:14;srowen;Looks like https://github.com/apache/spark/pull/1947 was merged and resolved this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to execute spark-shell in Windows OS,SPARK-3006,12733708,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,tsudukim,tsudukim,13/Aug/14 08:19,26/Aug/14 14:12,14/Jul/23 06:26,15/Aug/14 05:21,,,,,,,,1.1.0,,,,,,Windows,,,,0,,,,,,"when execute {{bin\spark-shell.cmd}} in Windows OS, I got errors like folloings:
{noformat}
Error: Cannot load main class from JAR: spark-shell
Run with --help for usage help or --verbose for debug output
{noformat}",Windows 8.1,apachespark,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3060,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411736,,,Tue Aug 26 14:12:24 UTC 2014,,,,,,,,,,"0|i1yu1j:",411727,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/14 08:44;tsudukim;This is because the option {{--class org.apache.spark.repl.Main}} follows argument {{spark-shell}}. Arguments should follow options. bash version of spark-shell is correct.;;;","13/Aug/14 08:47;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/1918;;;","15/Aug/14 05:16;tsudukim;The PR above has merged so now we can pass the ""submit"" options.
But we don't still have the way to pass the ""application"" options.
This problem is handled by another ticket [SPARK-3060].;;;","26/Aug/14 14:12;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2136;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark with Mesos fine-grained mode throws UnsupportedOperationException in MesosSchedulerBackend.killTask(),SPARK-3005,12733707,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xuzhongxing,xuzhongxing,13/Aug/14 08:18,08/Oct/14 01:32,14/Jul/23 06:26,08/Oct/14 01:32,1.0.2,,,,,,,,,,,,,Spark Core,,,,2,,,,,,"I am using Spark, Mesos, spark-cassandra-connector to do some work on a cassandra cluster.

During the job running, I killed the Cassandra daemon to simulate some failure cases. This results in task failures.

If I run the job in Mesos coarse-grained mode, the spark driver program throws an exception and shutdown cleanly.

But when I run the job in Mesos fine-grained mode, the spark driver program hangs.

The spark log is: 

{code}
 INFO [spark-akka.actor.default-dispatcher-4] 2014-08-13 15:58:15,794 Logging.scala (line 58) Cancelling stage 1
 INFO [spark-akka.actor.default-dispatcher-4] 2014-08-13 15:58:15,797 Logging.scala (line 79) Could not cancel tasks for stage 1
java.lang.UnsupportedOperationException
	at org.apache.spark.scheduler.SchedulerBackend$class.killTask(SchedulerBackend.scala:32)
	at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend.killTask(MesosSchedulerBackend.scala:41)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:185)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:183)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:183)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:183)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:176)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:176)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1075)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1061)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1061)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1061)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1031)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1031)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:635)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:635)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1234)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}","Spark 1.0.2, Mesos 0.18.1, spark-cassandra-connector",apachespark,mdaniel,mvherweg,pwendell,tstclair,xuzhongxing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/14 03:59;qqsun8819;SPARK-3005_1.diff;https://issues.apache.org/jira/secure/attachment/12661627/SPARK-3005_1.diff",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411735,,,Wed Oct 08 01:32:44 UTC 2014,,,,,,,,,,"0|i1yu1b:",411726,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,,,,,,"13/Aug/14 08:48;xuzhongxing;Some additional driver logs during the spark driver hang:
{code}
TRACE [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,908 Logging.scala (line 66) Checking for newly runnable parent stages
 INFO [Result resolver thread-1] 2014-08-13 15:58:15,908 Logging.scala (line 58) Removed TaskSet 1.0, whose tasks have all completed, from pool 
TRACE [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,909 Logging.scala (line 66) running: Set(Stage 1, Stage 2)
TRACE [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,909 Logging.scala (line 66) waiting: Set(Stage 0)
TRACE [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,909 Logging.scala (line 66) failed: Set()
DEBUG [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,909 Logging.scala (line 62) submitStage(Stage 0)
DEBUG [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,910 Logging.scala (line 62) missing: List(Stage 1, Stage 2)
DEBUG [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,910 Logging.scala (line 62) submitStage(Stage 1)
DEBUG [spark-akka.actor.default-dispatcher-2] 2014-08-13 15:58:15,910 Logging.scala (line 62) submitStage(Stage 2)
TRACE [spark-akka.actor.default-dispatcher-3] 2014-08-13 15:58:56,643 Logging.scala (line 66) Checking for hosts with no recent heart beats in BlockManagerMaster.
TRACE [spark-akka.actor.default-dispatcher-6] 2014-08-13 15:59:56,653 Logging.scala (line 66) Checking for hosts with no recent heart beats in BlockManagerMaster.
TRACE [spark-akka.actor.default-dispatcher-2] 2014-08-13 16:00:56,652 Logging.scala (line 66) Checking for hosts with no recent heart beats in BlockManagerMaster.
{code};;;","14/Aug/14 03:59;qqsun8819;a quick fix for fine grained killTask
;;;","14/Aug/14 04:20;xuzhongxing;Could adding an empty killTask method to  MesosSchedulerBackend fix this problem?
{code:title=MesosSchedulerBackend.scala}
override def killTask(taskId: Long, executorId: String, interruptThread: Boolean) {}
{code}
This works for my tests.;;;","14/Aug/14 08:07;apachespark;User 'xuzhongxing' has created a pull request for this issue:
https://github.com/apache/spark/pull/1940;;;","24/Aug/14 17:05;pwendell;Hey There - the message you are seeing does not itself indicate a failure. It is just saying that Mesos does not support cancellation (see SPARK-1749). The exact reason for the hanging is unclear.;;;","25/Aug/14 02:02;xuzhongxing;I was using spark to process data from Cassandra. And when Cassandra is under heavy load, the executors on the slaves throws timeout exception, and the tasks fail. Then the driver need to cancel the job.

In coarse-grained mode, it works fine. The coarse-grained backend has the killTask() method.

But in fine-grained mode, the backend didn't override the killTask() method. So it throws exception.

I could tune Cassandra to avoid read timeout. But I just want spark to exit when the job fails. Currently it just throws that operation-not-supported exception and hangs there. This is unacceptable behaviour.;;;","25/Aug/14 02:16;xuzhongxing;[SPARK-1749] didn't fix this problem. It just catches the UnsupportedOperationException and logs it. Then it sets ableToCancelStages = false. This is exactly the reason that causes the hang. Because the code only does cleanup when ableToCancelStages = true.
{code}
+ if (ableToCancelStages) {
+   job.listener.jobFailed(error)
+   cleanupStateForJobAndIndependentStages(job, resultStage)
+   listenerBus.post(SparkListenerJobEnd(job.jobId, JobFailed(error)))
{code}
The fact is that in the mesos fine-grained case, it is unnecessary to killTask(). So throwing UnsupportedOperationException and set ableToCancelStages = false is wrong behaviour for this case. We just need to do nothing in killTask() and let the driver do the rest of the cleanup.

The problem here is in the MesosSchedulerBackend. The MesosSchedulerBackend does not need to kill tasks, and should not throw UnsupportedOperationException. The tasks themselves already died and exited.
;;;","02/Sep/14 05:36;pwendell;I think it might be safe to just define the semantics of cancelTask to be ""best effort"" and have the default implementation be empty. From what I can tell the DAGScheduler is already resilient to a task finishing after the stage has been cleaned up (for other reasons I think it needs be resilient to this). So maybe we should just remove this entire ableToCancelStages logic in here and make it simpler. /cc [~kayousterhout] and [~markhamstra] who worked on this code.

Hey [~xuzhongxing] what do you mean that the ""tasks themselves already died and exited""? The code here is designed to cancel outstanding tasks that are still running. For instance, I have a job that has 500 tasks running. Then there is a failure of one task multiple time so I need to fail the stage, but there are still many tasks running. I think those tasks need to be killed still. Otherwise you have zombie tasks running. I think in mesos fine-grained mode we just won't be able to support this feature... but we should make it so it doesn't hang.
;;;","02/Sep/14 09:56;xuzhongxing;By ""tasks themselves already died and exited"", I mean that even if we do nothing in killTasks(), there won't be any zombie tasks left on the slaves. This is what I get from testing the Mesos fine-grained mode. If I'm wrong, please correct me. But the logic here is incomplete or inconsistent, and needs to be fixed.;;;","08/Oct/14 01:32;xuzhongxing;Resolved in https://github.com/apache/spark/pull/2453;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveThriftServer2 throws exception when the result set contains NULL,SPARK-3004,12733699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,13/Aug/14 07:02,13/Aug/14 23:51,14/Jul/23 06:26,13/Aug/14 23:51,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"To reproduce this issue with beeline:

{code}
$ cd $SPARK_HOME
$ ./bin/beeline -u jdbc:hive2://localhost:10000 -n lian
...
0: jdbc:hive2://localhost:10000> create table src1 (key int, value string);
...
0: jdbc:hive2://localhost:10000> load data local inpath './sql/hive/src/test/resources/data/files/kv3.txt' into table src1;
...
0: jdbc:hive2://localhost:10000> select * from src1 where key is null;
Error:  (state=,code=0)
{code}

Exception thrown from HiveThriftServer2:

{code}
java.lang.RuntimeException: Failed to check null bit for primitive int value.
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(Row.scala:145)
        at org.apache.spark.sql.hive.thriftserver.server.SparkSQLOperationManager$$anon$1.getNextRowSet(SparkSQLOperationManager.scala:80)
        at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:170)
        at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:417)
        at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:306)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:386)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1373)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1358)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:58)
        at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:55)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:526)
        at org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:55)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

The cause is that we didn't check {{isNullAt}} in {{SparkSQLOperationManager.getNextRowSet}}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411727,,,Wed Aug 13 09:17:41 UTC 2014,,,,,,,,,,"0|i1ytzj:",411718,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"13/Aug/14 09:17;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1920;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Hive UDFs that take arrays of structs as arguments,SPARK-2994,12733639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,13/Aug/14 00:20,14/Aug/14 00:36,14/Jul/23 06:26,14/Aug/14 00:36,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411667,,,Wed Aug 13 02:27:27 UTC 2014,,,,,,,,,,"0|i1ytm7:",411658,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"13/Aug/14 02:27;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1915;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
setting properties seems not effective,SPARK-2986,12733426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,guowei2,guowei2,12/Aug/14 09:12,14/Aug/14 00:46,14/Jul/23 06:26,14/Aug/14 00:46,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"setting properties like ""set spark.sql.shuffle.partitions=100"" seems not effective。
",,apachespark,guowei2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411454,,,Tue Aug 12 09:27:13 UTC 2014,,,,,,,,,,"0|i1ysc7:",411445,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/14 09:27;apachespark;User 'guowei2' has created a pull request for this issue:
https://github.com/apache/spark/pull/1904;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException on _temporary directory,SPARK-2984,12733399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,aash,aash,12/Aug/14 06:08,07/Jan/20 18:55,14/Jul/23 06:26,15/Apr/15 17:59,1.1.0,,,,,,,1.3.0,,,,,,Spark Core,,,,3,,,,,,"We've seen several stacktraces and threads on the user mailing list where people are having issues with a {{FileNotFoundException}} stemming from an HDFS path containing {{_temporary}}.

I ([~aash]) think this may be related to {{spark.speculation}}.  I think the error condition might manifest in this circumstance:

1) task T starts on a executor E1
2) it takes a long time, so task T' is started on another executor E2
3) T finishes in E1 so moves its data from {{_temporary}} to the final destination and deletes the {{_temporary}} directory during cleanup
4) T' finishes in E2 and attempts to move its data from {{_temporary}}, but those files no longer exist!  exception

Some samples:

{noformat}
14/08/11 08:05:08 ERROR JobScheduler: Error running job streaming job 1407744300000 ms.0
java.io.FileNotFoundException: File hdfs://hadoopc/user/csong/output/human_bot/-1407744300000.out/_temporary/0/task_201408110805_0000_m_000007 does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:654)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:712)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:708)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:708)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:360)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:310)
        at org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)
        at org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:126)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:841)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:724)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:643)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1068)
        at org.apache.spark.streaming.dstream.DStream$$anonfun$8.apply(DStream.scala:773)
        at org.apache.spark.streaming.dstream.DStream$$anonfun$8.apply(DStream.scala:771)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:41)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.streaming.scheduler.Job.run(Job.scala:32)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:172)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
-- Chen Song at http://apache-spark-user-list.1001560.n3.nabble.com/saveAsTextFiles-file-not-found-exception-td10686.html



{noformat}
I am running a Spark Streaming job that uses saveAsTextFiles to save results into hdfs files. However, it has an exception after 20 batches

result-1406312340000/_temporary/0/task_201407251119_0000_m_000003 does not exist.
{noformat}
and
{noformat}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /apps/data/vddil/real-time/checkpoint/temp: File does not exist. Holder DFSClient_NONMAPREDUCE_327993456_13 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2946)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2766)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor146.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1439)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1261)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
14/07/25 14:45:12 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://gnosis-01-01-01.crl.samsung.com/apps/data/vddil/real-time/checkpoint/checkpoint-1406324700000
{noformat}
-- Bill Jay at http://apache-spark-user-list.1001560.n3.nabble.com/saveAsTextFiles-file-not-found-exception-td10686.html




{noformat}
scala> d3.sample(false,0.01,1).map( pair => pair._2 ).saveAsTextFile(""10000.txt"")


14/06/09 22:06:40 ERROR TaskSetManager: Task 0.0:0 failed 4 times; aborting job
org.apache.spark.SparkException: Job aborted: Task 0.0:0 failed 4 times (most recent failure: Exception failure: java.io.IOException: The temporary job-output directory file:/data/spark-0.9.1-bin-hadoop1/10000.txt/_temporary doesn't exist!)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}
-- Oleg Proudnikov at http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201406.mbox/%3CCAFEaGwKYSXz2Gyviqu44-DvP-V1JzbVFAPd1x5dLHCQBOgTnjg@mail.gmail.com%3E




{noformat}
[INFO] 11 Dec 2013 12:00:33 - org.apache.spark.Logging$class - Loss was due to org.apache.hadoop.util.Shell$ExitCodeException
org.apache.hadoop.util.Shell$ExitCodeException: chmod: getting attributes of `/cygdrive/c/somepath/_temporary/_attempt_201312111200_0000_m_000000_0/part-00000': No such file or directory
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)
        at org.apache.hadoop.util.Shell.run(Shell.java:188)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:467)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:450)
        at org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:593)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:584)
        at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:427)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:465)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:781)
        at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:118)
        at org.apache.hadoop.mapred.SparkHadoopWriter.open(SparkHadoopWriter.scala:86)
        at org.apache.spark.rdd.PairRDDFunctions.writeToFile$1(PairRDDFunctions.scala:667)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:680)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:680)
        at org.apache.spark.scheduler.ResultTask.run(ResultTask.scala:99)
        at org.apache.spark.scheduler.local.LocalScheduler.runTask(LocalScheduler.scala:198)
        at org.apache.spark.scheduler.local.LocalActor$$anonfun$launchTask$1$$anon$1.run(LocalScheduler.scala:68)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
[INFO] 11 Dec 2013 12:00:33 - org.apache.spark.Logging$class - Remove TaskSet 0.0 from pool
[INFO] 11 Dec 2013 12:00:33 - org.apache.spark.Logging$class - Failed to run saveAsTextFile at Test.scala:19
Exception in thread ""main"" org.apache.spark.SparkException: Job failed: Task 0.0:0 failed more than 4 times; aborting job org.apache.hadoop.util.Shell$ExitCodeException: chmod: getting attributes of `/cygdrive/c/somepath/_temporary/_attempt_201312111200_0000_m_000000_0/part-00000': No such file or directory
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:760)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:758)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:758)
        at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:379)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:441)
        at org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:149)
{noformat}
On a Windows box!
-- Nathan Kronenfeld at http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCAEpWh49EvUEWdnsfKJGvU5MB9V5QsR=HQ=wHpufUmEetU19frg@mail.gmail.com%3E




{noformat}
14/07/29 16:16:57 ERROR executor.Executor: Exception in task ID 6087
 java.io.IOException: The temporary job-output directory hdfs://mybox:8020/path/to/a/dir/_temporary doesn't exist!
         at org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
         at org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:240)
         at org.apache.avro.mapred.AvroOutputFormat.getRecordWriter(AvroOutputFormat.java:154)
         at org.apache.hadoop.mapred.SparkHadoopWriter.open(SparkHadoopWriter.scala:90)
         at org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:728)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
         at org.apache.spark.scheduler.Task.run(Task.scala:53)
         at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
         at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
         at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
         at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)
{noformat}
and
{noformat}
14/07/29 16:16:57 ERROR executor.Executor: Exception in task ID 6158
 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /path/to/a/dir/_temporary/_attempt_201407291616_0000_m_0002
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2445)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2437)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2503)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2480)
         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:556)
         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:337)
         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44958)
         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1751)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1747)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1745)

         at org.apache.hadoop.ipc.Client.call(Client.java:1225)
         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
         at com.sun.proxy.$Proxy13.complete(Unknown Source)
         at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:606)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
         at com.sun.proxy.$Proxy13.complete(Unknown Source)
         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:329)
         at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:1769)
         at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1756)
         at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:66)
         at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:99)
         at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
         at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
         at org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:376)
         at org.apache.avro.mapred.AvroOutputFormat$1.close(AvroOutputFormat.java:163)
         at org.apache.hadoop.mapred.SparkHadoopWriter.close(SparkHadoopWriter.scala:102)
         at org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:737)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
         at org.apache.spark.scheduler.Task.run(Task.scala:53)
         at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
         at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
{noformat}
and
{noformat}
14/07/29 21:01:33 ERROR executor.Executor: Exception in task ID 150
 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /path/to/dir/main/_temporary/_attempt_201407292101_0000_m_000125_150/part-0012
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2445)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2437)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2503)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2480)
         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:556)
         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:337)
         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44958)
         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1751)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1747)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1745)
{noformat}
-- Andrew Ash",,aash,afvitale,ashrowty,bismillah,bleshik,dFuller,dmaverick,gaurav24,GauravGupta,giuseppe.bonaccorso,gphil,hnagar@gmail.com,ivan.vergiliev,jakubwaller,joshrosen,k.shaposhnikov@gmail.com,kaushik_srinivas,laurentcoder,lokeshrajaram,mkim,ogirardot,pauloricardomg,ravipesala,rdblue,rxin,sandeepb,soumdmw,srainville,stevel@apache.org,tsp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6800,SPARK-21288,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411427,,,Tue Jan 07 18:55:25 UTC 2020,,,,,,,,,,"0|i1ys67:",411418,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"15/Sep/14 16:26;gphil;I'm running into this as well. But to respond to this theory:

{quote}
I think this may be related to spark.speculation. I think the error condition might manifest in this circumstance:
1) task T starts on a executor E1
2) it takes a long time, so task T' is started on another executor E2
3) T finishes in E1 so moves its data from _temporary to the final destination and deletes the _temporary directory during cleanup
4) T' finishes in E2 and attempts to move its data from _temporary, but those files no longer exist! exception
{quote}

Speculation is not necessary for this to occur. I am consistently running into this while testing some code against ""local"" without speculation where I am trying to download, manipulate and merge 2 sets of data from S3 and serialize the resulting RDD using saveAsTextFile back to S3:

{code}
Job aborted due to stage failure: Task 3.0:754 failed 1 times, most recent failure: Exception failure in TID 762 on host localhost: java.io.FileNotFoundException: s3n://<bucket>/_temporary/_attempt_201409151537_0000_m_000754_762/part-00754.deflate: No such file or directory. org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:340) org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:165) org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172) org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132) org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:109) org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:786) org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:769) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) org.apache.spark.scheduler.Task.run(Task.scala:51) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:744) Driver stacktrace:
{code}

I'm happy to provide more information or help investigate further to figure this one out.

Edit: I forgot to mention that the file in question actually did exist on S3 when I checked after receiving this exception.;;;","16/Sep/14 17:31;aash;[~gphil], S3 consistency varies based on the region you're in.  I think the read-after-write would be ok for you, but the eventual consistency from the US standard region might not be.  What region were you in?

{quote}
Q: What data consistency model does Amazon S3 employ?

Amazon S3 buckets in the US West (Oregon), US West (Northern California), EU (Ireland), Asia Pacific (Singapore), Asia Pacific (Tokyo), Asia Pacific (Sydney), South America (Sao Paulo), and GovCloud (US) regions provide read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES. Amazon S3 buckets in the US Standard region provide eventual consistency.
{quote}
https://aws.amazon.com/s3/faqs/;;;","16/Sep/14 17:46;gphil;[~aash] --

Thanks for bringing this to my attention. I was beginning to suspect something along these lines--my buckets are in US Standard (but my mental model of what was going on in S3 was read-after-write consistency) so this incarnation of the issue is probably user error on my part.

I actually was able to accomplish what I was trying to do by retrying the failed tasks (and they succeeded on subsequent attempts) so this points to eventual consistency as the culprit here.;;;","11/Dec/14 11:48;pauloricardomg;We're also facing a similar issue when using S3N, described in detail on this thread: https://www.mail-archive.com/user@spark.apache.org/msg17253.html

Here is the relevant exception:

{code}
2014-12-10 19:05:13,823 ERROR [sparkDriver-akka.actor.default-dispatcher-16] scheduler.JobScheduler (Logging.scala:logError(96)) - Error runnin
g job streaming job 1418238300000 ms.0
java.io.FileNotFoundException: File s3n://BUCKET/_temporary/0/task_201412101900_0039_m_000033 does not exist.
        at org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(NativeS3FileSystem.java:506)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:360)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:310)
        at org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)
        at org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:126)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:995)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:878)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:845)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:803)
        at MyDumperClass$$anonfun$main$1.apply(IncrementalDumpsJenkins.scala:100)
        at MyDumperClass$$anonfun$main$1.apply(IncrementalDumpsJenkins.scala:79)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:41)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.streaming.scheduler.Job.run(Job.scala:32)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:172)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2014-12-10 19:05:13,829 INFO  [Driver] yarn.ApplicationMaster (Logging.scala:logInfo(59)) - Unregistering ApplicationMaster with FAILED
{code}

I'm quite sure it has to do with eventual consistency on S3, since it's common to publish files on S3 and they're not promptly available (when you try s3cmd ls s3://mybucket/whatever soon after a file is posted, for instance), only after a few seconds after they appear.  Is there already a configuration for retrying to fetch S3 files if they'e not found (maybe with some kind of exponential backoff)? Maybe this could be a solution, if it's not yet available.;;;","15/Apr/15 17:59;joshrosen;For speculative tasks, this should have been resolved in Spark 1.3+ by the new OutputCommitCoordinator.  I'm going to mark this issue as Fixed for now, but please comment if you still see this issue in Spark 1.3+ or if you see it even without using speculation.;;;","17/Jun/15 02:21;lokeshrajaram;[~joshrosen] I am running spark streaming job (1.3.1) in YARN cluster with Speculation ON. I see this issue happening. The job runs fine for a while and results in this issue as we increase the load. 

Starting stack trace of the issue(let me know if you need more info):

{panel}

15/06/16 21:14:45 INFO streaming.CheckpointWriter: Saving checkpoint for time 1434489285000 ms to file 'hdfs://xxxx:9000/usr/tmp/checkpoint/checkpoint-1434489285000'
15/06/16 21:14:49 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 7512.0 (TID 106806, hostname): java.io.FileNotFoundException: /mnt/var/lib/hadoop/tmp/nm-local-dir/usercache/hadoop/appcache/application_1433194836712_0037/blockmgr-e4c3162d-14a1-4f3d-8e3a-e2d90ad4dbe8/39/temp_shuffle_84312f99-301f-42c7-a20f-c048bb5c6173 (Input/output error)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$spillToPartitionFiles$1.apply(ExternalSorter.scala:361)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$spillToPartitionFiles$1.apply(ExternalSorter.scala:356)
	at scala.Array$.fill(Array.scala:267)
	at org.apache.spark.util.collection.ExternalSorter.spillToPartitionFiles(ExternalSorter.scala:356)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:211)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745) 
 {panel}

;;;","15/Sep/15 08:54;ivan.vergiliev;I'm also seeing this - Spark 1.3.1, Spark Standalone, speculation enabled. Any ideas on what we can do to avoid this (except for disabling speculation)?

Stack trace:

{code}
[2015-09-15 08:25:18,090] WARN  .jobserver.JobManagerActor [] [akka://JobServer/user/context-supervisor/default-context] - Exception from job 419b8ce6-8284-46a6-bd5e-55585c844105:
java.io.FileNotFoundException: File hdfs://FOLDER/_temporary/0/task_201509150825_115810_r_000033 does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:697)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:105)
        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:755)
        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:751)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:751)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:360)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:310)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1020)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:903)
        at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:798)
        at com.leanplum.spark.SessionsJob.runJob(SessionsJob.java:104)
        at spark.jobserver.JobManagerActor$$anonfun$spark$jobserver$JobManagerActor$$getJobFuture$4.apply(JobManagerActor.scala:235)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

Thanks,
Ivan;;;","15/Oct/15 20:38;tispratik;Am seeing the same issue on Spark 1.4.1. I am trying to save a dataframe as table ( saveAsTable ) using SaveMode.Overwrite.

{code}
15/10/15 16:19:57 INFO hadoop.ColumnChunkPageWriteStore: written 1,508B for [flight_id] INT64: 1,142 values, 1,441B raw, 1,464B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 563 entries, 4,504B raw, 563B comp}
15/10/15 16:19:57 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /warehouse/hive-user-tables/agg_imps_pratik/_temporary/0/_temporary/attempt_201510151447_0002_m_000023_0/part-r-00023-6778754e-ac4d-44ef-8ee8-fc87e89639bc.gz.parquet (inode 2376521862): File does not exist. Holder DFSClient_attempt_201510151447_0002_m_000023_0_529613711_162 does not have any open files.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3083)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2885)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2767)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:606)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:455)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
{code}


Also, i am not running in speculative mode.
{code}
.set(""spark.speculation"", ""false"")
{code};;;","12/Jan/16 08:14;k.shaposhnikov@gmail.com;I am seeing the same error message with Spark 1.6 and HDFS. This happens after an earlier job failure (ClassCastException);;;","13/Jan/16 10:04;ogirardot;Same error - Spark 1.5.1 no speculation enabled seen with large datasets (>150G) and 20 executors. 

{noformat}
16/01/13 10:56:57 WARN util.Utils: Suppressing exception in finally: No lease on /.../output/_temporary/0/_temporary/attempt_201601130901_0037_r_000060_0/../sub-output/TRX-r-00060.avro (inode 6654742): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_957274566_50, pendingcreates: 2]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3357)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3159)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3041)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
{noformat}

I'm using the AvroMultipleOutput.;;;","06/Jun/16 05:51;gaurav24;seeing similar errors with spark 1.6 
{noformat}

App > Caused by: java.io.FileNotFoundException: File s3n://{bucket}.../_temporary/0/task_201606041650_0053_m_000000/year=2016 does not exist.
App > 	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(NativeS3FileSystem.java:1471)
App > 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:366)
App > 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:368)
App > 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:315)
App > 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)
App > 	at org.apache.spark.sql.execution.datasources.BaseWriterContainer.commitJob(WriterContainer.scala:230)
App > 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:151)

{noformat}
;;;","08/Jun/16 18:14;sandeepb;Can this bug be reopened please ? I am seeing the issue with spark 1.6.1. as well on AWS. 
Caused by: java.io.FileNotFoundException: File s3n://xxxxxxx/_temporary/0/task_201606080516_0004_m_000079 does not exist.
at org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(NativeS3FileSystem.java:506)
  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:360)
  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:310)
  at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)
  at org.apache.spark.sql.execution.datasources.BaseWriterContainer.commitJob(WriterContainer.scala:230)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:151)
  ... 42 more
;;;","09/Jun/16 15:48;sandeepb;I tried with spark.speculation=false as well and it still gives the same failures;;;","12/Jun/16 10:35;stevel@apache.org;You cannot use speculation=true against S3 and not expect to have corrupted data. Please don't do that.;;;","27/Jun/16 13:05;ashrowty;Hi,

I would echo Sandeep's comments. I am seeing this same exception with Spark 1.6.1 on EMR writing partitioned parquet files to S3. I am still experimenting with it, but when I turn on EMRFS, the exceptions seem to go away. Most probably its the eventual consistency issue. Would be great if retries could be added within Spark for eventually consistent stores/filesystems.

-Ashish;;;","28/Jul/16 09:39;dmaverick;I've faced with such error in Spark 1.6.1 and Hadoop 2.6.2 even for local filesystem (during unit tests ) when I was trying to append/import partitions in parallel/simultaneously. 
The issue is gone when I switch to sequential import.;;;","28/Jul/16 15:02;gaurav24;[~dmaverick] Can you tell a little more on sequential import vs append ? didn't get that;;;","28/Jul/16 22:01;dmaverick;Something like this...
The problem occurs when I change importPartition to  importPartitionAsync  
 service.getHitRecordsFromFile return DataFrame.


{code}
import ....

import java.nio.file.{Path => JavaPath, Paths, Files}
class DefaultHitDataManager(service : SparkService) extends DataSourceManager with Logging {


  val proceeded_prefix = ""_proceeded_""
  val hit_data_pattern = ""*_data*""

  val dsCache = new ConcurrentHashMap[String, HitsDataSource]()


  override def loadDataSource(remoteLocationBase: URI, alias: String): Future[HitsDataSource] = {

    val dsWorkingDir = java.nio.Files.createTempDirectory(alias)
    val fs = FileSystem.get(remoteLocationBase, service.sqlContext.sparkContext.hadoopConfiguration)
    val loadFuture = scanPartitions(remoteLocationBase).flatMap { case newPartitions =>
      Future.traverse(newPartitions.toList)( {  case remotePartitionFile =>
        importPartitionAsync(alias, dsWorkingDir, fs, remotePartitionFile)
      }).flatMap { case statuses =>
        FileUtil.fullyDelete(dsWorkingDir.toFile)
        val ds = HitsDataSource(remoteLocationBase, alias)
        dsCache.put(ds.alias, ds)
        Future.successful(ds)
      }
    }
    loadFuture
  }
  ///XXX: async loading causes race condition in FileSystem and things like that https://issues.apache.org/jira/browse/SPARK-2984
  def importPartitionAsync(alias: String, dsWorkingDir: JavaPath, partitionFS: FileSystem,
                           remotePartitionFile: FileStatus) = Future {
    importPartition(alias, dsWorkingDir,partitionFS, remotePartitionFile )
  }

  def importPartition(alias: String, dsWorkingDir: JavaPath, partitionFS: FileSystem,
                      remotePartitionFile: FileStatus) = {
    val partitionWorkingDir = Files.createTempDirectory(dsWorkingDir, remotePartitionFile.getPath.getName)
    val localPath = new Path(""file://"" + partitionWorkingDir.toString, remotePartitionFile.getPath.getName)

    partitionFS.copyToLocalFile(remotePartitionFile.getPath, localPath)
    val unzippedPartition = s""$partitionWorkingDir/hit_data*.tsv""

      val filePath = localPath.toUri.getPath
      val untarCommand = s""tar -xzf $filePath  --wildcards --no-anchored '$hit_data_pattern'""
      val shellCmd = Array(""bash"", ""-c"", untarCommand.toString)
      val shexec = new ShellCommandExecutor(shellCmd, partitionWorkingDir.toFile)
      shexec.execute()
      val exitcode = shexec.getExitCode
      if (exitcode != 0) {
        throw new IOException(s""Error untarring file $filePath. Tar process exited with exit code $exitcode"")
      }
      val partitionData = service.getHitRecordsFromFile(""file://"" + unzippedPartition)
      partitionData.coalesce(1).write.partitionBy(""createdAtYear"", ""createdAtMonth"").
        mode(SaveMode.Append).saveAsTable(alias)

      val newPartitionPath = new Path(remotePartitionFile.getPath.getParent,
        proceeded_prefix + remotePartitionFile.getPath.getName)
      partitionFS.rename(remotePartitionFile.getPath, newPartitionPath)
  }

  def scanPartitions(locationBase: URI) = Future {
    val fs = FileSystem.get(locationBase, service.sqlContext.sparkContext.hadoopConfiguration)
    val location = new Path(locationBase)
    val newPartitions  = if (fs.isDirectory(location)) {
      fs.listStatus(location, new PathFilter {
        override def accept(path: Path): Boolean = {
          !path.getName.contains(proceeded_prefix)
        }
      })
    } else {
      Array(fs.getFileStatus(location))
    }
    logDebug(s""Found new partitions : $newPartitions"")
    newPartitions
  }
}
{code}

I hope this will help;;;","24/Oct/16 13:50;bleshik;I'm getting a similar exception on Spark 1.6.0:
{code}
java.io.FileNotFoundException: No such file or directory 's3://bucket/path/part-00001'
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:507)
	at org.apache.hadoop.fs.FileSystem.getFileBlockLocations(FileSystem.java:704)
	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1696)
	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1681)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:268)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.elasticsearch.spark.rdd.EsSpark$.saveToEs(EsSpark.scala:59)
	at org.elasticsearch.spark.rdd.EsSpark$.saveJsonToEs(EsSpark.scala:95)
	at org.elasticsearch.spark.package$SparkJsonRDDFunctions.saveJsonToEs(package.scala:48)
{code};;;","24/Oct/16 14:33;stevel@apache.org;Alexy, can you describe your layout a bit more

# are you using Amazon EMR?
# why s3:// URLs? 
# what hadoop-* JARs are you using (it's not that easy to tell in spark-1.6...what did you download?)

If you can, use Hadoop 2.7.x+ and then switch to s3a.

What I actually suspect here, looking at the code listing, is that the following sequence is happening

* previous work completes, deletes/renames _temporary
* next code gets a list of all files (globStatus(pat))
* Then iterates through, getting info about each one.
* During that listing, the _temp dir goes away, which breaks the code.


Seems to me the listing logic in {{FileInputFormat.singleThreadedListStatus}} could be more forgiving about FNFEs: if the file is no longer there, well, skip it.

However, S3A in Hadoop 2.8 (and in HDP2.5, I'll note) replaces this very inefficient directory listing with one optimised for S3A (HADOOP-13208), which is much less likely to exhibit this problem.

Even so, some more resilience would be good; I'll make a note in the relevant bits of the Hadoop code;;;","18/Nov/16 22:33;giuseppe.bonaccorso;I'm facing the same issue with EMR 5.0.1 with Spark 2.0.1 and S3 (with read-after-write consistency). After a few hours of streaming processing and data saving in Parquet format, I got always this exception:

{code:java}
java.io.FileNotFoundException: No such file or directory: s3a://xxx/_temporary/0/task_xxxx
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1004)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:745)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:426)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)
	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)
	at org.apache.spark.sql.execution.datasources.BaseWriterContainer.commitJob(WriterContainer.scala:222)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:144)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:510)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:488)
{code};;;","19/Nov/16 12:37;stevel@apache.org;That sounds like a separate issue...could you open a new JIRA?

Also, are you really using EMR 5.01 and an s3a URL? Interesting;;;","21/Feb/17 13:28;laurentcoder;related issue (still open): SPARK-10109;;;","07/Apr/17 14:44;hnagar@gmail.com;Is there any work going on this issue, or anything related to this, as it seems nobody has been able to resolve this, and a lot of people including me have this issue?;;;","07/Apr/17 15:02;stevel@apache.org;For s3a commits, HADOOP-13786 is going to be the fix. This not some trivial ""directory missing"" problem, it is the fundamental issue that rename() isn't how you commit work into an eventually consistent object store whose s3 client mimics rename by copying all the data from one blob to another. See [the design document|https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3a_committer.md]. 

It's currently at demo quality: appears to work, just not been tested at scale or with fault injection. Netflix have been using the core code though.Works for ORC that is, Parquet will take a bit more work due to bits of the spark code which explicitly look for ParquetOutputCommitter subclasses; something will need to be done there.

;;;","17/Oct/17 11:34;soumdmw;I'm facing the same issues with Spark 2.0.2 on DC/OS with HDFS [Parquet files].
After few hours of streaming when the load increases and forces multiple batches writing to same location simultaneously, this error is reproduced.

java.io.IOException: Failed to rename FileStatus{path=hdfs://namenode/xxx/xxx/parquet/snappy/customer_demographics/datetime=2017101710/_temporary/0/task_201710171034_0026_m_000002/part-r-00002-59deea14-b221-4a91-bcd3-78e2dbaf6b91.snappy.parquet; isDirectory=false; length=2614; replication=3; blocksize=134217728; modification_time=1508236440393; access_time=1508236440331; owner=root; group=hdfs; permission=rw-r--r--; isSymlink=false} to hdfs://namenode/xxx/xxx/parquet/snappy/customer_demographics//datetime=2017101710/part-r-00002-59deea14-b221-4a91-bcd3-78e2dbaf6b91.snappy.parquet 

Spark configurations used :
        .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
        .set(""spark.sql.tungsten.enabled"", ""true"")
        .set(""spark.speculation"",""false"")
        .set(""spark.sql.parquet.mergeSchema"", ""false"")
        .set(""spark.mapreduce.fileoutputcommitter.algorithm.version"", ""2"")

Config in the properties file while spark submit :
spark.streaming.concurrentJobs 4

;;;","17/Oct/17 12:33;stevel@apache.org;bq. multiple batches writing to same location simultaneously


Hadoop {{FileOutputCommitter}} cleans up $dest/_temporary while committing or aborting a job.

if you are writing >1 job to the same directory tree simultaneously, expect the job cleanup in one task to break the others. You could try overloading ParquetOutputCommitter.cleanupJob() to stop this, but it's probably safer to work out why output to the same path is happening in parallel and stop it.;;;","18/Oct/17 06:49;soumdmw;[~stevel@apache.org] Multiple batches/jobs run at same time as we have enabled concurrentJobs to 4 and in cases where load increases suddenly, jobs run parallel.
The directory tree comprises of the partitions, current dest folder being the current hour and hence same dir. 
We can add batch time for each job to separate the parent folder but we may end up with lot of directories and we don't want that for current scenario.
Is there a simpler way to not delete _temporary folder if any write is currently going on.;;;","01/Nov/17 00:53;dFuller;Similar issue as well. Not related to S3 or HDFS as I am reading files directly from linux file system.

{code:java}
Caused by: java.io.FileNotFoundException: /tmp/spark-b24b4f13-d1d3-4d4b-986e-420f64febac3/executor-7cbd0dcc-0b8f-4210-8b35-ea72efd3d2a7/blockmgr-81fe3028-4834-40e4-b5ca-e731a03286f1/03/shuffle_33528_20_0.index.a80d3f03-224d-4915-a7a1-26de8b20b59b (No such file or directory)
        at java.io.FileOutputStream.open0(Native Method)
        at java.io.FileOutputStream.open(FileOutputStream.java:270)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
        at org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:144)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        ... 1 more
{code};;;","01/Nov/17 10:49;stevel@apache.org;Darron: different stack trace, different parts of the code, different issue. This one is specifically about the {{_temporary/}} path in job commits. Your one looks more like the path to that temp directory doesn't exist: either the dir is missing/not created or part of the fs has just been unmounted.;;;","01/Nov/17 11:08;stevel@apache.org;[~soumdmw] you asked

bq. is there a simpler way to not delete _temporary folder if any write is currently going on.

Afraid not. The classic committers all assume exclusive access to the dest, do various sequences of operations (which end up being non-atomic). >1 writer will cause confusion.  The only possible category where any vaguely viable multiuse story would be SaveMode.Append *and* every file being generated with unique names so there's no conflict.

Now, the partitioned committer which we are close to adding to Hadoop does do that for s3, but even there I'm killing all outstanding multipart uploads to the dest dir after job commit or abort to keep costs down (these can be from failed tasks & will run up bills if not cleaned up).

After that work is in, we could think about adding some option about ""fs.s3a.committer.multiple.active.jobs"" flag which will
(a) warn that things may get confused & caller must clean up.
(b) fail if the output mode isn't append (overwrite is list + delete + commit, so vulnerable to race conditions)
(c) not do list * cancel of all uncommitted multiparts. Caller will be expected to cleanup via CLI as part of their workflow.

This does nothing for HDFS output: someone will get to write their own commit algorithms there, which is not something I would encourage people to do unless they want to get into the depths of [distributed commit algorithms as used in Hadoop & Spark|https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/committer_architecture.md].

Simpler to do a workaround like: output independently and then merge in to the dest tree using an algorithm similar to the v1 {{mergePaths()}} algorithm in the doc I've just linked to.

[~rdblue]: any thoughts?;;;","01/Nov/17 15:35;rdblue;I don't have a good solution here. You could maybe isolate every write to a new location, so you get separate _temporary directories, but then you end up having a separate final directory for each write and have to union the data. That's not ideal and is one of the drawbacks of how we write data -- our writes must always overwrite a partition and can't append (each write creates only new folders). Probably easier to override your committer and change the behavior.;;;","23/Dec/18 04:58;GauravGupta;I am having similar issues while using spark 1.6 even when I have set spark.speculation=false.I am trying to save the rdd as saveAsTextFile in hdfs when I am encountering this error.Can this issue be reopened please?;;;","02/Jan/19 20:02;stevel@apache.org;Gaurav, if this has returned in a 2.x version against HDFS, best to open a new JIRA and mark as related to this one.;;;","07/Jan/20 04:08;afvitale;[~stevel@apache.org] :

Is your recommended solution for HDFS still ""Simpler to do a workaround like: output independently and then merge in to the dest tree using an algorithm similar to the v1 {{mergePaths()}} algorithm in the doc I've just linked to."" as you indicated in comment https://issues.apache.org/jira/browse/SPARK-2984?focusedCommentId=16233937&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16233937

As part of your recommendation, is it guaranteed that parquet filenames will be unique across jobs?

Also, when ""outputting independently"", is it ok to use v2 commit algorithm?

Thanks

 ;;;","07/Jan/20 18:55;stevel@apache.org;bq. As part of your recommendation, is it guaranteed that parquet filenames will be unique across jobs?

no idea. S3A committer defaults to inserting a UUID into the filename to meet that guarantee.

bq. Also, when ""outputting independently"", is it ok to use v2 commit algorithm?

Only if each independent job fails completely if there's a failure/timeout during task commit (i.e do not attempt to commit >1 task attempt for the same task). Spark does not currently do that , AFAIK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitionStrategy: VertexID hash overflow,SPARK-2981,12733383,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,larryxiao,larryxiao,12/Aug/14 03:24,08/Sep/14 08:26,14/Jul/23 06:26,08/Sep/14 08:26,1.0.2,,,,,,,1.0.3,1.1.0,1.2.0,,,,GraphX,,,,0,newbie,,,,,"In EdgePartition1D, a PartitionID is calculated by multiplying VertexId with a mixingPrime (1125899906842597L) then cast to Int, and mod numParts.

The Long is overflowed, and when cast to Int:

{quote}
scala> (1125899906842597L*1).toInt
res1: Int = -27

scala> (1125899906842597L*2).toInt
res2: Int = -54

scala> (1125899906842597L*3).toInt
res3: Int = -81
{quote}
As the cast produce number that are multiplies of 3, the partition is not useable when partitioning to multiples of 3.

for example when you partition to 6 or 9 parts:
{quote}
14/08/12 09:26:21 INFO GraphXPartition: GRAPHX: psrc Array((0,4347084), (1,0), (2,0), (3,3832578), (4,0), (5,0))
14/08/12 09:26:21 INFO GraphXPartition: GRAPHX: pdst Array((0,4347084), (1,0), (2,0), (3,3832578), (4,0), (5,0)) 

14/08/12 09:21:46 INFO GraphXPartition: GRAPHX: psrc Array((0,8179662), (1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0))
14/08/12 09:21:46 INFO GraphXPartition: GRAPHX: pdst Array((0,8179662), (1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)) 

so the vertices are partitioned to 0,3 for 6; and 0 for 9
{quote}

I think solution is to cast after mod.
{quote}
scala> (1125899906842597L*3)
res4: Long = 3377699720527791

scala> (1125899906842597L*3) % 9
res5: Long = 3

scala> ((1125899906842597L*3) % 9).toInt
res5: Int = 3
{quote}",,ankurd,apachespark,larryxiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411411,,,Mon Sep 08 08:26:12 UTC 2014,,,,,,,,,,"0|i1ys2n:",411402,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/14 06:56;apachespark;User 'larryxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/1902;;;","08/Sep/14 08:26;ankurd;Reopening temporarily to correct the Fix Version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix handling of short shuffle manager names in ShuffleBlockManager,SPARK-2977,12733322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,11/Aug/14 19:00,11/Sep/14 16:12,14/Jul/23 06:26,16/Aug/14 21:56,1.1.0,,,,,,,,,,,,,Shuffle,Spark Core,,,0,,,,,,"Since we allow short names for {{spark.shuffle.manager}}, all code that reads that configuration property should be prepared to handle the short names.

See my comment at https://github.com/apache/spark/pull/1799#discussion_r16029607 (opening this as a JIRA so we don't forget to fix it).",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411350,,,Fri Aug 15 23:51:59 UTC 2014,,,,,,,,,,"0|i1yrpr:",411342,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Aug/14 23:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1976;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK_LOCAL_DIRS may cause problems when running in local mode,SPARK-2975,12733298,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,11/Aug/14 17:45,20/Aug/14 17:16,14/Jul/23 06:26,20/Aug/14 17:16,1.0.0,1.1.0,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"If we're running Spark in local mode and {{SPARK_LOCAL_DIRS}} is set, the {{Executor}} modifies SparkConf so that this value overrides {{spark.local.dir}}.  Normally, this is safe because the modification takes place before SparkEnv is created.  In local mode, the Executor uses an existing SparkEnv rather than creating a new one, so it winds up with a DiskBlockManager that created local directories with the original {{spark.local.dir}} setting, but other components attempt to use directories specified in the _new_ {{spark.local.dir}}, leading to problems.

I discovered this issue while testing Spark 1.1.0-snapshot1, but I think it will also affect Spark 1.0 (haven't confirmed this, though).

(I posted some comments at https://github.com/apache/spark/pull/299#discussion-diff-15975800, but also opening this JIRA so this isn't forgotten.)",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2974,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411326,,,Sun Aug 17 20:06:48 UTC 2014,,,,,,,,,,"0|i1yrkf:",411318,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"12/Aug/14 00:08;joshrosen;I'm raising the priority of this issue to 'critical', since it causes problems when running on a cluster if some tasks are small enough to be run locally on the driver.

Here's an example exception:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 21 in stage 0.0 failed 1 times, most recent failure: Lost task 21.0 in stage 0.0 (TID 21, localhost): java.io.IOException: No such file or directory
        java.io.UnixFileSystem.createFileExclusively(Native Method)
        java.io.File.createNewFile(File.java:1006)
        java.io.File.createTempFile(File.java:1989)
        org.apache.spark.util.Utils$.fetchFile(Utils.scala:335)
        org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:342)
        org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:340)
        scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:340)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:180)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code};;;","17/Aug/14 00:31;joshrosen;The driver's configuration properties seem to be propagated to the workers, so it's probably not safe to have SPARK_LOCAL_DIR override spark.local.dir by mutating the SparkConf when creating SparkEnv, since this would cause a driver-local override to affect every machine on the cluster.

Is the set of local directories a property of workers' environments, like SPARK_HOME, in which case we probably don't want to propagate it from drivers to workers?  Or is there a use-case for allowing the driver to tell workers which local directories to use (maybe I want to create two SparkContexts and configure them to use different disks or something)?;;;","17/Aug/14 20:06;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.getLocalDir() may return non-existent spark.local.dir directory,SPARK-2974,12733291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,11/Aug/14 17:26,20/Aug/14 05:43,14/Jul/23 06:26,20/Aug/14 05:43,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"The patch for [SPARK-2324] modified Spark to ignore a certain number of invalid local directories.  Unfortunately, the {{Utils.getLocalDir()}} method returns the _first_ local directory from {{spark.local.dir}}, which might not exist.  This can lead to confusing FileNotFound errors when executors attempt to fetch files. 

(I commented on this at https://github.com/apache/spark/pull/1274#issuecomment-51537965, but I'm opening a JIRA so we don't forget to fix it).",,apachespark,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2325,,,,,SPARK-2975,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411319,,,Wed Aug 20 05:43:29 UTC 2014,,,,,,,,,,"0|i1yriv:",411311,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"17/Aug/14 20:06;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2002;;;","20/Aug/14 05:43;pwendell;Issue resolved by pull request 2002
[https://github.com/apache/spark/pull/2002];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-sql script ends with IOException when EventLogging is enabled,SPARK-2970,12733211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,sarutak,sarutak,11/Aug/14 11:16,29/Aug/14 00:24,14/Jul/23 06:26,29/Aug/14 00:24,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"When spark-sql script run with spark.eventLog.enabled set true, it ends with IOException because FileLogger can not create APPLICATION_COMPLETE file in HDFS.

It's is because a shutdown hook of SparkSQLCLIDriver is executed after a shutdown hook of org.apache.hadoop.fs.FileSystem is executed.

When spark.eventLog.enabled is true, the hook of SparkSQLCLIDriver finally try to create a file to mark the application finished but the hook of FileSystem try to close FileSystem.",CDH5.1.0 (Hadoop 2.3.0),apachespark,lian cheng,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3062,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411239,,,Wed Aug 20 17:14:23 UTC 2014,,,,,,,,,,"0|i1yr13:",411231,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"11/Aug/14 11:55;sarutak;I noticed it's not caused by the reason above.
It's caused by shutdown hook of FileSystem.
I have already resolved it to execute shutdown hook for stopping SparkSQLContext before the shutdown hook for FileSystem.;;;","11/Aug/14 12:02;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1891;;;","11/Aug/14 15:08;lian cheng;[~sarutak] Would you mind to update the issue description? Otherwise it can be confusing for people that don't see your comments below. Thanks.;;;","11/Aug/14 15:24;sarutak;[~liancheng] Thank you pointing my mistake. I've modified the description.;;;","15/Aug/14 17:21;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1970;;;","18/Aug/14 01:07;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2007;;;","20/Aug/14 17:14;sarutak;Due to this issue, application is not marked as finished because a file APPLICATION_COMPLETE is not created.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullabilities of Explode.,SPARK-2968,12733204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,11/Aug/14 10:13,12/Aug/14 03:19,14/Jul/23 06:26,12/Aug/14 03:19,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Output nullabilities of {{Explode}} could be detemined by {{ArrayType.containsNull}} or {{MapType.valueContainsNull}}.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411232,,,Mon Aug 11 10:16:43 UTC 2014,,,,,,,,,,"0|i1yqzj:",411224,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 10:16;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1888;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several SQL unit test failed when sort-based shuffle is enabled,SPARK-2967,12733203,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,jerryshao,jerryshao,11/Aug/14 10:11,21/Aug/14 01:12,14/Jul/23 06:26,20/Aug/14 22:52,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Several SQLQuerySuite unit test failed when sort-based shuffle is enabled. Seems SQL test uses GenericMutableRow  which will make ExternalSorter's internal buffer all refered to the same object finally because of object's mutability. Seems row should be copied when feeding into ExternalSorter.

The error shows below, though have many failures, I only pasted part of them:

{noformat}
 SQLQuerySuite:
 - SPARK-2041 column name equals tablename
 - SPARK-2407 Added Parser of SQL SUBSTR()
 - index into array
 - left semi greater than predicate
 - index into array of arrays
 - agg *** FAILED ***
   Results do not match for query:
   Aggregate ['a], ['a,SUM('b) AS c1#38]
    UnresolvedRelation None, testData2, None
   
   == Analyzed Plan ==
   Aggregate [a#4], [a#4,SUM(CAST(b#5, LongType)) AS c1#38L]
    SparkLogicalPlan (ExistingRdd [a#4,b#5], MapPartitionsRDD[7] at mapPartitions at basicOperators.scala:215)
   
   == Physical Plan ==
   Aggregate false, [a#4], [a#4,SUM(PartialSum#40L) AS c1#38L]
    Exchange (HashPartitioning [a#4], 200)
     Aggregate true, [a#4], [a#4,SUM(CAST(b#5, LongType)) AS PartialSum#40L]
      ExistingRdd [a#4,b#5], MapPartitionsRDD[7] at mapPartitions at basicOperators.scala:215
   
   == Results ==
   !== Correct Answer - 3 ==   == Spark Answer - 3 ==
   !Vector(1, 3)               [1,3]
   !Vector(2, 3)               [1,3]
   !Vector(3, 3)               [1,3] (QueryTest.scala:53)
 - aggregates with nulls
 - select *
 - simple select
 - sorting *** FAILED ***
   Results do not match for query:
   Sort ['a ASC,'b ASC]
    Project [*]
     UnresolvedRelation None, testData2, None
   
   == Analyzed Plan ==
   Sort [a#4 ASC,b#5 ASC]
    Project [a#4,b#5]
     SparkLogicalPlan (ExistingRdd [a#4,b#5], MapPartitionsRDD[7] at mapPartitions at basicOperators.scala:215)
   
   == Physical Plan ==
   Sort [a#4 ASC,b#5 ASC], true
    Exchange (RangePartitioning [a#4 ASC,b#5 ASC], 200)
     ExistingRdd [a#4,b#5], MapPartitionsRDD[7] at mapPartitions at basicOperators.scala:215
   
   == Results ==
   !== Correct Answer - 6 ==   == Spark Answer - 6 ==
   !Vector(1, 1)               [3,2]
   !Vector(1, 2)               [3,2]
   !Vector(2, 1)               [3,2]
   !Vector(2, 2)               [3,2]
   !Vector(3, 1)               [3,2]
   !Vector(3, 2)               [3,2] (QueryTest.scala:53)
 - limit
 - average
 - average overflow *** FAILED ***
   Results do not match for query:
   Aggregate ['b], [AVG('a) AS c0#90,'b]
    UnresolvedRelation None, largeAndSmallInts, None
   
   == Analyzed Plan ==
   Aggregate [b#3], [AVG(CAST(a#2, LongType)) AS c0#90,b#3]
    SparkLogicalPlan (ExistingRdd [a#2,b#3], MapPartitionsRDD[4] at mapPartitions at basicOperators.scala:215)
   
   == Physical Plan ==
   Aggregate false, [b#3], [(CAST(SUM(PartialSum#93L), DoubleType) / CAST(SUM(PartialCount#94L), DoubleType)) AS c0#90,b#3]
    Exchange (HashPartitioning [b#3], 200)
     Aggregate true, [b#3], [b#3,COUNT(CAST(a#2, LongType)) AS PartialCount#94L,SUM(CAST(a#2, LongType)) AS PartialSum#93L]
      ExistingRdd [a#2,b#3], MapPartitionsRDD[4] at mapPartitions at basicOperators.scala:215
   
   == Results ==
   !== Correct Answer - 2 ==   == Spark Answer - 2 ==
   !Vector(2.0, 2)             [2.147483645E9,1]
   !Vector(2.147483645E9, 1)   [2.147483645E9,1] (QueryTest.scala:53)
{noformat}

",,apachespark,igor.berman,jerryshao,marmbrus,matei,rxin,sandyr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411231,,,Thu Aug 21 01:12:44 UTC 2014,,,,,,,,,,"0|i1yqzb:",411223,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"12/Aug/14 07:32;matei;Good catch, this is a difference in behavior with sort-based shuffle. [~rxin] [~marmbrus] would it be possible to not pass the same object here?;;;","12/Aug/14 07:46;marmbrus;Yes, its possible and seems to fix the issue.  However I think we'll only wanna do it when sort based shuffle is on, since its pretty expensive.;;;","12/Aug/14 08:06;jerryshao;Hi Matei and Michael, thanks a lot for looking into this issue. I think some users' codes may also encounter this problem when using same mutable object, like in mapPartitions(), maybe we should add some docs to warn this.;;;","20/Aug/14 21:22;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2066;;;","21/Aug/14 01:12;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2072;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HashOuterJoin output nullabilities.,SPARK-2965,12733192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,11/Aug/14 08:53,12/Aug/14 03:16,14/Jul/23 06:26,12/Aug/14 03:16,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Output attributes of opposite side of {{OuterJoin}} should be nullable.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411220,,,Mon Aug 11 09:01:49 UTC 2014,,,,,,,,,,"0|i1yqwv:",411212,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 09:01;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove duplicated code from spark-sql and start-thriftserver.sh,SPARK-2964,12733176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sarutak,sarutak,11/Aug/14 06:53,27/Aug/14 00:34,14/Jul/23 06:26,27/Aug/14 00:34,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"In spark-sql script, they expect -s option but it's wrong. It's typo for -S (large S). We need to fix that.
And now, we have bin/utils.sh, so let's improve spark-sql and start-thriftserver.sh to leverage utils.sh.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411204,,,Mon Aug 11 07:11:53 UTC 2014,,,,,,,,,,"0|i1yqtb:",411196,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 07:11;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1886;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The description about how to build for using CLI and Thrift JDBC server is absent in proper document ,SPARK-2963,12733165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,11/Aug/14 05:31,23/Aug/14 05:35,14/Jul/23 06:26,23/Aug/14 05:35,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Currently, if we'd like to use HiveServer or CLI for SparkSQL, we need to use -Phive-thriftserver option when building but it's description is incomplete.",,apachespark,lian cheng,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411193,,,Sat Aug 23 05:35:57 UTC 2014,,,,,,,,,,"0|i1yqqv:",411185,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 06:16;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1885;;;","11/Aug/14 15:31;lian cheng;Actually [there is|https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md#running-the-thrift-jdbc-server], but the Spark CLI part is incomplete. Would you mind to update the Issue title and description? Thanks.;;;","11/Aug/14 15:35;sarutak;I've updated this title and Github's one.;;;","21/Aug/14 09:50;sarutak;I reopend this issue because of regression.;;;","21/Aug/14 10:17;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2080;;;","23/Aug/14 05:35;pwendell;Thanks - I've merged your fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark executables fail to start via symlinks,SPARK-2960,12733151,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,roji,roji,11/Aug/14 02:28,13/Nov/15 08:23,14/Jul/23 06:26,04/Nov/15 10:49,,,,,,,,1.6.0,,,,,,Deploy,,,,0,,,,,,The current scripts (e.g. pyspark) fail to run when they are executed via symlinks. A common Linux scenario would be to have Spark installed somewhere (e.g. /opt) and have a symlink to it in /usr/bin.,,apachespark,jerryshao,mikeri,patrungel,roji,tgraves,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3482,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411179,,,Fri Nov 13 08:23:00 UTC 2015,,,,,,,,,,"0|i1yqnr:",411171,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/14 18:56;srowen;I suggest this be marked a dupe of https://issues.apache.org/jira/browse/SPARK-3482 as the latter appears to be the same and has an open PR.;;;","07/Oct/14 19:48;apachespark;User 'roji' has created a pull request for this issue:
https://github.com/apache/spark/pull/1875;;;","06/Apr/15 13:52;patrungel;This now formed a loop of three tickets (SPARK-2960, SPARK-3482 and SPARK-4162) all three resolved as duplicate; two PR-s (#1875 and #2386) are closed but not merged. Apparently this issue doesn't progress at all.

Is there anything that can be done to burst through?

I could draft a new PR; can this ticket be re-opened?;;;","06/Apr/15 14:50;srowen;Not sure what happened there -- probably my fault in any event -- but this one is duplicated, rather than is the duplicate. There was a PR but it wasn't accepted, so that shouldn't resolve it either. As far as I know the issue is still valid;;;","02/Sep/15 17:40;mikeri;Has there been any progress on this?  It looks like there was a pull request that appeared to fix the issue (see earlier comments), but it ended up getting closed without any action being taken on it?;;;","09/Sep/15 14:19;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/8669;;;","14/Sep/15 09:09;srowen;As an addendum, I think the title is misleading. It's really more like, ""if Spark scripts are symlinked, they work, but the logic to discover the Spark installation this way doesn't"". Shoudl this not be configured by {{SPARK_HOME}} and/or {{SPARK_CONF_DIR}} if set? right now some of these scripts don't use them or override them unilaterally.;;;","14/Sep/15 11:58;patrungel;The title of the issue is not that misleading, when one doesn't get spark-something running after 'spark-something' was typed in, it's commonly known as 'spark executables fail to start'. Following the symlinks does fix the issue in hand.

Having executables {quote}configured by {{SPARK_HOME}} and/or {{SPARK_CONF_DIR}} {quote} would be a nice solution, I'd vote for that.
This implies scripts treating those configurations as read-only and quitting early and loudly if the latter is missing or crippled. 
That's some rework though, not a bug to fix.;;;","04/Nov/15 10:49;srowen;Issue resolved by pull request 8669
[https://github.com/apache/spark/pull/8669];;;","13/Nov/15 05:04;yinxusen;Here we use the SPARK_HOME in those shell scripts may cause confusion. If users have already set up a SPARK_HOME to another dir, the bin/spark_shell script will use the libs in that dir, other than the current one. I think we'd better add a warning message for users who have another SPARK_HOME setting. Especially when the two spark directories are not the same version, there will be errors.;;;","13/Nov/15 06:04;jerryshao;Hi [~yinxusen], from script's view point, it cannot detect which SPARK_HOME is the Spark you wanted, if user have multiple Spark installed, maybe it would be better for user to distinguish.;;;","13/Nov/15 08:23;srowen;I don't think that's a problem. If {{SPARK_HOME}} is explicitly set, it means it's intended to be respected as the primary installation. This is for production deployments, not development. I don't see a use case where you have two simultaneous production deployments on one environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Test code fails to compile with ""mvn compile"" without ""install"" ",SPARK-2955,12733121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,10/Aug/14 20:02,15/Jan/15 09:08,14/Jul/23 06:26,15/Aug/14 05:09,1.0.2,,,,,,,1.2.0,,,,,,Build,,,,0,build,compile,scalatest,test,test-compile,"(This is the corrected follow-up to https://issues.apache.org/jira/browse/SPARK-2903 )

Right now, ""mvn compile test-compile"" fails to compile Spark. (Don't worry; ""mvn package"" works, so this is not major.) The issue stems from test code in some modules depending on test code in other modules. That is perfectly fine and supported by Maven.

It takes extra work to get this to work with scalatest, and this has been attempted: https://github.com/apache/spark/blob/master/sql/catalyst/pom.xml#L86

This formulation is not quite enough, since the SQL Core module's tests fail to compile for lack of finding test classes in SQL Catalyst, and likewise for most Streaming integration modules depending on core Streaming test code. Example:

{code}
[error] /Users/srowen/Documents/spark/sql/core/src/test/scala/org/apache/spark/sql/QueryTest.scala:23: not found: type PlanTest
[error] class QueryTest extends PlanTest {
[error]                         ^
[error] /Users/srowen/Documents/spark/sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala:28: package org.apache.spark.sql.test is not a value
[error]   test(""SPARK-1669: cacheTable should be idempotent"") {
[error]   ^
...
{code}

The issue I believe is that generation of a test-jar is bound here to the compile phase, but the test classes are not being compiled in this phase. It should bind to the test-compile phase.

It works when executing ""mvn package"" or ""mvn install"" since test-jar artifacts are actually generated available through normal Maven mechanisms as each module is built. They are then found normally, regardless of scalatest configuration.

It would be nice for a simple ""mvn compile test-compile"" to work since the test code is perfectly compilable given the Maven declarations.

On the plus side, this change is low-risk as it only affects tests.
[~yhuai] made the original scalatest change and has glanced at this and thinks it makes sense.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411149,,,Fri Aug 15 05:09:19 UTC 2014,,,,,,,,,,"0|i1yqhb:",411141,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/14 20:07;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1879;;;","15/Aug/14 05:09;pwendell;Issue resolved by pull request 1879
[https://github.com/apache/spark/pull/1879];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark MLlib serialization tests fail on Python 2.6,SPARK-2954,12733092,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,10/Aug/14 07:07,09/Jan/15 17:49,14/Jul/23 06:26,11/Aug/14 18:55,1.1.0,,,,,,,0.9.3,1.0.3,1.1.0,,,,PySpark,,,,0,,,,,,"The PySpark MLlib tests currently fail on Python 2.6 due to problems unpacking data from bytearray using struct.unpack:

{code}
**********************************************************************
File ""pyspark/mllib/_common.py"", line 181, in __main__._deserialize_double
Failed example:
    _deserialize_double(_serialize_double(1L)) == 1.0
Exception raised:
    Traceback (most recent call last):
      File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/doctest.py"", line 1253, in __run
        compileflags, 1) in test.globs
      File ""<doctest __main__._deserialize_double[4]>"", line 1, in <module>
        _deserialize_double(_serialize_double(1L)) == 1.0
      File ""pyspark/mllib/_common.py"", line 194, in _deserialize_double
        return struct.unpack(""d"", ba[offset:])[0]
    error: unpack requires a string argument of length 8
**********************************************************************
File ""pyspark/mllib/_common.py"", line 184, in __main__._deserialize_double
Failed example:
    _deserialize_double(_serialize_double(sys.float_info.max)) == x
Exception raised:
    Traceback (most recent call last):
      File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/doctest.py"", line 1253, in __run
        compileflags, 1) in test.globs
      File ""<doctest __main__._deserialize_double[6]>"", line 1, in <module>
        _deserialize_double(_serialize_double(sys.float_info.max)) == x
      File ""pyspark/mllib/_common.py"", line 194, in _deserialize_double
        return struct.unpack(""d"", ba[offset:])[0]
    error: unpack requires a string argument of length 8
**********************************************************************
File ""pyspark/mllib/_common.py"", line 187, in __main__._deserialize_double
Failed example:
    _deserialize_double(_serialize_double(sys.float_info.max)) == y
Exception raised:
    Traceback (most recent call last):
      File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/doctest.py"", line 1253, in __run
        compileflags, 1) in test.globs
      File ""<doctest __main__._deserialize_double[8]>"", line 1, in <module>
        _deserialize_double(_serialize_double(sys.float_info.max)) == y
      File ""pyspark/mllib/_common.py"", line 194, in _deserialize_double
        return struct.unpack(""d"", ba[offset:])[0]
    error: unpack requires a string argument of length 8
**********************************************************************
{code}

It looks like one solution is to wrap the {{bytearray}} with {{buffer()}}: http://stackoverflow.com/a/15467046/590203",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411120,,,Sun Aug 10 07:31:40 UTC 2014,,,,,,,,,,"0|i1yqb3:",411112,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/14 07:31;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1874;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable logging actor messages at DEBUG level,SPARK-2952,12733084,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,10/Aug/14 04:59,12/Aug/14 21:56,14/Jul/23 06:26,12/Aug/14 21:56,,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"Logging actor messages can be very useful for debugging what went wrong in a distributed setting. For example, yesterday we ran into a problem in which we have no idea what was going on with the actor messages. 

",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411112,,,Sun Aug 10 05:01:38 UTC 2014,,,,,,,,,,"0|i1yq9b:",411104,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"10/Aug/14 05:01;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1870;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SerDeUtils.pythonToPairRDD fails on RDDs of pickled array.arrays in Python 2.6,SPARK-2951,12733079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,joshrosen,joshrosen,10/Aug/14 01:29,04/May/15 03:53,14/Jul/23 06:26,16/Sep/14 01:57,1.1.0,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"With Python 2.6, calling SerDeUtils.pythonToPairRDD() on an RDD of pickled Python array.arrays will fail with this exception:

{code}
ava.lang.ClassCastException: java.lang.String cannot be cast to java.util.ArrayList
        net.razorvine.pickle.objects.ArrayConstructor.construct(ArrayConstructor.java:33)
        net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
        net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
        net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
        net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
        org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToPairRDD$1$$anonfun$5.apply(SerDeUtil.scala:106)
        org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToPairRDD$1$$anonfun$5.apply(SerDeUtil.scala:106)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:898)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:880)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

I think this is due to a difference in how array.array is pickled in Python 2.6 vs. Python 2.7.  To see this, run the following script:

{code}
from pickletools import dis, optimize
from pickle import dumps, loads, HIGHEST_PROTOCOL
from array import array

arr = array('d', [1, 2, 3])

#protocol = HIGHEST_PROTOCOL
protocol = 0

pickled = dumps(arr, protocol=protocol)
pickled = optimize(pickled)
unpickled = loads(pickled)

print arr
print unpickled

print dis(pickled)
{code}

In Python 2.7, this outputs

{code}
array('d', [1.0, 2.0, 3.0])
array('d', [1.0, 2.0, 3.0])
    0: c    GLOBAL     'array array'
   13: (    MARK
   14: S        STRING     'd'
   19: (        MARK
   20: l            LIST       (MARK at 19)
   21: F        FLOAT      1.0
   26: a        APPEND
   27: F        FLOAT      2.0
   32: a        APPEND
   33: F        FLOAT      3.0
   38: a        APPEND
   39: t        TUPLE      (MARK at 13)
   40: R    REDUCE
   41: .    STOP
highest protocol among opcodes = 0
None
{code}

whereas 2.6 outputs

{code}
array('d', [1.0, 2.0, 3.0])
array('d', [1.0, 2.0, 3.0])
    0: c    GLOBAL     'array array'
   13: (    MARK
   14: S        STRING     'd'
   19: S        STRING     '\x00\x00\x00\x00\x00\x00\xf0?\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x08@'
  110: t        TUPLE      (MARK at 13)
  111: R    REDUCE
  112: .    STOP
highest protocol among opcodes = 0
None
{code}

I think the Java-side depickling library doesn't expect this pickled format, causing this failure.

I noticed this when running PySpark's unit tests on 2.6 because the TestOuputFormat.test_newhadoop test failed.

I think that this issue affects all of the methods that might need to depickle arrays in Java, including all of the Hadoop output format methods.

How should we try to fix this?  Require that users upgrade to 2.7 if they want to use code that requires this?  Open a bug with the depickling library maintainers?  Try to hack in our own pickling routines for arrays if we detect that we're using 2.6?",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7314,SPARK-3013,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411107,,,Wed Dec 10 20:17:08 UTC 2014,,,,,,,,,,"0|i1yq87:",411099,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/14 23:50;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2365;;;","16/Sep/14 01:57;joshrosen;Issue resolved by pull request 2365
[https://github.com/apache/spark/pull/2365];;;","10/Dec/14 20:17;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark doesn't work on Python 2.6,SPARK-2948,12733059,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,sarutak,sarutak,09/Aug/14 20:51,09/Jan/15 17:49,14/Jul/23 06:26,11/Aug/14 18:55,1.1.0,,,,,,,0.9.3,1.0.3,1.1.0,,,,PySpark,,,,0,,,,,,"In serializser.py, collections.namedtuple is redefined as follows.

{code}
    def namedtuple(name, fields, verbose=False, rename=False):                                                                                                
        cls = _old_namedtuple(name, fields, verbose, rename)                                                                                                  
        return _hack_namedtuple(cls)                                                                                                                          
                                 
{code}

The number of arguments is 4 but the number of arguments of namedtuple for Python 2.6 is 3 so mismatch is occurred.",CentOS 6.5 / Python 2.6.6,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411087,,,Sun Aug 10 07:32:10 UTC 2014,,,,,,,,,,"0|i1yq3r:",411079,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/14 21:01;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1868;;;","10/Aug/14 07:32;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1874;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure with push down of conjunctive parquet predicates,SPARK-2935,12732972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,08/Aug/14 23:56,14/Aug/14 00:41,14/Jul/23 06:26,14/Aug/14 00:41,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,411000,,,Sat Aug 09 00:01:37 UTC 2014,,,,,,,,,,"0|i1ypkv:",410993,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"09/Aug/14 00:01;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1863;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getAllowedLocalityLevel() throws ArrayIndexOutOfBoundsException,SPARK-2931,12732910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,08/Aug/14 19:18,22/Oct/14 22:18,14/Jul/23 06:26,15/Aug/14 06:42,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"When running Spark Perf's sort-by-key benchmark on EC2 with v1.1.0-snapshot, I get the following errors (one per task):

{code}
14/08/08 18:54:22 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, ip-172-31-14-30.us-west-2.compute.internal, PROCESS_LOCAL, 1003 bytes)
14/08/08 18:54:22 INFO cluster.SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@ip-172-31-9-213.us-west-2.compute.internal:58901/user/Executor#1436065036] with ID 0
14/08/08 18:54:22 ERROR actor.OneForOneStrategy: 1
java.lang.ArrayIndexOutOfBoundsException: 1
  at org.apache.spark.scheduler.TaskSetManager.getAllowedLocalityLevel(TaskSetManager.scala:475)
  at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:409)
  at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7$$anonfun$apply$2.apply$mcVI$sp(TaskSchedulerImpl.scala:261)
  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
  at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7.apply(TaskSchedulerImpl.scala:257)
  at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7.apply(TaskSchedulerImpl.scala:254)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
  at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:254)
  at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:254)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:254)
  at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.makeOffers(CoarseGrainedSchedulerBackend.scala:153)
  at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:103)
  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
  at akka.actor.ActorCell.invoke(ActorCell.scala:456)
  at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
  at akka.dispatch.Mailbox.run(Mailbox.scala:219)
  at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

This causes the job to hang.

I can deterministically reproduce this by re-running the test, either in isolation or as part of the full performance testing suite.","Spark EC2, spark-1.1.0-snapshot1, sort-by-key spark-perf benchmark",apachespark,joshrosen,kayousterhout,lirui,mridulm80,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2353,,,,,,,,,,,,,,,,,,,"09/Aug/14 18:11;joshrosen;scala-sort-by-key.err;https://issues.apache.org/jira/secure/attachment/12660814/scala-sort-by-key.err","09/Aug/14 19:59;mridulm80;test.patch;https://issues.apache.org/jira/secure/attachment/12660819/test.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410938,,,Fri Aug 15 06:42:38 UTC 2014,,,,,,,,,,"0|i1yp7z:",410931,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Aug/14 20:04;kayousterhout;Do you know of any way to reproduce this locally without running the full spark-perf suite?;;;","08/Aug/14 20:25;joshrosen;It's pretty quick to set up a local spark-perf that runs the 1.1.10-snapshot1 binary distribution.  Unfortunately, I tried this and was unable to reproduce the issue.  Maybe I need to run multiple local workers with different hostnames, which I'll try next.;;;","08/Aug/14 20:31;kayousterhout;I tried doing something similar to spark-perf and could not reproduce this problem; will try running it on multiples machines shortly.  In case it's useful to you, here's what I did:

val randKeys = sc.parallelize(1 to 10, 10).flatMap { x => val r = new Random(x); (1 to 20).map{ x => r.nextInt(10) }}
val kv = randKeys.map((_, 1))
kv.sortByKey().collect();;;","09/Aug/14 02:01;joshrosen;This isn't the easiest bug to reproduce.  I tried running the same spark-perf test on a smaller cluster composed of 20 m1.xlarge instances and it worked fine.  I've been able to consistently reproduce it on a cluster of 8 r3.8xlarge nodes.

It looks like the problem was introduced in https://github.com/apache/spark/pull/1313, since it works fine if I revert to the previous commit.;;;","09/Aug/14 08:07;pwendell;[~matei] Hey Matei - IIRC you looked at this patch a bunch. Do you have any guesses as to what is causing this?;;;","09/Aug/14 08:18;kayousterhout;Josh and I chatted a bit about this offline.  I suspect what's happening is:

Here we recompute myLocalityLevels: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693

...but don't reset currentLocalityLevel.

So, it seems like if the number of locality levels decreases, currentLocalityLevel can end being higher than the set of allowed levels.  If this is indeed the problem, it looks like the code was last changed by the patch you mentioned Patrick/Josh.;;;","09/Aug/14 12:54;mridulm80;[~kayousterhout] this is weird, I remember mentioned this exact same issue in some PR for 1.1 (trying to find which one, though not 1313 iirc); and I think it was supposed to have been addressed.
We had observed this issue of currentLocalityLevel running away when we had internally merged the pr.

Strange that it was not addressed, speaks volumes of me not following up on my reviews !;;;","09/Aug/14 18:11;joshrosen;@ [~kayousterhout]: I can see how that code in {{executorLost()}} could be problematic, but I don't think that's what's triggering this bug since I don't see any ""Re-queueing tasks for ..."" messages in my log.

I've attached a full log from a failing test run.  If it helps, I can re-run at DEBUG logging or add extra logging statements.;;;","09/Aug/14 18:49;mridulm80;Checking more, it might have been for an internal review : not sure, cant find an external reference.
So the issue is as [~kayousterhout] described, after locality levels are updated (as part of an executor loss, etc); the index is not updated accordingly (or reset).

The earlier code was assuming the population of the data structures wont change once created - which is no longer the case.
A testcase to simulate this should be possible - rough sketch :
a) Add two executors one PROCESS_LOCAL executor exec1 and one ANY executors exec2.
my locality levels should now contain all levels now due to exec1
b) schedule a task on the process_local executors, and wait sufficiently such that level is now at rack or any.
c) fail the process_local executor - iirc currently this will not cause recomputation of levels.
d) Add an ANY executor exec3 - this will trigger computeValidLocalityLevels in executorAdded (exec failure does not, we probably should add that).
e) Now, any invocation of getAllowedLocalityLevel will cause the exception Josh mentioned.


and ensure the corresponding cleanup is triggered; to cause an update
;;;","09/Aug/14 19:59;mridulm80;A patch to showcase the exception;;;","09/Aug/14 19:59;mridulm80;[~joshrosen] [~kayousterhout] Added a patch which deterministically showcases the bug - should be easy to fix it now I hope :-);;;","11/Aug/14 01:52;lirui;I think this may be introduced in [#892|https://github.com/apache/spark/pull/892]. When the valid locality levels are recomputed I didn't reset the current level. Sorry about this.;;;","11/Aug/14 16:45;joshrosen;Thanks for investigating and reproducing this issue.  Is someone planning to open a PR with a fix?  If not, I can probably do it later this afternoon, since this bug is a blocker for many of the spark-perf tests that I'm running.;;;","11/Aug/14 21:07;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1896;;;","15/Aug/14 06:42;pwendell;This was fixed in:
https://github.com/apache/spark/pull/1896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TorrentBroadcast should use the user specified serializer,SPARK-2928,12732879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,rxin,rxin,08/Aug/14 17:42,08/Aug/14 23:59,14/Jul/23 06:26,08/Aug/14 23:59,,,,,,,,1.0.3,1.1.0,,,,,Spark Core,,,,0,,,,,,"TorrentBroadcast.blockifyObject and unBlockifyObject always use the Java serializer to serialize data. Since TorrentBroadcast is now turned on by default, we'd need to fix it to use the user specified serializer (which is set in SparkEnv).

",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2897,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410907,,,2014-08-08 17:42:23.0,,,,,,,,,,"0|i1yp13:",410900,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a conf to configure if we always read Binary columns stored in Parquet as String columns,SPARK-2927,12732876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,08/Aug/14 17:21,12/Sep/14 21:09,14/Jul/23 06:26,14/Aug/14 18:05,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Based on Parquet spec (https://github.com/Parquet/parquet-format), ""strings are stored as byte arrays (binary) with a UTF8 annotation"". However, if the data generator does not follow it, we will only read binary values back instead of string values.",,apachespark,chutium,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2699,,,,,,SPARK-2699,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410904,,,Fri Aug 15 14:29:58 UTC 2014,,,,,,,,,,"0|i1yp0f:",410897,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Aug/14 17:26;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1855;;;","15/Aug/14 14:29;chutium;SPARK-2699 could also be closed, these two ticket are almost the same. only one comment about auto-detection: https://github.com/apache/spark/pull/1855#discussion-diff-16294353;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/spark-sql shell throw unrecognized option error when set --driver-java-options,SPARK-2925,12732773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,08/Aug/14 07:31,14/Aug/14 18:04,14/Jul/23 06:26,14/Aug/14 18:04,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"run cmd like this will get the error

bin/spark-sql --driver-java-options '-Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,address=8788,server=y,suspend=y'

Error: Unrecognized option '-Xnoagent'.
Run with --help for usage help or --verbose for debug output
",,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410801,,,Fri Aug 08 07:41:49 UTC 2014,,,,,,,,,,"0|i1yodr:",410794,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Aug/14 07:41;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/1851;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark web ui: Internal Error: Missing Template ERR_DNS_FAIL,SPARK-2922,12732751,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hansen,hansen,08/Aug/14 03:34,08/Aug/14 09:52,14/Jul/23 06:26,08/Aug/14 04:07,1.0.0,,,,,,,,,,,,,Spark Core,,,,0,,,,,,,,hansen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410779,,,Fri Aug 08 09:52:45 UTC 2014,,,,,,,,,,"0|i1yo8v:",410772,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/14 09:52;srowen;This doesn't seem to be anything to do with Spark. It's a Windows system error?
http://superuser.com/questions/221633/getting-err-dns-fail-when-loading-a-local-webserver-page;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TorrentBroadcast does not support broadcast compression ,SPARK-2920,12732746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gq,joesu,joesu,08/Aug/14 02:44,08/Aug/14 23:58,14/Jul/23 06:26,08/Aug/14 23:58,1.0.0,1.1.0,,,,,,1.0.3,1.1.0,,,,,,,,,0,,,,,,"TorrentBroadcast always broadcast uncompressed content. The spark option ""spark.broadcast.compress"" has no effect for TorrentBroadcast. 

",,joesu,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2897,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410774,,,2014-08-08 02:44:20.0,,,,,,,,,,"0|i1yo7r:",410767,,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic support  for analyze command in HiveQl,SPARK-2919,12732745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,08/Aug/14 02:36,08/Aug/14 18:25,14/Jul/23 06:26,08/Aug/14 18:25,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"We need to support sql(""analyze tableName"")",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410773,,,Fri Aug 08 04:42:17 UTC 2014,,,,,,,,,,"0|i1yo7j:",410766,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Aug/14 04:42;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1848;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid CTAS creates table in logical plan analyzing.,SPARK-2917,12732742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,08/Aug/14 02:15,11/Sep/14 18:58,14/Jul/23 06:26,11/Sep/14 18:58,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Currently, the CTAS will create the table in logical plan analyzing, right before the real execution triggered.
E.g. in
CREATE TABLE s1 as select * from src where key=0;",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2918,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410770,,,Fri Aug 08 02:21:57 UTC 2014,,,,,,,,,,"0|i1yo6v:",410763,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"08/Aug/14 02:21;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/1846;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[MLlib] While running regression tests with dense vectors of length greater than 1000, the treeAggregate blows up after several iterations",SPARK-2916,12732741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,brkyvz,brkyvz,08/Aug/14 01:54,16/Aug/14 05:56,14/Jul/23 06:26,16/Aug/14 05:56,,,,,,,,,,,,,,MLlib,Spark Core,,,0,,,,,,"While running any of the regression algorithms with gradient descent, the treeAggregate blows up after several iterations.

Observed on EC2 cluster with 16 nodes, matrix dimensions of 1,000,000 x 5,000

In order to replicate the problem, use aggregate multiple times, maybe over 50-60 times.

Testing lead to the possible workaround:
setting 
`spark.cleaner.referenceTracking false`

seems to help. So the problem is most probably related to the cleanup.
",,brkyvz,huasanyelao,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410769,,,Sat Aug 16 05:56:41 UTC 2014,,,,,,,,,,"0|i1yo6n:",410762,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Aug/14 08:57;mengxr;[~brkyvz] I tried running computeColumnSummaryStatistics() 300 times with matrix size 1,000,000 x 10,000 on a 16 nodes m3.2xlarge cluster. I used the latest branch-1.1. Could you try again with the latest branch-1.1?;;;","08/Aug/14 09:00;brkyvz;will do;;;","15/Aug/14 22:26;pwendell;Just to document for posterity - this was narrowed down and is just a symptom of SPARK-3015.;;;","16/Aug/14 05:56;pwendell;Fixed by virtue of SPARK-3015;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Storage summary table UI glitch when using sparkSQL,SPARK-2915,12732740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,falaki,falaki,falaki,08/Aug/14 01:32,07/Oct/14 18:47,14/Jul/23 06:26,07/Oct/14 18:47,1.0.2,,,,,,,1.1.1,1.2.0,,,,,Web UI,,,,0,WebUI,,,,,"When using sqlContext.cacheTable() a registered table. the name of the RDD becomes a very large string (related to the query that created the sqlRDD). As a result the first columns of the storage tab in SparkUI becomes very long and the other columns become squashed.

Since the name of the RDD is not human readable, we can simply set ellipsis in the first cell (which will hide the rest of string). Alternatively we can fix the RDD name to a more readable and shorter name.",Standalone,falaki,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3827,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410768,,,Tue Oct 07 18:47:39 UTC 2014,,,,,,,,,,"0|i1yo6f:",410761,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/14 18:47;joshrosen;Fixed by https://github.com/apache/spark/pull/2687;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonRDD.nullTypeToStringType does not convert all NullType to StringType,SPARK-2908,12732664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,07/Aug/14 20:38,08/Aug/14 18:10,14/Jul/23 06:26,08/Aug/14 18:10,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Failed on
{code}
val schemaRDD = jsonRDD(sparkContext.parallelize(Seq(""""""{""record"": [{""children"": null}]}"""""")))
{code}
{code}
schemaRDD.schema
StructType(ArrayBuffer(StructField(record,ArrayType(StructType(ArrayBuffer(StructField(children,NullType,true))),false),true)))
{code}
You can see the children field is still NullType.",,apachespark,nchammas,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2695,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410692,,,Thu Aug 07 20:57:12 UTC 2014,,,,,,,,,,"0|i1ynpr:",410685,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"07/Aug/14 20:57;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1840;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-sql shows 'sbin' instead of 'bin' in the 'usage' string,SPARK-2905,12732539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,olegd,olegd,olegd,07/Aug/14 12:54,07/Aug/14 22:53,14/Jul/23 06:26,07/Aug/14 22:52,1.1.0,,,,,,,1.1.0,,,,,,Deploy,,,,0,,,,,,"Usage: ./sbin/spark-sql [options] [cli option]

Should be ./bin/spark-sql [options] [cli option]",,apachespark,olegd,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410567,,,Thu Aug 07 22:52:17 UTC 2014,,,,,,,,,,"0|i1ymy7:",410561,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/14 13:02;apachespark;User 'dosoft' has created a pull request for this issue:
https://github.com/apache/spark/pull/1835;;;","07/Aug/14 22:52;pwendell;Issue resolved by pull request 1835
[https://github.com/apache/spark/pull/1835];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inputBytes aren't aggregated for stages like other task metrics,SPARK-2900,12732475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,sandyr,sandyr,07/Aug/14 06:29,18/Aug/14 05:39,14/Jul/23 06:26,18/Aug/14 05:39,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,This issue was caused by a race between SPARK-2099 and SPARK-1683,,apachespark,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410503,,,Mon Aug 18 05:39:26 UTC 2014,,,,,,,,,,"0|i1ymk7:",410497,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/14 06:42;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/1826;;;","18/Aug/14 05:39;pwendell;Issue resolved by pull request 1826
[https://github.com/apache/spark/pull/1826];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to connect to daemon,SPARK-2898,12732466,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,07/Aug/14 05:19,15/Jun/15 11:34,14/Jul/23 06:26,10/Aug/14 20:02,1.1.0,,,,,,,1.1.0,,,,,,PySpark,,,,0,,,,,,"There is a deadlock  in handle_sigchld() because of logging

--------------------------------------------------------------------
Java options: -Dspark.storage.memoryFraction=0.66 -Dspark.serializer=org.apache.spark.serializer.JavaSerializer -Dspark.executor.memory=3g -Dspark.locality.wait=60000000
Options: SchedulerThroughputTest --num-tasks=10000 --num-trials=4 --inter-trial-wait=1
--------------------------------------------------------------------
14/08/06 22:09:41 WARN JettyUtils: Failed to create UI on port 4040. Trying again on port 4041. - Failure(java.net.BindException: Address already in use)
worker 50114 crashed abruptly with exit status 1
14/08/06 22:10:37 ERROR Executor: Exception in task 1476.0 in stage 1.0 (TID 11476)
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:101)
	... 10 more
14/08/06 22:10:37 WARN PythonWorkerFactory: Failed to open socket to Python daemon:
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:241)
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:68)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/08/06 22:10:37 ERROR Executor: Exception in task 1478.0 in stage 1.0 (TID 11478)
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/08/06 22:10:37 WARN PythonWorkerFactory: Assuming that daemon unexpectedly quit, attempting to restart
14/08/06 22:10:37 WARN TaskSetManager: Lost task 1476.0 in stage 1.0 (TID 11476, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)
        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
14/08/06 22:10:37 ERROR TaskSetManager: Task 1476 in stage 1.0 failed 1 times; aborting job
14/08/06 22:10:37 WARN TaskSetManager: Lost task 1478.0 in stage 1.0 (TID 11478, localhost): java.io.EOFException: 
        java.io.DataInputStream.readInt(DataInputStream.java:392)
        org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)
        org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
        org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
        org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
        org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)


Another one:

Daemon failed to fork PySpark worker: [Errno 35] Resource temporarily unavailable
14/08/07 12:04:37 ERROR Executor: Exception in task 15579.0 in stage 0.0 (TID 15579)
java.lang.IllegalStateException: Python daemon failed to launch worker
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:71)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/08/07 12:04:37 WARN TaskSetManager: Lost task 15579.0 in stage 0.0 (TID 15579, localhost): java.lang.IllegalStateException: Python daemon failed to launch worker
        org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:71)
        org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
        org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
        org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
        org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
14/08/07 12:04:37 ERROR TaskSetManager: Task 15579 in stage 0.0 failed 1 times; aborting job



worker 17037 crashed abruptly with exit status 1
14/08/07 12:06:34 ERROR Executor: Exception in task 19607.0 in stage 0.0 (TID 19607)
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/08/07 12:06:34 WARN PythonWorkerFactory: Failed to open socket to Python daemon:
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:241)
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:68)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
4/08/07 12:06:34 ERROR Executor: Exception in task 19604.0 in stage 0.0 (TID 19604)
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:101)
	... 10 more
14/08/07 12:06:34 WARN PythonWorkerFactory: Assuming that daemon unexpectedly quit, attempting to restart
14/08/07 12:06:34 WARN TaskSetManager: Lost task 19604.0 in stage 0.0 (TID 19604, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)
        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
14/08/07 12:06:34 ERROR TaskSetManager: Task 19604 in stage 0.0 failed 1 times; aborting job
14/08/07 12:06:34 WARN TaskSetManager: Lost task 19607.0 in stage 0.0 (TID 19607, localhost): java.io.EOFException:
        java.io.DataInputStream.readInt(DataInputStream.java:392)
        org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)
        org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)
        org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
        org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
        org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)


14/08/07 13:29:01 WARN PythonWorkerFactory: Assuming that daemon unexpectedly quit, attempting to restart
14/08/07 13:29:01 ERROR Executor: Exception in task 9085.0 in stage 0.0 (TID 9085)
java.io.IOException: Cannot run program ""python"": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:149)
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:89)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:184)
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1022)
	... 14 more
14/08/07 13:29:01 ERROR Executor: Exception in task 9084.0 in stage 0.0 (TID 9084)
java.io.IOException: Cannot run program ""python"": error=316, Unknown error: 316
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:149)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:79)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: error=316, Unknown error: 316
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:184)
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1022)
	... 13 more


",,apachespark,davies,taylor_tails,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410494,,,Mon Jun 15 11:34:09 UTC 2015,,,,,,,,,,"0|i1ymi7:",410488,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/14 21:57;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1842;;;","15/Jun/15 11:34;taylor_tails;FYI 

java.io.IOException: Cannot run program ""python"": error=316, Unknown error: 316

I have seen this error to occur on mac because lib/jspawnhelper is missing execute permissions in your jre.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"org.apache.spark.broadcast.TorrentBroadcast does use the serializer class specified in the spark option ""spark.serializer""",SPARK-2897,12732463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gq,joesu,joesu,07/Aug/14 04:51,08/Aug/14 23:59,14/Jul/23 06:26,08/Aug/14 23:59,1.0.2,1.1.0,,,,,,1.0.3,1.1.0,,,,,Spark Core,,,,0,,,,,,"HTTPBroadcast will changes the serializer according to the setting in ""spark.serializer"".

However, TorrentBroadcast does not read that option. It always uses Java built-in serializer. ( Specifically, it uses org.apache.spark.util.Utils.serialize() and org.apache.spark.util.Utils.deserialize() ). 

With TorrentBroadcast, Spark programs can not broadcast objects that can be serialized with custom Kryo serializer but not with Java built-in serializer. 

",,apachespark,gq,joesu,pllee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2928,,,,,,SPARK-2920,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410491,,,Thu Aug 07 13:46:40 UTC 2014,,,,,,,,,,"0|i1ymhj:",410485,,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,,"07/Aug/14 13:46;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/1836;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not swallow exception when cannot find custom Kryo registrator,SPARK-2893,12732449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,grahamdennis,grahamdennis,grahamdennis,07/Aug/14 01:58,14/Aug/14 09:25,14/Jul/23 06:26,14/Aug/14 09:25,1.0.2,,,,,,,1.0.3,1.1.0,,,,,Spark Core,,,,0,,,,,,"If for any reason a custom Kryo registrator cannot be found, the task (& application) should fail, because otherwise problems could occur later during data serialisation / deserialisation.  See SPARK-2878 for an example.",,apachespark,grahamdennis,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410477,,,Thu Aug 07 06:47:26 UTC 2014,,,,,,,,,,"0|i1ymef:",410471,,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,,"07/Aug/14 06:47;apachespark;User 'GrahamDennis' has created a pull request for this issue:
https://github.com/apache/spark/pull/1827;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Socket Receiver does not stop when streaming context is stopped,SPARK-2892,12732446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,07/Aug/14 01:44,09/Feb/15 20:46,14/Jul/23 06:26,09/Feb/15 20:46,1.0.2,,,,,,,1.2.1,,,,,,DStreams,,,,6,,,,,,"Running NetworkWordCount with
{quote}      
ssc.start(); Thread.sleep(10000); ssc.stop(stopSparkContext = false); Thread.sleep(60000)
{quote}

gives the following error

{quote}
14/08/06 18:37:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10047 ms on localhost (1/1)
14/08/06 18:37:13 INFO DAGScheduler: Stage 0 (runJob at ReceiverTracker.scala:275) finished in 10.056 s
14/08/06 18:37:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
14/08/06 18:37:13 INFO SparkContext: Job finished: runJob at ReceiverTracker.scala:275, took 10.179263 s
14/08/06 18:37:13 INFO ReceiverTracker: All of the receivers have been terminated
14/08/06 18:37:13 WARN ReceiverTracker: All of the receivers have not deregistered, Map(0 -> ReceiverInfo(0,SocketReceiver-0,null,false,localhost,Stopped by driver,))
14/08/06 18:37:13 INFO ReceiverTracker: ReceiverTracker stopped
14/08/06 18:37:13 INFO JobGenerator: Stopping JobGenerator immediately
14/08/06 18:37:13 INFO RecurringTimer: Stopped timer for JobGenerator after time 1407375433000
14/08/06 18:37:13 INFO JobGenerator: Stopped JobGenerator
14/08/06 18:37:13 INFO JobScheduler: Stopped JobScheduler
14/08/06 18:37:13 INFO StreamingContext: StreamingContext stopped successfully
14/08/06 18:37:43 INFO SocketReceiver: Stopped receiving
14/08/06 18:37:43 INFO SocketReceiver: Closed socket to localhost:9999
{quote}",,cfregly,helena_e,huitseeker,ilayaperumalg,joshrosen,lbustelo,liqusha,markfisher,nchammas,pwendell,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4802,SPARK-5035,SPARK-2464,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410474,,,Mon Feb 09 20:46:10 UTC 2015,,,,,,,,,,"0|i1ymdr:",410468,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/14 13:08;helena_e;I see the same with 1.0.2 streaming, with or without stopGracefully = true

ssc.stop(stopSparkContext = false, stopGracefully = true)

ERROR 08:26:21,139 Deregistered receiver for stream 0: Stopped by driver
 WARN 08:26:21,211 Stopped executor without error
 WARN 08:26:21,213 All of the receivers have not deregistered, Map(0 -> ReceiverInfo(0,ActorReceiver-0,null,false,host,Stopped by driver,))
;;;","04/Sep/14 16:58;helena_e;I wonder if the ERROR should be a WARN or INFO since it occurs as a result of ReceiverSupervisorImpl receiving a StopReceiver, and "" Deregistered receiver for stream"" seems like the expected behavior.


DEBUG 13:00:22,418 Stopping JobScheduler
 INFO 13:00:22,441 Received stop signal
 INFO 13:00:22,441 Sent stop signal to all 1 receivers
 INFO 13:00:22,442 Stopping receiver with message: Stopped by driver: 
 INFO 13:00:22,442 Called receiver onStop
 INFO 13:00:22,443 Deregistering receiver 0
ERROR 13:00:22,445 Deregistered receiver for stream 0: Stopped by driver
 INFO 13:00:22,445 Stopped receiver 0;;;","05/Sep/14 17:54;tdas;I intended it to be ERROR to catch such issues where receivers dont stop properly. If a program is supposed to shutdown after stopping the streaming context, then this probably not much of a problem as everything of Spark is torn down anyways. But a SparkContext is going to be reused, then this is indeed a problem. ;;;","18/Sep/14 15:28;lbustelo;Any update on this? Will it get fixed for 1.0.3 or 1.1.0;;;","25/Nov/14 16:58;lbustelo;Update?;;;","10/Dec/14 00:54;ilayaperumalg;It looks like this one and the issue mentioned in SPARK-4802 (ReceiverInfo removal at ReceiverTracker upon deregistering receiver) are related. I believe the following warning message is the result of receiverInfo not being removed at ReceiverTracker by the ReceiverTrackerActor when the corresponding receiver is deregistered.

""WARN ReceiverTracker: All of the receivers have not deregistered, Map(0 -> ReceiverInfo(0,SocketReceiver-0,null,false,localhost,Stopped by driver,))""

From what I can see so far, closing the streaming context stops the receiver only in ""local"" mode.

In ""cluster"" mode, using the Spark standalone cluster I noticed that when the ReceiverTracker at the driver sends the ""StopReceiver"" message as a result of streaming context close,  it couldn't reach to the ReceiverSupervisorImpl's actor that is running at the executor node.  At the same time, the ReceiverSupervisorImpl at the executor could send the messages such as RegisterReceiver, AddBlock back to the ReceiverTrackerActor at the driver.

It would be great if someone could explain what might be going on from ReceiverTracker -> ReceiverSupervisorImpl actor at executor when sending the stop signal in the distributed mode case.

Thanks!;;;","10/Dec/14 06:23;srowen;This does indeed look like the same issue. Since SPARK-4802 has an open PR I think it should continue there.;;;","10/Dec/14 16:48;markfisher;[~srowen] SPARK-4802 is only related to the receiverInfo not being removed. This issue is actually much more critical, given that Receivers do not seem to stop other than in local mode. Please reopen.
;;;","10/Dec/14 23:25;ilayaperumalg;To add more info:

When the ReceiverTracker sends the ""StopReceiver"" message to the receiver actor at the executor, it's ReceiverLauncher thread always times out and I notice the corresponding job is cancelled only because of stopping the DAGScheduler. This throws the exception[1] while at the executor side the worker node throws this info[2]

Exception[1]:
INFO sparkDriver-akka.actor.default-dispatcher-14 cluster.SparkDeploySchedulerBackend - Asking each executor to shut down
15:06:53,783 1.1.0.SNAP  INFO Thread-40 scheduler.DAGScheduler - Job 1 failed: start at SparkDriver.java:109, took 72.739141 s
Exception in thread ""Thread-40"" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:701)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:701)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.postStop(DAGScheduler.scala:1428)
	at akka.actor.Actor$class.aroundPostStop(Actor.scala:475)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundPostStop(DAGScheduler.scala:1375)
	at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
	at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
	at akka.actor.ActorCell.terminate(ActorCell.scala:369)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

info[2]
INFO LocalActorRef: Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%40127.0.0.1%3A52219-2#1619424601] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14/12/10 15:06:53 ERROR EndpointWriter: AssociationError [akka.tcp://sparkWorker@localhost:51262] <- [akka.tcp://sparkExecutor@localhost:52217]: Error [Shut down address: akka.tcp://sparkExecutor@localhost:52217] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@localhost:52217
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.

Please note that I am running everything on localhost and the same thing works fine on ""local"" mode and the above issue only arises on ""cluster"" mode. I tried changing the hostname to 127.0.0.1 but noticed the same.

Any clues on what might be going on here would help a lot.
Thanks!;;;","11/Dec/14 16:59;srowen;OK. The issues may have a common cause but that can be deferred until the other JIRA is resolved. If it turns out to resolve this, great.;;;","24/Dec/14 21:48;tdas;[~ilayaperumalg] Now that SPARK-4802 has been solved could you check whether this issue has been resolved?

;;;","31/Dec/14 01:56;tdas;[~ilayaperumalg] Ping, any thoughts?;;;","31/Dec/14 09:24;joshrosen;I think that I've found the reason that this works in local mode and not on a real cluster; see SPARK-5035  / https://github.com/apache/spark/pull/3857;;;","31/Dec/14 12:07;ilayaperumalg;[~joshrosen] This indeed fixes the issue on stopping the receiver. thanks!

P.S: I deleted my prior comment from the JIRA as the error I noticed was due to some setup issue on my cluster environment.;;;","09/Feb/15 20:46;pwendell;I believe this is fixed by SPARK-5035, so I'm closing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL should allow SELECT with duplicated columns,SPARK-2890,12732433,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,huangjs,huangjs,07/Aug/14 00:17,03/Jul/15 18:32,14/Jul/23 06:26,16/Sep/14 18:43,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Spark reported error java.lang.IllegalArgumentException with messages:

java.lang.IllegalArgumentException: requirement failed: Found fields with the same name.
        at scala.Predef$.require(Predef.scala:233)
        at org.apache.spark.sql.catalyst.types.StructType.<init>(dataTypes.scala:317)
        at org.apache.spark.sql.catalyst.types.StructType$.fromAttributes(dataTypes.scala:310)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertToString(ParquetTypes.scala:306)
        at org.apache.spark.sql.parquet.ParquetTableScan.execute(ParquetTableOperations.scala:83)
        at org.apache.spark.sql.execution.Filter.execute(basicOperators.scala:57)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:85)
        at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:433)

After trial and error, it seems it's caused by duplicated columns in my select clause.

I made the duplication on purpose for my code to parse correctly. I think we should allow users to specify duplicated columns as return value.

Jianshi",,apachespark,btiernay,huangjs,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8817,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410461,,,Tue Jan 20 17:00:10 UTC 2015,,,,,,,,,,"0|i1ymav:",410455,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"07/Aug/14 07:20;huangjs;In previous versions, there was no warnings but the result is buggy which contains nulls in duplicated columns.

Jianshi
;;;","11/Aug/14 18:34;yhuai;What is the semantic when you have columns with same names?;;;","12/Aug/14 05:08;huangjs;My use case:

The result will be parsed into (id, type, start, end, properties) tuples. Properties might or might not contain any of (id, type, start end). So it's easier just to list them at the end and not to worry about duplicated names.

Jianshi;;;","13/Aug/14 06:58;huangjs;I think the fault is on my side. I should've changed project the duplicated columns into different names.

So the current behavior makes sense. I'll close this issue.

Jianshi;;;","29/Aug/14 22:12;marmbrus;There are cases where this does actually make it impossible to read data.  For example when you are trying read a schema that becomes ambiguous due to case insensitive resolution. So I think we should do something here.;;;","29/Aug/14 22:15;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/2209;;;","30/Aug/14 01:52;yhuai;Do we also want to handle ambiguity introduced by join?

For example, 
{code:sql}
CREATE TABLE t1 (key int, value string);
CREATE TABLE t2 (key int, value string);
SELECT x.key, x.value, y.key, y.value FROM t1 LEFT OUTER JOIN t2 ON (t1.key = t2.key);
{code}

For this kind of cases, there will be an exception when users try to get the schema of the result SchemaRDD. I feel it is better to keep the exception and ask users to use AS to rename those columns.;;;","20/Jan/15 16:02;btiernay;What if you request {{SELECT x.\*, y.\*}}? If there are 20 columns on each side, is the user required to specify them all?;;;","20/Jan/15 17:00;yhuai;[~btiernay] Oh, seems the comments thread of this JIRA is not quite clear on if this issue has been resolved. Actually, we have relaxed this restriction (https://github.com/apache/spark/pull/2209/files is the change). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark creates Hadoop Configuration objects inconsistently ,SPARK-2889,12732420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,06/Aug/14 23:20,02/Sep/14 15:26,14/Jul/23 06:26,30/Aug/14 21:48,,,,,,,,1.2.0,,,,,,Spark Core,,,,0,,,,,,"Looking through Spark code, there are 3 ways used to get Configuration objects:

- Use SparkContext.hadoopConfiguration
- Use SparkHadoopUtil.newConfiguration
- Call {{new Configuration()}} directly

Only the first one supports setting hadoop configs via {{spark.hadoop.*}} properties. We should probably make everybody agree about how to do things.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3347,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410448,,,Thu Aug 07 23:36:34 UTC 2014,,,,,,,,,,"0|i1ym7z:",410442,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/14 23:36;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix addColumnMetadataToConf in HiveTableScan,SPARK-2888,12732418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,06/Aug/14 22:32,08/Aug/14 18:02,14/Jul/23 06:26,08/Aug/14 18:02,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410446,,,Wed Aug 06 22:37:09 UTC 2014,,,,,,,,,,"0|i1ym7j:",410440,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Aug/14 22:37;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1817;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD.countApproxDistinct() is wrong when RDD has more one partition,SPARK-2887,12732402,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,06/Aug/14 21:39,07/Aug/14 04:41,14/Jul/23 06:26,07/Aug/14 04:29,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,07/Aug/14 00:00,0,,,,,,"

scala> sc.makeRDD(1 to 1000, 10).countApproxDistinct(0.01)
res0: Long = 101",,apachespark,davies,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410430,,,Thu Aug 07 04:41:47 UTC 2014,,,,,,,,,,"0|i1ym33:",410418,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Aug/14 21:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1812;;;","07/Aug/14 04:24;pwendell;I merged this into 1.1 - [~davies] could you submit a PR for 1.0 and/or 0.9? It didn't merge cleanly into those branches.;;;","07/Aug/14 04:29;pwendell;Actually I looked back and I don't think this bug is relevant for 1.0 or 0.9;;;","07/Aug/14 04:41;davies;Yes, it only happens in master and 1.1, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create binary builds in parallel with release script,SPARK-2884,12732383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,06/Aug/14 19:55,27/Aug/14 07:35,14/Jul/23 06:26,27/Aug/14 07:35,,,,,,,,,,,,,,Build,Project Infra,,,0,,,,,,,,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410411,,,Wed Aug 27 07:35:04 UTC 2014,,,,,,,,,,"0|i1ylz3:",410400,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"27/Aug/14 07:35;pwendell;This was fixed in:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a5ae720745d744ec29741b49d2d362f362d53fa4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy is now default codec - could lead to conflicts since uses /tmp,SPARK-2881,12732338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,tgraves,tgraves,06/Aug/14 17:08,01/Sep/14 05:49,14/Jul/23 06:26,18/Aug/14 00:07,1.1.0,,,,,,,1.1.0,1.2.0,,,,,Spark Core,,,,0,,,,,,"I was using spark master branch and I ran into an issue with Snappy since its now the default codec for shuffle. 

The issue was that someone else had run with snappy and it created /tmp/snappy-*.so but it had restrictive permissions so I was not able to use it or remove it.   This caused my spark job to not start.  

I was running in yarn client mode at the time.  Yarn cluster mode shouldn't have this issue since we change the java.io.tmpdir. 
I assume this would also affect standalone mode.

I'm not sure if this is a true blocker but wanted to file it as one at first and let us decide.",,apachespark,mridulm80,pwendell,taroleo,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-7916,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410366,,,Mon Aug 18 00:07:27 UTC 2014,,,,,,,,,,"0|i1ylp3:",410355,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/14 18:45;mridulm80;To add, this will affect spark whenever tmp directory is not overridden via java.io.tmpdir to something ephemeral.

So local, yarn client, standalone should be affected by default (unless I missed something in the scripts).
I am not very of how mesos runs jobs, so cant comment about that, anyone care to add ?


A workaround I can think of is to always set 'org.xerial.snappy.tempdir' to a randomly generated directory under ""java.io.tmpdir"" as part of spark startup (only) once : which will cause snappy to use that directory and avoid this issue. 


Since snappy is the default codec now, I am +1 on marking this as a blocker for release;;;","07/Aug/14 23:08;pwendell;What about using the existing spark local dir for this?;;;","17/Aug/14 01:36;pwendell;Actually since this uses a static system property it might be better to just do [~mridul]'s suggestion and keep this simple. One possibility is just a static code block in the Snappy compression codec itself that sets it to a random sub directory of /tmp/;;;","17/Aug/14 02:30;pwendell;I filed a bug to have snappy deal with this better out of the box.

https://github.com/xerial/snappy-java/issues/84;;;","17/Aug/14 02:42;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/1991;;;","17/Aug/14 02:59;pwendell;It turns out this is actually fixed in newer versions of snappy-java. I'm a little hesitant to do a version bump this late in the release cycle, but we do have the option of bumping the version as well.;;;","17/Aug/14 03:06;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/1994;;;","17/Aug/14 03:11;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/1995;;;","17/Aug/14 03:26;taroleo;This problem is fixed in snappy-java 1.1.1.3. 
But for your convenience, I applied a hot fix and released 1.0.5.3:
https://github.com/xerial/snappy-java/commit/89277ddb7a9982126d444af3a290a1d68953ac66

This version 1.0.5.3 is now available in Maven central:
http://central.maven.org/maven2/org/xerial/snappy/snappy-java/
;;;","17/Aug/14 17:31;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/1999;;;","17/Aug/14 22:49;pwendell;Okay I've merged a change in branch-1.1 updating the version to snappy-java 1.0.5.3 so this is no longer blocking Spark 1.1. I've also submitted a patch to the master branch updating to 1.1.1.3. We can merge that when tests pass.;;;","18/Aug/14 00:07;pwendell;Fixed in master branch via:
https://github.com/apache/spark/pull/1995;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent Kryo serialisation with custom Kryo Registrator,SPARK-2878,12732249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,grahamdennis,grahamdennis,grahamdennis,06/Aug/14 08:19,28/Feb/15 03:19,14/Jul/23 06:26,16/Aug/14 22:00,1.0.0,1.0.2,,,,,,1.0.3,1.1.0,,,,,Spark Core,,,,2,,,,,,"The custom Kryo Registrator (a class with the org.apache.spark.serializer.KryoRegistrator trait) is not used with every Kryo instance created, and this causes inconsistent serialisation and deserialisation.

The Kryo Registrator is sometimes not used because of a ClassNotFound exception that only occurs if it *isn't* the Worker thread (of an Executor) that tries to create the KryoRegistrator.

A complete description of the problem and a project reproducing the problem can be found at https://github.com/GrahamDennis/spark-kryo-serialisation

I have currently only tested this with Spark 1.0.0, but will try to test against 1.0.2.","Linux RedHat EL 6, 4-node Spark cluster.",aash,apachespark,grahamdennis,hammer,pwendell,rxin,sorenmacbeth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3046,,,SPARK-3070,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410278,,,Sat Aug 16 22:00:56 UTC 2014,,,,,,,,,,"0|i1yl5z:",410268,,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,,"06/Aug/14 08:40;grahamdennis;I've now reproduced this with Spark 1.0.2;;;","07/Aug/14 00:44;grahamdennis;This issue is possibly the root cause of a number of similar problems on the web:
https://spark-project.atlassian.net/browse/SPARK-755
https://mail-archives.apache.org/mod_mbox/spark-user/201310.mbox/%3CCABeDAjGP5cZvRyVQVq8XVqcmiPnxG1bAi+DJ3TowREeQ6oPRjQ@mail.gmail.com%3E
https://groups.google.com/forum/#!topic/spark-users/gJMjhm8cby0
http://stackoverflow.com/questions/19629360/spark-with-cassandra-failed-to-register-spark-kryo-registrator
http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-Issue-on-Spark-1-0-1-Mesos-0-18-2-td7498.html
http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html;;;","07/Aug/14 01:19;grahamdennis;Stacktrace of the root cause ""Failed to run spark.kryo.registrator"" error, demonstrating that Kryo deserialisation is happening on a ConnectionManager thread:

{code}
14/08/06 18:37:52 ERROR serializer.KryoSerializer: Failed to run spark.kryo.registrator
java.lang.ClassNotFoundException: org.example.MyRegistrator
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:78)
	at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:76)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:76)
	at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:133)
	at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:95)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:995)
	at org.apache.spark.storage.BlockManager.dataSerialize(BlockManager.scala:1005)
	at org.apache.spark.storage.MemoryStore.getBytes(MemoryStore.scala:102)
	at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:384)
	at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:359)
	at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90)
	at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)
	at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
	at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:662)
	at org.apache.spark.network.ConnectionManager$$anon$9.run(ConnectionManager.scala:504)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code};;;","07/Aug/14 02:17;sorenmacbeth;Great find! 

This has definitely bitten me. I found a lame workround, but It would be fantastic to have this well and truly fixed. Flambo needs kryo to function (not to mention that kryo is much faster than java serialization almost all the time).  ;;;","07/Aug/14 02:23;grahamdennis;I should mention that my current hacky workaround is to set `spark.task.maxFailures` to something very large (1000) because the failures are non-deterministic, and despite task failures, progress does happen.  If you have a better workaround, I'm interested!;;;","07/Aug/14 03:40;sorenmacbeth;My particular case _feels_ a bit different, although I suspect the root
case is likely the same. In my case, intermittently, some of the scala
collection classes that are part of chill and part of the default kryo
serializer wouldn't get registered in my mesos slave tasks. I fixed it my
explicitly registering them in my custom serializer to ensure the the class
in question, scala's `Some`, always had a consistent id in kryo.

I haven't commented out my hack in >= 1.0.0 to see if it's still an issue
or not.


On Wed, Aug 6, 2014 at 7:23 PM, Graham Dennis (JIRA) <jira@apache.org>




-- 
http://about.me/soren
;;;","07/Aug/14 19:10;aash;I think I'm seeing this too in Spark 1.0.1, stacktraces appear in the webui that look like this:

{noformat}
java.lang.IndexOutOfBoundsException (java.lang.IndexOutOfBoundsException: Index: 95, Size: 0}
  java.util.ArrayList.rangeCheck(ArrayList.java:635)
  java.util.ArrayList.get(ArrayList.java:411)
  com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
  com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:773)
  com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:727)
  org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:118)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.readNextItem(ExternalAppendOnlyMap.scala:386)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.hasNext(ExternalAppendOnlyMap.scala:404)
  scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:847)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.org$apache$spark$util$collection$ExternalAppendOnlyMap$ExternalIterator$$getMorePairs(ExternalAppendOnlyMap.scala:252)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator$$anonfun$next$1.apply(ExternalAppendOnlyMap.scala:313)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator$$anonfun$next$1.apply(ExternalAppendOnlyMap.scala:311)
  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:311)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:226)
  scala.collection.Iterator$class.foreach(Iterator.scala:727)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.foreach(ExternalAppendOnlyMap.scala:226)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
  org.apache.spark.scheduler.Task.run(Task.scala:51)
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  java.lang.Thread.run(Thread.java:745)
{noformat}

{noformat}
  java.lang.ClassCastException (java.lang.ClassCastException: java.lang.String cannot be cast to scala.Tuple2}
  org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.readNextItem(ExternalAppendOnlyMap.scala:386)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.hasNext(ExternalAppendOnlyMap.scala:404)
  scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:847)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.org$apache$spark$util$collection$ExternalAppendOnlyMap$ExternalIterator$$getMorePairs(ExternalAppendOnlyMap.scala:252)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator$$anonfun$next$1.apply(ExternalAppendOnlyMap.scala:313)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator$$anonfun$next$1.apply(ExternalAppendOnlyMap.scala:311)
  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:311)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:226)
  scala.collection.Iterator$class.foreach(Iterator.scala:727)
  org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.foreach(ExternalAppendOnlyMap.scala:226)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
  org.apache.spark.scheduler.Task.run(Task.scala:51)
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  java.lang.Thread.run(Thread.java:745)
{noformat};;;","08/Aug/14 06:11;apachespark;User 'ash211' has created a pull request for this issue:
https://github.com/apache/spark/pull/1850;;;","11/Aug/14 02:41;grahamdennis;Here's the problem as I see it: To use a custom kryo registrator, the application jar must be available to the executor JVM.  Currently, the application jar isn't added to the classpath on launch, and so needs to be added later.  This happens when a task is sent to the executor JVM.  But the only reason the executor JVM can deserialise the task is because the closure serialiser can be different to the normal object serialiser, and it defaults to the Java serialiser.  If you were to try and use the kryo serialiser to serialise the closure, you'd have a chicken-and-egg problem: to know what jars the task needs, you need to deserialise the task, but to deserialise the task you need the application jars that contain the custom kryo registrator.

A similar problem would occur if you tried to set a custom serialiser that only existed in the application jar.

So my question is this: is there a reason that the application jar isn't added to (the end of) the classpath of the executor JVMs at launch time?  This would allow the application jar to contain a custom serialiser and/or a custom kryo registrator.  Additional jars can still be added to the executors later, but the user can't intend for these to modify the behaviour of the kryo registrator (as that would almost certainly lead to inconsistencies).;;;","11/Aug/14 11:07;grahamdennis;I've created a pull request with work-in-progress changes that I'd like feedback on: https://github.com/apache/spark/pull/1890;;;","11/Aug/14 11:11;apachespark;User 'GrahamDennis' has created a pull request for this issue:
https://github.com/apache/spark/pull/1890;;;","16/Aug/14 22:00;pwendell;Fixed by SPARK-3046 via:
https://github.com/apache/spark/pull/1972;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetastoreRelation should use SparkClassLoader when creating the tableDesc,SPARK-2877,12732245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,06/Aug/14 07:47,08/Aug/14 18:16,14/Jul/23 06:26,08/Aug/14 18:16,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410274,,,Wed Aug 06 17:47:29 UTC 2014,,,,,,,,,,"0|i1yl5b:",410264,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Aug/14 17:47;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1806;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD.partitionBy loads entire partition into memory,SPARK-2876,12732238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,NathanHowell,NathanHowell,06/Aug/14 07:28,14/Apr/21 05:57,14/Jul/23 06:26,17/Feb/15 00:43,1.0.1,,,,,,,1.1.0,,,,,,PySpark,,,,0,,,,,,"{{RDD.partitionBy}} fails due to an OOM in the PySpark daemon process when given a relatively large dataset. It seems that the use of {{BatchedSerializer(UNLIMITED_BATCH_SIZE)}} is suspect, most other RDD methods use {{self._jrdd_deserializer}}.

{code}
y = x.keyBy(...)
z = y.partitionBy(512) # fails
z = y.repartition(512) # succeeds
{code}",,davies,joshrosen,NathanHowell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ZOOKEEPER-704,,,,,,,,,SPARK-2538,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410267,,,Tue Feb 17 00:41:27 UTC 2015,,,,,,,,,,"0|i1yl3r:",410257,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/15 00:23;joshrosen;[~davies], I'm going through old PySpark issues and it looks like this one could still be relevant: BatchedSerializer is still used in a few places and its default batch size hasn't changed.  Is there still a reason to call BatchedSerializer directly anywhere, or can I just replace its uses with AutoBatchedSerializer?;;;","17/Feb/15 00:41;davies;[~joshrosen] BatchedSerializer is still needed in some places(for example, parallelize()), also it's base class of AutoBatchedSerializer, so we can not replace all of them into AutoBatchedSerializer. I should had checked that all the occuries of BatchedSerializer are needed.

This JIR should had been fixed by spilling the data during aggregation, it's not the problem of BatchedSerializer.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaRDD#javaToPython failed on come cases,SPARK-2875,12732223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,yhuai,yhuai,06/Aug/14 05:04,06/Aug/14 18:09,14/Jul/23 06:26,06/Aug/14 18:09,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"See http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-jsonRDD-and-jsonFile-in-pyspark-td11461.html#a11517

We can infer the schema. But, SchemaRDD#javaToPython failed on some cases when we bring the data back to the Python side.",,apachespark,nchammas,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410252,,,Wed Aug 06 06:47:09 UTC 2014,,,,,,,,,,"0|i1yl0f:",410242,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Aug/14 06:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1802;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL related scripts don't show complete usage message,SPARK-2874,12732219,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,06/Aug/14 04:19,06/Aug/14 19:30,14/Jul/23 06:26,06/Aug/14 19:30,1.0.1,1.0.2,,,,,,,,,,,,,,,,0,,,,,,"Due to [SPARK-2678|https://issues.apache.org/jira/browse/SPARK-2678], {{--help}} is shadowed by {{spark-submit}}, thus {{bin/spark-sql}} and {{sbin/start-thriftserver2.sh}} can't show application customized usage messages.",,apachespark,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2678,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410248,,,Wed Aug 06 19:30:14 UTC 2014,,,,,,,,,,"0|i1ykzj:",410238,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Aug/14 04:27;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1801;;;","06/Aug/14 19:30;pwendell;https://github.com/apache/spark/pull/1801;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential leak of Jdbc Connection and PreparedStatement in case of error in JdbcRDD,SPARK-2869,12732121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,javadba,javadba,javadba,05/Aug/14 20:24,07/Aug/14 23:19,14/Jul/23 06:26,06/Aug/14 01:19,1.0.0,1.1.0,,,,,,1.0.3,1.1.0,,,,,Spark Core,,,,0,jdbc,rdd,,,,"Within the compute() method of the JdbcRDD there is constructor logic that includes obtaining a JDBC connection, creating a PreparedStatement, and generating a ResultSet. This logic is not apparently protected within a try/catch block. In case of error/exception it would appear then to leak connections/statmenets/resultSets",All,apachespark,javadba,zgl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410150,,,Tue Aug 05 22:08:48 UTC 2014,,,,,,,,,,"0|i1ykev:",410143,,,,,,,,,,,Flex-BlazeDS Sprint 1,,,1.1.0,,,,,,,,,,,"05/Aug/14 22:08;apachespark;User 'javadba' has created a pull request for this issue:
https://github.com/apache/spark/pull/1792;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY attributes must appear in SELECT clause,SPARK-2866,12732096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,05/Aug/14 18:38,06/Aug/14 03:56,14/Jul/23 06:26,06/Aug/14 03:56,,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410125,,,Wed Aug 06 01:22:21 UTC 2014,,,,,,,,,,"0|i1yk9r:",410119,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Aug/14 01:22;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential deadlock: tasks could hang forever waiting to fetch a remote block even though most tasks finish,SPARK-2865,12732094,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,ConcreteVitamin,ConcreteVitamin,05/Aug/14 18:28,15/Aug/14 15:53,14/Jul/23 06:26,15/Aug/14 15:53,1.0.1,1.1.0,,,,,,,,,,,,Shuffle,Spark Core,,,0,,,,,,"In the application I tested, most of the tasks out of 128 tasks could finish, but sometimes (pretty deterministically) either 1 or 3 tasks would just hang forever (> 5 hrs with no progress at all) with the following stack trace. There were no apparent failures from the UI, also the nodes where the stuck tasks were running had no apparent memory/CPU/disk pressures.

{noformat}
""Executor task launch worker-0"" daemon prio=10 tid=0x00007f32ec003800 nid=0xaac waiting on condition [0x00007f33f4428000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3e0d7198e8> (a scala.concurrent.impl.Promise$CompletionLatch)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.network.ConnectionManager.sendMessageReliablySync(ConnectionManager.scala:832)
        at org.apache.spark.storage.BlockManagerWorker$.syncGetBlock(BlockManagerWorker.scala:122)
        at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:497)
        at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:495)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:495)
        at org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:481)
        at org.apache.spark.storage.BlockManager.get(BlockManager.scala:524)
        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

This behavior does *not* appear on 1.0 (reusing the same cluster), but appears on the master branch as of Aug 4, 2014 *and* 1.0.1. Further, I tried out [this patch|https://github.com/apache/spark/pull/1758], and it didn't fix the behavior.

When this behavior happened, the driver printed out the following line repeatedly:

{noformat}
14/08/04 23:32:42 WARN storage.BlockManagerMasterActor: Removing BlockManager BlockManagerId(7, ip-172-31-6-74.us-west-1.compute.internal, 59408, 0) with no recent heart beats: 67331ms exceeds 45000ms
{noformat}",16-node EC2 r3.2xlarge cluster,ConcreteVitamin,joshrosen,llai,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410123,,,Fri Aug 15 15:53:01 UTC 2014,,,,,,,,,,"0|i1yk9b:",410117,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"07/Aug/14 07:34;ConcreteVitamin;Just wanted to point out that I used the aforementioned Github patch in my experiment when it's not at its final state.  I haven't retried since it got new fixes and merged into master.;;;","07/Aug/14 18:07;ConcreteVitamin;This might be related to the fact that the default broadcast is changed to TorrentBroadcastFactory *after* 1.0.;;;","15/Aug/14 15:53;pwendell;I believe this has been resolved by virtue of other patches to the connection manager and other components.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix random seed in Word2Vec,SPARK-2864,12732076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,05/Aug/14 17:22,05/Aug/14 23:24,14/Jul/23 06:26,05/Aug/14 23:24,1.1.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,"The random seed is not fixed in word2vec, making the unit tests fail randomly.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410105,,,Tue Aug 05 23:24:21 UTC 2014,,,,,,,,,,"0|i1yk4n:",410099,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"05/Aug/14 22:07;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1790;;;","05/Aug/14 23:24;mengxr;Issue resolved by pull request 1790
[https://github.com/apache/spark/pull/1790];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DoubleRDDFunctions.histogram() throws exception for some inputs,SPARK-2862,12731997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nrchandan,nrchandan,nrchandan,05/Aug/14 11:32,18/Aug/14 16:53,14/Jul/23 06:26,18/Aug/14 16:53,0.9.0,0.9.1,1.0.0,1.0.1,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"histogram method call throws an IndexOutOfBoundsException when the choice of bucketCount partitions the RDD in irrational increments e.g. 

scala> val r = sc.parallelize(6 to 99)
r: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:12

scala> r.histogram(9)
java.lang.IndexOutOfBoundsException: 9
at scala.collection.immutable.NumericRange.apply(NumericRange.scala:124)
at scala.collection.immutable.NumericRange$$anon$1.apply(NumericRange.scala:176)
at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:66)
at scala.collection.IterableLike$class.copyToArray(IterableLike.scala:237)
at scala.collection.AbstractIterable.copyToArray(Iterable.scala:54)
at scala.collection.TraversableOnce$class.copyToArray(TraversableOnce.scala:241)
at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:105)
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:249)
at scala.collection.AbstractTraversable.toArray(Traversable.scala:105)
at org.apache.spark.rdd.DoubleRDDFunctions.histogram(DoubleRDDFunctions.scala:116)
at $iwC$$iwC$$iwC$$iwC.<init>(<console>:15)
at $iwC$$iwC$$iwC.<init>(<console>:20)
at $iwC$$iwC.<init>(<console>:22)
at $iwC.<init>(<console>:24)
at <init>(<console>:26)","Scala version 2.9.2 (OpenJDK 64-Bit Server VM, Java 1.7.0_55) running on Ubuntu 14.04",apachespark,nrchandan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410026,,,Tue Aug 05 14:53:26 UTC 2014,,,,,,,,,,"0|i1yjn3:",410020,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/14 12:21;srowen;It looks like a Scala bug, which I see you've already found and proposed a workaround for. I was about to paste this simpler proof of the bug, so here it is for spectators:

scala> val increment = (99.0 - 6.0) / 9.0
increment: Double = 10.333333333333334
scala> Range.Double.inclusive(6.0, 99.0, increment).toArray
java.lang.IndexOutOfBoundsException: 9
...

Range.Double.inclusive(6.0, 99.0, 10.333333333333333) is fine as is your ""6.0 to (99.0, increment). I think you can open a bug for the Scala class.;;;","05/Aug/14 12:22;apachespark;User 'nrchandan' has created a pull request for this issue:
https://github.com/apache/spark/pull/1787;;;","05/Aug/14 14:53;nrchandan;[~srowen] My colleague [~Shiti] is working on the Scala bug. She is the one who suggested this solution. Apparently, toSeq and toList don't error out but toArray does.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doc comment of DoubleRDDFunctions.histogram is incorrect,SPARK-2861,12731987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,nrchandan,nrchandan,nrchandan,05/Aug/14 10:37,09/Aug/14 07:47,14/Jul/23 06:26,09/Aug/14 07:46,0.9.0,0.9.1,1.0.0,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"The documentation comment of histogram method of DoubleRDDFunctions class in source file DoubleRDDFunctions.scala is  inconsistent. This might confuse somebody reading the documentation.

Comment in question:
{code}
  /**
   * Compute a histogram using the provided buckets. The buckets are all open
   * to the left except for the last which is closed
   *  e.g. for the array
   *  [1, 10, 20, 50] the buckets are [1, 10) [10, 20) [20, 50]
   *  e.g 1<=x<10 , 10<=x<20, 20<=x<50
   *  And on the input of 1 and 50 we would have a histogram of 1, 0, 0
{code}

The buckets are all open to the right (NOT left) except for the last which is closed
For the example quoted, the last bucket should be 20<=x<=50.
Also, the histogram result on input of 1 and 50 would be 1, 0, 1 (NOT 1, 0, 0). This works correctly in Spark but the doc comment is incorrect.
",,apachespark,nrchandan,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410016,,,Sat Aug 09 07:46:53 UTC 2014,,,,,,,,,,"0|i1yjkv:",410009,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/14 10:47;apachespark;User 'nrchandan' has created a pull request for this issue:
https://github.com/apache/spark/pull/1786;;;","09/Aug/14 07:46;pwendell;Issue resolved by pull request 1786
[https://github.com/apache/spark/pull/1786];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolving CASE WHEN throws None.get exception,SPARK-2860,12731968,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,05/Aug/14 08:55,05/Aug/14 18:37,14/Jul/23 06:26,05/Aug/14 18:37,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409997,,,Tue Aug 05 09:22:30 UTC 2014,,,,,,,,,,"0|i1yjgv:",409991,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"05/Aug/14 09:22;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1785;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
master.ui.port and worker.ui.port are ignored,SPARK-2857,12731924,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,05/Aug/14 04:22,05/Nov/14 10:45,14/Jul/23 06:26,05/Aug/14 07:40,1.0.2,,,,,,,1.1.0,,,,,,Deploy,,,,0,,,,,,"... but documented. This is because SparkConf only picks system properties up if they are prefixed with ""spark.""",,andrewor,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409953,,,Tue Aug 05 07:40:45 UTC 2014,,,,,,,,,,"0|i1yj7b:",409947,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/14 05:51;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1779;;;","05/Aug/14 07:40;pwendell;Issue resolved by pull request 1779
[https://github.com/apache/spark/pull/1779];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decrease initial buffer size for Kryo to 64KB,SPARK-2856,12731915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,matei,matei,05/Aug/14 03:04,07/Aug/14 00:03,14/Jul/23 06:26,07/Aug/14 00:03,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"SPARK-2543 allowed us to have a max size for Kryo buffers that's bigger than the initial one, so there's no need to start the initial one at 2 MB. Instead I'd give it support for fractional values and start it as low as 0.1 MB, or even 0.065 (65 KB). It will avoid a lot of unnecessary memory allocation in hash-based shuffle.",,apachespark,matei,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2503,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409944,,,Thu Aug 07 00:03:22 UTC 2014,,,,,,,,,,"0|i1yj5b:",409938,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"05/Aug/14 05:57;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1780;;;","07/Aug/14 00:03;pwendell;[~rxin] think you forgot to close this so I'm closing it now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark test cases crashed for no reason,SPARK-2855,12731892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,codingcat,codingcat,04/Aug/14 23:28,28/Aug/14 17:02,14/Jul/23 06:26,28/Aug/14 17:02,1.1.0,,,,,,,,,,,,,PySpark,,,,0,,,,,,"I met this for several times, 

all scala/java test cases passed, but pyspark test cases just crashed

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17875/consoleFull

",,codingcat,davies,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409921,,,Thu Aug 28 17:02:06 UTC 2014,,,,,,,,,,"0|i1yj07:",409915,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 13:55;farrellee;[~zhunansjtu] the link you supplied no longer works, please include the test failure in a comment on this jira;;;","28/Aug/14 14:02;codingcat;I guess they have fixed this.....Jenkins side mistake?;;;","28/Aug/14 14:03;codingcat;[~joshrosen]?;;;","28/Aug/14 16:53;joshrosen;Do you recall the actual exception?  Was it a Py4J error (something like ""connection to GatewayServer failed?"").  It seems like we've been experiencing some flakiness in these tests and I wonder whether it's due to some system resource being exhausted, such as ephemeral ports.;;;","28/Aug/14 16:59;codingcat;no....

https://github.com/apache/spark/pull/1313

search ""This particular failure was my fault,"";;;","28/Aug/14 17:02;joshrosen;That issue should be fixed now, so I'm going to mark this JIRA as resolved.  Feel free to re-open (or open a new issue) if you notice flaky PySpark tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/spark-submit should respect spark.driver.* for client mode,SPARK-2849,12731838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,andrewor14,andrewor,04/Aug/14 20:21,05/Nov/14 10:43,14/Jul/23 06:26,20/Aug/14 22:02,1.0.2,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"We currently ignore all spark.driver.* for client mode. I understand that these are originally intended for the driver when it is launched on one of the worker machines (i.e. for cluster mode). However, from the names spark.driver.* themselves, the user should expect these options to work regardless of what deploy mode they're running on. This has been a source of confusion on the mailing list.",,andrewor,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409867,,,Wed Aug 20 22:02:12 UTC 2014,,,,,,,,,,"0|i1yipj:",409861,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"04/Aug/14 20:26;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1770;;;","08/Aug/14 02:11;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1845;;;","20/Aug/14 22:02;pwendell;Issue resolved by pull request 1845
[https://github.com/apache/spark/pull/1845];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add configureInputJobPropertiesForStorageHandler to initialization of job conf,SPARK-2846,12731829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,alexliu68,alexliu68,04/Aug/14 19:36,20/Aug/14 23:15,14/Jul/23 06:26,20/Aug/14 23:15,1.0.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"The existing implementation bypass StorageHandler and other Hive integration API. I test CassandraStorageHandler on the latest Spark Sql, it fails due to some job properties configuration in StorageHandler API are bypassed.",,alexliu68,apachespark,bcantoni,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/14 06:24;alexliu68;2846.txt;https://issues.apache.org/jira/secure/attachment/12659829/2846.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409858,,,Wed Aug 13 18:52:27 UTC 2014,,,,,,,,,,"0|i1yinj:",409852,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"05/Aug/14 06:30;alexliu68;Patch is attached.;;;","08/Aug/14 18:28;marmbrus;Hi [~alexliu68],  Could you submit this patch as a pull request against Spark's github account? https://github.com/apache/spark

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark;;;","13/Aug/14 18:29;alexliu68;pull @ https://github.com/apache/spark/pull/1927;;;","13/Aug/14 18:52;apachespark;User 'alexliu68' has created a pull request for this issue:
https://github.com/apache/spark/pull/1927;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Existing JVM Hive Context not correctly used in Python Hive Context,SPARK-2844,12731820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ahirreddy,ahirreddy,ahirreddy,04/Aug/14 18:42,12/Aug/14 03:07,14/Jul/23 06:26,12/Aug/14 03:07,,,,,,,,1.1.0,,,,,,PySpark,SQL,,,0,,,,,,"Unlike the SQLContext, assing an existing JVM HiveContext object into the Python HiveContext constructor does not actually re-use that object. Instead it will create a new HiveContext.",,ahirreddy,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409849,,,Mon Aug 04 18:47:42 UTC 2014,,,,,,,,,,"0|i1yilr:",409844,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"04/Aug/14 18:43;ahirreddy;https://github.com/apache/spark/pull/1768;;;","04/Aug/14 18:47;apachespark;User 'ahirreddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/1768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow creating external tables in metastore,SPARK-2825,12731681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,ilikerps,ilikerps,04/Aug/14 07:17,28/Jul/15 19:30,14/Jul/23 06:26,28/Jul/15 19:30,,,,,,,,,,,,,,SQL,,,,0,,,,,,"External tables are useful for creating a metastore entry that points at pre-existing data. There is an easy workaround to use registerAsTable within SparkSQL itself, but this is only transient, and has to be re-run for every Spark session.",,ilikerps,KevinZwx,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409710,,,Mon Aug 04 21:27:16 UTC 2014,,,,,,,,,,"0|i1yhqn:",409702,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/14 21:27;marmbrus;CREATE EXTERNAL TABLE already works as far as I know and should have the same semantics as Hive.

The only operation that is not supported is CREATE EXTERNAL TABLE tablename AS SELECT ... This operation is heavier weight than just creating a pointer, since the query needs to be executed to calculate the data.  I'd assume that this is for a case when you want to create a new table, but want the data to exist outside of warehouse.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow saving Parquet files to the HiveMetastore,SPARK-2824,12731680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,ilikerps,ilikerps,04/Aug/14 07:08,16/Sep/15 08:01,14/Jul/23 06:26,15/Sep/15 20:53,,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Currently, we use one code path for reading all data from the Hive Metastore, and this precludes writing or loading data as custom ParquetRelations (they are always MetastoreRelations).",,apachespark,ilikerps,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409709,,,Tue Sep 15 20:53:20 UTC 2015,,,,,,,,,,"0|i1yhqf:",409701,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/14 07:36;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/1764;;;","15/Sep/15 20:53;marmbrus;We have much better support for working with parquet and metastore tables now :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX jobs throw IllegalArgumentException,SPARK-2823,12731678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,luluorta,luluorta,04/Aug/14 06:45,24/Jan/15 03:35,14/Jul/23 06:26,24/Jan/15 03:35,,,,,,,,1.2.1,1.3.0,,,,,GraphX,,,,1,,,,,,"If the users set “spark.default.parallelism” and the value is different with the EdgeRDD partition number, GraphX jobs will throw IllegalArgumentException:

14/07/26 21:06:51 WARN DAGScheduler: Creating new stage failed due to exception - job: 1
java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
        at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:60)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:54)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getShuffleMapStage(DAGScheduler.scala:1
97)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:272)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:269)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$visit$1(DAGScheduler.scala:269)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:274)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:269)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$visit$1(DAGScheduler.scala:269)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:274)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:269)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$visit$1(DAGScheduler.scala:269)
        at org.apache.spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:279)
        at org.apache.spark.scheduler.DAGScheduler.newStage(DAGScheduler.scala:219)
        at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:672)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1184)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",,ankurd,apachespark,joshrosen,luluorta,maropu,pedrorodriguez,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5351,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409707,,,Sat Jan 24 03:35:23 UTC 2015,,,,,,,,,,"0|i1yhpz:",409699,,,,,,,,,,,,,,1.0.3,1.1.2,1.2.1,1.3.0,,,,,,,,"04/Aug/14 07:16;apachespark;User 'luluorta' has created a pull request for this issue:
https://github.com/apache/spark/pull/1763;;;","03/Sep/14 02:30;ankurd;Issue resolved by pull request 1763
[https://github.com/apache/spark/pull/1763];;;","04/Sep/14 06:51;ankurd;I had to revert this because of SPARK-3400.;;;","16/Dec/14 00:17;joshrosen;I've removed the ""Fix Versions"" from this JIRA because its fix was reverted.;;;","20/Jan/15 02:45;pedrorodriguez;I just ran into this bug while testing LDA code (hasn't ever happened before, calls into GraphX). I am not sure what causes it, but rather than running it in cluster or locally, I am running it on a Docker container. It also looks like if you run with local[2], the problem could be reproduced.

Is this being worked on or should I maybe try looking into it soon?;;;","23/Jan/15 07:14;joshrosen;I think that the Snappy error was caused by a different change, so it should be safe to re-attempt the original fix.  There's a PR for this at https://github.com/apache/spark/pull/4136 (opened for SPARK-5351, a possible duplicate of this issue);;;","23/Jan/15 16:42;pedrorodriguez;I looked into this more and it looks like in addition to a bug in GraphX there is a bug in Spark core with handling textFile and zip https://issues.apache.org/jira/browse/SPARK-5385. Not certain if they may be related.;;;","24/Jan/15 03:35;ankurd;Issue resolved by pull request 4136
https://github.com/apache/spark/pull/4136;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"add  ""show create table"" support",SPARK-2817,12731663,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,tianyi,tianyi,04/Aug/14 03:29,13/Aug/14 23:51,14/Jul/23 06:26,13/Aug/14 23:51,1.0.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"In spark sql component, the ""show create table"" syntax had been disabled.
We thought it is a useful funciton to describe a hive table.",,apachespark,tianyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2847,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409692,,,Mon Aug 04 05:17:03 UTC 2014,,,,,,,,,,"0|i1yhmn:",409684,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/14 05:17;apachespark;User 'tianyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/1760;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveThriftServer throws NPE when executing native commands,SPARK-2814,12731607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,03/Aug/14 13:31,22/Oct/14 06:55,14/Jul/23 06:26,03/Aug/14 19:35,1.1.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"After [PR #1686|https://github.com/apache/spark/pull/1686], {{HiveThriftServer2}} throws exception when executing native commands.

The reason is that initialization of {{HiveContext.sessionState.out}} and {{HiveContext.sessionState.err}} were made lazy, while {{HiveThriftServer2}} uses an overriden version of {{HiveContext}} that doesn't know how to initialize these two streams. When {{HiveContext.runHive}} tries to write to {{HiveContext.sessionState.out}}, an NPE is throw.

Reproduction steps:

# Start HiveThriftServer2
# Connect to it via beeline
# Execute `set;`

Exception thrown:
{code}
======================
HIVE FAILURE OUTPUT
======================


======================
END HIVE FAILURE OUTPUT
======================

14/08/03 21:30:55 ERROR SparkSQLOperationManager: Error executing query:
java.lang.NullPointerException
        at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:210)
        at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:173)
        at org.apache.spark.sql.hive.HiveContext.set(HiveContext.scala:144)
        at org.apache.spark.sql.execution.SetCommand.sideEffectResult$lzycompute(commands.scala:59)
        at org.apache.spark.sql.execution.SetCommand.sideEffectResult(commands.scala:50)
        ...
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4037,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409636,,,Sun Aug 03 13:36:35 UTC 2014,,,,,,,,,,"0|i1yhav:",409631,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"03/Aug/14 13:36;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1753;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove scalalogging dependency in Spark SQL,SPARK-2804,12731570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,02/Aug/14 20:53,02/Aug/14 22:19,14/Jul/23 06:26,02/Aug/14 22:19,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,For the 1.1. release we should just remove scala logging and rely on spark logging for the SQL library.,,marmbrus,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409599,,,Sat Aug 02 22:19:19 UTC 2014,,,,,,,,,,"0|i1yh2n:",409594,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Aug/14 22:19;marmbrus;Resolved here: https://github.com/apache/spark/commit/4c477117bb1ffef463776c86f925d35036f96b7a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct several small errors in Flume module pom.xml files,SPARK-2798,12731456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,01/Aug/14 23:30,15/Jan/15 09:08,14/Jul/23 06:26,25/Aug/14 20:29,,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"(EDIT) Since the scalatest issue was since resolved, this is now about a few small problems in the Flume Sink pom.xml 

- scalatest is not declared as a test-scope dependency
- Its Avro version doesn't match the rest of the build
- Its Flume version is not synced with the other Flume module
- The other Flume module declares its dependency on Flume Sink slightly incorrectly, hard-coding the Scala 2.10 version
- It depends on Scala Lang directly, which it shouldn't",,apachespark,nchammas,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409528,,,Mon Aug 25 21:08:53 UTC 2014,,,,,,,,,,"0|i1ygnb:",409523,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/14 23:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1726;;;","25/Aug/14 20:31;srowen;[~tdas] Cool, I think this closes SPARK-3169 too if I understand correctly;;;","25/Aug/14 21:08;tdas;Naah, that was already closed by the fix I did on friday (https://github.com/apache/spark/pull/2101). Maven and therefore make-distribution should work fine with that fix. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaRDDs don't support unpersist(),SPARK-2797,12731429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,nchammas,nchammas,01/Aug/14 21:56,12/Sep/14 17:08,14/Jul/23 06:26,03/Aug/14 00:56,1.0.2,,,,,,,1.1.0,,,,,,PySpark,SQL,,,0,,,,,,"Looks like something simple got missed in the Java layer?

{code}
>>> from pyspark.sql import SQLContext
>>> sqlContext = SQLContext(sc)
>>> raw = sc.parallelize(['{""a"": 5}'])
>>> events = sqlContext.jsonRDD(raw)
>>> events.printSchema()
root
 |-- a: IntegerType

>>> events.cache()
PythonRDD[45] at RDD at PythonRDD.scala:37
>>> events.unpersist()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/spark/python/pyspark/sql.py"", line 440, in unpersist
    self._jschema_rdd.unpersist()
  File ""/root/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"", line 537, in __call__
  File ""/root/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o108.unpersist. Trace:
py4j.Py4JException: Method unpersist([]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
	at py4j.Gateway.invoke(Gateway.java:251)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
>>> events.unpersist
<bound method SchemaRDD.unpersist of PythonRDD[45] at RDD at PythonRDD.scala:37>
{code}

Note that the {{unpersist}} method exists but cannot be called without raising the shown error.

This is on {{1.0.2-rc1}}.",,apachespark,nchammas,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3500,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409501,,,Sun Aug 03 03:49:19 UTC 2014,,,,,,,,,,"0|i1yghj:",409496,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"01/Aug/14 22:11;nchammas;[~marmbrus] / [~yhuai]: I don't know if this affects all SchemaRDDs, or just those created via {{jsonRDD()}}. I also didn't check if this affects Scala. Gotta run now, but just wanted to pop in this ticket real quick.;;;","02/Aug/14 22:49;yhuai;I guess the problem is that when we create a Python SchemaRDD, we use a Scala SchemaRDD as the base SchemaRDD. When I look at the context.py, we are using a JavaRDD as the base RDD of a Python RDD. ;;;","02/Aug/14 22:51;yhuai;But, I do not understand why cache works but unpersist fails...;;;","02/Aug/14 23:24;yhuai;Oh. I see the problem. In Scala RDD, we are using default parameter for the unpersist method. Seems we will not be able to call it without an input parameter.;;;","02/Aug/14 23:31;yhuai;I think we also need to refactoring the sql.py later. Some variable names are misleading (For example, _jschema_rdd is a Scala SchemaRDD instead of a JavaSchemaRDD).;;;","02/Aug/14 23:36;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1745;;;","03/Aug/14 03:49;nchammas;{quote}
I think we also need to refactoring the sql.py later. Some variable names are misleading (For example, _jschema_rdd is a Scala SchemaRDD instead of a JavaSchemaRDD).
{quote}

Yin, is there a JIRA to track that? (Do we need one?)

By the way, thank you for resolving this so quickly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecisionTree bug with ordered categorical features,SPARK-2796,12731428,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,01/Aug/14 21:50,01/Aug/14 22:53,14/Jul/23 06:26,01/Aug/14 22:53,1.0.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,"In DecisionTree, the method sequentialBinSearchForOrderedCategoricalFeatureInClassification() indexed bins from 0 to (math.pow(2, featureCategories.toInt - 1) - 1).  This upper bound is the bound for unordered categorical features, not ordered ones.  The upper bound should be the arity (i.e., max value) of the feature.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409500,,,Fri Aug 01 21:56:12 UTC 2014,,,,,,,,,,"0|i1yghb:",409495,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"01/Aug/14 21:56;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark zip() doesn't work properly if RDDs have different serializers,SPARK-2790,12731398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,joshrosen,joshrosen,01/Aug/14 19:48,19/Aug/14 21:47,14/Jul/23 06:26,19/Aug/14 21:47,1.0.0,1.1.0,,,,,,1.1.0,,,,,,PySpark,,,,0,,,,,,"In PySpark, attempting to {{zip()}} two RDDs may fail if the RDDs have different serializers (e.g. batched vs. unbatched), even if those RDDs have the same number of partitions and same numbers of elements.  This problem occurs in the MLlib Python APIs, where we might want to zip a JavaRDD of LabelledPoints with a JavaRDD of batch-serialized Python objects.

This is problematic because whether zip() succeeds or errors depends on the partitioning / batching strategy, and we don't want to surface the serialization details to users.",,apachespark,josephkb,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409470,,,Mon Aug 11 18:42:23 UTC 2014,,,,,,,,,,"0|i1ygav:",409466,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 18:42;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid asserts for unimplemented hive features,SPARK-2785,12731265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,adav,marmbrus,01/Aug/14 05:15,02/Aug/14 23:48,14/Jul/23 06:26,02/Aug/14 23:48,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409337,,,Sat Aug 02 22:06:26 UTC 2014,,,,,,,,,,"0|i1yfhj:",409333,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Aug/14 22:06;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1742;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make language configurable using SQLConf instead of hql/sql functions,SPARK-2784,12731262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,01/Aug/14 04:42,03/Aug/14 19:35,14/Jul/23 06:26,03/Aug/14 19:35,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409334,,,Sat Aug 02 23:37:13 UTC 2014,,,,,,,,,,"0|i1yfgv:",409330,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Aug/14 23:37;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1746;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic support for analyze in HiveContext,SPARK-2783,12731261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,marmbrus,marmbrus,01/Aug/14 04:41,03/Aug/14 22:07,14/Jul/23 06:26,03/Aug/14 22:07,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409333,,,Sat Aug 02 21:56:45 UTC 2014,,,,,,,,,,"0|i1yfgn:",409329,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Aug/14 21:56;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1741;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spearman correlation computes wrong ranks when numPartitions > RDD size,SPARK-2782,12731256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dorx,dorx,dorx,01/Aug/14 03:07,01/Aug/14 04:24,14/Jul/23 06:26,01/Aug/14 04:24,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,The getRanks logic inside of SpearmanCorrelation returns the wrong ranks when numPartitions > size for the input RDDs.,,apachespark,dorx,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409328,,,Fri Aug 01 04:24:25 UTC 2014,,,,,,,,,,"0|i1yffj:",409324,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/14 03:16;apachespark;User 'dorx' has created a pull request for this issue:
https://github.com/apache/spark/pull/1710;;;","01/Aug/14 04:24;mengxr;Issue resolved by pull request 1710
[https://github.com/apache/spark/pull/1710];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyzer should check resolution of LogicalPlans,SPARK-2781,12731251,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,staple,staple,01/Aug/14 02:07,11/Sep/14 04:02,14/Jul/23 06:26,11/Sep/14 04:02,1.0.1,1.1.0,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,"Currently the Analyzer’s CheckResolution rule checks that all attributes are resolved by searching for unresolved Expressions.  But some LogicalPlans, including Union, contain custom implementations of the resolve attribute that validate other criteria in addition to checking for attribute resolution of their descendants.  These LogicalPlans are not currently validated by the CheckResolution implementation.

As a result, it is currently possible to execute a query generated from unresolved LogicalPlans.  One example is a UNION query that produces rows with different data types in the same column:

{noformat}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext._
case class T1(value:Seq[Int])
val t1 = sc.parallelize(Seq(T1(Seq(0,1))))
t1.registerAsTable(""t1"")
sqlContext.sql(""SELECT value FROM t1 UNION SELECT 2 FROM t1”).collect()
{noformat}

In this example, the type coercion implementation cannot unify array and integer types.  One row contains an array in the returned column and the other row contains an integer.  The result is:

{noformat}
res3: Array[org.apache.spark.sql.Row] = Array([List(0, 1)], [2])
{noformat}

I believe fixing this is a first step toward improving validation for Union (and similar) plans.  (For instance, Union does not currently validate that its children contain the same number of columns.)
",,apachespark,glenn.strycker@gmail.com,marmbrus,staple,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409323,,,Fri Aug 01 02:23:37 UTC 2014,,,,,,,,,,"0|i1yfef:",409319,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"01/Aug/14 02:12;marmbrus;We actually fixed this a long time ago: https://github.com/apache/spark/commit/b3e768e154bd7175db44c3ffc3d8f783f15ab776;;;","01/Aug/14 02:16;apachespark;User 'staple' has created a pull request for this issue:
https://github.com/apache/spark/pull/1706;;;","01/Aug/14 02:16;marmbrus;I'm sorry... I thought this was stale and did not read it carefully. Reopening.;;;","01/Aug/14 02:23;staple;No problem, I think the current validation checks Expressions but there are some cases where LogicalPlans might not be resolved even though Expressions are resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
asInstanceOf[Map[...]] should use scala.collection.Map instead of scala.collection.immutable.Map,SPARK-2779,12731233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yhuai,yhuai,yhuai,01/Aug/14 00:22,01/Aug/14 04:34,14/Jul/23 06:26,01/Aug/14 04:34,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409305,,,Fri Aug 01 00:26:01 UTC 2014,,,,,,,,,,"0|i1yfan:",409301,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"01/Aug/14 00:26;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1705;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenerateMIMAIgnore fails scalastyle check due to long line,SPARK-2771,12731101,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,yuzhihong@gmail.com,yuzhihong@gmail.com,31/Jul/14 16:14,31/Jul/14 22:40,14/Jul/23 06:26,31/Jul/14 22:40,,,,,,,,,,,,,,,,,,0,,,,,,"I got the following error building master branch:
{code}
[INFO] --- scalastyle-maven-plugin:0.4.0:check (default) @ spark-tools_2.10 ---
error file=/homes/hortonzy/spark/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala message=File line length exceeds 100 characters line=118
Saving to outputFile=/homes/hortonzy/spark/tools/scalastyle-output.xml
Processed 3 file(s)
{code}
This is caused by 3rd line below:
{code}
    classSymbol.typeSignature.members.filterNot(x =>
      x.fullName.startsWith(""java"") || x.fullName.startsWith(""scala""))
        .filter(x => isPackagePrivate(x) || isDeveloperApi(x) || isExperimental(x)).map(_.fullName) ++
{code}",,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/14 16:17;yuzhihong@gmail.com;spark-2771-v1.txt;https://issues.apache.org/jira/secure/attachment/12658942/spark-2771-v1.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409173,,,Thu Jul 31 16:17:45 UTC 2014,,,,,,,,,,"0|i1yehr:",409169,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/14 16:15;srowen;Already on it :)
https://github.com/apache/spark/pull/1690;;;","31/Jul/14 16:17;yuzhihong@gmail.com;Patch v1 shortens line 118 to 100 chars wide.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL CLI doens't output error message if query failed.,SPARK-2767,12731017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,31/Jul/14 08:48,01/Aug/14 18:42,14/Jul/23 06:26,01/Aug/14 18:42,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409089,,,Thu Jul 31 08:51:01 UTC 2014,,,,,,,,,,"0|i1ydz3:",409085,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"31/Jul/14 08:51;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/1686;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalaReflectionSuite  throw an llegalArgumentException in JDK 6,SPARK-2766,12730999,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,31/Jul/14 05:54,01/Aug/14 04:07,14/Jul/23 06:26,01/Aug/14 04:07,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,,,apachespark,gq,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409071,,,Fri Aug 01 04:07:44 UTC 2014,,,,,,,,,,"0|i1ydv3:",409067,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/14 06:06;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/1683;;;","01/Aug/14 04:07;pwendell;Issue resolved by pull request 1683
[https://github.com/apache/spark/pull/1683];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkILoop leaks memory in multi-repl configurations,SPARK-2762,12730984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,timhunter,timhunter,timhunter,31/Jul/14 03:08,31/Jul/14 17:26,14/Jul/23 06:26,31/Jul/14 17:25,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"When subclassing SparkILoop and instantiating multiple objects, the SparkILoop instances do not get garbage collected.",,matei,timhunter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409056,,,Thu Jul 31 17:26:03 UTC 2014,,,,,,,,,,"0|i1ydrr:",409052,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/14 17:26;matei;PR: https://github.com/apache/spark/pull/1674;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching tables from multiple databases does not work,SPARK-2760,12730960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,31/Jul/14 00:29,27/Oct/14 00:08,14/Jul/23 06:26,27/Oct/14 00:08,1.1.0,,,,,,,1.2.0,,,,,,SQL,,,,0,,,,,,,,cfregly,dongxu,marmbrus,ravipesala,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2775,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409032,,,Mon Oct 27 00:08:43 UTC 2014,,,,,,,,,,"0|i1ydmf:",409028,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"27/Oct/14 00:08;marmbrus;This was fixed with the caching overhaul.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Mima test for Spark Sink after 1.1.0 is released,SPARK-2757,12730945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hshreedharan,hshreedharan,hshreedharan,30/Jul/14 23:12,23/Jan/15 18:27,14/Jul/23 06:26,01/Jan/15 01:00,1.1.0,,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"We are adding it in 1.1.0, so it is excluded from Mima right now. Once we release 1.1.0, we should add it to Mima so we do binary compat checks.",,apachespark,hshreedharan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2719,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409017,,,Tue Dec 30 12:48:15 UTC 2014,,,,,,,,,,"0|i1ydjb:",409014,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"29/Dec/14 17:18;srowen;[~hshreedharan] Is this still an issue? Exclusions are tied to particular Spark releases, so if something was excluded for 1.1 (compared to 1.0) then it is not automatically excluded for 1.2 (compared to 1.1). Does something need to change in MimaBuild or MimaExcludes?;;;","29/Dec/14 23:15;hshreedharan;Actually yes, the SparkSink and the FlumePollingReceiver code is in Mima excludes right now and we should remove it from that, so we actually compare against a previous release to make sure nothing broke compat.;;;","30/Dec/14 12:48;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3842;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decision Tree bugs,SPARK-2756,12730940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,30/Jul/14 22:58,01/Aug/14 03:53,14/Jul/23 06:26,01/Aug/14 03:53,1.0.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,"3 bugs:

Bug 1: Indexing is inconsistent for aggregate calculations for unordered features (in multiclass classification with categorical features, where the features had few enough values such that they could be considered unordered, i.e., isSpaceSufficientForAllCategoricalSplits=true).

* updateBinForUnorderedFeature indexed agg as (node, feature, featureValue, binIndex), where
** featureValue was from arr (so it was a feature value)
** binIndex was in [0,…, 2^(maxFeatureValue-1)-1)
* The rest of the code indexed agg as (node, feature, binIndex, label).

Bug 2: calculateGainForSplit (for classification):
* It returns dummy prediction values when either the right or left children had 0 weight.  These are incorrect for multiclass classification.

Bug 3: Off-by-1 when finding thresholds for splits for continuous features.
* When finding thresholds for possible splits for continuous features in DecisionTree.findSplitsBins, the thresholds were set according to individual training examples’ feature values.  This can cause problems for small datasets.
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,409012,,,Fri Aug 01 03:53:10 UTC 2014,,,,,,,,,,"0|i1ydi7:",409009,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 23:01;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1673;;;","01/Aug/14 03:53;mengxr;Issue resolved by pull request 1673
[https://github.com/apache/spark/pull/1673];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark sql cli should not exit when get a exception,SPARK-2752,12730804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,30/Jul/14 15:07,03/Aug/14 19:46,14/Jul/23 06:26,03/Aug/14 19:46,1.0.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408877,,,Wed Jul 30 15:21:08 UTC 2014,,,,,,,,,,"0|i1ycon:",408875,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 15:21;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/1661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark SQL Java tests aren't compiling in Jenkins' Maven builds; missing junit:junit dep",SPARK-2749,12730774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,30/Jul/14 12:15,15/Jan/15 09:08,14/Jul/23 06:26,31/Jul/14 19:19,1.0.1,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"The Maven-based builds in the build matrix have been failing for a few days:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/

On inspection, it looks like the Spark SQL Java tests don't compile:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/244/consoleFull

I confirmed it by repeating the command vs master:

mvn -Dhadoop.version=1.0.4 -Dlabel=centos -DskipTests clean package

The problem is that this module doesn't depend on JUnit. In fact, none of the modules do, but com.novocode:junit-interface (the SBT-JUnit bridge) pulls it in, in most places. However this module doesn't depend on com.novocode:junit-interface

Adding the junit:junit dependency fixes the compile problem. In fact, the other modules with Java tests should probably depend on it explicitly instead of happening to get it via com.novocode:junit-interface, since that is a bit SBT/Scala-specific (and I am not even sure it's needed).",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408847,,,Thu Jul 31 19:19:23 UTC 2014,,,,,,,,,,"0|i1ychz:",408845,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/14 12:21;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1660;;;","31/Jul/14 08:51;srowen;Thanks! PR merged so this can be closed as fixed.;;;","31/Jul/14 14:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1690;;;","31/Jul/14 19:19;pwendell;Issue resolved by pull request 1690
[https://github.com/apache/spark/pull/1690];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Loss of precision for small arguments to Math.exp, Math.log",SPARK-2748,12730765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,30/Jul/14 11:33,15/Jan/15 09:08,14/Jul/23 06:26,30/Jul/14 15:57,1.0.1,,,,,,,1.1.0,,,,,,GraphX,MLlib,,,0,,,,,,"In a few places in MLlib, an expression of the form log(1.0 + p) is evaluated. When p is so small that 1.0 + p == 1.0, the result is 0.0. However the correct answer is very near p. This is why Math.log1p exists.

Similarly for one instance of exp(m) - 1 in GraphX; there's a special Math.expm1 method.

While the errors occur only for very small arguments, given their use in machine learning algorithms, this is entirely possible.

Also, while we're here, naftaliharris discovered a case in Python where 1 - 1 / (1 + exp(margin)) is less accurate than exp(margin) / (1 + exp(margin)). I don't think there's a JIRA on that one, so maybe this can serve as an umbrella for all of these related issues.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408838,,,Wed Jul 30 15:57:05 UTC 2014,,,,,,,,,,"0|i1ycfz:",408836,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 11:40;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1659;;;","30/Jul/14 11:41;srowen;PR: https://github.com/apache/spark/pull/1659
See also: https://github.com/apache/spark/pull/1652;;;","30/Jul/14 15:57;mengxr;Issue resolved by pull request 1659
[https://github.com/apache/spark/pull/1659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
git diff --dirstat can miss sql changes and not run Hive tests,SPARK-2747,12730722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,rxin,30/Jul/14 08:36,30/Jul/14 16:29,14/Jul/23 06:26,30/Jul/14 16:29,,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"dev/run-tests use ""git diff --dirstat master"" to check whether sql is changed. However, --dirstat won't show sql if sql's change is negligible (e.g. 1k loc change in core, and only 1 loc change in hive).

We should use ""git diff --name-only master"" instead.",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408795,,,Wed Jul 30 08:41:00 UTC 2014,,,,,,,,,,"0|i1yc6n:",408793,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 08:41;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1656;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set SBT_MAVEN_PROFILES only when it is not set explicitly by the user,SPARK-2746,12730718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,rxin,30/Jul/14 07:50,30/Jul/14 18:53,14/Jul/23 06:26,30/Jul/14 18:53,,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"dev/run-tests always sets SBT_MAVEN_PROFILES, which is not desired. As a matter of fact, Jenkins is failing for older Hadoop versions because the YARN profile is always on.

{code}
  export SBT_MAVEN_PROFILES=""-Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0""
{code}",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408791,,,Wed Jul 30 07:56:01 UTC 2014,,,,,,,,,,"0|i1yc5r:",408789,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 07:56;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1655;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet has issues with capital letters and case insensitivity,SPARK-2743,12730689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,30/Jul/14 03:17,31/Jul/14 18:16,14/Jul/23 06:26,31/Jul/14 18:16,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408762,,,Wed Jul 30 04:40:58 UTC 2014,,,,,,,,,,"0|i1ybzj:",408760,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 04:40;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1647;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The variable inputFormatInfo and inputFormatMap never used,SPARK-2742,12730685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,meiyoula,meiyoula,30/Jul/14 02:40,22/Aug/14 15:46,14/Jul/23 06:26,22/Aug/14 15:45,,,,,,,,1.2.0,,,,,,YARN,,,,0,,,,,,"the ClientArguments class has two never used variables, one is inputFormatInfo, the other is inputFormatMap",,meiyoula,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408758,,,Fri Aug 22 15:46:04 UTC 2014,,,,,,,,,,"0|i1ybyn:",408756,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/14 08:19;pwendell;Oops - I closed accidentially;;;","22/Aug/14 15:46;tgraves;https://github.com/apache/spark/pull/1614;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename registerAsTable to registerTempTable,SPARK-2739,12730660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,30/Jul/14 00:12,03/Aug/14 01:32,14/Jul/23 06:26,03/Aug/14 01:32,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408733,,,Sat Aug 02 22:26:10 UTC 2014,,,,,,,,,,"0|i1ybtb:",408732,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Aug/14 22:26;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1743;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redundant imports in BlockManagerSuite,SPARK-2738,12730658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sandyr,sandyr,sandyr,30/Jul/14 00:03,01/Aug/14 06:13,14/Jul/23 06:26,01/Aug/14 06:13,1.0.0,,,,,,,1.1.0,,,,,,,,,,0,,,,,,BlockManagerSuite imports the same classes multiple times,,apachespark,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408731,,,Fri Aug 01 06:13:13 UTC 2014,,,,,,,,,,"0|i1ybsv:",408730,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/14 00:06;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/1642;;;","01/Aug/14 06:13;pwendell;Issue resolved by pull request 1642
[https://github.com/apache/spark/pull/1642];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastExceptions when collect()ing JavaRDDs' underlying Scala RDDs,SPARK-2737,12730609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,29/Jul/14 21:14,23/Sep/15 08:21,14/Jul/23 06:26,31/Jul/14 05:42,0.8.0,0.9.0,1.0.0,,,,,1.1.0,,,,,,Java API,,,,0,,,,,,"The Java API's use of fake ClassTags doesn't seem to cause any problems for Java users, but it can lead to issues when passing JavaRDDs' underlying RDDs to Scala code (e.g. in the MLlib Java API wrapper code).  If we call {{collect()}} on a Scala RDD with an incorrect ClassTag, this causes ClassCastExceptions when we try to allocate an array of the wrong type (for example, see SPARK-2197).

There are a few possible fixes here.  An API-breaking fix would be to completely remove the fake ClassTags and require Java API users to pass {{java.lang.Class}} instances to all {{parallelize()}} calls and add {{returnClass}} fields to all {{Function}} implementations.  This would be extremely verbose.

Instead, I propose that we add internal APIs to ""repair"" a Scala RDD with an incorrect ClassTag by wrapping it and overriding its ClassTag.  This should be okay for cases where the Scala code that calls {{collect()}} knows what type of array should be allocated, which is the case in the MLlib wrappers.",,apachespark,glenn.strycker@gmail.com,josephkb,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4489,,,,,,,,,SPARK-2197,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408682,,,Wed Sep 23 08:21:43 UTC 2015,,,,,,,,,,"0|i1ybif:",408680,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/14 21:26;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1639;;;","30/Jul/14 00:38;josephkb;Relating to [SPARK-2197 Spark invoke DecisionTree by Java | https://issues.apache.org/jira/browse/SPARK-2197], this makes the Java DecisionTree test get farther, but does not fix it completely.  Will examine logs more.;;;","22/Sep/15 22:08;glenn.strycker@gmail.com;I am getting a similar error in Spark 1.3.0... see a new ticket I created:  https://issues.apache.org/jira/browse/SPARK-10762;;;","23/Sep/15 08:21;srowen;[~glenn.strycker@gmail.com] you can use JIRA to link issues if you're pretty sure they're related. It's more visible than in a comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DROP TABLE should also uncache table,SPARK-2734,12730559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,29/Jul/14 18:33,16/Apr/15 18:02,14/Jul/23 06:26,31/Jul/14 00:31,1.0.2,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Steps to reproduce:
{code}
hql(""CREATE TABLE test(a INT)"")
hql(""CACHE TABLE test"")
hql(""DROP TABLE test"")
hql(""SELECT * FROM test"")
{code}",,apachespark,arushkharbanda,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408632,,,Thu Apr 16 18:02:05 UTC 2015,,,,,,,,,,"0|i1yb7j:",408630,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"30/Jul/14 05:31;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1650;;;","16/Apr/15 11:16;arushkharbanda;This issue is occurring for me, i am using Spark - 1.3.0. ;;;","16/Apr/15 18:02;marmbrus;How do you know it occurring?  What queries are you running?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When retrieving a value from a Map, GetItem evaluates key twice",SPARK-2730,12730531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yhuai,yhuai,yhuai,29/Jul/14 16:51,29/Jul/14 19:24,14/Jul/23 06:26,29/Jul/14 19:24,,,,,,,,1.0.3,1.1.0,,,,,SQL,,,,0,,,,,,"{code}
override def eval(input: Row): Any = {
    val value = child.eval(input)
    if (value == null) {
      null
    } else {
      val key = ordinal.eval(input)
      if (key == null) {
        null
      } else {
        if (child.dataType.isInstanceOf[ArrayType]) {
          // TODO: consider using Array[_] for ArrayType child to avoid
          // boxing of primitives
          val baseValue = value.asInstanceOf[Seq[_]]
          val o = key.asInstanceOf[Int]
          if (o >= baseValue.size || o < 0) {
            null
          } else {
            baseValue(o)
          }
        } else {
          val baseValue = value.asInstanceOf[Map[Any, _]]
          val key = ordinal.eval(input) // We evaluate the key again at here
          baseValue.get(key).orNull
        }
      }
    }
  }
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408604,,,Tue Jul 29 17:01:28 UTC 2014,,,,,,,,,,"0|i1yb1b:",408602,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/14 17:01;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forgot to match Timestamp type in ColumnBuilder,SPARK-2729,12730516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chutium,chutium,29/Jul/14 16:29,02/Aug/14 16:16,14/Jul/23 06:26,01/Aug/14 18:36,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"after SPARK-2710 we can create a table in Spark SQL with ColumnType Timestamp from jdbc.

when i try to
{code}
sqlContext.cacheTable(""myJdbcTable"")
{code}

then
{code}
sqlContext.sql(""select count(*) from myJdbcTable"")
{code}

i got exception:

{code}
scala.MatchError: 8 (of class java.lang.Integer)
        at org.apache.spark.sql.columnar.ColumnBuilder$.apply(ColumnBuilder.scala:146)
{code}

i checked the code ColumnBuilder.scala:146

it is just missing a match of Timestamp typeid.

so it is easy to fix.",,apachespark,chutium,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408589,,,Sat Aug 02 16:16:10 UTC 2014,,,,,,,,,,"0|i1yaxz:",408587,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"29/Jul/14 17:01;apachespark;User 'chutium' has created a pull request for this issue:
https://github.com/apache/spark/pull/1636;;;","31/Jul/14 08:01;lian cheng;Steps to reproduce this bug within {{sbt -Phive hive/console}}:
{code}
scala> hql(""create table dates as select cast('2011-01-01 01:01:01' as timestamp) from src"")
...
scala> cacheTable(""dates"")
...
scala> hql(""select count(*) from dates"").collect()
...
14/07/31 16:00:37 ERROR executor.Executor: Exception in task 0.0 in stage 3.0 (TID 6)
scala.MatchError: 8 (of class java.lang.Integer)
        at org.apache.spark.sql.columnar.ColumnBuilder$.apply(ColumnBuilder.scala:146)
        at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$1$$anonfun$2.apply(InMemoryColumnarTableScan.scala:48)
        at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$1$$anonfun$2.apply(InMemoryColumnarTableScan.scala:47)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$1.apply(InMemoryColumnarTableScan.scala:47)
        at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$1.apply(InMemoryColumnarTableScan.scala:46)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:595)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:595)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:261)
        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:261)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:228)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:261)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:228)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:112)
        at org.apache.spark.scheduler.Task.run(Task.scala:51)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:189)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}
;;;","02/Aug/14 16:16;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1738;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SortOrder in ShuffleDependency and HashShuffleReader,SPARK-2726,12730405,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,29/Jul/14 05:18,29/Jul/14 08:14,14/Jul/23 06:26,29/Jul/14 08:14,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"SPARK-2125 introduced a SortOrder in ShuffleDependency and HashShuffleReader. However, the key ordering already includes the SortOrder information since an Ordering can be reversed easily. This is similar to Java's Comparator interface. Rarely does an API accept both a Comparator as well as a SortOrder.

We should remove the SortOrder.",,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2125,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408478,,,2014-07-29 05:18:17.0,,,,,,,,,,"0|i1ya9r:",408476,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix MapType compatibility issues with reading Parquet datasets,SPARK-2721,12730358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,rrusso2007,rrusso2007,28/Jul/14 23:47,27/Aug/14 08:10,14/Jul/23 06:26,27/Aug/14 08:10,1.0.1,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Parquet-thrift (along with most likely other implementations of parquet) supports null values in a map and this makes any thrift generated parquet files that contain a map unreadable by spark sql due to the following code in parquet-thrift for generating the schema for maps:

{code:title=parquet.thrift.ThriftSchemaConverter.java|borderStyle=solid}
  @Override
  public void visit(ThriftType.MapType mapType) {
    final ThriftField mapKeyField = mapType.getKey();
    final ThriftField mapValueField = mapType.getValue();

    //save env for map
    String mapName = currentName;
    Type.Repetition mapRepetition = currentRepetition;

    //=========handle key
    currentFieldPath.push(mapKeyField);
    currentName = ""key"";
    currentRepetition = REQUIRED;
    mapKeyField.getType().accept(this);
    Type keyType = currentType;//currentType is the already converted type
    currentFieldPath.pop();

    //=========handle value
    currentFieldPath.push(mapValueField);
    currentName = ""value"";
    currentRepetition = OPTIONAL;
    mapValueField.getType().accept(this);
    Type valueType = currentType;
    currentFieldPath.pop();

    if (keyType == null && valueType == null) {
      currentType = null;
      return;
    }

    if (keyType == null && valueType != null)
      throw new ThriftProjectionException(""key of map is not specified in projection: "" + currentFieldPath);

    //restore Env
    currentName = mapName;
    currentRepetition = mapRepetition;
    currentType = ConversionPatterns.mapType(currentRepetition, currentName,
            keyType,
            valueType);
  }
{code}

Which causes an error on the spark side when we reach this step in the toDataType function that asserts that both the key and value are of repetition level REQUIRED:

{code:title=org.apache.spark.sql.parquet.ParquetTypes.scala|borderStyle=solid}
        case ParquetOriginalType.MAP => {
          assert(
            !groupType.getFields.apply(0).isPrimitive,
            ""Parquet Map type malformatted: expected nested group for map!"")
          val keyValueGroup = groupType.getFields.apply(0).asGroupType()
          assert(
            keyValueGroup.getFieldCount == 2,
            ""Parquet Map type malformatted: nested group should have 2 (key, value) fields!"")
          val keyType = toDataType(keyValueGroup.getFields.apply(0))
          println(""here"")
          assert(keyValueGroup.getFields.apply(0).getRepetition == Repetition.REQUIRED)
          val valueType = toDataType(keyValueGroup.getFields.apply(1))
          assert(keyValueGroup.getFields.apply(1).getRepetition == Repetition.REQUIRED)
          new MapType(keyType, valueType)
        }
{code}

Currently I have modified parquet-thrift to use repetition REQUIRED just to make spark sql able to work on the parquet files since we don't actually use null values in our maps. However it would be preferred to use parquet-thrift and spark sql out of the box and have them work nicely together with our existing thrift data types without having to modify dependencies.",,marmbrus,rrusso2007,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3036,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408431,,,Wed Aug 27 08:10:39 UTC 2014,,,,,,,,,,"0|i1ya0f:",408429,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"27/Aug/14 08:10;marmbrus;I think this was fixed by [SPARK-3036].  Please reopen if you are still having problems.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN does not handle spark configs with quotes or backslashes,SPARK-2718,12730340,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,andrewor14,andrewor,28/Jul/14 22:17,05/Nov/14 10:43,14/Jul/23 06:26,03/Sep/14 19:16,1.0.2,,,,,,,1.1.0,,,,,,YARN,,,,0,,,,,,"Say we have the following config:
{code}
spark.app.name spark shell with spaces and ""quotes "" and \ backslashes \
{code}

This works in standalone mode but not in YARN mode. This is because standalone mode uses Java's ProcessBuilder, which handles these cases nicely, but YARN mode uses org.apache.hadoop.yarn.api.records.ContainerLaunchContext, which does not. As a result, submitting an application to YARN with the given config leads to the following exception:

{code}
line 0: unexpected EOF while looking for matching `""'
syntax error: unexpected end of file
  at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
  at org.apache.hadoop.util.Shell.run(Shell.java:418)
  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
  ...
{code}",,andrewor,andrewor14,apachespark,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2722,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408413,,,Wed Sep 03 19:16:51 UTC 2014,,,,,,,,,,"0|i1y9wn:",408412,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/14 18:34;vanzin;Ran into this while working on some other stuff, so I'll work on a fix.;;;","01/Aug/14 23:11;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1724;;;","03/Sep/14 13:37;tgraves;[~andrewor]  the pr for this is closed, can we resolve the jira also?;;;","03/Sep/14 19:16;andrewor14;Yes, thanks for the reminder.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Having clause with no references fails to resolve,SPARK-2716,12730259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,28/Jul/14 17:30,30/Jul/14 01:14,14/Jul/23 06:26,30/Jul/14 01:14,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"For example:
{code}
SELECT a FROM b GROUP BY a HAVING COUNT(*) > 1
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408332,,,Tue Jul 29 21:50:52 UTC 2014,,,,,,,,,,"0|i1y9fr:",408336,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"29/Jul/14 21:50;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[APACHE-SPARK] [CORE] Build Fails: Case class 'TaskUIData' makes sbt complaining.,SPARK-2708,12730084,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,miccagiann,miccagiann,miccagiann,27/Jul/14 19:07,27/Jul/14 22:13,14/Jul/23 06:26,27/Jul/14 20:01,1.0.1,,,,,,,,,,,,,Spark Core,,,,0,patch,,,,,Build procedure fails due to numerous errors appearing in files located in Apache Spark Core project's 'org.apache.spark.ui' directory where case class 'TaskUIData' appears to be undefined. However the problem seems more complicated since the class is imported correctly to the aforementioned files.,,miccagiann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408157,,,Sun Jul 27 22:13:45 UTC 2014,,,,,,,,,,"0|i1y8d3:",408162,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/14 19:49;srowen;I don't see any failures when I run the tests from master just now. Jenkins seems to be succeeding too, or at least, the failed builds don't seem to be due to a TaskUIData class: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/  

The class TaskUIData is present in org.apache.spark.ui.jobs.UIData. It was added pretty recently: https://github.com/apache/spark/commits/72e9021eaf26f31a82120505f8b764b18fbe8d48/core/src/main/scala/org/apache/spark/ui/jobs/UIData.scala

Maybe you need to do a clean build?;;;","27/Jul/14 19:58;miccagiann;I am doing it right now! I am going to report as soon as possible.

Thanks!;;;","27/Jul/14 20:01;miccagiann;Yes, you are right! Thanks for the help. I am closing this issue as resolved.
Thanks again guys!;;;","27/Jul/14 20:06;srowen;(Nit: might mark it as Not A Problem or something, lest someone go looking for something that fixed this.);;;","27/Jul/14 22:13;miccagiann;This issue is resolved. Initially there was no problem so this issue has been closed without any patches provided.
The solution is just to perform a clean build on the apache spark project.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong stage description in Web UI ,SPARK-2705,12730065,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,27/Jul/14 09:06,27/Jul/14 19:36,14/Jul/23 06:26,27/Jul/14 19:36,1.0.1,1.0.2,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"Type of stage description object in the stage table of Web UI should be a {{String}}, but an {{Option\[String\]}} is used. See [here|https://github.com/apache/spark/blob/aaf2b735fddbebccd28012006ee4647af3b3624f/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala#L125].",,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408138,,,Sun Jul 27 19:36:01 UTC 2014,,,,,,,,,,"0|i1y88v:",408143,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"27/Jul/14 11:36;lian cheng;PR: https://github.com/apache/spark/pull/1524;;;","27/Jul/14 19:36;pwendell;Fixed via: https://github.com/apache/spark/pull/1524;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager threads should be named and daemon,SPARK-2704,12730053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,26/Jul/14 19:45,26/Jul/14 22:01,14/Jul/23 06:26,26/Jul/14 22:01,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"handleMessageExecutor, handleReadWriteExecutor, and handleConnectExecutor are not marked as daemon and not named. I think there exists some condition in which Spark programs won't terminate because of this.

Stack dumped (look at pool-1-thread-3, pool-2-thread-1, etc)
{code}
2014-07-26 10:53:59
Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.51-b03 mixed mode):

""Attach Listener"" daemon prio=5 tid=0x00007fc03480d800 nid=0x16303 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""ForkJoinPool-5-worker-15"" daemon prio=5 tid=0x00007fc032b6f800 nid=0x5713 waiting on condition [0x000000011bf69000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007f5b5de28> (a scala.concurrent.forkjoin.ForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-akka.actor.default-dispatcher-15"" daemon prio=5 tid=0x00007fc031a9c800 nid=0x580b waiting on condition [0x000000011be66000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-akka.actor.default-dispatcher-14"" daemon prio=5 tid=0x00007fc0358d3800 nid=0x3b17 waiting on condition [0x00000001111c9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""DestroyJavaVM"" prio=5 tid=0x00007fc031bc7000 nid=0xd17 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Result resolver thread-3"" daemon prio=5 tid=0x00007fc0350e3000 nid=0x16003 waiting on condition [0x000000011bd63000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10198> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""Result resolver thread-2"" daemon prio=5 tid=0x00007fc031a9b800 nid=0x15e03 waiting on condition [0x000000011bc60000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10198> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""Result resolver thread-1"" daemon prio=5 tid=0x00007fc034aa4000 nid=0x15c03 waiting on condition [0x000000011bb5d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10198> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""Result resolver thread-0"" daemon prio=5 tid=0x00007fc034aa0800 nid=0x15a03 waiting on condition [0x000000011ba5a000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10198> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-1-thread-4"" prio=5 tid=0x00007fc032254000 nid=0x15803 waiting on condition [0x000000011b957000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44098> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-1-thread-3"" prio=5 tid=0x00007fc032253000 nid=0x15603 waiting on condition [0x000000011b854000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44098> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-2-thread-4"" prio=5 tid=0x00007fc03590a000 nid=0x15403 waiting on condition [0x000000011b751000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10300> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-2-thread-3"" prio=5 tid=0x00007fc035909800 nid=0x15203 waiting on condition [0x000000011b64e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10300> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-3-thread-1"" prio=5 tid=0x00007fc031a9b000 nid=0x15003 waiting on condition [0x000000011b54b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb54198> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-1-thread-2"" prio=5 tid=0x00007fc034a9e800 nid=0x14e03 waiting on condition [0x000000011b448000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44098> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-2-thread-2"" prio=5 tid=0x00007fc034a95800 nid=0x14c03 waiting on condition [0x000000011b345000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10300> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-1-thread-1"" prio=5 tid=0x00007fc032a79800 nid=0x14a03 waiting on condition [0x000000011b242000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44098> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""pool-2-thread-1"" prio=5 tid=0x00007fc031bc3000 nid=0x14803 waiting on condition [0x000000011b13f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb10300> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""spark-akka.actor.default-dispatcher-16"" daemon prio=5 tid=0x00007fc0321a2800 nid=0x14603 waiting on condition [0x000000011b03c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-15"" daemon prio=5 tid=0x00007fc0321a1800 nid=0x14403 waiting on condition [0x000000011af39000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""File appending thread for /scratch/rxin/spark-1/work/app-20140726103835-0000/1/stderr"" daemon prio=5 tid=0x00007fc0321a8000 nid=0x13c07 runnable [0x000000011ad33000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:272)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	- locked <0x00000007feb2a238> (a java.lang.UNIXProcess$ProcessPipeInputStream)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1218)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)

""File appending thread for /scratch/rxin/spark-1/work/app-20140726103835-0000/1/stdout"" daemon prio=5 tid=0x00007fc0350b5000 nid=0x14007 runnable [0x000000011ab2d000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:272)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	- locked <0x00000007feb12b60> (a java.lang.UNIXProcess$ProcessPipeInputStream)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1218)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)

""File appending thread for /scratch/rxin/spark-1/work/app-20140726103835-0000/0/stderr"" daemon prio=5 tid=0x00007fc034102000 nid=0x14307 runnable [0x000000011ae36000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:272)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	- locked <0x00000007feb46478> (a java.lang.UNIXProcess$ProcessPipeInputStream)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1218)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)

""File appending thread for /scratch/rxin/spark-1/work/app-20140726103835-0000/0/stdout"" daemon prio=5 tid=0x00007fc034a9b800 nid=0x14207 runnable [0x000000011ac30000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:272)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	- locked <0x00000007feb221e8> (a java.lang.UNIXProcess$ProcessPipeInputStream)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1218)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)

""process reaper"" daemon prio=5 tid=0x00007fc0321a0800 nid=0x13a03 runnable [0x000000011201b000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.UNIXProcess.waitForProcessExit(Native Method)
	at java.lang.UNIXProcess.access$200(UNIXProcess.java:54)
	at java.lang.UNIXProcess$3.run(UNIXProcess.java:174)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""process reaper"" daemon prio=5 tid=0x00007fc034171800 nid=0x13803 runnable [0x0000000111fc4000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.UNIXProcess.waitForProcessExit(Native Method)
	at java.lang.UNIXProcess.access$200(UNIXProcess.java:54)
	at java.lang.UNIXProcess$3.run(UNIXProcess.java:174)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""ExecutorRunner for app-20140726103835-0000/0"" daemon prio=5 tid=0x00007fc03592a000 nid=0x13603 in Object.wait() [0x000000011aa2a000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feb38140> (a java.lang.UNIXProcess)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:210)
	- locked <0x00000007feb38140> (a java.lang.UNIXProcess)
	at org.apache.spark.deploy.worker.ExecutorRunner.fetchAndRunExecutor(ExecutorRunner.scala:160)
	at org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRunner.scala:62)

""ExecutorRunner for app-20140726103835-0000/1"" daemon prio=5 tid=0x00007fc031b89800 nid=0x13403 in Object.wait() [0x000000011a927000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feb544b0> (a java.lang.UNIXProcess)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:210)
	- locked <0x00000007feb544b0> (a java.lang.UNIXProcess)
	at org.apache.spark.deploy.worker.ExecutorRunner.fetchAndRunExecutor(ExecutorRunner.scala:160)
	at org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRunner.scala:62)

""spark-akka.actor.default-dispatcher-14"" daemon prio=5 tid=0x00007fc035931800 nid=0x13203 waiting on condition [0x000000011a824000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-17"" daemon prio=5 tid=0x00007fc0330e3000 nid=0x13003 waiting on condition [0x000000011a721000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-13"" daemon prio=5 tid=0x00007fc0330e2000 nid=0x12e03 waiting on condition [0x000000011a61e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-16"" daemon prio=5 tid=0x00007fc034a45000 nid=0x12c03 waiting on condition [0x000000011a51b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker1-akka.actor.default-dispatcher-14"" daemon prio=5 tid=0x00007fc031b81800 nid=0x12a03 waiting on condition [0x000000011a418000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb226e8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-15"" daemon prio=5 tid=0x00007fc0350b2800 nid=0x12803 waiting on condition [0x000000011a315000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker1-akka.actor.default-dispatcher-13"" daemon prio=5 tid=0x00007fc035931000 nid=0x12603 waiting on condition [0x000000011a212000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb226e8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""Hashed wheel timer #4"" daemon prio=5 tid=0x00007fc03592d800 nid=0x12403 waiting on condition [0x000000011a10f000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:744)

""Hashed wheel timer #3"" daemon prio=5 tid=0x00007fc03414e000 nid=0x12203 waiting on condition [0x000000011a00c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:744)

""Hashed wheel timer #1"" daemon prio=5 tid=0x00007fc03592d000 nid=0x12003 waiting on condition [0x0000000119f09000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:744)

""Spark Context Cleaner"" daemon prio=5 tid=0x00007fc034a3e000 nid=0x11e03 in Object.wait() [0x0000000119e06000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feb38390> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked <0x00000007feb38390> (a java.lang.ref.ReferenceQueue$Lock)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:117)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:115)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:115)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1218)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:114)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)

""qtp1722016480-116"" daemon prio=5 tid=0x00007fc034160800 nid=0x11c03 waiting on condition [0x0000000119d03000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb2a688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-115"" daemon prio=5 tid=0x00007fc0322c6000 nid=0x11a03 waiting on condition [0x0000000119c00000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb2a688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-114"" daemon prio=5 tid=0x00007fc0358ff000 nid=0x11803 waiting on condition [0x0000000119afd000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb2a688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-113"" daemon prio=5 tid=0x00007fc0322bb000 nid=0x11603 waiting on condition [0x00000001199fa000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb2a688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-112 Acceptor1 SelectChannelConnector@0.0.0.0:50373"" daemon prio=5 tid=0x00007fc0322ba000 nid=0x11403 runnable [0x00000001198f7000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)
	- locked <0x00000007febf63b8> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-111 Acceptor0 SelectChannelConnector@0.0.0.0:50373"" daemon prio=5 tid=0x00007fc03415f800 nid=0x11203 waiting for monitor entry [0x00000001197f4000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:225)
	- waiting to lock <0x00000007febf63b8> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-110 Selector1"" daemon prio=5 tid=0x00007fc0350b0800 nid=0x11003 runnable [0x00000001196f1000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb2aeb0> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb2aea0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb2ad70> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1722016480-109 Selector0"" daemon prio=5 tid=0x00007fc034a3c800 nid=0x10e03 runnable [0x00000001195ee000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb2ab68> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb2ab58> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb2aa18> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""New I/O server boss #24"" daemon prio=5 tid=0x00007fc03415f000 nid=0x10c03 runnable [0x00000001194eb000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb38b28> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb38b38> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb38ad8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #23"" daemon prio=5 tid=0x00007fc032a50000 nid=0x10a03 runnable [0x00000001193e8000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb2b4c0> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb2b4d0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb2b470> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #22"" daemon prio=5 tid=0x00007fc034a3b000 nid=0x10803 runnable [0x00000001192e5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb244b8> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb244c8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb2b978> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O boss #21"" daemon prio=5 tid=0x00007fc0329f1800 nid=0x10603 runnable [0x00000001191e2000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb3b328> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb3b318> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb3b0d8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #20"" daemon prio=5 tid=0x00007fc0329f1000 nid=0x10403 runnable [0x00000001190df000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb24780> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb24790> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb24730> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #19"" daemon prio=5 tid=0x00007fc0329ee000 nid=0x10203 runnable [0x0000000118fdc000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb25018> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb25008> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb24c10> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-100"" daemon prio=5 tid=0x00007fc0358fe000 nid=0x10003 waiting on condition [0x0000000118ed9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb25850> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-99"" daemon prio=5 tid=0x00007fc0358fd800 nid=0xfe03 waiting on condition [0x0000000118dd6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb25850> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-98"" daemon prio=5 tid=0x00007fc03220b800 nid=0xfc03 waiting on condition [0x0000000118cd3000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb25850> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-97"" daemon prio=5 tid=0x00007fc03220b000 nid=0xfa03 waiting on condition [0x0000000118bd0000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb25850> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-96 Acceptor1 SelectChannelConnector@0.0.0.0:50371"" daemon prio=5 tid=0x00007fc03220a000 nid=0xf803 runnable [0x0000000118acd000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)
	- locked <0x00000007feb264a8> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-95 Acceptor0 SelectChannelConnector@0.0.0.0:50371"" daemon prio=5 tid=0x00007fc032209800 nid=0xf603 waiting for monitor entry [0x00000001189ca000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:225)
	- waiting to lock <0x00000007feb264a8> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-94 Selector1"" daemon prio=5 tid=0x00007fc032208800 nid=0xf403 runnable [0x00000001188c7000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb26018> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb26008> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb25ed8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1769933516-93 Selector0"" daemon prio=5 tid=0x00007fc031b8b000 nid=0xf203 runnable [0x00000001187c4000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feb25cd0> (a sun.nio.ch.Util$2)
	- locked <0x00000007feb25cc0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feb25b90> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""sparkWorker2-akka.actor.default-dispatcher-6"" daemon prio=5 tid=0x00007fc03414f800 nid=0xf003 waiting on condition [0x00000001186c1000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-akka.actor.default-dispatcher-5"" daemon prio=5 tid=0x00007fc0349d0000 nid=0xee03 waiting on condition [0x00000001185be000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-akka.actor.default-dispatcher-4"" daemon prio=5 tid=0x00007fc031ba8800 nid=0xec03 waiting on condition [0x00000001184bb000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-akka.actor.default-dispatcher-3"" daemon prio=5 tid=0x00007fc031ba7800 nid=0xea03 waiting on condition [0x00000001183b8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-akka.actor.default-dispatcher-2"" daemon prio=5 tid=0x00007fc0349cf800 nid=0xe803 waiting on condition [0x00000001182b5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb00c88> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker2-scheduler-1"" daemon prio=5 tid=0x00007fc0349df800 nid=0xe603 waiting on condition [0x00000001181b2000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:744)

""New I/O server boss #18"" daemon prio=5 tid=0x00007fc0329eb800 nid=0xe403 runnable [0x00000001180af000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed3bb20> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed3bb30> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed3bad0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #17"" daemon prio=5 tid=0x00007fc032212800 nid=0xe203 runnable [0x0000000117fac000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed94608> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed94618> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed945b8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #16"" daemon prio=5 tid=0x00007fc03227f000 nid=0xe003 runnable [0x0000000117ea9000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed949e0> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed949f0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fec5d230> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O boss #15"" daemon prio=5 tid=0x00007fc03227e800 nid=0xde03 runnable [0x0000000117da6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fec5f068> (a sun.nio.ch.Util$2)
	- locked <0x00000007fec5f058> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fec5ee18> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #14"" daemon prio=5 tid=0x00007fc0329cf000 nid=0xdc03 runnable [0x0000000117ca3000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed94d78> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed94d88> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed94d28> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #13"" daemon prio=5 tid=0x00007fc0329cc800 nid=0xda03 runnable [0x0000000117ba0000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed611c0> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed611d0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed61170> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""sparkWorker1-akka.actor.default-dispatcher-5"" daemon prio=5 tid=0x00007fc0329d5000 nid=0xd803 waiting on condition [0x0000000117a9d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb226e8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker1-akka.actor.default-dispatcher-4"" daemon prio=5 tid=0x00007fc031ac8000 nid=0xd603 waiting on condition [0x000000011799a000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb226e8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker1-akka.actor.default-dispatcher-3"" daemon prio=5 tid=0x00007fc0329d4000 nid=0xd403 waiting on condition [0x0000000117897000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb226e8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker1-akka.actor.default-dispatcher-2"" daemon prio=5 tid=0x00007fc0329d2800 nid=0xd203 waiting on condition [0x0000000117794000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb226e8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkWorker1-scheduler-1"" daemon prio=5 tid=0x00007fc0329d2000 nid=0xd003 waiting on condition [0x0000000117691000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-73"" daemon prio=5 tid=0x00007fc0340a4000 nid=0xce03 waiting on condition [0x000000011758e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed61af8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-72"" daemon prio=5 tid=0x00007fc0329a6800 nid=0xcc03 waiting on condition [0x000000011748b000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed61af8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-71"" daemon prio=5 tid=0x00007fc0358fc800 nid=0xca03 waiting on condition [0x0000000117388000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed61af8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-70"" daemon prio=5 tid=0x00007fc031ba7000 nid=0xc803 waiting on condition [0x0000000117285000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed61af8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-69 Acceptor1 SelectChannelConnector@0.0.0.0:50369"" daemon prio=5 tid=0x00007fc0358fc000 nid=0xc603 waiting for monitor entry [0x0000000117182000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:225)
	- waiting to lock <0x00000007fed95600> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-68 Acceptor0 SelectChannelConnector@0.0.0.0:50369"" daemon prio=5 tid=0x00007fc031ba6000 nid=0xc403 runnable [0x000000011707f000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)
	- locked <0x00000007fed95600> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-67 Selector1"" daemon prio=5 tid=0x00007fc031ba5800 nid=0xc203 runnable [0x0000000116f7c000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed1ce30> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed1ce40> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed1cde0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp1858873692-66 Selector0"" daemon prio=5 tid=0x00007fc031ac9800 nid=0xc003 runnable [0x0000000116e79000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed61f78> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed61f68> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed61e38> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""New I/O server boss #12"" daemon prio=5 tid=0x00007fc0329a3800 nid=0xbe03 runnable [0x0000000116d76000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed1d608> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed1d618> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed1d5b8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #11"" daemon prio=5 tid=0x00007fc0329a3000 nid=0xbc03 runnable [0x0000000116c73000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed62408> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed62418> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed623b8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #10"" daemon prio=5 tid=0x00007fc03227d800 nid=0xba03 runnable [0x0000000116b70000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fedb21d8> (a sun.nio.ch.Util$2)
	- locked <0x00000007fedb21e8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed62948> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O boss #9"" daemon prio=5 tid=0x00007fc032225000 nid=0xb803 runnable [0x0000000116a6d000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fedb23e0> (a sun.nio.ch.Util$2)
	- locked <0x00000007fedb23f0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fedb2390> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #8"" daemon prio=5 tid=0x00007fc032224800 nid=0xb603 runnable [0x000000011696a000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fedf1110> (a sun.nio.ch.Util$2)
	- locked <0x00000007fedf1120> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fedf10c0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #7"" daemon prio=5 tid=0x00007fc0330e7000 nid=0xb403 runnable [0x0000000116867000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fedf1660> (a sun.nio.ch.Util$2)
	- locked <0x00000007fedf1650> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fedf1530> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""sparkMaster-akka.actor.default-dispatcher-7"" daemon prio=5 tid=0x00007fc03419b000 nid=0xb203 waiting on condition [0x0000000116764000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-6"" daemon prio=5 tid=0x00007fc0329f6800 nid=0xb003 waiting on condition [0x0000000116661000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-5"" daemon prio=5 tid=0x00007fc0328eb000 nid=0xae03 waiting on condition [0x000000011655e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-4"" daemon prio=5 tid=0x00007fc031ac7000 nid=0xac03 waiting on condition [0x000000011645b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-3"" daemon prio=5 tid=0x00007fc032221000 nid=0xaa03 waiting on condition [0x0000000116358000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-akka.actor.default-dispatcher-2"" daemon prio=5 tid=0x00007fc032220000 nid=0xa803 waiting on condition [0x0000000116255000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb22518> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkMaster-scheduler-1"" daemon prio=5 tid=0x00007fc0349e3800 nid=0xa603 waiting on condition [0x0000000116152000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:744)

""Timer-0"" daemon prio=5 tid=0x00007fc0349d5800 nid=0xa403 in Object.wait() [0x0000000114005000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007fedf2b70> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000007fedf2b70> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""SparkListenerBus"" daemon prio=5 tid=0x00007fc032993000 nid=0xa207 waiting on condition [0x0000000115e9c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fedf2ed8> (a java.util.concurrent.Semaphore$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1218)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)

""qtp205855311-49"" daemon prio=5 tid=0x00007fc03598b800 nid=0x9e03 waiting on condition [0x0000000114726000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed80c10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-48"" daemon prio=5 tid=0x00007fc034a12800 nid=0x9c03 waiting on condition [0x0000000114623000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed80c10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-47"" daemon prio=5 tid=0x00007fc031a2f800 nid=0x9a03 waiting on condition [0x0000000114520000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed80c10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-46"" daemon prio=5 tid=0x00007fc0340ac800 nid=0x9803 waiting on condition [0x000000011441d000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fed80c10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-45 Acceptor1 SelectChannelConnector@0.0.0.0:4040"" daemon prio=5 tid=0x00007fc032992800 nid=0x9603 waiting for monitor entry [0x000000011431a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:225)
	- waiting to lock <0x00000007fed818a0> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-44 Acceptor0 SelectChannelConnector@0.0.0.0:4040"" daemon prio=5 tid=0x00007fc03412f000 nid=0x9403 runnable [0x0000000114217000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)
	- locked <0x00000007fed818a0> (a java.lang.Object)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-43 Selector1"" daemon prio=5 tid=0x00007fc032991800 nid=0x9203 runnable [0x0000000114114000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed81560> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed81550> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed81420> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""qtp205855311-42 Selector0"" daemon prio=5 tid=0x00007fc032991000 nid=0x9003 runnable [0x0000000113f02000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fed811e0> (a sun.nio.ch.Util$2)
	- locked <0x00000007fed811d0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fed810a0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.eclipse.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.eclipse.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""SPARK_CONTEXT cleanup timer"" daemon prio=5 tid=0x00007fc034124000 nid=0x8e03 in Object.wait() [0x0000000112774000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007fee79858> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000007fee79858> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""qtp2111378219-38"" daemon prio=5 tid=0x00007fc034092000 nid=0x8c03 waiting on condition [0x0000000113dff000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-37"" daemon prio=5 tid=0x00007fc032262000 nid=0x8a03 waiting on condition [0x0000000113cfc000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-36"" daemon prio=5 tid=0x00007fc03494f800 nid=0x8803 waiting on condition [0x0000000113bf9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-35"" daemon prio=5 tid=0x00007fc03297e000 nid=0x8603 waiting on condition [0x0000000113af6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-34"" daemon prio=5 tid=0x00007fc032261000 nid=0x8403 waiting on condition [0x00000001139f3000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-33"" daemon prio=5 tid=0x00007fc032260800 nid=0x8203 waiting on condition [0x00000001138f0000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-32"" daemon prio=5 tid=0x00007fc034939000 nid=0x8003 waiting on condition [0x00000001137ed000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007fee79be0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:744)

""qtp2111378219-31 Acceptor0 SocketConnector@0.0.0.0:50367"" daemon prio=5 tid=0x00007fc034895000 nid=0x7e03 runnable [0x00000001136ea000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at org.eclipse.jetty.server.bio.SocketConnector.accept(SocketConnector.java:117)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:744)

""Connection manager future execution context-0"" daemon prio=5 tid=0x00007fc03587d800 nid=0x7c03 waiting on condition [0x00000001135e7000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feeaea38> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""BROADCAST_VARS cleanup timer"" daemon prio=5 tid=0x00007fc032257800 nid=0x7a03 in Object.wait() [0x00000001134e4000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feeaf038> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000007feeaf038> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""BLOCK_MANAGER cleanup timer"" daemon prio=5 tid=0x00007fc032256800 nid=0x7803 in Object.wait() [0x00000001133e1000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007fee0d090> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000007fee0d090> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""connection-manager-thread"" daemon prio=5 tid=0x00007fc0319b4000 nid=0x7603 runnable [0x00000001132de000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007fee7a690> (a sun.nio.ch.Util$2)
	- locked <0x00000007fee7a6a0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fee7a640> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:303)
	at org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)

""SHUFFLE_BLOCK_MANAGER cleanup timer"" daemon prio=5 tid=0x00007fc0330ec000 nid=0x7403 in Object.wait() [0x00000001131db000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feeaf3c8> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000007feeaf3c8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""MAP_OUTPUT_TRACKER cleanup timer"" daemon prio=5 tid=0x00007fc03504b800 nid=0x7203 in Object.wait() [0x00000001130d8000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feee46b0> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000007feee46b0> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""New I/O server boss #6"" daemon prio=5 tid=0x00007fc03197b800 nid=0x7003 runnable [0x0000000112fd5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feee4ab8> (a sun.nio.ch.Util$2)
	- locked <0x00000007feee4ac8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feee4a68> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #5"" daemon prio=5 tid=0x00007fc03198e800 nid=0x6e03 runnable [0x0000000112ed2000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feeaf720> (a sun.nio.ch.Util$2)
	- locked <0x00000007feeaf730> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007fee92108> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #4"" daemon prio=5 tid=0x00007fc03197a800 nid=0x6c03 runnable [0x0000000112dcf000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feee5530> (a sun.nio.ch.Util$2)
	- locked <0x00000007feee5540> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feee54e0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O boss #3"" daemon prio=5 tid=0x00007fc031978000 nid=0x6a03 runnable [0x0000000112ccc000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feebf658> (a sun.nio.ch.Util$2)
	- locked <0x00000007feebf648> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feebf408> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #2"" daemon prio=5 tid=0x00007fc03587d000 nid=0x6803 runnable [0x0000000112bc9000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feebfd08> (a sun.nio.ch.Util$2)
	- locked <0x00000007feebfcf8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feebfbd8> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""New I/O worker #1"" daemon prio=5 tid=0x00007fc032926800 nid=0x6603 runnable [0x0000000112ac6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000007feec04c8> (a sun.nio.ch.Util$2)
	- locked <0x00000007feec04b8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000007feec00c0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""spark-akka.actor.default-dispatcher-5"" daemon prio=5 tid=0x00007fc03191f000 nid=0x6403 waiting on condition [0x00000001129c3000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-4"" daemon prio=5 tid=0x00007fc031911800 nid=0x6203 waiting on condition [0x00000001128c0000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-3"" daemon prio=5 tid=0x00007fc0328e9000 nid=0x6003 waiting on condition [0x00000001125a9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-2"" daemon prio=5 tid=0x00007fc032940000 nid=0x5e03 waiting on condition [0x00000001124a6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007feb44260> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-scheduler-1"" daemon prio=5 tid=0x00007fc032133000 nid=0x5c07 waiting on condition [0x0000000112281000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:744)

""Service Thread"" daemon prio=5 tid=0x00007fc035000000 nid=0x5303 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" daemon prio=5 tid=0x00007fc034801000 nid=0x5103 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" daemon prio=5 tid=0x00007fc031810800 nid=0x4f03 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" daemon prio=5 tid=0x00007fc031810000 nid=0x4d03 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" daemon prio=5 tid=0x00007fc03180f000 nid=0x3903 in Object.wait() [0x0000000111098000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007feee6f30> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked <0x00000007feee6f30> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)

""Reference Handler"" daemon prio=5 tid=0x00007fc032013000 nid=0x3703 in Object.wait() [0x0000000110f95000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007fee0dac0> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
	- locked <0x00000007fee0dac0> (a java.lang.ref.Reference$Lock)

""VM Thread"" prio=5 tid=0x00007fc03283e800 nid=0x3503 runnable 

""GC task thread#0 (ParallelGC)"" prio=5 tid=0x00007fc03200f800 nid=0x11f runnable 

""GC task thread#1 (ParallelGC)"" prio=5 tid=0x00007fc032010000 nid=0x252b runnable 

""GC task thread#2 (ParallelGC)"" prio=5 tid=0x00007fc032010800 nid=0x2903 runnable 

""GC task thread#3 (ParallelGC)"" prio=5 tid=0x00007fc032011800 nid=0x2b03 runnable 

""GC task thread#4 (ParallelGC)"" prio=5 tid=0x00007fc032012000 nid=0x2d03 runnable 

""GC task thread#5 (ParallelGC)"" prio=5 tid=0x00007fc032012800 nid=0x2f03 runnable 

""GC task thread#6 (ParallelGC)"" prio=5 tid=0x00007fc03180c000 nid=0x3103 runnable 

""GC task thread#7 (ParallelGC)"" prio=5 tid=0x00007fc03180c800 nid=0x3303 runnable 

""VM Periodic Task Thread"" prio=5 tid=0x00007fc032846800 nid=0x5503 waiting on condition 

JNI global references: 322
{code}",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408126,,,Sat Jul 26 19:50:43 UTC 2014,,,,,,,,,,"0|i1y86f:",408132,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"26/Jul/14 19:50;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
" ConnectionManager throws out of ""Could not find reference for received ack message  xxx"" exception",SPARK-2701,12730036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gq,gq,26/Jul/14 15:04,17/Aug/14 00:55,14/Jul/23 06:26,17/Aug/14 00:55,1.0.0,1.0.1,,,,,,1.1.0,,,,,,Spark Core,,,,1,,,,,,"{noformat}
14/07/26 22:05:15 INFO network.ConnectionManager: Key not valid ? sun.nio.ch.SelectionKeyImpl@13c7f699
14/07/26 22:05:15 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(tuan222,49423)
14/07/26 22:05:15 INFO spark.CacheManager: Partition rdd_51_418 not found, computing it
14/07/26 22:05:15 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
14/07/26 22:05:15 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Getting 1024 non-empty blocks out of 1024 blocks
14/07/26 22:05:15 INFO network.ConnectionManager: key already cancelled ? sun.nio.ch.SelectionKeyImpl@13c7f699
java.nio.channels.CancelledKeyException
        at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
        at org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/07/26 22:05:15 INFO network.ConnectionManager: Notifying org.apache.spark.network.ConnectionManager$MessageStatus@67c56c61
14/07/26 22:05:15 ERROR storage.BlockFetcherIterator$BasicBlockFetcherIterator: Could not get block(s) from ConnectionManagerId(tuan222,49423)
14/07/26 22:05:15 INFO network.SendingConnection: Initiating connection to [tuan222/172.16.1.222:49423]
14/07/26 22:05:15 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Started 79 remote fetches in 69 ms
14/07/26 22:05:15 INFO network.ConnectionManager: Removing ReceivingConnection to ConnectionManagerId(tuan222,49423)
14/07/26 22:05:15 ERROR executor.ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[pool-1-thread-2,5,main]
java.lang.Error: java.lang.Exception: Could not find reference for received ack message 222491
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.Exception: Could not find reference for received ack message 222491
        at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:647)
        at org.apache.spark.network.ConnectionManager$$anon$9.run(ConnectionManager.scala:504)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        ... 2 more
14/07/26 22:05:15 WARN network.SendingConnection: Error finishing connection to tuan222/172.16.1.222:49423
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
        at org.apache.spark.network.SendingConnection.finishConnect(Connection.scala:318)
        at org.apache.spark.network.ConnectionManager$$anon$7.run(ConnectionManager.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
14/07/26 22:05:15 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(tuan222,49423)
14/07/26 22:05:15 INFO network.ConnectionManager: Notifying org.apache.spark.network.ConnectionManager$MessageStatus@1324c9dc
14/07/26 22:05:15 ERROR storage.BlockFetcherIterator$BasicBlockFetcherIterator: Could not get block(s) from ConnectionManagerId(tuan222,49423)
14/07/26 22:05:15 INFO network.ConnectionManager: Handling connection error on connection to ConnectionManagerId(tuan222,49423)
14/07/26 22:05:15 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(tuan222,49423)
{noformat}",,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408109,,,Sun Aug 17 00:55:12 UTC 2014,,,,,,,,,,"0|i1y82v:",408116,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/14 00:55;gq;https://github.com/apache/spark/pull/1632;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce default spark.serializer.objectStreamReset ,SPARK-2696,12729979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,falaki,25/Jul/14 22:55,26/Jul/14 08:06,14/Jul/23 06:26,26/Jul/14 08:06,1.0.0,,,,,,,1.0.3,1.1.0,,,,,Spark Core,,,,0,configuration,,,,,"The current default value of spark.serializer.objectStreamReset is 10,000. 
When trying to re-partition (e.g., to 64 partitions) a large file (e.g., 500MB), containing 1MB records, the serializer will cache 10000 x 1MB x 64 = 640 GB which will cause it to go out of memory.

We think 100 would be a more reasonable default value for this configuration parameter.",,apachespark,falaki,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,408052,,,Fri Jul 25 23:11:05 UTC 2014,,,,,,,,,,"0|i1y7qf:",408060,,,,,,,,,,,,,,1.0.3,,,,,,,,,,,"25/Jul/14 23:11;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/1595;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for UDAF Hive Aggregates like PERCENTILE,SPARK-2693,12729822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ravi.pesala,marmbrus,marmbrus,25/Jul/14 19:12,03/Oct/14 18:25,14/Jul/23 06:26,03/Oct/14 18:25,,,,,,,,1.2.0,,,,,,SQL,,,,3,,,,,,"{code}
SELECT MIN(field1), MAX(field2), AVG(field3), PERCENTILE(field4), year,month,day FROM  raw_data_table  GROUP BY year, month, day

MIN, MAX and AVG functions work fine for me, but with PERCENTILE, I get an error as shown below.

Exception in thread ""main"" java.lang.RuntimeException: No handler for udf class org.apache.hadoop.hive.ql.udf.UDAFPercentile
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$.lookupFunction(hiveUdfs.scala:69)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$4$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:115)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$4$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:113)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
{code}

This aggregate extends UDAF, which we don't yet have a wrapper for.
",,apachespark,gvramana,marmbrus,nemccarthy,nfo,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407895,,,Fri Oct 03 18:25:46 UTC 2014,,,,,,,,,,"0|i1y6rr:",407904,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"26/Aug/14 13:46;ravipesala;UDAF is deprecated in HIve, Though there can be few functions which could have implemented using this interface. We can support the same in spark for backward compatability. 
As you mentioned supporting UDAF in spark requires to write a wrapper.

Please assign it to me.;;;","01/Oct/14 20:00;apachespark;User 'ravipesala' has created a pull request for this issue:
https://github.com/apache/spark/pull/2620;;;","03/Oct/14 18:25;marmbrus;Issue resolved by pull request 2620
[https://github.com/apache/spark/pull/2620];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove use of println in ActorHelper,SPARK-2689,12729788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,matei,matei,25/Jul/14 17:49,17/Sep/15 20:45,14/Jul/23 06:26,25/Jul/14 17:50,,,,,,,,1.1.0,,,,,,DStreams,,,,0,,,,,,,,matei,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407861,,,Fri Jul 25 17:49:32 UTC 2014,,,,,,,,,,"0|i1y6k7:",407870,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/14 17:49;matei;Pull request: https://github.com/apache/spark/pull/1372;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unidoc failed because org.apache.spark.util.CallSite uses Java keywords as value names,SPARK-2683,12729662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,25/Jul/14 06:08,25/Jul/14 18:50,14/Jul/23 06:26,25/Jul/14 18:50,,,,,,,,1.1.0,,,,,,Documentation,Spark Core,,,0,,,,,,,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2682,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407736,,,Fri Jul 25 06:20:53 UTC 2014,,,,,,,,,,"0|i1y5sf:",407745,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"25/Jul/14 06:20;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1585;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc generated from Scala source code is not in javadoc's index,SPARK-2682,12729659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,25/Jul/14 05:30,25/Jul/14 20:00,14/Jul/23 06:26,25/Jul/14 20:00,,,,,,,,1.1.0,,,,,,Documentation,,,,0,,,,,,Seems genjavadocSettings was deleted from SparkBuild. We need to add it back to let unidoc generate javadoc for our scala code.,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407733,,,Fri Jul 25 05:50:43 UTC 2014,,,,,,,,,,"0|i1y5rz:",407743,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"25/Jul/14 05:50;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1584;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`Spark-submit` overrides user application options,SPARK-2678,12729602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,lian cheng,lian cheng,24/Jul/14 22:59,15/Aug/14 18:54,14/Jul/23 06:26,07/Aug/14 23:03,1.0.1,1.0.2,,,,,,1.1.0,,,,,,Deploy,,,,0,,,,,,"Here is an example:
{code}
./bin/spark-submit --class Foo some.jar --help
{code}
SInce {{--help}} appears behind the primary resource (i.e. {{some.jar}}), it should be recognized as a user application option. But it's actually overriden by {{spark-submit}} and will show {{spark-submit}} help message.

When directly invoking {{spark-submit}}, the constraints here are:

# Options before primary resource should be recognized as {{spark-submit}} options
# Options after primary resource should be recognized as user application options

The tricky part is how to handle scripts like {{spark-shell}} that delegate  {{spark-submit}}. These scripts allow users specify both {{spark-submit}} options like {{--master}} and user defined application options together. For example, say we'd like to write a new script {{start-thriftserver.sh}} to start the Hive Thrift server, basically we may do this:
{code}
$SPARK_HOME/bin/spark-submit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal $@
{code}
Then user may call this script like:
{code}
./sbin/start-thriftserver.sh --master spark://some-host:7077 --hiveconf key=value
{code}
Notice that all options are captured by {{$@}}. If we put it before {{spark-internal}}, they are all recognized as {{spark-submit}} options, thus {{--hiveconf}} won't be passed to {{HiveThriftServer2}}; if we put it after {{spark-internal}}, they *should* all be recognized as options of {{HiveThriftServer2}}, but because of this bug, {{--master}} is still recognized as {{spark-submit}} option and leads to the right behavior.

Although currently all scripts using {{spark-submit}} work correctly, we still should fix this bug, because it causes option name collision between {{spark-submit}} and user application, and every time we add a new option to {{spark-submit}}, some existing user applications may break. However, solving this bug may cause some incompatible changes.

The suggested solution here is using {{--}} as separator of {{spark-submit}} options and user application options. For the Hive Thrift server example above, user should call it in this way:
{code}
./sbin/start-thriftserver.sh --master spark://some-host:7077 -- --hiveconf key=value
{code}
And {{SparkSubmitArguments}} should be responsible for splitting two sets of options and pass them correctly.",,andrewor,apachespark,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2880,SPARK-2894,,,,,SPARK-2874,SPARK-2110,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407676,,,Sun Aug 10 04:17:03 UTC 2014,,,,,,,,,,"0|i1y5fb:",407686,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"31/Jul/14 19:21;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1699;;;","01/Aug/14 10:36;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1715;;;","06/Aug/14 19:29;pwendell;Fixed in 1.1.0 via:
https://github.com/apache/spark/pull/1801;;;","07/Aug/14 07:41;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1831;;;","07/Aug/14 23:02;pwendell;Fixed in 1.0.3 via:
https://github.com/apache/spark/pull/1831;;;","08/Aug/14 21:27;apachespark;User 'chutium' has created a pull request for this issue:
https://github.com/apache/spark/pull/1861;;;","10/Aug/14 04:17;pwendell;This issue was fixed by:
https://github.com/apache/spark/pull/1825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BasicBlockFetchIterator#next can wait forever,SPARK-2677,12729591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,24/Jul/14 21:58,12/May/16 10:21,14/Jul/23 06:26,16/Aug/14 21:17,0.9.2,1.0.0,1.0.1,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"In BasicBlockFetchIterator#next, it waits fetch result on result.take.

{code}

    override def next(): (BlockId, Option[Iterator[Any]]) = {
      resultsGotten += 1
      val startFetchWait = System.currentTimeMillis()
      val result = results.take()
      val stopFetchWait = System.currentTimeMillis()
      _fetchWaitTime += (stopFetchWait - startFetchWait)
      if (! result.failed) bytesInFlight -= result.size
      while (!fetchRequests.isEmpty &&
        (bytesInFlight == 0 || bytesInFlight + fetchRequests.front.size <= maxBytesInFlight)) {
        sendRequest(fetchRequests.dequeue())
      }
      (result.blockId, if (result.failed) None else Some(result.deserialize()))
    }
{code}

But, results is implemented as LinkedBlockingQueue so if remote executor hang up, fetching Executor waits forever.
",,apachespark,derrickburns,fairchildljb,gq,llai,peng.zhang,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2681,,,,,,,,SPARK-2717,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407665,,,Thu May 12 10:21:13 UTC 2016,,,,,,,,,,"0|i1y5cv:",407675,,,,,,,,,,,,,,1.0.3,1.1.0,,,,,,,,,,"27/Jul/14 18:08;pwendell;Just as an FYI - this has been observed also in several earlier versions of Spark. I think one issue is that we don't have timeouts in the conneciton manger code. If a JVM goes into GC thrashing and becomes un-responsive (but still alive), then you can get stuck here.;;;","27/Jul/14 18:14;gq;If {{yarn.scheduler.fair.preemption}} is set to {{true}} in production cluster, This issue will appear frequently.;;;","28/Jul/14 09:00;gq;[~pwendell] , [~sarutak] How about the following solution?
https://github.com/apache/spark/pull/1619;;;","28/Jul/14 15:01;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/1619;;;","29/Jul/14 06:40;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1632;;;","11/Aug/14 02:22;sarutak;SPARK-2538 was resolved but there is still this issue.
I tried to resolve this issue in https://github.com/apache/spark/pull/1632;;;","21/Dec/14 02:19;derrickburns;Appear to still happen in 1.1.1:

2014-12-20 22:54:00,574 INFO  [connection-manager-thread] network.ConnectionManager (Logging.scala:logInfo(59)) - Key not valid ? sun.nio.ch.SelectionKeyImpl@6045fa68
2014-12-20 22:54:00,574 INFO  [handle-read-write-executor-0] network.ConnectionManager (Logging.scala:logInfo(59)) - Removing SendingConnection to ConnectionManagerId(ip-10-89-134-186.us-west-2.compute.internal,49171)
2014-12-20 22:54:00,574 INFO  [handle-read-write-executor-2] network.ConnectionManager (Logging.scala:logInfo(59)) - Removing ReceivingConnection to ConnectionManagerId(ip-10-89-134-186.us-west-2.compute.internal,49171)
2014-12-20 22:54:00,575 INFO  [sparkDriver-akka.actor.default-dispatcher-14] cluster.YarnClientSchedulerBackend (Logging.scala:logInfo(59)) - Executor 7 disconnected, so removing it
2014-12-20 22:54:00,576 ERROR [handle-read-write-executor-2] network.ConnectionManager (Logging.scala:logError(75)) - Corresponding SendingConnection to ConnectionManagerId(ip-10-89-134-186.us-west-2.compute.internal,49171) not found
2014-12-20 22:54:00,576 ERROR [sparkDriver-akka.actor.default-dispatcher-14] cluster.YarnClientClusterScheduler (Logging.scala:logError(75)) - Lost executor 7 on ip-10-89-134-186.us-west-2.compute.internal: remote Akka client disassociated
2014-12-20 22:54:00,576 INFO  [connection-manager-thread] network.ConnectionManager (Logging.scala:logInfo(80)) - key already cancelled ? sun.nio.ch.SelectionKeyImpl@6045fa68
java.nio.channels.CancelledKeyException
	at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:392)
	at org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:145);;;","12/May/16 10:21;fairchildljb;It seems this issue still can occur on spark 1.5.2 release version;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchFailedException should be thrown when local fetch has failed,SPARK-2670,12729516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,24/Jul/14 18:03,07/Sep/14 08:09,14/Jul/23 06:26,01/Aug/14 07:03,1.0.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"In BasicBlockFetchIterator, when remote fetch has failed, then FetchResult which size is -1 is set to results.

{code}
       case None => {
          logError(""Could not get block(s) from "" + cmId)
          for ((blockId, size) <- req.blocks) {
            results.put(new FetchResult(blockId, -1, null))
          }
{code}

The size -1 means fetch fail and BlockStoreShuffleFetcher#unpackBlock throws FetchFailedException so that we can retry.

But, when local fetch has failed, the failed FetchResult is not set.
So, we cannot retry for the FetchResult.",,apachespark,qwertymaniac,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1667,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407590,,,Thu Jul 24 19:15:43 UTC 2014,,,,,,,,,,"0|i1y4y7:",407604,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"24/Jul/14 19:15;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1578;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop configuration is not localised when submitting job in yarn-cluster mode,SPARK-2669,12729489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,rossmohax,rossmohax,24/Jul/14 16:12,17/Apr/15 19:22,14/Jul/23 06:26,17/Apr/15 19:22,1.0.0,,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"I'd like to propose a fix for a problem when Hadoop configuration is not localized when job is submitted in yarn-cluster mode. Here is a description from github pull request https://github.com/apache/spark/pull/1574

This patch fixes a problem when Spark driver is run in the container
managed by YARN ResourceManager it inherits configuration from a
NodeManager process, which can be different from the Hadoop
configuration present on the client (submitting machine). Problem is
most vivid when fs.defaultFS property differs between these two.

Hadoop MR solves it by serializing client's Hadoop configuration into
job.xml in application staging directory and then making Application
Master to use it. That guarantees that regardless of execution nodes
configurations all application containers use same config identical to
one on the client side.

This patch uses similar approach. YARN ClientBase serializes
configuration and adds it to ClientDistributedCacheManager under
""job.xml"" link name. ClientDistributedCacheManager is then utilizes
Hadoop localizer to deliver it to whatever container is started by this
application, including the one running Spark driver.

YARN ClientBase also adds ""SPARK_LOCAL_HADOOPCONF"" env variable to AM
container request which is then used by SparkHadoopUtil.newConfiguration
to trigger new behavior when machine-wide hadoop configuration is merged
with application specific job.xml (exactly how it is done in Hadoop MR).

SparkContext is then follows same approach, adding
SPARK_LOCAL_HADOOPCONF env to all spawned containers to make them use
client-side Hadopo configuration.

Also all the references to ""new Configuration()"" which might be executed
on YARN cluster side are changed to use SparkHadoopUtil.get.conf

Please note that it fixes only core Spark, the part which I am
comfortable to test and verify the result. I didn't descend into
steaming/shark directories, so things might need to be changed there too.
",,apachespark,rossmohax,sandyr,stevel@apache.org,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407563,,,Wed Jan 21 19:48:15 UTC 2015,,,,,,,,,,"0|i1y4s7:",407577,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,"24/Jul/14 16:15;apachespark;User 'redbaron' has created a pull request for this issue:
https://github.com/apache/spark/pull/1574;;;","21/Jan/15 19:48;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4142;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deal with `--conf` options in spark-submit that relate to flags,SPARK-2664,12729372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,pwendell,pwendell,24/Jul/14 06:25,21/Sep/14 01:42,14/Jul/23 06:26,31/Jul/14 18:52,,,,,,,,1.1.0,,,,,,,,,,1,,,,,,"If someone sets a spark conf that relates to an existing flag `--master`, we should set it correctly like we do with the defaults file. Otherwise it can have confusing semantics. I noticed this after merging it, otherwise I would have mentioned it in the review.

I think it's as simple as modifying loadDefaults to check the user-supplied options also. We might change it to loadUserProperties since it's no longer just the defaults file.",,apachespark,berngp,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3620,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407446,,,Thu Jul 31 18:52:15 UTC 2014,,,,,,,,,,"0|i1y42f:",407460,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"24/Jul/14 07:18;sandyr;I think the right behavior here is worth a little thought.  What's the mental model we expect the user to have about the relationship between properties specified through --conf and properties that get their own flag?  My first thought is - if we're ok with taking properties like master through --conf, is there a point (beyond compatibility) in having flags for these properties at all?

Flags that aren't Spark confs are there because they impact what happens before the SparkContext is created.  These fall into a couple categories:
1. Flags that have no property Spark conf equivalent like --executor-cores 
2. Flags that have a direct Spark conf equivalent like --executor-cores (spark.executor.memory)
3. Flags that impact a Spark conf like --deploy-mode (which can mean we set spark.master to yarn-cluster)

I think the two ways to look at it are:
1. We're OK with taking properties that have related flags.  In the case of a property in the 2nd category, we have a policy over which takes precedence.  In the case of a property in the 3rd category, we have some (possibly complex) resolution logic.  This approach would be the most accepting, but requires the user to have a model of how these conflicts get resolved.
2. We're not OK with taking properties that have related flags.  --conf specifies property that gets passed to the SparkContext and has no effect on anything that happens before it's created. To save users from themselves, if someone passes spark.master or spark.app.name through --conf, we ignore it or throw an error.

I'm a little more partial to approach 2 because I think the mental model is a little simpler.

Either way, we should probably enforce the same behavior when a config comes from the defaults file.

Lastly, how do we allow setting a default for one of these special flags?  E.g. make it so that all jobs run on YARN or Mesos by default.  With approach 1, this is relatively straightforward - we use the same logic we'd use on a property that comes in through --conf for making defaults take effect.  We might need to add spark properties for flags that don't have them already like --executor-cores.  With approach 2, we'd need to add support in the defaults file or somewhere else for specifying flag defaults.;;;","25/Jul/14 05:50;pwendell;Hey Sandy,

The reason why we originally allowed spark-defaults.conf to support spark options that have corresponding flags (which is 1 here) is because there was no other way for users to set things like the master in a configuration file. This seems like something we need to support (at least, IIRC it was one reason some packagers wanted a configuration file in the first place). So my proposal here was to just treat --conf as the same way. I'd also be okay to just throw an exception if users try to set one of these as a --conf when it corresponds to a flag, but then it deviates a bit from the behavior of the config file which could be confusing.

In terms of adding new configs for flags that don't currently have a corresponding conf. Personally, I'm also open to doing that. It would certainly simplify the fact that we have two different concepts at the moment for which there is not a 1:1 mapping. In my experience users have been most confused about the fact that those flags and spark conf properties are partially-but-not-completely overlapping. They haven't been confused about precedence order as much, since we state it clearly.

In the shorter term, given that we can't revert the behavior of spark-defaults.conf, I'd prefer to either use that same behavior for flags (the original proposal) or to throw an exception if any of the ""reserved"" properties are set via the --conf flag instead of their dedicated flag (we can always make it accept more liberally later). Otherwise it's super confusing what happens if the user sets --conf spark.master and also --master.;;;","30/Jul/14 19:01;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/1665;;;","31/Jul/14 18:52;pwendell;Issue resolved by pull request 1665
[https://github.com/apache/spark/pull/1665];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE for JsonProtocol,SPARK-2662,12729323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,24/Jul/14 01:45,24/Jul/14 05:51,14/Jul/23 06:26,24/Jul/14 05:51,,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,,,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407397,,,Thu Jul 24 01:46:22 UTC 2014,,,,,,,,,,"0|i1y3rj:",407411,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 01:46;gq;PR: https://github.com/apache/spark/pull/1511;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveQL: Division operator should always perform fractional division,SPARK-2659,12729315,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,marmbrus,marmbrus,marmbrus,24/Jul/14 00:48,15/Jul/19 06:35,14/Jul/23 06:26,28/Jul/14 01:35,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28395,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407389,,,Thu Jul 24 00:55:53 UTC 2014,,,,,,,,,,"0|i1y3pr:",407403,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"24/Jul/14 00:55;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1557;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveQL: 1 = true should evaluate to true,SPARK-2658,12729309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,marmbrus,marmbrus,marmbrus,24/Jul/14 00:19,24/Jul/14 05:53,14/Jul/23 06:26,24/Jul/14 05:53,,,,,,,,1.0.1,1.1.0,,,,,,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407383,,,Thu Jul 24 00:30:44 UTC 2014,,,,,,,,,,"0|i1y3of:",407397,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 00:30;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1556;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching tables larger than memory causes OOMs,SPARK-2650,12729227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,23/Jul/14 18:13,12/Aug/14 06:12,14/Jul/23 06:26,12/Aug/14 03:22,1.0.0,1.0.1,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"The logic for setting up the initial column buffers is different for Spark SQL compared to Shark and I'm seeing OOMs when caching tables that are larger than available memory (where shark was okay).

Two suspicious things: the intialSize is always set to 0 so we always go with the default.  The default looks like it was copied from code like 10 * 1024 * 1024... but in Spark SQL its 10 * 102 * 1024.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2902,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407301,,,Tue Aug 12 06:12:10 UTC 2014,,,,,,,,,,"0|i1y36n:",407317,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"04/Aug/14 07:31;lian cheng;Did some experiments and came to some conclusions:

# Needless to say, the {{10 * 1024 * 104}} is definitely a typo, but it's not related to the OOMs. More reasonable initial buffer sizes don't help solving these OOMs.
# The OOMs are also not related to whether the table size is larger than available memory. The cause is that the process of building in-memory columnar buffers is memory consuming, and multiple tasks building buffers in parallel eat too much memory altogether.
# According to 2, reducing parallelism or increasing executor memory can workaround this issue. For example, a {{HiveThriftServer2}} started with default executor memory (512MB) and {{--total-executor-cores=1}} could cache a 1.7GB table.
# Shark performs better than Spark SQL in this case, but still OOMs when the table gets larger: caching a 1.8GB table with default Shark configurations makes Shark OOM too.

I'm investigating why Spark SQL consumes more memory than Shark when building in-memory columnar buffers.;;;","04/Aug/14 19:03;lian cheng;Some additional comments after more experiments and some improvements:

# How exactly the OOMs occur when caching a large table (assume N cores and M memory are available within a single executor):
#- Say the table is so large that the underlying RDD is divided into X partitions (usually X >> N, let's assume this here)
#- When caching the table, N tasks are executed in parallel, building column buffers, each of them is memory consuming. Say, each task consumes Y memory in average.
#- At some point, memory consumptions of all N parallel tasks altogether, namely N * Y exceeds M, and an OOM is thrown
#- All tasks fail and retry, but fail again, until the driver stops retrying, and the job fail
#- I guess the reason that this issue hasn't been reported is that, usually N * Y < holds in production.
# Initial buffer sizes do affect the OOMs in a subtle way:
#- Too large an initial buffer size implies, apparently, larger memory consumption
#- Too small an initial buffer size causes the {{ColumnBuilder}} keeps allocating larger buffers to ensure enough free space to hold more elements (12.5% larger at a time). Thus 212.5% larger space is required to finish growing a buffer (112.5% for the new buffer + 100% for the original one).
#- A well estimated initial buffer size should be 1) large enough to avoid buffer growing, and 2) small enough to avoid memory waste. For example, by hand tuning, 5MB can be a good initial size for an executor with 512M memory and 1 core to cache a 1.7GB table without any problem. But this value varies between different cluster configurations.
# An apparent approach to help fixing this issue is to try reducing the memory consumption during the column building process.
#- [PR #1769|https://github.com/apache/spark/pull/1769] is submitted to reduce memory consumption of the column building proces.
# Another approach is to estimate the initial buffer size. To do this, Shark uses an estimated table partition size by leveraging HDFS block size and column element default size. We can use similar approach in Spark SQL for Hive tables, and some configurable initial size for non-Hive tables.
#- Currently {{InMemoryRelation}} resides in package {{org.apache.spark.sql.columnar}} and doesn't know anything about Hive tables. We can add an {{estimatedPartitionSize}} method, and override it in a new {{InMemoryMetastoreRelation}} to estimate RDD partition sizes of a Hive table. This will be done in another separate PR.;;;","04/Aug/14 19:07;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1769;;;","10/Aug/14 21:27;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1880;;;","12/Aug/14 06:12;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1901;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log4j initialization not quite compatible with log4j 2.x,SPARK-2646,12729161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,srowen,srowen,23/Jul/14 13:05,12/Mar/15 13:11,14/Jul/23 06:26,31/Jul/14 19:27,1.0.0,1.0.1,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"The logging code that handles log4j initialization leads to an stack overflow error when used with log4j 2.x, which has just been released. This occurs even a downstream project has correctly adjusted SLF4J bindings, and that is the right thing to do for log4j 2.x, since it is effectively a separate project from 1.x.

Here is the relevant bit of Logging.scala:

{code}
  private def initializeLogging() {
    // If Log4j is being used, but is not initialized, load a default properties file
    val binder = StaticLoggerBinder.getSingleton
    val usingLog4j = binder.getLoggerFactoryClassStr.endsWith(""Log4jLoggerFactory"")
    val log4jInitialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized && usingLog4j) {
      val defaultLogProps = ""org/apache/spark/log4j-defaults.properties""
      Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match {
        case Some(url) =>
          PropertyConfigurator.configure(url)
          log.info(s""Using Spark's default log4j profile: $defaultLogProps"")
        case None =>
          System.err.println(s""Spark was unable to load $defaultLogProps"")
      }
    }
    Logging.initialized = true

    // Force a call into slf4j to initialize it. Avoids this happening from mutliple threads
    // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html
    log
  }
{code}

The first minor issue is that there is a call to a logger inside this method, which is initializing logging. In this situation, it ends up causing the initialization to be called recursively until the stack overflow. It would be slightly tidier to log this only after Logging.initialized = true. Or not at all. But it's not the root problem, or else, it would not work at all now. 

The calls to log4j classes here always reference log4j 1.2 no matter what. For example, there is not getAllAppenders in log4j 2.x. That's fine. Really, ""usingLog4j"" means ""using log4j 1.2"" and ""log4jInitialized"" means ""log4j 1.2 is initialized"".

usingLog4j should be false for log4j 2.x, because the initialization only matters for log4j 1.2. But, it's true, and that's the real issue. And log4jInitialized is always false, since calls to the log4j 1.2 API are stubs and no-ops in this setup, where the caller has swapped in log4j 2.x. Hence the loop.

This is fixed, I believe, if ""usingLog4j"" can be false for log4j 2.x. The SLF4J static binding class has the same name for both versions, unfortunately, which causes the issue. However they're in different packages. For example, if the test included ""... and begins with org.slf4j"", it should work, as the SLF4J binding for log4j 2.x is provided by log4j 2.x at the moment, and is in package org.apache.logging.slf4j.

Of course, I assume that SLF4J will eventually offer its own binding. I hope to goodness they at least name the binding class differently, or else this will again not work. But then some other check can probably be made.

(Credit to Agust Egilsson for finding this; at his request I'm opening a JIRA for him. I'll propose a PR too.)",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6305,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407235,,,Thu Jul 31 19:27:12 UTC 2014,,,,,,,,,,"0|i1y2rz:",407251,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 14:00;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1547;;;","31/Jul/14 19:27;pwendell;Issue resolved by pull request 1547
[https://github.com/apache/spark/pull/1547];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark driver calls System.exit(50) after calling SparkContext.stop() the second time ,SPARK-2645,12729142,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rekhajoshm,vokom,vokom,23/Jul/14 10:58,30/Jun/15 21:00,14/Jul/23 06:26,30/Jun/15 21:00,1.0.0,,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"In some cases my application calls SparkContext.stop() after it has already stopped and this leads to stopping JVM that runs spark driver.
E.g
This program should run forever
{code}
JavaSparkContext context = new JavaSparkContext(""spark://12.34.21.44:7077"", ""DummyApp"");
        try {
            JavaRDD<Integer> rdd = context.parallelize(Arrays.asList(1, 2, 3));
            rdd.count();
        } catch (Throwable e) {
            e.printStackTrace();
        }
        try {
            context.cancelAllJobs();
            context.stop();
            //call stop second time
            context.stop();
        } catch (Throwable e) {
            e.printStackTrace();
        }
        Thread.currentThread().join();
{code}
but it finishes with exit code 50 after calling SparkContext.stop() the second time.
Also it throws an exception like this
{code}
org.apache.spark.ServerStateException: Server is already stopped
	at org.apache.spark.HttpServer.stop(HttpServer.scala:122) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.HttpFileServer.stop(HttpFileServer.scala:48) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:81) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.SparkContext.stop(SparkContext.scala:984) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.dead(SparkDeploySchedulerBackend.scala:92) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.deploy.client.AppClient$ClientActor.markDead(AppClient.scala:178) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.deploy.client.AppClient$ClientActor$$anonfun$registerWithMaster$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AppClient.scala:96) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:790) ~[spark-core_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.deploy.client.AppClient$ClientActor$$anonfun$registerWithMaster$1.apply$mcV$sp(AppClient.scala:91) [spark-core_2.10-1.0.0.jar:1.0.0]
	at akka.actor.Scheduler$$anon$9.run(Scheduler.scala:80) [akka-actor_2.10-2.2.3-shaded-protobuf.jar:na]
	at akka.actor.LightArrayRevolverScheduler$$anon$3$$anon$2.run(Scheduler.scala:241) [akka-actor_2.10-2.2.3-shaded-protobuf.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:42) [akka-actor_2.10-2.2.3-shaded-protobuf.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3-shaded-protobuf.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.4.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.4.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.4.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.4.jar:na]
{code}
One remark is that this behavior is only reproducible when I call SparkContext.cancellAllJobs() before calling SparkContext.stop()",,apachespark,rekhajoshm,vokom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407216,,,Wed Jun 24 04:47:41 UTC 2015,,,,,,,,,,"0|i1y2nr:",407232,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,"23/Jan/15 18:19;srowen;Is this still an issue? I do not see any call to {{System.exit(50)}} anymore.;;;","25/Jan/15 11:19;vokom;[~sowen], are you using the same spark master url that is mentioned in the code snippet (""spark://12.34.21.44:7077"" or just spark://invalid.host:7077) ? 
Now it exits with exit code 1. ;;;","25/Jan/15 12:17;srowen;Just checking if you believe this is still an issue, since I don't see any code that exits with status 50 as mentioned in the description. I have not tested this at all myself. If it's still a problem, is the fix simply to handle multiple calls to {{stop()}} better?;;;","24/Jun/15 04:44;apachespark;User 'rekhajoshm' has created a pull request for this issue:
https://github.com/apache/spark/pull/6973;;;","24/Jun/15 04:47;rekhajoshm;[~sowen] [~vokom] Hi. On a quick look, with my setup latest 1.5.0-SNAPSHOT., I believe this can still happen.
If Executor hits on an unhandled exception, the system will exit with code 50 (SparkExitCode.UNCAUGHT_EXCEPTION)
{code}
//SparkUncaughtExceptionHandler//
if (!Utils.inShutdown()) {
  if (exception.isInstanceOf[OutOfMemoryError]) {
    System.exit(SparkExitCode.OOM)
  } else {
    System.exit(SparkExitCode.UNCAUGHT_EXCEPTION)
  }
}
.. . 
private[spark] object SparkExitCode {
  /** The default uncaught exception handler was reached. */
  val UNCAUGHT_EXCEPTION = 50
{code}

Git patch is to defensively avoid a stop if already done and/or handling exception at SparkEnv.Please review.Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stages web ui has ERROR when pool name is None,SPARK-2643,12729122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,yantangzhai,yantangzhai,23/Jul/14 09:02,29/Sep/14 07:53,14/Jul/23 06:26,29/Sep/14 07:53,,,,,,,,,,,,,,Web UI,,,,0,,,,,,"14/07/23 16:01:44 WARN servlet.ServletHandler: /stages/
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:313)
        at scala.None$.get(Option.scala:311)
        at org.apache.spark.ui.jobs.StageTableBase.stageRow(StageTable.scala:132)
        at org.apache.spark.ui.jobs.StageTableBase.org$apache$spark$ui$jobs$StageTableBase$$renderStageRow(StageTable.scala:150)
        at org.apache.spark.ui.jobs.StageTableBase$$anonfun$toNodeSeq$1.apply(StageTable.scala:52)
        at org.apache.spark.ui.jobs.StageTableBase$$anonfun$toNodeSeq$1.apply(StageTable.scala:52)
        at org.apache.spark.ui.jobs.StageTableBase$$anonfun$stageTable$1.apply(StageTable.scala:61)
        at org.apache.spark.ui.jobs.StageTableBase$$anonfun$stageTable$1.apply(StageTable.scala:61)
        at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:376)
        at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:376)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1085)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1077)
        at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:980)
        at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:980)
        at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:969)
        at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:969)
        at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:974)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.xml.NodeBuffer.$amp$plus(NodeBuffer.scala:38)
        at scala.xml.NodeBuffer.$amp$plus(NodeBuffer.scala:40)
        at org.apache.spark.ui.jobs.StageTableBase.stageTable(StageTable.scala:60)
        at org.apache.spark.ui.jobs.StageTableBase.toNodeSeq(StageTable.scala:52)
        at org.apache.spark.ui.jobs.JobProgressPage.render(JobProgressPage.scala:91)
        at org.apache.spark.ui.WebUI$$anonfun$attachPage$1.apply(WebUI.scala:65)
        at org.apache.spark.ui.WebUI$$anonfun$attachPage$1.apply(WebUI.scala:65)
        at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:70)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:370)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:744)

14/07/23 16:01:44 WARN server.AbstractHttpConnection: /stages/
java.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted()Z
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:370)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:744)


This is because:
(1) One stage for example the stage 10 has been completed and then it's resubmitted. 
(2) Then JobProgressListener.trimIfNecessary is triggered by another SparkListenerStageCompleted event. The StageUIData of the stage 10 will be removed from memory although the stage 10 is active at present.
(3) JobProgressPage will not get the StageUIData of the stage 10 any more.",,apachespark,gq,yantangzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1208,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407196,,,Mon Sep 29 07:53:14 UTC 2014,,,,,,,,,,"0|i1y2jb:",407212,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/14 11:49;apachespark;User 'YanTangZhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1854;;;","29/Sep/14 07:53;srowen;Discussion suggests this was fixed by a related change: https://github.com/apache/spark/pull/1854#issuecomment-55061571;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark submit doesn't pick up executor instances from properties file,SPARK-2641,12729103,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kjsingh,kjsingh,23/Jul/14 07:48,17/Sep/15 20:34,14/Jul/23 06:26,07/Jan/15 10:32,1.0.0,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"When running spark-submit in Yarn cluster mode, we provide properties file using --properties-file option.

spark.executor.instances=5
spark.executor.memory=2120m
spark.executor.cores=3

The submitted job picks up the cores and memory, but not the correct instances.

I think the issue is here in org.apache.spark.deploy.SparkSubmitArguments:

// Use properties file as fallback for values which have a direct analog to
    // arguments in this script.
    master = Option(master).getOrElse(defaultProperties.get(""spark.master"").orNull)
    executorMemory = Option(executorMemory)
      .getOrElse(defaultProperties.get(""spark.executor.memory"").orNull)
    executorCores = Option(executorCores)
      .getOrElse(defaultProperties.get(""spark.executor.cores"").orNull)
    totalExecutorCores = Option(totalExecutorCores)
      .getOrElse(defaultProperties.get(""spark.cores.max"").orNull)
    name = Option(name).getOrElse(defaultProperties.get(""spark.app.name"").orNull)
    jars = Option(jars).getOrElse(defaultProperties.get(""spark.jars"").orNull)

Along with these defaults, we should also set default for instances:

numExecutors=Option(numExecutors).getOrElse(defaultProperties.get(""spark.executor.instances"").orNull)

PS: spark.executor.instances is also not mentioned on http://spark.apache.org/docs/latest/configuration.html",,apachespark,kjsingh,mxm,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3620,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407177,,,Wed Jul 30 09:41:25 UTC 2014,,,,,,,,,,"0|i1y2f3:",407193,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/14 09:41;apachespark;User 'kjsingh' has created a pull request for this issue:
https://github.com/apache/spark/pull/1657;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In ""local[N]"", free cores of the only executor should be touched by ""spark.task.cpus"" for every finish/start-up of tasks.",SPARK-2640,12729094,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,woshilaiceshide,woshilaiceshide,woshilaiceshide,23/Jul/14 06:49,17/Sep/15 20:33,14/Jul/23 06:26,23/Jul/14 18:06,,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,,,apachespark,mxm,woshilaiceshide,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407168,,,Wed Jul 23 07:10:29 UTC 2014,,,,,,,,,,"0|i1y2d3:",407184,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 07:10;apachespark;User 'woshilaiceshide' has created a pull request for this issue:
https://github.com/apache/spark/pull/1544;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix race condition at SchedulerBackend.isReady in standalone mode,SPARK-2635,12729074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,li-zhihui,li-zhihui,li-zhihui,23/Jul/14 02:48,09/Aug/14 05:55,14/Jul/23 06:26,09/Aug/14 05:54,1.0.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"In SPARK-1946(PR #900), configuration spark.scheduler.minRegisteredExecutorsRatio was introduced. However, in standalone mode, there is a race condition where isReady() can return true because totalExpectedExecutors has not been correctly set.

Because expected executors is uncertain in standalone mode, the PR try to use CPU cores(--total-executor-cores) as expected resources to judge whether SchedulerBackend is ready.",,li-zhihui,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1946,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407148,,,Sat Aug 09 05:54:13 UTC 2014,,,,,,,,,,"0|i1y28n:",407164,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 02:53;li-zhihui;PR https://github.com/apache/spark/pull/1525;;;","09/Aug/14 05:54;pwendell;Issue resolved by pull request 1525
[https://github.com/apache/spark/pull/1525];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapOutputTrackerWorker.mapStatuses should be thread-safe,SPARK-2634,12729072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zsxwing,zsxwing,23/Jul/14 02:33,26/Sep/14 01:24,14/Jul/23 06:26,26/Sep/14 01:24,1.0.0,,,,,,,1.2.0,,,,,,Spark Core,,,,0,easyfix,,,,,"MapOutputTrackerWorker.mapStatuses will be used concurrently, so it should be a thread-safe Map.",,apachespark,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407146,,,Fri Sep 26 01:24:16 UTC 2014,,,,,,,,,,"0|i1y287:",407162,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 03:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/1541;;;","26/Sep/14 01:24;joshrosen;Issue resolved by pull request 1541
[https://github.com/apache/spark/pull/1541];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Importing a method of class in Spark REPL causes the REPL to pulls in unnecessary stuff.,SPARK-2632,12729048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,yhuai,yhuai,23/Jul/14 00:26,07/Feb/20 17:23,14/Jul/23 06:26,01/Aug/14 05:58,1.0.0,1.0.1,,,,,,1.1.0,,,,,,Spark Shell,,,,0,,,,,," Master is affected by this bug. To reproduce the exception, you can start a local cluster (sbin/start-all.sh) then open a spark shell.

{code}
class X() { println(""What!""); def y = 3 }
val x = new X
import x.y
case class Person(name: String, age: Int)
sc.textFile(""examples/src/main/resources/people.txt"").map(_.split("","")).map(p => Person(p(0), p(1).trim.toInt)).collect
{code}
Then you will find the exception. I am attaching the stack trace below...
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 0.0 (TID 0) had a not serializable result: $iwC$$iwC$$iwC$$iwC$X
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1045)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1029)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1027)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1027)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:632)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:632)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:632)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1230)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,apachespark,prashant,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2610,SPARK-2576,,,,,,SPARK-5149,SPARK-5150,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407122,,,Tue Jul 29 09:35:54 UTC 2014,,,,,,,,,,"0|i1y22v:",407138,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"23/Jul/14 00:26;yhuai;[~prashant] Can you take a look at this issue? Thanks:);;;","23/Jul/14 04:52;yhuai;I have created a a [REPL test|https://github.com/yhuai/spark/commit/93ae2183f2f32d81ed488e23fd88b84057d385fc].;;;","23/Jul/14 16:50;prashant;I am looking at this, thanks for creating the test. ;;;","23/Jul/14 18:01;yhuai;Seems the exception triggered by importing a method of a non-serializable class is not a new bug. So, I guess we do not want to block our release on it.;;;","29/Jul/14 09:35;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1635;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In-memory Compression is not configured with SQLConf,SPARK-2631,12728998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,22/Jul/14 22:15,30/Jul/14 01:21,14/Jul/23 06:26,30/Jul/14 01:21,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407072,,,Tue Jul 29 20:56:01 UTC 2014,,,,,,,,,,"0|i1y1rz:",407089,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"29/Jul/14 20:56;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop SparkContext in all examples,SPARK-2626,12728947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,andrewor14,andrewor,22/Jul/14 19:34,05/Nov/14 10:43,14/Jul/23 06:26,01/Oct/14 18:28,1.0.1,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,starter,,,,,"Event logs rely on sc.stop() to close the file. If this is never closed, the history server will not be able to find the logs.",,andrewor,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,407021,,,Wed Oct 01 18:28:52 UTC 2014,,,,,,,,,,"0|i1y1h3:",407040,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/14 10:35;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2575;;;","01/Oct/14 18:28;joshrosen;Issue resolved by pull request 2575
[https://github.com/apache/spark/pull/2575];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct doc and usage of preservesPartitioning,SPARK-2617,12728816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,22/Jul/14 09:28,23/Jul/14 07:59,14/Jul/23 06:26,23/Jul/14 07:59,1.0.1,,,,,,,1.1.0,,,,,,Documentation,MLlib,Spark Core,,0,,,,,,"The name `preservesPartitioning` is ambiguous: 1) preserves the indices of partitions, 2) preserves the partitioner. The latter is correct and `preservesPartitioning` should really be called `preservesPartitioner`. Unfortunately, this is already part of the API and we cannot change.

We should be clear in the doc and fix wrong usages.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406890,,,Tue Jul 22 09:36:07 UTC 2014,,,,,,,,,,"0|i1y0nz:",406909,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"22/Jul/14 09:36;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1526;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""=="" support for HiveQl",SPARK-2615,12728788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,22/Jul/14 07:32,23/Jul/14 01:14,14/Jul/23 06:26,23/Jul/14 01:14,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"Currently, if passing ""=="" other than ""="" in expression of Hive QL, will cause exception.",,apachespark,chenghao,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406862,,,Wed Jul 23 00:55:36 UTC 2014,,,,,,,,,,"0|i1y0hr:",406881,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/14 07:35;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/1522;;;","22/Jul/14 16:19;yhuai;Based on Hive language manual (https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF), ""=="" is invalid. But, Hive actually treats ""=="" as ""="". I have sent an email to hive-dev list to ask it.;;;","23/Jul/14 00:55;chenghao;Yes, that's true.
But ""=="" is actually used in lots of unit test queries, without this being fixed in SparkSQL, some of them may not able to pass the unit test. 
For example:
https://github.com/apache/hive/blob/trunk/ql/src/test/queries/clientpositive/groupby_grouping_id1.q

Probably we should create a Jira issue for Hive, either for the documentation or source code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALS has data skew for popular product,SPARK-2612,12728776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,peng.zhang,peng.zhang,peng.zhang,22/Jul/14 05:51,22/Jul/14 09:41,14/Jul/23 06:26,22/Jul/14 09:41,1.0.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,,,,,,"Usually there are some popular products which are related with many users in Rating inputs. 
groupByKey() in updateFeatures() may cause one extra Shuffle stage to gather data of the popular product to one task, because it's RDD's partitioner may be not used as the join() partitioner. 
The following join() need to shuffle from the aggregated product data. The shuffle block can easily be bigger than 2G, and shuffle failed as mentioned in SPARK-1476
And increasing blocks number doesn't work.  

IMHO, groupByKey() should use the same partitioner as the other RDD in join(). So groupByKey() and join() will be in the same stage, and shuffle data from many previous tasks will not trigger ""2G"" limits.",,apachespark,glenn.strycker@gmail.com,mengxr,peng.zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406850,,,Tue Jul 22 06:10:24 UTC 2014,,,,,,,,,,"0|i1y0f3:",406869,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/14 06:10;apachespark;User 'renozhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/1521;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos doesn't handle spark.executor.extraJavaOptions correctly (among other things),SPARK-2608,12728609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,scwf,scwf,21/Jul/14 16:11,28/Aug/14 18:08,14/Jul/23 06:26,27/Aug/14 19:41,1.0.0,,,,,,,1.1.0,,,,,,Mesos,,,,0,,,,,,"mesos scheduler backend use spark-class/spark-executor to launch executor backend, this will lead to problems:
1 when set spark.executor.extraJavaOptions CoarseMesosSchedulerBackend  will throw error
2 spark.executor.extraJavaOptions and spark.executor.extraLibraryPath set in sparkconf will not be valid",,apachespark,pwendell,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2921,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406684,,,Wed Aug 27 19:41:21 UTC 2014,,,,,,,,,,"0|i1xzev:",406704,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"21/Jul/14 16:15;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/1513;;;","16/Aug/14 15:46;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/1986;;;","23/Aug/14 08:07;apachespark;User 'tnachen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2103;;;","26/Aug/14 20:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2145;;;","27/Aug/14 17:47;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/2161;;;","27/Aug/14 19:41;pwendell;Resolved by:
https://github.com/apache/spark/pull/2161

However, it would be nice to have a solution longer term that deals with quoted strings. One issue is whether this is feasible given the CommandInfo interface in mesos.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaRDD unionall prevents caching,SPARK-2607,12728606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,thierry.herrmann,thierry.herrmann,21/Jul/14 16:04,15/Sep/15 20:50,14/Jul/23 06:26,15/Sep/15 20:50,1.0.0,,,,,,,1.5.0,,,,,,SQL,,,,2,cache,union,,,,"This driver program submitted with spark-submit:

{code:title=TestUnion.scala|borderStyle=solid}
val sc = new org.apache.spark.SparkContext(conf)
  val sqlCtx = new SQLContext(sc)
  val rddForDay1 = sqlCtx.parquetFile(s""hdfs://dell-715-09/user/hive/warehouse/mytable/uivr_year=2014/uivr_month=5/uivr_day=1"")
  val rddForDay2 = sqlCtx.parquetFile(s""hdfs://dell-715-09/user/hive/warehouse/mytable/uivr_year=2014/uivr_month=5/uivr_day=2"")
  rddForDay1.cache
  rddForDay2.cache
  rddForDay1 union rddForDay2 count
{code}

generates these line in the log, thanks to the .cache calls:

{noformat}
14/07/21 11:38:49 INFO BlockManagerInfo: Added rdd_1_0 in memory on dell-715-12.neura-local.com:39169 (size: 689.7 MB, free: 8.0 GB)
14/07/21 11:38:49 INFO BlockManagerInfo: Added rdd_0_0 in memory on dell-715-12.neura-local.com:39169 (size: 744.4 MB, free: 7.2 GB)
{noformat}

If I replace union with unionAll, these lines are not present anymore in the log which makes me think the RDDs are not cached anymore.","Linux vb2 3.13.0-30-generic #54-Ubuntu SMP Mon Jun 9 22:45:01 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
",berngp,glenn.strycker@gmail.com,gvramana,marmbrus,ravipesala,thierry.herrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406681,,,Tue Sep 15 20:50:29 UTC 2015,,,,,,,,,,"0|i1xze7:",406701,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/15 20:50;marmbrus;I beleive this is fixed in 1.5.  Please reopen if you are still having issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In some cases, pages display incorrect in spark UI",SPARK-2606,12728585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,21/Jul/14 14:03,23/Jul/14 01:35,14/Jul/23 06:26,23/Jul/14 01:35,,,,,,,,1.1.0,,,,,,Web UI,YARN,,,0,,,,,,"In yarn-client mode, if the user browse spark UI,in the process of start. the spark UI pages display incorrect,after the start is completed.",,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406660,,,Mon Jul 21 14:04:38 UTC 2014,,,,,,,,,,"0|i1xz9j:",406680,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"21/Jul/14 14:04;gq;PR: https://github.com/apache/spark/pull/1501;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Application hangs on yarn in edge case scenario of executor memory requirement,SPARK-2604,12728506,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,twinkle,twinkle,21/Jul/14 10:37,15/Dec/14 05:28,14/Jul/23 06:26,15/Dec/14 05:28,1.0.0,,,,,,,,,,,,,Spark Core,,,,1,,,,,,"In yarn environment, let's say :
MaxAM = Maximum allocatable memory
ExecMem - Executor's memory

if (MaxAM > ExecMem && ( MaxAM - ExecMem) > 384m ))
  then Maximum resource validation fails w.r.t executor memory , and application master gets launched, but when resource is allocated and again validated, they are returned and application appears to be hanged.

Typical use case is to ask for executor memory = maximum allowed memory as per yarn config",,apachespark,tgraves,twinkle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2140,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406582,,,Thu Dec 11 21:58:33 UTC 2014,,,,,,,,,,"0|i1xysf:",406602,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/14 10:38;twinkle;Please assign this issue to me.;;;","21/Jul/14 13:43;tgraves;Just to clarify, are you referring to the max checks in ClientBase.verifyClusterResources?  Basically they don't take into account for the memory overhead.;;;","21/Jul/14 13:47;tgraves;Also note that it shouldn't hang, it should fail after a certain number of retries. The AM retries is configured by the resource manager, the executor failure number is (although this only work in yarn-cluster mode) There is Pr up to fix in client mode if that is what you are using.

private val maxNumExecutorFailures = sparkConf.getInt(""spark.yarn.max.executor.failures"",
    sparkConf.getInt(""spark.yarn.max.worker.failures"", math.max(args.numExecutors * 2, 3)))
;;;","21/Jul/14 15:42;twinkle;For Executors, In verifyClusterResources we do not take into account the overhead, where as in YarnAllocationHandler.scala, following def is provided:

def isResourceConstraintSatisfied(container: Container): Boolean = {
    container.getResource.getMemory >= (executorMemory + YarnAllocationHandler.MEMORY_OVERHEAD)
  }

In the case,when container is not allocated with enough memory to satisfy the condition, container is release. As executor has not been launched, it is not counted as failures. Please see the code below:

for (container <- allocatedContainers) {
        if (isResourceConstraintSatisfied(container)) {
          // Add the accepted `container` to the host's list of already accepted,
          // allocated containers
          val host = container.getNodeId.getHost
          val containersForHost = hostToContainers.getOrElseUpdate(host,
            new ArrayBuffer[Container]())
          containersForHost += container
        } else {
          // Release container, since it doesn't satisfy resource constraints.
          releaseContainer(container)
        }
      }

So allocation happens and container is then returned and not counted as failed, due to which only App master is launched.;;;","22/Jul/14 13:11;twinkle;I tried running in yarn-cluster mode. After setting property of spark.yarn.max.executor.failures to some number. Application do gets failed, but with misleading exception ( pasted at the end ). Instead of handling the condition this way, probably we should be doing the check for the overhead memory amount at the validation itself. Please share your thoughts, if you think otherwise.

Stacktrace :
Application application_1405933848949_0024 failed 2 times due to Error launching appattempt_1405933848949_0024_000002. Got exception: java.net.ConnectException: Call From NN46/192.168.156.46 to localhost:51322 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
at java.lang.reflect.Constructor.newInstance(Unknown Source)
at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
at org.apache.hadoop.ipc.Client.call(Client.java:1414)
at org.apache.hadoop.ipc.Client.call(Client.java:1363)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
at com.sun.proxy.$Proxy28.startContainers(Unknown Source)
at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:249)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source);;;","23/Jul/14 14:35;tgraves;Yes  we should be adding the overhead in at the check.;;;","24/Jul/14 11:10;apachespark;User 'twinkle-sachdeva' has created a pull request for this issue:
https://github.com/apache/spark/pull/1571;;;","24/Jul/14 11:11;twinkle;Please review  the pull request : https://github.com/apache/spark/pull/1571;;;","11/Dec/14 21:58;srowen;PR comments suggest this was fixed by SPARK-2140? https://github.com/apache/spark/pull/1571;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
py4j.Py4JException on sc.pickleFile,SPARK-2601,12728437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,kmatzen,kmatzen,20/Jul/14 17:33,27/Jul/14 00:37,14/Jul/23 06:26,27/Jul/14 00:37,1.1.0,,,,,,,1.1.0,,,,,,PySpark,,,,0,,,,,,"{code:title=test.py}
from pyspark import SparkContext

text_filename = 'README.md'
pickled_filename = 'pickled_file'

sc = SparkContext('local', 'Test Pipeline')

text_file = sc.textFile(text_filename)
text_file.saveAsPickleFile(pickled_filename)

pickled_file = sc.pickleFile(pickled_filename)
print pickled_file.first()
{code}

{code:title=bin/spark-submit test.py}
14/07/20 13:29:21 INFO SecurityManager: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
14/07/20 13:29:21 INFO SecurityManager: Changing view acls to: kmatzen
14/07/20 13:29:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kmatzen)
14/07/20 13:29:22 INFO Slf4jLogger: Slf4jLogger started
14/07/20 13:29:22 INFO Remoting: Starting remoting
14/07/20 13:29:22 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@en-cs-lr455-gr23.cs.cornell.edu:56405]
14/07/20 13:29:22 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@en-cs-lr455-gr23.cs.cornell.edu:56405]
14/07/20 13:29:22 INFO SparkEnv: Registering MapOutputTracker
14/07/20 13:29:22 INFO SparkEnv: Registering BlockManagerMaster
14/07/20 13:29:22 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140720132922-85ff
14/07/20 13:29:22 INFO ConnectionManager: Bound socket to port 35372 with id = ConnectionManagerId(en-cs-lr455-gr23.cs.cornell.edu,35372)
14/07/20 13:29:22 INFO MemoryStore: MemoryStore started with capacity 294.9 MB
14/07/20 13:29:22 INFO BlockManagerMaster: Trying to register BlockManager
14/07/20 13:29:22 INFO BlockManagerMasterActor: Registering block manager en-cs-lr455-gr23.cs.cornell.edu:35372 with 294.9 MB RAM
14/07/20 13:29:22 INFO BlockManagerMaster: Registered BlockManager
14/07/20 13:29:22 INFO HttpFileServer: HTTP File server directory is /tmp/spark-4261985b-cefa-410b-913f-8e5eed6d4bb9
14/07/20 13:29:22 INFO HttpServer: Starting HTTP Server
14/07/20 13:29:27 INFO SparkUI: Started SparkUI at http://en-cs-lr455-gr23.cs.cornell.edu:4040
14/07/20 13:29:28 INFO Utils: Copying /home/kmatzen/spark/test.py to /tmp/spark-1f6dc228-f540-4824-b128-2d9b98a7ddcc/test.py
14/07/20 13:29:28 INFO SparkContext: Added file file:/home/kmatzen/spark/test.py at http://128.84.103.23:45332/files/test.py with timestamp 1405877368118
14/07/20 13:29:28 INFO MemoryStore: ensureFreeSpace(34134) called with curMem=0, maxMem=309225062
14/07/20 13:29:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 33.3 KB, free 294.9 MB)
14/07/20 13:29:28 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)
14/07/20 13:29:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/07/20 13:29:28 WARN LoadSnappy: Snappy native library not loaded
14/07/20 13:29:28 INFO FileInputFormat: Total input paths to process : 1
14/07/20 13:29:28 INFO SparkContext: Starting job: saveAsObjectFile at NativeMethodAccessorImpl.java:-2
14/07/20 13:29:28 INFO DAGScheduler: Got job 0 (saveAsObjectFile at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=false)
14/07/20 13:29:28 INFO DAGScheduler: Final stage: Stage 0(saveAsObjectFile at NativeMethodAccessorImpl.java:-2)
14/07/20 13:29:28 INFO DAGScheduler: Parents of final stage: List()
14/07/20 13:29:28 INFO DAGScheduler: Missing parents: List()
14/07/20 13:29:28 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[4] at saveAsObjectFile at NativeMethodAccessorImpl.java:-2), which has no missing parents
14/07/20 13:29:28 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at saveAsObjectFile at NativeMethodAccessorImpl.java:-2)
14/07/20 13:29:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
14/07/20 13:29:28 INFO TaskSetManager: Re-computing pending task lists.
14/07/20 13:29:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 5627 bytes)
14/07/20 13:29:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
14/07/20 13:29:28 INFO Executor: Fetching http://128.84.103.23:45332/files/test.py with timestamp 1405877368118
14/07/20 13:29:28 INFO Utils: Fetching http://128.84.103.23:45332/files/test.py to /tmp/fetchFileTemp7128156481344843652.tmp
14/07/20 13:29:28 INFO BlockManager: Found block broadcast_0 locally
14/07/20 13:29:28 INFO HadoopRDD: Input split: file:/home/kmatzen/spark/README.md:0+4506
14/07/20 13:29:28 INFO PythonRDD: Times: total = 218, boot = 196, init = 20, finish = 2
14/07/20 13:29:28 INFO FileOutputCommitter: Saved output of task 'attempt_201407201329_0000_m_000000_0' to file:/home/kmatzen/spark/pickled_file
14/07/20 13:29:28 INFO SparkHadoopWriter: attempt_201407201329_0000_m_000000_0: Committed
14/07/20 13:29:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1755 bytes result sent to driver
14/07/20 13:29:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 431 ms on localhost (1/1)
14/07/20 13:29:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
14/07/20 13:29:28 INFO DAGScheduler: Stage 0 (saveAsObjectFile at NativeMethodAccessorImpl.java:-2) finished in 0.454 s
14/07/20 13:29:28 INFO SparkContext: Job finished: saveAsObjectFile at NativeMethodAccessorImpl.java:-2, took 0.518879869 s
14/07/20 13:29:28 INFO MemoryStore: ensureFreeSpace(34158) called with curMem=34134, maxMem=309225062
14/07/20 13:29:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 33.4 KB, free 294.8 MB)
14/07/20 13:29:28 INFO FileInputFormat: Total input paths to process : 1
Traceback (most recent call last):
  File ""/home/kmatzen/spark/test.py"", line 12, in <module>
    print pickled_file.first()
  File ""/home/kmatzen/spark/python/pyspark/rdd.py"", line 984, in first
    return self.take(1)[0]
  File ""/home/kmatzen/spark/python/pyspark/rdd.py"", line 970, in take
    res = self.context.runJob(self, takeUpToNumLeft, p, True)
  File ""/home/kmatzen/spark/python/pyspark/context.py"", line 714, in runJob
    it = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions, allowLocal)
  File ""/home/kmatzen/spark/python/pyspark/rdd.py"", line 1600, in _jrdd
    class_tag)
  File ""/home/kmatzen/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"", line 669, in __call__
  File ""/home/kmatzen/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:
py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.FlatMappedRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)
	at py4j.Gateway.invoke(Gateway.java:213)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:744)
{code}

Looks like this is related:
https://issues.apache.org/jira/browse/SPARK-1034",,apachespark,kmatzen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1034,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406513,,,Sat Jul 26 22:40:41 UTC 2014,,,,,,,,,,"0|i1xyd3:",406533,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/14 22:40;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1605;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangePartitioner's binary search does not use the given Ordering,SPARK-2598,12728426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,rxin,20/Jul/14 07:20,20/Jul/14 18:06,14/Jul/23 06:26,20/Jul/14 18:06,1.0.0,1.0.1,,,,,,1.0.2,1.1.0,,,,,,,,,0,,,,,,"RangePartitioner uses binary search to find the right partition, but K is not always Comparable. ",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406502,,,Sun Jul 20 07:45:18 UTC 2014,,,,,,,,,,"0|i1xyan:",406522,,,,,,,,,,,,,,1.0.2,1.1.0,,,,,,,,,,"20/Jul/14 07:45;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Populate pull requests on JIRA automatically,SPARK-2596,12728412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,19/Jul/14 23:30,20/Jul/14 01:19,14/Jul/23 06:26,20/Jul/14 01:19,,,,,,,,1.1.0,,,,,,Project Infra,,,,0,,,,,,For a bunch of reasons we should automatically populate a JIRA with information about new pull requests when they arrive. I've written a small python script to do this that we can run from Jenkins every 5 or 10 minutes to keep things in Sync.,,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406488,,,Sun Jul 20 01:19:47 UTC 2014,,,,,,,,,,"0|i1xy7j:",406508,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"20/Jul/14 01:19;pwendell;Issue resolved by pull request 1496
[https://github.com/apache/spark/pull/1496];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add config property to disable incremental collection used in Thrift server,SPARK-2590,12728358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,19/Jul/14 06:01,12/Aug/14 03:08,14/Jul/23 06:26,12/Aug/14 03:08,,,,,,,,1.1.0,,,,,,SQL,,,,1,,,,,,"{{SparkSQLOperationManager}} uses {{RDD.toLocalIterator}} to collect the result set one partition at a time. This is useful to avoid OOM when the result is large, but introduces extra job scheduling costs as each partition is collected with a separate job. Users may want to disable this when the result set is expected to be small.

*UPDATE* Incremental collection hurts performance because tasks of the last stage of the RDD DAG generated from the SQL query plan are executed sequentially. Thus we decided to disable it by default.",,apachespark,gerweck,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2591,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406434,,,Fri Aug 08 08:56:34 UTC 2014,,,,,,,,,,"0|i1xxvj:",406454,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Aug/14 08:56;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/1853;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error message is incorrect in make-distribution.sh,SPARK-2587,12728349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mwagner,mwagner,mwagner,19/Jul/14 00:51,20/Jul/14 03:25,14/Jul/23 06:26,20/Jul/14 03:25,,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"SPARK-2526 removed some options in favor of using Maven profiles, but it now gives incorrect guidance for those that try to use the old --with-hive flag: ""--with-hive' is no longer supported, use Maven option -Pyarn""",,mwagner,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406425,,,Sun Jul 20 03:25:27 UTC 2014,,,,,,,,,,"0|i1xxtj:",406445,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/14 03:25;pwendell;Issue resolved by pull request 1489
[https://github.com/apache/spark/pull/1489];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionManager cannot distinguish whether error occurred or not,SPARK-2583,12728247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,sarutak,sarutak,18/Jul/14 17:42,09/Aug/14 07:26,14/Jul/23 06:26,07/Aug/14 00:29,,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"ConnectionManager#handleMessage sent empty messages to another peer if some error occurred or not in onReceiveCalback.

{code}
         val ackMessage = if (onReceiveCallback != null) {
            logDebug(""Calling back"")
            onReceiveCallback(bufferMessage, connectionManagerId)
          } else {
            logDebug(""Not calling back as callback is null"")
            None
          }

          if (ackMessage.isDefined) {
            if (!ackMessage.get.isInstanceOf[BufferMessage]) {
              logDebug(""Response to "" + bufferMessage + "" is not a buffer message, it is of type ""
                + ackMessage.get.getClass)
            } else if (!ackMessage.get.asInstanceOf[BufferMessage].hasAckId) {
              logDebug(""Response to "" + bufferMessage + "" does not have ack id set"")
              ackMessage.get.asInstanceOf[BufferMessage].ackId = bufferMessage.id
            }
          }

        // We have no way to tell peer whether error occurred or not
          sendMessage(connectionManagerId, ackMessage.getOrElse {
            Message.createBufferMessage(bufferMessage.id)
          })
        }
{code}",,apachespark,pwendell,sarutak,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2942,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406352,,,Thu Aug 07 00:29:53 UTC 2014,,,,,,,,,,"0|i1xxdj:",406373,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Jul/14 01:02;sarutak;PR: https://github.com/apache/spark/pull/1490;;;","21/Jul/14 06:02;pwendell;Hey [~sarutak] - I'm curious - what is the behavior you were seeing without this patch? In what cases were exceptions being thrown and what was the consequence of ignoring the error?;;;","21/Jul/14 17:14;sarutak;Hi [~pwendell], 
When I simulate disk fault on shuffle, I saw following behavior.

I simulated disk fault on shuffle.
I deleted bucket file on remote executor.
Then, Remote executor threw FileNotFoundException.
But fetching executor still wait.
Maybe waiting on future.onSuccess in BasicBlockFetcherIterator#sendRequest.

I think, BasicBlockFetcherIterator expects ack message rom ConnectionManager through BlockManagerWorker but ConnectionManager#handleMesasge doesn't reply back when FileNotFoundException (maybe also some other exception) is thrown.
It was because BlockManagerWorker#onBlockMessageReceive return None even if error has occurred.

Considering this case, I think following 3 improvement is needed.

1) BlockManagerWorker#onBlockMessageReceive should not return None when error has occurred. I think, None means no message but error is the message which aught to be reply back

2) ConnectionManager should not reply empty message when some error has occurred.

3) When onReceiveCallback throws Exception, ConnectionManager should replay back that error has occurred. In this case, onReceiveCallback doesn't throw exception but return None, but I think any other callback functions may throw exceptions.



;;;","24/Jul/14 17:42;sarutak;I have added some test cases to my PR for this issue.;;;","04/Aug/14 01:11;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1758;;;","07/Aug/14 00:29;pwendell;Issue resolved by pull request 1758
[https://github.com/apache/spark/pull/1758];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broken pipe collecting schemardd results,SPARK-2580,12728228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,farrellee,farrellee,18/Jul/14 16:12,29/Jul/14 12:40,14/Jul/23 06:26,29/Jul/14 07:32,1.0.0,,,,,,,1.0.3,1.1.0,,,,,PySpark,SQL,,,0,py4j,pyspark,,,,"{code}
from pyspark.sql import SQLContext
sqlCtx = SQLContext(sc)
# size of cluster impacts where this breaks (i.e. 2**15 vs 2**2)
data = sc.parallelize([{'name': 'index', 'value': 0}] * 2**20)
sdata = sqlCtx.inferSchema(data)
sdata.first()
{code}

result: note - result returned as well as error
{code}
>>> sdata.first()
14/07/18 12:10:25 INFO SparkContext: Starting job: runJob at PythonRDD.scala:290
14/07/18 12:10:25 INFO DAGScheduler: Got job 43 (runJob at PythonRDD.scala:290) with 1 output partitions (allowLocal=true)
14/07/18 12:10:25 INFO DAGScheduler: Final stage: Stage 52(runJob at PythonRDD.scala:290)
14/07/18 12:10:25 INFO DAGScheduler: Parents of final stage: List()
14/07/18 12:10:25 INFO DAGScheduler: Missing parents: List()
14/07/18 12:10:25 INFO DAGScheduler: Computing the requested partition locally
14/07/18 12:10:25 INFO PythonRDD: Times: total = 45, boot = 3, init = 40, finish = 2
14/07/18 12:10:25 INFO SparkContext: Job finished: runJob at PythonRDD.scala:290, took 0.048348426 s
{u'name': u'index', u'value': 0}
>>> PySpark worker failed with exception:
Traceback (most recent call last):
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/worker.py"", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/serializers.py"", line 191, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/serializers.py"", line 124, in dump_stream
    self._write_with_length(obj, stream)
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/serializers.py"", line 139, in _write_with_length
    stream.write(serialized)
IOError: [Errno 32] Broken pipe

Traceback (most recent call last):
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/daemon.py"", line 130, in launch_worker
    worker(listen_sock)
  File ""/home/matt/Documents/Repositories/spark/dist/python/pyspark/daemon.py"", line 119, in worker
    outfile.flush()
IOError: [Errno 32] Broken pipe
{code}",fedora 21 local and rhel 7 clustered (standalone),apachespark,farrellee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406333,,,Tue Jul 29 12:40:25 UTC 2014,,,,,,,,,,"0|i1xx9j:",406354,,,,,,,,,,,,,,1.0.2,,,,,,,,,,,"18/Jul/14 16:15;farrellee;fyi, this was discovered during spark summit 2014 using the wiki_parquet example.;;;","29/Jul/14 00:31;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/1625;;;","29/Jul/14 12:40;farrellee;+1 lgtm, validated;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OUTER JOINs cause ClassCastException,SPARK-2578,12728180,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,teots,teots,18/Jul/14 10:10,10/Aug/16 04:56,14/Jul/23 06:26,10/Aug/16 04:56,1.0.0,,,,,,,2.0.0,,,,,,SQL,,,,1,,,,,,"When I run a hql query that contains a right or a left outer join I always get this exception:

{code}
org.apache.spark.SparkDriverExecutionException: Execution error
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:849)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1231)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.ClassCastException: scala.collection.mutable.HashSet cannot be cast to scala.collection.mutable.BitSet
	at org.apache.spark.sql.execution.BroadcastNestedLoopJoin$$anonfun$7.apply(joins.scala:338)
	at org.apache.spark.rdd.RDD$$anonfun$19.apply(RDD.scala:811)
	at org.apache.spark.rdd.RDD$$anonfun$19.apply(RDD.scala:808)
	at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:845)
	... 10 more
{code} 

This can easily reproduced using the queries and data from this [tutorial|http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/]. Only the last query was modified to use a right outer join.

{code}
hiveContext.hql(""SELECT a.year, a.player_id, a.runs from batting a RIGHT OUTER JOIN (SELECT year, max(runs) runs FROM batting GROUP BY year ) b ON (a.year = b.year AND a.runs = b.runs)"").collect.foreach(println)
{code}

I compiled the 1.0.1 release myself with the following command: ./make-distribution.sh --hadoop=2.2.0 --with-yarn --with-hive --tgz",,dimazhiyanov,dongjoon,nadenf,neelesh77,smolav,teots,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2998,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406285,,,Wed Aug 10 04:56:41 UTC 2016,,,,,,,,,,"0|i1xwyv:",406306,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/16 04:56;dongjoon;Now, Spark 2.0 supports the scenario without exceptions.
{code}
scala> spark.version
res0: String = 2.0.0

scala> sql(""create table batting (player_id STRING, year INT, runs INT)"")
res1: org.apache.spark.sql.DataFrame = []

scala> sql(""SELECT a.year, a.player_id, a.runs from batting a RIGHT OUTER JOIN (SELECT year, max(runs) runs FROM batting GROUP BY year ) b  ON (a.year = b.year AND a.runs = b.runs)"").show
+----+---------+----+                                                           
|year|player_id|runs|
+----+---------+----+
+----+---------+----+
{code}
;;;","10/Aug/16 04:56;dongjoon;It's resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File upload to viewfs is broken due to mount point resolution,SPARK-2577,12728176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,jira.shegalov,jira.shegalov,18/Jul/14 10:01,13/Oct/14 16:09,14/Jul/23 06:26,23/Jul/14 02:05,,,,,,,,1.1.0,,,,,,YARN,,,,0,,,,,,"YARN client resolves paths of uploaded artifacts. When a viewfs path is resolved, the filesystem changes to the target file system. However, the original fs is passed to {{ClientDistributedCacheManager#addResource}}. 

{code}
14/07/18 01:30:31 INFO yarn.Client: Uploading file:/Users/gshegalov/workspace/spark-tw/assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop3.0.0-SNAPSHOT.jar to viewfs:/user/gshegalov/.sparkStaging/application_1405479201490_0049/spark-assembly-1.1.0-SNAPSHOT-hadoop3.0.0-SNAPSHOT.jar
Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: hdfs://ns1:8020/user/gshegalov/.sparkStaging/application_1405479201490_0049/spark-assembly-1.1.0-SNAPSHOT-hadoop3.0.0-SNAPSHOT.jar, expected: viewfs:/
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.getUriPath(ViewFileSystem.java:116)
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.getFileStatus(ViewFileSystem.java:345)
	at org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:72)
	at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:236)
	at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:229)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:229)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:37)
	at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:74)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:81)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:136)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:320)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:28)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}

There are two options:
# do not resolve path because symlinks are currently disabled in Hadoop
# pass the correct filesystem object",,jira.shegalov,qiaohaijun,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3927,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406281,,,Fri Jul 18 10:33:45 UTC 2014,,,,,,,,,,"0|i1xwxz:",406302,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"18/Jul/14 10:33;jira.shegalov;https://github.com/apache/spark/pull/1483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
slave node throws NoClassDefFoundError $line11.$read$ when executing a Spark QL query on HDFS CSV file,SPARK-2576,12728174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,svend,svend,18/Jul/14 09:54,07/Feb/20 17:23,14/Jul/23 06:26,01/Aug/14 05:59,1.0.1,,,,,,,1.0.2,1.1.0,,,,,Spark Core,SQL,,,0,,,,,,"Execution of SQL query against HDFS systematically throws a class not found exception on slave nodes when executing .

(this was originally reported on the user list: http://apache-spark-user-list.1001560.n3.nabble.com/spark1-0-1-spark-sql-error-java-lang-NoClassDefFoundError-Could-not-initialize-class-line11-read-tc10135.html)

Sample code (ran from spark-shell): 

{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

import sqlContext.createSchemaRDD

case class Car(timestamp: Long, objectid: String, isGreen: Boolean)

// I get the same error when pointing to the folder ""hdfs://vm28:8020/test/cardata""
val data = sc.textFile(""hdfs://vm28:8020/test/cardata/part-00000"")

val cars = data.map(_.split("","")).map ( ar => Car(ar(0).toLong, ar(1), ar(2).toBoolean))
cars.registerAsTable(""mcars"")
val allgreens = sqlContext.sql(""SELECT objectid from mcars where isGreen = true"")
allgreens.collect.take(10).foreach(println)
{code}

Stack trace on the slave nodes: 

{code}

I0716 13:01:16.215158 13631 exec.cpp:131] Version: 0.19.0
I0716 13:01:16.219285 13656 exec.cpp:205] Executor registered on slave 20140714-142853-485682442-5050-25487-2
14/07/16 13:01:16 INFO MesosExecutorBackend: Registered with Mesos as executor ID 20140714-142853-485682442-5050-25487-2
14/07/16 13:01:16 INFO SecurityManager: Changing view acls to: mesos,mnubohadoop
14/07/16 13:01:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mesos, mnubohadoop)
14/07/16 13:01:17 INFO Slf4jLogger: Slf4jLogger started
14/07/16 13:01:17 INFO Remoting: Starting remoting
14/07/16 13:01:17 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@vm23:38230]
14/07/16 13:01:17 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@vm23:38230]
14/07/16 13:01:17 INFO SparkEnv: Connecting to MapOutputTracker: akka.tcp://spark@vm28:41632/user/MapOutputTracker
14/07/16 13:01:17 INFO SparkEnv: Connecting to BlockManagerMaster: akka.tcp://spark@vm28:41632/user/BlockManagerMaster
14/07/16 13:01:17 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140716130117-8ea0
14/07/16 13:01:17 INFO MemoryStore: MemoryStore started with capacity 294.9 MB.
14/07/16 13:01:17 INFO ConnectionManager: Bound socket to port 44501 with id = ConnectionManagerId(vm23-hulk-priv.mtl.mnubo.com,44501)
14/07/16 13:01:17 INFO BlockManagerMaster: Trying to register BlockManager
14/07/16 13:01:17 INFO BlockManagerMaster: Registered BlockManager
14/07/16 13:01:17 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ccf6f36c-2541-4a25-8fe4-bb4ba00ee633
14/07/16 13:01:17 INFO HttpServer: Starting HTTP Server
14/07/16 13:01:18 INFO Executor: Using REPL class URI: http://vm28:33973
14/07/16 13:01:18 INFO Executor: Running task ID 2
14/07/16 13:01:18 INFO HttpBroadcast: Started reading broadcast variable 0
14/07/16 13:01:18 INFO MemoryStore: ensureFreeSpace(125590) called with curMem=0, maxMem=309225062
14/07/16 13:01:18 INFO MemoryStore: Block broadcast_0 stored as values to memory (estimated size 122.6 KB, free 294.8 MB)
14/07/16 13:01:18 INFO HttpBroadcast: Reading broadcast variable 0 took 0.294602722 s
14/07/16 13:01:19 INFO HadoopRDD: Input split: hdfs://vm28:8020/test/cardata/part-00000:23960450+23960451
I0716 13:01:19.905113 13657 exec.cpp:378] Executor asked to shutdown
14/07/16 13:01:20 ERROR Executor: Exception in task ID 2
java.lang.NoClassDefFoundError: $line11/$read$
    at $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
    at $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$1.next(Iterator.scala:853)
    at scala.collection.Iterator$$anon$1.head(Iterator.scala:840)
    at org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:181)
    at org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:176)
    at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
    at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassNotFoundException: $line11.$read$
    at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:65)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    ... 27 more
Caused by: java.lang.ClassNotFoundException: $line11.$read$
    at java.lang.ClassLoader.findClass(Unknown Source)
    at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
    at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:60)
    ... 29 more
{code}


Note that running a simple map+reduce job on the same hdfs files with the same installation works fine: 

{code}
# this works
val data = sc.textFile(""hdfs://vm28:8020/test/cardata/"")

val lineLengths = data.map(s => s.length)
val totalLength = lineLengths.reduce((a, b) => a + b)
{code}

The hdfs files contain just plain csv files: 

{code}
$ hdfs dfs -tail /test/cardata/part-00000
14/07/16 13:18:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
1396396560000,2ea211cc-ea01-435a-a190-98a6dd5ccd0a,false,Ivory,chrysler,New Caledonia,1970,0.0,0.0,0.0,0.0,38.24645296229051,99.41880649743675,26.619177092584696
1396396620000,2ea211cc-ea01-435a-a190-98a6dd5ccd0a,false,Ivory,chrysler,New Caledonia,1970,1.3637951832478066,0.5913309707002152,56.6895043678199,96.54451566032114,100.76632815433682,92.29189473832957,7.009760456230157
1396396680000,2ea211cc-ea01-435a-a190-98a6dd5ccd0a,false,Ivory,chrysler,New Caledonia,1970,-3.405565593143888,0.8104753585926928,41.677424397834905,36.57019235002255,8.974008103729105,92.94054149986701,11.673872282136195
1396396740000,2ea211cc-ea01-435a-a190-98a6dd5ccd0a,false,Ivory,chrysler,New Caledonia,1970,2.6548062807597854,0.6180832371072019,40.88058181777176,24.47455760837969,37.42027121601756,93.97373842452362,16.48937328407166
{code}

spark-env.sh look like this: 

{code}
export SPARK_LOCAL_IP=vm28
export MESOS_NATIVE_LIBRARY=/usr/local/etc/mesos-0.19.0/build/src/.libs/libmesos.so
export SPARK_EXECUTOR_URI=hdfs://vm28:8020/apps/spark/spark-1.0.1-2.3.0-mr1-cdh5.0.2-hive.tgz
{code}


","One Mesos 0.19 master without zookeeper and 4 mesos slaves. 

JDK 1.7.51 and Scala 2.10.4 on all nodes. 

HDFS from CDH5.0.3

Spark version: I tried both with the pre-built CDH5 spark package available from http://spark.apache.org/downloads.html and by packaging spark with sbt 0.13.2, JDK 1.7.51 and scala 2.10.4 as explained here http://mesosphere.io/learn/run-spark-on-mesos/

All nodes are running Debian 3.2.51-1 x86_64 GNU/Linux and have 
",apachespark,chutium,marmbrus,mxm,nchammas,prashant,svend,ueshin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2632,,,,,SPARK-1199,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406279,,,Tue Jul 29 09:36:10 UTC 2014,,,,,,,,,,"0|i1xwxj:",406300,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"18/Jul/14 09:55;svend;I see another person is reporting a similar issue on the mailing list with a similar stack (Spark 1.0.1 and CDH 5.0.3): 
http://apache-spark-user-list.1001560.n3.nabble.com/spark1-0-1-spark-sql-error-java-lang-NoClassDefFoundError-Could-not-initialize-class-line11-read-tc10135.html;;;","18/Jul/14 16:36;yhuai;Regarding
{code}
$line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
$line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
{code}
what is your input at console line 19?;;;","19/Jul/14 02:23;svend;actually I do not have a line 19 ???

I just re-ran these 8 lines of code, with no additional line breaks, on a freshly started session:

{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.createSchemaRDD
case class Car(timestamp: Long, objectid: String, isGreen: Boolean)
val data = sc.textFile(""hdfs://vm28:8020/test/cardata"")
val cars = data.map(_.split("","")).map ( ar => Car(ar(0).toLong, ar(1), ar(2).toBoolean))
cars.registerAsTable(""mcars"")
val allgreens = sqlContext.sql(""SELECT objectid from mcars where isGreen = true"")
allgreens.collect.take(10).foreach(println)
{code}


I first get this error: 

{code}
java.lang.ExceptionInInitializerError
    at $line10.$read$$iwC.<init>(<console>:6)
    at $line10.$read.<init>(<console>:26)
    at $line10.$read$.<init>(<console>:30)
    at $line10.$read$.<clinit>(<console>)
    at $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
    at $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$1.next(Iterator.scala:853)
    at scala.collection.Iterator$$anon$1.head(Iterator.scala:840)
    at org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:181)
    at org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:176)
    at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
    at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.NullPointerException
    at $line3.$read$$iwC$$iwC.<init>(<console>:8)
    at $line3.$read$$iwC.<init>(<console>:14)
    at $line3.$read.<init>(<console>:16)
    at $line3.$read$.<init>(<console>:20)
    at $line3.$read$.<clinit>(<console>)
    ... 31 more
{code}

(I do not know where do line 26, 30 or 12 come from)

Then right after this one: 

{code}
java.lang.NoClassDefFoundError: Could not initialize class $line10.$read$
    at $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
    at $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$1.next(Iterator.scala:853)
    at scala.collection.Iterator$$anon$1.head(Iterator.scala:840)
    at org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:181)
    at org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:176)
    at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
    at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
{code}

;;;","19/Jul/14 02:24;svend;



I just noted that running the same spark-shell code with the session launched like this: 

{code}
./bin/spark-shell
{code}

(without connecting to mesos) computes the result successfully. 

Could it be that the 

{code}
case class Car(timestamp: Long, objectid: String, isGreen: Boolean)
{code}

defined in the shell is not made available to the remote slaves on mesos?
;;;","21/Jul/14 08:38;marmbrus;This has been reported as [a regression since 1.0.0|http://people.apache.org/~marmbrus/docs/catalyst/#org.apache.spark.sql.catalyst.expressions.package].;;;","21/Jul/14 16:29;yhuai;I did a quick test. Seems ""import sqlContext.createSchemaRDD"" introduced the problem. 

I am describing details below....

The simple test data is
{code:title=csv.text|borderStyle=solid}
1111111111111,abcde1,true
2111111111111,abcde2,true
3111111111111,abcde3,true
4111111111111,abcde4,true
5111111111111,abcde5,true
6111111111111,abcde6,true
7111111111111,abcde7,true
8111111111111,abcde8,true
{code}

The test code is 
{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.createSchemaRDD // Seems this line introduced the exception.
case class Car(timestamp: Long, objectid: String, isGreen: Boolean)
val data = sc.textFile(""csv.text"")
val cars = data.map(_.split("","")).map ( ar => new Car(ar(0).toLong, ar(1), ar(2).toBoolean))
cars.collect
{code}
Except creating a sqlContext and importing sqlContext.createSchemaRDD, this snippet basically does not have any sql-specific operation.

I found when you comment out 
{code}
import sqlContext.createSchemaRDD
{code}
the program is fine....
However, if you include that line, you will see the exception
{code}
java.lang.NoClassDefFoundError: Could not initialize class $line10.$read$
{code}

Then, instead of importing createSchemaRDD, I tried to explicitly create a SchemaRDD from a RDD and the query works.
Here is the snippet. 
{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
case class Car(timestamp: Long, objectid: String, isGreen: Boolean)

val data = sc.textFile(""/Users/yhuai/Desktop/csv.text"")
val cars = data.map(_.split("","")).map ( ar => new Car(ar(0).toLong, ar(1), ar(2).toBoolean))
val schemaRDD1 = sqlContext.createSchemaRDD(cars)

schemaRDD1.registerAsTable(""mcars"")
val allgreens = sqlContext.sql(""SELECT objectid from mcars where isGreen = true"")
allgreens.collect.take(10).foreach(println)
{code}

I also tried on master. The behavior is the same except the different exception, which is 
{code}
14/07/21 09:19:27 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, yins-mbp): java.lang.ExceptionInInitializerError: 
        $line10.$read$$iwC.<init>(<console>:6)
        $line10.$read.<init>(<console>:26)
        $line10.$read$.<init>(<console>:30)
        $line10.$read$.<clinit>(<console>)
        $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
        $line12.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:19)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        scala.collection.AbstractIterator.to(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:762)
        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:762)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1096)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1096)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:112)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:189)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        java.lang.Thread.run(Thread.java:745)
{code};;;","21/Jul/14 17:11;yhuai;Forgot to mention. I used a local cluster and spark shell for my test (sbin/start-all.sh and then bin/spark-shell).;;;","21/Jul/14 17:32;svend;Great Yin!

I confirm this workaround is fixing the execution on mesos as well, I removed the import and created the schemaRDD as you did and the job now executes successfully.;;;","21/Jul/14 17:46;svend;Maybe this tuto should be updated until a fix is released, it's the main landing place for Spark QL beginners (like me): 

http://spark.apache.org/docs/1.0.1/sql-programming-guide.html
;;;","21/Jul/14 18:32;yhuai;[~svend] Let me see what I can do about the 1.0.1 doc. ;;;","21/Jul/14 18:35;yhuai;I tried a standalone application (org.apache.spark.examples.sql.RDDRelation) and it's fine. I feel the problem is caused by REPL. ;;;","21/Jul/14 20:46;yhuai;After SPARK-1199 has been reverted, my test works now (branch 1.0).;;;","22/Jul/14 05:34;yhuai;I will try to add a Spark REPL test. ;;;","23/Jul/14 04:33;yhuai;OK. I have created a [REPL test|https://github.com/yhuai/spark/commit/93ae2183f2f32d81ed488e23fd88b84057d385fc]. I tried to run it on the current master, current branch-1.0, and 1.0.1. None of them can pass the test. So, seems after we import a method, REPL always thinks the class that the method belongs to should be serialized and be shipped to workers even if the class is irrelevant.;;;","23/Jul/14 18:18;yhuai;[~prashant] I have also created a [REPL test|https://github.com/yhuai/spark/commit/d77c515f741c65d807cdc147fb32d92e2039bc97] for importing SQLContext.createSchemaRDD. SQLContext is a serializable class. Seems the issue is slightly different with SPARK-2632. The exception is caused by the following code in SparkImports.scala
{code}
case x: ClassHandler =>
        // I am trying to guess if the import is a defined class
        // This is an ugly hack, I am not 100% sure of the consequences.
        // Here we, let everything but ""defined classes"" use the import with val.
        // The reason for this is, otherwise the remote executor tries to pull the
        // classes involved and may fail.
          for (imv <- x.definedNames) {
            val objName = req.lineRep.readPath
            code.append(""import "" + objName + "".INSTANCE"" + req.accessPath + "".`"" + imv + ""`\n"")
          }
{code}
Can you take a look at it as well? Thank you:);;;","24/Jul/14 11:42;prashant;Looking at it.;;;","28/Jul/14 10:59;chutium;i get same problem, 1.0.1, standalone cluster;;;","29/Jul/14 09:36;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/1635;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"This file ""make-distribution.sh"" has an error, please fix it",SPARK-2573,12728152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,wangshaoxin26,wangshaoxin26,18/Jul/14 08:14,03/Aug/14 05:32,14/Jul/23 06:26,03/Aug/14 05:32,1.0.0,,,,,,,,,,,,,Build,,,,0,build,,,,,"line 61: 
echo ""Error: '--with-hive' is no longer supported, use Maven option -Pyarn"".

It should be use -Phive.Otherwise, i like the oldest more,why update it",,wangshaoxin26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406257,,,2014-07-18 08:14:39.0,,,,,,,,,,"0|i1xwsv:",406278,,,,,,,,,,,,,,1.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle read bytes are reported incorrectly for stages with multiple shuffle dependencies,SPARK-2571,12728108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,18/Jul/14 02:30,18/Jul/14 22:07,14/Jul/23 06:26,18/Jul/14 21:43,0.9.3,1.0.1,,,,,,1.1.0,,,,,,Web UI,,,,0,,,,,,"In BlockStoreShuffleFetcher, we set the shuffle metrics for a task to include information about data fetched from one BlockFetcherIterator.  When tasks have multiple shuffle dependencies (e.g., a stage that joins two datasets together), the metrics will get set based on data fetched from the last BlockFetcherIterator to complete, rather than the sum of all data fetched from all BlockFetcherIterators.  This can lead to dramatically underreporting the shuffle read bytes.

Thanks [~andrewor14] and [~rxin] for helping to diagnose this issue.",,glenn.strycker@gmail.com,kayousterhout,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406213,,,Fri Jul 18 21:43:05 UTC 2014,,,,,,,,,,"0|i1xwj3:",406234,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"18/Jul/14 21:43;kayousterhout;https://github.com/apache/spark/commit/7b971b91caeebda57f1506ffc4fd266a1b379290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException from HiveFromSpark(examples),SPARK-2570,12728105,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chenghao,chenghao,chenghao,18/Jul/14 02:10,18/Jul/14 06:25,14/Jul/23 06:26,18/Jul/14 06:25,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"The Exception is thrown when run the example of HiveFromSpark
Exception in thread ""main"" java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(Row.scala:145)
	at org.apache.spark.examples.sql.hive.HiveFromSpark$.main(HiveFromSpark.scala:45)
	at org.apache.spark.examples.sql.hive.HiveFromSpark.main(HiveFromSpark.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406210,,,Fri Jul 18 02:11:23 UTC 2014,,,,,,,,,,"0|i1xwif:",406231,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/14 02:11;chenghao;https://github.com/apache/spark/pull/1475;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Customized UDFs in hive not running with Spark SQL,SPARK-2569,12728103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,jackyhung,jackyhung,18/Jul/14 02:01,23/Jul/14 23:27,14/Jul/23 06:26,23/Jul/14 23:27,1.0.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"start spark-shell,
init (like create hiveContext, import ._ ect, make sure the jar including the UDFs is in classpath)

hql(""CREATE TEMPORARY FUNCTION t_ts AS 'udf.Timestamp'""), which is successful. 

then i tried hql(""select t_ts(time) from data_common where xxxx limit 1"").collect().foreach(println), which failed with NullPointException 

we had discussion about it in the mail list.
http://apache-spark-user-list.1001560.n3.nabble.com/run-sparksql-hiveudf-error-throw-NPE-td8888.html#a9006

java.lang.NullPointerException org.apache.spark.sql.hive.HiveFunctionFactory$class.getFunctionClass(hiveUdfs.scala:117) org.apache.spark.sql.hive.HiveUdf.getFunctionClass(hiveUdfs.scala:157) org.apache.spark.sql.hive.HiveFunctionFactory$class.createFunction(hiveUdfs.scala:119) org.apache.spark.sql.hive.HiveUdf.createFunction(hiveUdfs.scala:157) org.apache.spark.sql.hive.HiveUdf.function$lzycompute(hiveUdfs.scala:170) org.apache.spark.sql.hive.HiveUdf.function(hiveUdfs.scala:170) org.apache.spark.sql.hive.HiveSimpleUdf.method$lzycompute(hiveUdfs.scala:181) org.apache.spark.sql.hive.HiveSimpleUdf.method(hiveUdfs.scala:180) org.apache.spark.sql.hive.HiveSimpleUdf.wrappers$lzycompute(hiveUdfs.scala:186) org.apache.spark.sql.hive.HiveSimpleUdf.wrappers(hiveUdfs.scala:186) org.apache.spark.sql.hive.HiveSimpleUdf.eval(hiveUdfs.scala:220) org.apache.spark.sql.catalyst.expressions.MutableProjection.apply(Projection.scala:64) org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:160) org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153) org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:580) org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:580) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:261) org.apache.spark.rdd.RDD.iterator(RDD.scala:228) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)


","linux or mac, hive 0.9.0 and hive 0.13.0 with hadoop 1.0.4, scala 2.10.3, spark 1.0.0",apachespark,jackyhung,poseidon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406208,,,Wed Jul 23 20:10:51 UTC 2014,,,,,,,,,,"0|i1xwhz:",406229,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"23/Jul/14 20:10;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1552;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resubmitted stage sometimes remains as active stage in the web UI,SPARK-2567,12728099,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,tsudukim,tsudukim,18/Jul/14 01:32,25/Jul/14 22:18,14/Jul/23 06:26,25/Jul/14 22:17,,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"When a stage is resubmitted because of executor lost for example, sometimes more than one resubmitted task appears in the web UI and one stage remains as active even after the job has finished.",,apachespark,codingcat,matei,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1726,,,,,,,,,,,,,,"18/Jul/14 01:34;tsudukim;SPARK-2567.png;https://issues.apache.org/jira/secure/attachment/12656411/SPARK-2567.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406204,,,Fri Jul 25 22:18:24 UTC 2014,,,,,,,,,,"0|i1xwh3:",406225,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/14 00:55;tsudukim;I found this lines in log file.
{noformat}
14/07/17 18:07:36 DEBUG DAGScheduler: Stage Stage 1 is actually done; true 3 3
{noformat}
so I think this is because extra stage which has no tasks to be executed is submitted, and then SparkListenerStageSubmitted is called but SparkListenerStageCompleted is not called.;;;","21/Jul/14 21:04;tsudukim;in submitMissingTasks of DAGScheduler.scala:
{code:title=DAGScheduler.scala}
    ...
    listenerBus.post(SparkListenerStageSubmitted(stageToInfos(stage), properties))

    if (tasks.size > 0) {
      ...
      try {
        SparkEnv.get.closureSerializer.newInstance().serialize(tasks.head)
      } catch {
        case e: NotSerializableException =>
          abortStage(stage, ""Task not serializable: "" + e.toString)
          runningStages -= stage
          return
        case NonFatal(e) => // Other exceptions, such as IllegalArgumentException from Kryo.
          abortStage(stage, s""Task serialization failed: $e\n${e.getStackTraceString}"")
          runningStages -= stage
          return
      }

      logInfo(""Submitting "" + tasks.size + "" missing tasks from "" + stage + "" ("" + stage.rdd + "")"")
      myPending ++= tasks
      logDebug(""New pending tasks: "" + myPending)
      taskScheduler.submitTasks(
        new TaskSet(tasks.toArray, stage.id, stage.newAttemptId(), stage.jobId, properties))
      stageToInfos(stage).submissionTime = Some(clock.getTime())
    } else {
      logDebug(""Stage "" + stage + "" is actually done; %b %d %d"".format(
        stage.isAvailable, stage.numAvailableOutputs, stage.numPartitions))
      runningStages -= stage
    }
{code}
SparkListenerStageSubmitted is posted before the check if
* stage has tasks to be run
* tasks are serializable

If the stage doesn't pass this check, this TaskSet is not submitted.
As a result, the corresponding SparkListenerStageCompleted will never be posted.

So I think SparkListenerStageSubmitted should be posted after the check.
;;;","21/Jul/14 21:16;tsudukim;PRed: https://github.com/apache/spark/pull/1516;;;","21/Jul/14 21:20;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/1516;;;","23/Jul/14 19:04;tsudukim;I noticed now but the cause of [SPARK-1726] seems to be the same.;;;","24/Jul/14 05:06;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/1566;;;","25/Jul/14 22:18;matei;I've merged this into 1.1 because the patch didn't apply in branch-1.0; we can revisit that later though if this is too annoying on 1.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repartitioning a SchemaRDD breaks resolution,SPARK-2561,12728053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,17/Jul/14 21:33,23/Jul/14 18:14,14/Jul/23 06:26,23/Jul/14 18:14,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,406158,,,2014-07-17 21:33:32.0,,,,,,,,,,"0|i1xw73:",406178,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stabilize the computation of logistic function in pyspark,SPARK-2552,12727878,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,17/Jul/14 08:14,21/Jul/14 01:41,14/Jul/23 06:26,21/Jul/14 01:41,,,,,,,,1.1.0,,,,,,MLlib,PySpark,,,0,Starter,,,,,"exp(1000) throws an error in python. For logistic function, we can use either 1 / ( 1 + exp( -x ) ) or 1 - 1 / (1 + exp( x ) ) to compute its value which ensuring exp always takes a negative value.",,mengxr,miccagiann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405983,,,Mon Jul 21 01:41:06 UTC 2014,,,,,,,,,,"0|i1xv4f:",406003,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"19/Jul/14 02:11;miccagiann;Hi Xiangrui,

From what I have seen so far, this error affects the prediction made using the
_predict_ method of _LogisticRegressionModel_ defined in 
_spark/python/pyspark/mllib/classification.py_ file. Is there any other occurence of
this issue in another file as well???

I can see two solutions in order to solve this issue:
a) Either check if the dot product between coeffs and data attributes gives a value in the desired range [ -745, 709 ] and if not to just set it to the one of these two values.

b) To create specific math functions in Java such as Logistic Function, SoftMax,
etc.. and call them via py4j in python2.7 and store the result in a 'decimal.Decimal' variable.

Thanks,
Michael;;;","19/Jul/14 07:00;mengxr;It is not necessary to check the ranges because exp never underflows on a negative number. So the function is just

{code}
def logistic(x):
  if x > 0:
    return 1 / (1 + math.exp(-x))
  else
    return 1 - 1 / (1 + math.exp(x))
{code}
;;;","19/Jul/14 08:53;mengxr;PR: https://github.com/apache/spark/pull/1493;;;","20/Jul/14 16:19;miccagiann;Xiangrui Meng,

Sorry about posting in this topic:
Would you find useful to create the following spark packages:
a) One that will include all the math functions commonly used for ML tasks.
b) One that will include distributions such as Uniform, Gaussian etc...

Let me know what you are thinking.

Thanks,
Michael;;;","21/Jul/14 01:41;mengxr;Issue resolved by pull request 1493
[https://github.com/apache/spark/pull/1493];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaRecoverableWordCount is missing,SPARK-2548,12727858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,mengxr,mengxr,17/Jul/14 06:31,10/Nov/14 22:17,14/Jul/23 06:26,10/Nov/14 19:49,0.9.2,1.0.1,,,,,,1.1.1,1.2.0,,,,,Documentation,DStreams,,,0,starter,,,,,JavaRecoverableWordCount was mentioned in the doc but not in the codebase. We need to rewrite the example because the code was lost during the migration from spark/spark-incubating to apache/spark.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405963,,,Mon Nov 10 21:43:59 UTC 2014,,,,,,,,,,"0|i1xuzz:",405983,,,,,,,,,,,,,,0.9.3,1.0.3,1.1.0,,,,,,,,,"28/Sep/14 16:20;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2564;;;","10/Nov/14 21:43;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/3188;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration object thread safety issue,SPARK-2546,12727850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,aash,aash,17/Jul/14 05:20,15/Sep/15 07:21,14/Jul/23 06:26,19/Oct/14 07:40,0.9.1,1.0.2,1.1.0,1.2.0,,,,1.0.3,1.1.1,1.2.0,,,,Spark Core,,,,0,,,,,,"// observed in 0.9.1 but expected to exist in 1.0.1 as well

This ticket is copy-pasted from a thread on the dev@ list:

{quote}
We discovered a very interesting bug in Spark at work last week in Spark 0.9.1 — that the way Spark uses the Hadoop Configuration object is prone to thread safety issues.  I believe it still applies in Spark 1.0.1 as well.  Let me explain:

Observations
 - Was running a relatively simple job (read from Avro files, do a map, do another map, write back to Avro files)
 - 412 of 413 tasks completed, but the last task was hung in RUNNING state
 - The 412 successful tasks completed in median time 3.4s
 - The last hung task didn't finish even in 20 hours
 - The executor with the hung task was responsible for 100% of one core of CPU usage
 - Jstack of the executor attached (relevant thread pasted below)

Diagnosis

After doing some code spelunking, we determined the issue was concurrent use of a Configuration object for each task on an executor.  In Hadoop each task runs in its own JVM, but in Spark multiple tasks can run in the same JVM, so the single-threaded access assumptions of the Configuration object no longer hold in Spark.

The specific issue is that the AvroRecordReader actually _modifies_ the JobConf it's given when it's instantiated!  It adds a key for the RPC protocol engine in the process of connecting to the Hadoop FileSystem.  When many tasks start at the same time (like at the start of a job), many tasks are adding this configuration item to the one Configuration object at once.  Internally Configuration uses a java.lang.HashMap, which isn't threadsafe… The below post is an excellent explanation of what happens in the situation where multiple threads insert into a HashMap at the same time.

http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html

The gist is that you have a thread following a cycle of linked list nodes indefinitely.  This exactly matches our observations of the 100% CPU core and also the final location in the stack trace.

So it seems the way Spark shares a Configuration object between task threads in an executor is incorrect.  We need some way to prevent concurrent access to a single Configuration object.


Proposed fix

We can clone the JobConf object in HadoopRDD.getJobConf() so each task gets its own JobConf object (and thus Configuration object).  The optimization of broadcasting the Configuration object across the cluster can remain, but on the other side I think it needs to be cloned for each task to allow for concurrent access.  I'm not sure the performance implications, but the comments suggest that the Configuration object is ~10KB so I would expect a clone on the object to be relatively speedy.

Has this been observed before?  Does my suggested fix make sense?  I'd be happy to file a Jira ticket and continue discussion there for the right way to fix.


Thanks!
Andrew


P.S.  For others seeing this issue, our temporary workaround is to enable spark.speculation, which retries failed (or hung) tasks on other machines.


{noformat}
""Executor task launch worker-6"" daemon prio=10 tid=0x00007f91f01fe000 nid=0x54b1 runnable [0x00007f92d74f1000]
   java.lang.Thread.State: RUNNABLE
    at java.util.HashMap.transfer(HashMap.java:601)
    at java.util.HashMap.resize(HashMap.java:581)
    at java.util.HashMap.addEntry(HashMap.java:879)
    at java.util.HashMap.put(HashMap.java:505)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
    at org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
    at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
    at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
    at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
    at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
    at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
    at org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
    at org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
    at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
    at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
    at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
{noformat}
{quote}",,aash,ankurmitujjain,apachespark,ash211,capricornius,gq,joshrosen,ozawa,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2585,,SPARK-10611,SPARK-1097,HADOOP-11209,,,,,,SPARK-2521,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405955,,,Fri Jul 17 15:24:58 UTC 2015,,,,,,,,,,"0|i1xuy7:",405975,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"17/Jul/14 06:19;aash;On the thread:

Me:

{quote}
Reynold's recent announcement of the broadcast RDD object patch may also have implications of the right path forward here.  I'm not sure I fully understand the implications though: https://github.com/apache/spark/pull/1452

""Once this is committed, we can also remove the JobConf broadcast in HadoopRDD.""
{quote}


[~pwendell]:

{quote}
I think you are correct and a follow up to SPARK-2521 will end up
fixing this. The desing of SPARK-2521 automatically broadcasts RDD
data in tasks and the approach creates a new copy of the RDD and
associated data for each task. A natural follow-up to that patch is to
stop handling the jobConf separately (since we will now broadcast all
referents of the RDD itself) and just have it broadcasted with the
RDD. I'm not sure if Reynold plans to include this in SPARK-2521 or
afterwards, but it's likely we'd do that soon.
{quote};;;","31/Jul/14 05:48;pwendell;Ideally we should merge either this or SPARK-2585 in the 1.1 release.;;;","05/Aug/14 19:57;joshrosen;Hi Andrew,

Do you have any way to reliably reproduce this issue?  I'm considering implementing a clone()-based approach and I'd like to have a way to test whether I've fixed this bug.;;;","05/Aug/14 21:37;ash211;I don't have a reliable repro that's in a unit test format.  On my prod
cluster though it reproduces quite reliably!  I'd suggest using the
AvroInputFormat on sizable files and a large number of partitions -- I had
O(400) partitions and O(15GB) of data in that dataset.

Sidenote -- the trouble with unit testing race conditions is that you have
to run them for a long time in an error-prone situation and hope that the
behavior is triggered.  You could verify that the Configuration objects
each partition gets are equal() but not reference equal, but that's not
directly testing for the race condition.



;;;","15/Aug/14 22:02;pwendell;Hey Andrew I think due to us cutting SPARK-2585 from this release it will remain broken in Spark 1.1. We could look into a solution based on clone()'ing the conf for future patch releases in the 1.1 branch.;;;","15/Aug/14 22:11;ash211;Ok I'll stay on the lookout for this bug and ping here again if we observe
this.  Luckily we haven't seen this particular issue since, but that's
mostly been because other things are causing problems.

We have a few bugs now that are nondeterministically broken in Spark and
cause jobs to fail/hang, but if we retry the job several times (and
spark.speculation helps somewhat) we can usually eventually get a job to
complete.  I can share that list if you're interested of what's highest on
our minds right now.


On Fri, Aug 15, 2014 at 6:03 PM, Patrick Wendell (JIRA) <jira@apache.org>

;;;","25/Sep/14 20:18;aash;Another proposed fix: extend JobConf as a shim and replace the Hadoop one with one that's threadsafe;;;","26/Sep/14 02:36;joshrosen;JobConf has a _ton_ of methods and it's not clear whether we can get away with synchronizing only some of them.

I'm going to look into using Scala macro annotations (http://docs.scala-lang.org/overviews/macros/annotations.html) to create a {{@synchronizeAll}} macro for adding synchronization to all methods of a class.;;;","26/Sep/14 02:56;joshrosen;A synchronization wrapper (whether written by hand or generated using macros) might introduce an unwanted runtime dependency on the exact compile-time version of Hadoop that we used.  For example, say we compile against Hadoop 1.x and run on Hadoop 1.y (where y > x) and the runtime version of JobConf contains methods that were not present in the version that we wrapped at compile-time.  What happens in this case?

Before we explore this option, I should probably re-visit SPARK-2585 to see if I can understand why the patch seemed to introduce a performance regression, since that approach is Hadoop version agnostic.;;;","06/Oct/14 19:28;joshrosen;I've decided to go with the cloning approach, since this seems simplest and safest.

It looks like SparkContext has a public {{hadoopConfiguration}} {{val}} that holds a re-used Configuration object.  It looks like this may have been purposely exposed to allow users to set Hadoop configuration properties (see how it's mentioned in docs/storage-openstack-swift.md; the Spark EC2 instructions also mention using this attribute to set S3 credentials).  This object is used as the default Hadoop configuration in the {{newAPIHadoopRDD}} and {{saveAsHadoop*}} methods; it's also read in many other places inside of Spark.

While SPARK-2585 addressed sharing of the Configuration objects in executors, it seems that we still might face races in the driver if multiple threads are sharing a SparkContext and one thread mutates the shared configuration while another thread submits a job that reads it.

This seems like a tricky problem to fix.  I don't think that we can change {{SparkContext.hadoopConfiguration}} to return a copy of the configuration object, since it seems that the shared / mutating semantics are required by some existing code.  At the same time, we can't simply clone the return value before using it in our internal driver-side code since a) we can't lock out writers/mutators while performing the clone() and b) the change in semantics might break existing user-code.  Essentially, I don't think that there's anything that we can do that's guaranteed to be safe once a Configuration has been exposed to multiple threads; we need to perform the cloning before the object has been shared.;;;","06/Oct/14 20:00;joshrosen;Here are a few ""in the wild"" examples of how {{sc.hadoopConfiguration}} is currently used, to give a sense of the impact of any changes that we might make here.

[Setting elasticserach configuration properties|https://github.com/barnybug/spark-elasticsearch-blogpost/blob/master/Main.scala#L23]:

{code}
sc.hadoopConfiguration.set(""es.resource"", ""syslog/entry"")
output.saveAsHadoopFile[ESOutputFormat](""-"")
{code}

[Setting S3 credentials|http://stackoverflow.com/a/26156429/590203]:

{code}
val conf = new SparkConf().setAppName(""Simple Application"").setMaster(""local"")      
val sc = new SparkContext(conf)
val hadoopConf=sc.hadoopConfiguration;
hadoopConf.set(""fs.s3.impl"", ""org.apache.hadoop.fs.s3native.NativeS3FileSystem"")
hadoopConf.set(""fs.s3.awsAccessKeyId"",myAccessKey)
hadoopConf.set(""fs.s3.awsSecretAccessKey"",mySecretKey)
{code}

There's a lot more examples here: https://github.com/search?utf8=%E2%9C%93&q=%22sc.hadoopconfiguration%22&type=Code&ref=searchresults

The most common use-case seems to be setting S3 credentials.  One option would be to slowly deprecate the existing {{hadoopConfiguration}} field in favor of methods for setting S3 credentials.  Currently, you can set these options in SparkConf before creating the SparkContext; unfortunately, this isn't an option for users that want to set configurations after starting SparkContext (e.g. IPython notebook users).  I suppose that these users could work with a clone of the configuration object and manually pass that object into Spark methods.

If we did add a SparkContext-wide setting for changing Hadoop configurations, then in multi-user shared-SparkContext environments we run the risk of users overwriting each others' S3 credentials.;;;","06/Oct/14 20:33;joshrosen;For now, let's ignore the design issue of whether the current API is confusing in multi-user shared-SparkContext environments.  If we want to keep the current API without any driver-side thread-safety issues, is there anything that we can do?

Maybe we can add a very limited amount of synchronization to Configuration.  [Looking at a recent version of Configuration.java|https://github.com/apache/hadoop/blob/d989ac04449dc33da5e2c32a7f24d59cc92de536/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L666], it seems that the private {{updatingResource}} HashMap and {{finalParameters}} HashSet fields the only non-thread-safe collections in Configuration (Java's {{Properties}} class is thread-safe).

My hunch is that the {{updatingResource}} HashMap was the map referred to by the stacktrace posted in this issue.  We might be able to use reflection to find this field and inject a synchronized HashMap instead.;;;","06/Oct/14 21:41;aash;Excellent research Josh!

I agree that we should pass for now on the driver-side thread-safety issues.  All the issues I've encountered so far have been in multiple accesses on the executor side, which the cloning on access approach seems to take care of.;;;","06/Oct/14 23:35;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2684;;;","17/Oct/14 21:59;aash;We tested Josh's patch, confirming the fix and measuring the perf regression at ~8%;;;","19/Oct/14 07:40;joshrosen;Issue resolved by pull request 2684
[https://github.com/apache/spark/pull/2684];;;","19/Oct/14 07:45;joshrosen;I've fixed this in HadoopRDD and applied my fix to all branches.  Note that the fix is currently guarded by a configuration option, {{spark.hadoop.cloneConf}}.  This is in order to avoid unexpected performance regressions when users who were unaffected by this issue choose to upgrade to 1.1.1 or 1.0.3.  We'll probably make cloning the default in 1.2.0 and may spend some more time trying to understand its performance implications.

Note that this does not address the potential for thread-safety issues due to Configuration-sharing on the driver.  As described upthread, this is a much harder issue to fix.  Since I'm not aware of any cases where this has caused issues on the driver, I'm inclined to wait things out and address that if it's discovered to be an issue.  

I've opened HADOOP-11209 to try to fix the Configuration thread-safety issues upstream, so hopefully this won't be a problem in the future.;;;","22/Jan/15 05:22;ozawa;Now HADOOP-11209, the problem reported by [~joshrosen], is resolved by [~varun_saxena]'s contribution. Thanks for your reporting.;;;","15/Jul/15 10:29;ankurmitujjain;This exists in SPARK 1.4 too...;;;","15/Jul/15 11:29;ozawa;[~anknai] cc: [~joshrosen] the problem is fixed in Hadoop 2.7. Could you build spark with hadoop.version=2.7.1? I'll also backport the patch to 2.6.x, but it takes a bit time to release.;;;","15/Jul/15 11:33;ankurmitujjain;Thanks Tsuyoshi...
I thought fix is already done for version 1.0.3, 1.1.1, 1.2.0. 
So 1.4.0 should have this fix with it....

Anyways this means that on EMR we will face this issue as they are using Hadoop 2.4.0;;;","17/Jul/15 15:24;joshrosen;[~ankurmitujjain], you can try setting {{spark.hadoop.cloneConf=true}} in your SparkConf in order to enable additional defensive copying that is designed to guard against this issue. This setting is off by default because this cloning is actually fairly expensive because new {{Configuration}} objects are costly to create.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Workaround Timezone specific Hive tests,SPARK-2537,12727805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,lian cheng,lian cheng,17/Jul/14 00:00,17/Sep/15 20:57,14/Jul/23 06:26,05/Sep/14 20:54,1.0.1,1.1.0,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"Several Hive tests in {{HiveCompatibilitySuite}} are timezone sensitive:

- {{timestamp_1}}
- {{timestamp_2}}
- {{timestamp_3}}
- {{timestamp_udf}}

Their answers differ between different timezones. Caching golden answers naively cause build failures in other timezones. Currently these tests are blacklisted. A not so clever solution is to cache golden answers of all timezones for these tests, then select the right version for the current build according to system timezone.",,lian cheng,mxm,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405910,,,Fri Sep 05 20:53:13 UTC 2014,,,,,,,,,,"0|i1xuo7:",405930,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"05/Sep/14 20:53;lian cheng;PR [#1440|https://github.com/apache/spark/pull/1440] fixes this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid pulling in the entire RDD or PairRDDFunctions in various operators,SPARK-2534,12727801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,rxin,rxin,16/Jul/14 23:33,05/Aug/14 19:18,14/Jul/23 06:26,17/Jul/14 23:37,,,,,,,,1.0.2,1.1.0,,,,,Spark Core,,,,0,,,,,,"The way groupByKey is written actually pulls the entire PairRDDFunctions into the 3 closures, sometimes resulting in gigantic task sizes:

{code}
  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = {
    // groupByKey shouldn't use map side combine because map side combine does not
    // reduce the amount of data shuffled and requires all map side data be inserted
    // into a hash table, leading to more objects in the old gen.
    def createCombiner(v: V) = ArrayBuffer(v)
    def mergeValue(buf: ArrayBuffer[V], v: V) = buf += v
    def mergeCombiners(c1: ArrayBuffer[V], c2: ArrayBuffer[V]) = c1 ++ c2
    val bufs = combineByKey[ArrayBuffer[V]](
      createCombiner _, mergeValue _, mergeCombiners _, partitioner, mapSideCombine=false)
    bufs.mapValues(_.toIterable)
  }
{code}

Changing the functions from def to val would solve it. ",,gq,rxin,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405906,,,Wed Jul 16 23:36:13 UTC 2014,,,,,,,,,,"0|i1xunb:",405926,,,,,,,,,,,,,,1.0.2,1.1.0,,,,,,,,,,"16/Jul/14 23:36;sandyr;Yowza;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Relax incorrect assumption of one ExternalAppendOnlyMap per thread,SPARK-2530,12727745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,16/Jul/14 19:11,05/Nov/14 10:43,14/Jul/23 06:26,06/Oct/14 22:42,1.0.1,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"Originally reported by Matei.

Our current implementation of EAOM assumes only one map is created per task. This is not true in the following case, however:

{code}
rdd1.join(rdd2).reduceByKey(...)
{code}

This is because reduce by key does a map side combine, which creates an EAOM that streams from an EAOM previously created by the same thread to aggregate values from the join.

The more concerning thing is the following: we currently maintain a global shuffle memory map (thread ID -> memory used by that thread to shuffle). If we create two EAOMs in the same thread, the memory occupied by the first map may be clobbered by that occupied by the second. This has very adverse consequences if the first map is huge but the second is just starting out, in which case we end up believing that we use much less memory than we actually do.",,andrewor,glenn.strycker@gmail.com,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2711,,,,,,,,,,,SPARK-2711,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405850,,,Mon Oct 06 22:42:44 UTC 2014,,,,,,,,,,"0|i1xubb:",405870,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 22:42;matei;This was fixed by SPARK-2711.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean the closure in foreach and foreachPartition,SPARK-2529,12727738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,16/Jul/14 18:31,25/Jul/14 21:13,14/Jul/23 06:26,25/Jul/14 08:10,,,,,,,,1.0.2,1.1.0,,,,,,,,,0,,,,,,Somehow we didn't clean the closure for foreach and foreachPartition. Should do that.,,apachespark,llai,markhamstra,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405843,,,Fri Jul 25 03:15:00 UTC 2014,,,,,,,,,,"0|i1xu9r:",405863,,,,,,,,,,,,,,1.0.2,1.1.0,,,,,,,,,,"25/Jul/14 02:16;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/1583;;;","25/Jul/14 03:15;markhamstra;Actually, we were cleaning those closures, but that was removed in https://github.com/apache/spark/commit/6b288b75d4c05f42ad3612813dc77ff824bb6203 -- not sure why.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify make-distribution.sh to just pass through Maven options,SPARK-2526,12727713,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,16/Jul/14 17:11,17/Jul/14 08:02,14/Jul/23 06:26,17/Jul/14 08:02,,,,,,,,1.1.0,,,,,,Build,,,,0,,,,,,"There is a some complexity make-distribution.sh around selecting profiles. This is both annoying to maintain and also limits the number of ways that packagers can use this. For instance, it's not possible to build with separate HDFS and YARN versions, and supporting this with our current flags would get pretty complicated. We should just allow the user to pass a list of profiles directly to make-distribution.sh - the Maven build itself is already parameterized to support this. We also now have good docs explaining the use of profiles in the Maven build.

All of this logic was more necessary when we used SBT for the package build, but we haven't done that for several versions.",,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405818,,,Thu Jul 17 08:02:54 UTC 2014,,,,,,,,,,"0|i1xu4f:",405839,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"17/Jul/14 08:02;pwendell;Issue resolved by pull request 1445
[https://github.com/apache/spark/pull/1445];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing document about spark.deploy.retainedDrivers,SPARK-2524,12727660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,16/Jul/14 14:28,20/Jul/14 03:48,14/Jul/23 06:26,20/Jul/14 03:48,,,,,,,,1.1.0,,,,,,Deploy,,,,0,,,,,,"The configuration on spark.deploy.retainedDrivers is undocumented but actually used
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L60",,lianhuiwang,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405765,,,Sun Jul 20 03:48:08 UTC 2014,,,,,,,,,,"0|i1xtsv:",405787,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/14 03:48;pwendell;Issue resolved by pull request 1443
[https://github.com/apache/spark/pull/1443];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For partitioned Hive tables, partition-specific ObjectInspectors should be used.",SPARK-2523,12727612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,16/Jul/14 08:37,30/Jul/14 20:10,14/Jul/23 06:26,28/Jul/14 18:29,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"In HiveTableScan.scala, ObjectInspector was created for all of the partition based records, which probably causes ClassCastException if the object inspector is not identical among table & partitions.",,apachespark,chenghao,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405718,,,Wed Jul 30 20:10:59 UTC 2014,,,,,,,,,,"0|i1xtif:",405740,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"16/Jul/14 08:39;chenghao;This is the follow up for 
https://github.com/apache/spark/pull/1408
https://github.com/apache/spark/pull/1390

The PR is https://github.com/apache/spark/pull/1439;;;","16/Jul/14 08:43;chenghao;[~yhuai] Can you review the code for me?;;;","16/Jul/14 17:23;yhuai;Yeah, no problem. Can you add a case which can trigger the bug in the description?;;;","17/Jul/14 04:41;chenghao;sbt/sbt hive/console
{code:title=prepare.scala|borderStyle=solid}
hql(""CREATE TABLE add_part_test (key STRING, value STRING) PARTITIONED BY (ds STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe' STORED AS RCFILE"").collect
hql(""from src insert into table add_part_test PARTITION (ds='2010-01-01') select 100,100 limit 1"").collect
hql(""ALTER TABLE add_part_test set SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe"").collect
hql(""from src insert into table add_part_test PARTITION (ds='2010-01-02') select 200,200 limit 1"").collect
hql(""select * from add_part_test"").collect.mkString(""\n"")
{code}
{panel:title=Output (Without this PR)}
14/07/17 12:12:02 WARN scheduler.TaskSetManager: Loss was due to java.lang.ClassCastException
java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyString
	at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(LazyStringObjectInspector.java:52)
	at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(LazyStringObjectInspector.java:28)
	at org.apache.spark.sql.hive.HiveInspectors$class.unwrapData(hiveUdfs.scala:287)
	at org.apache.spark.sql.hive.execution.HiveTableScan.unwrapData(HiveTableScan.scala:48)
	at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$attributeFunctions$1$$anonfun$apply$3.apply(HiveTableScan.scala:101)
	at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$attributeFunctions$1$$anonfun$apply$3.apply(HiveTableScan.scala:99)
	at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$12$$anonfun$apply$5.apply(HiveTableScan.scala:203)
	at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$12$$anonfun$apply$5.apply(HiveTableScan.scala:200)
{panel}
And 
{panel:title=Output (With this PR)}
[100,100,2010-01-01]
[200,200,2010-01-02]
{panel}

{code:title=analysis.sql|borderStyle=solid}
hql(""DESCRIBE EXTENDED add_part_test partition (ds='2010-01-01')"").collect.mkString(""\n"")
hql(""DESCRIBE EXTENDED add_part_test partition (ds='2010-01-02')"").collect.mkString(""\n"")
{code}
You probably will see the different output like:

{panel:title=Output}
Detailed Partition Information	Partition(values: 2010-01-01, ... serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe...)	
Detailed Partition Information	Partition(values: 2010-01-02, ... serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe...)	
{panel};;;","17/Jul/14 04:57;chenghao;I think the root cause is the when ALTER table with different SERDE, it will not affect the existed partitions, but table and all subsequent partitions will inherit from it, so we got different SerDEs for different Partitions when new partitions added afterward. 

The original implementation of HiveTableScan gets the ObjectInspectors from the TableDesc, which is not correct for the existed partitions in this case.

This PR solve that by utilizing the partition desc for SerDe instantiation, and convert it into Catalyst MutableRow directly while scanning the partition, I think it's more straightforward as we don't need the ObjectInspector for the downstream operators. (In Shark we do have to make a uniform ObjectInspector for downstream operators, that's why we have to serialize the row and then deserialize it again in [PR1390|https://github.com/apache/spark/pull/1390]);;;","17/Jul/14 15:37;yhuai;I see. Although we are using the right SerDe to deserialize a row, we are using the wrong ObjectInspector to extract fields (in attributeFunctions)... Also, creating Rows in TableReader makes sense. Will finish my review soon.;;;","30/Jul/14 20:10;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/1669;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix foldability of Substring expression.,SPARK-2518,12727585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,16/Jul/14 06:21,18/Jul/14 22:01,14/Jul/23 06:26,18/Jul/14 22:01,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"This is a follow-up of [#1428|https://github.com/apache/spark/pull/1428].",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405691,,,Wed Jul 16 06:26:50 UTC 2014,,,,,,,,,,"0|i1xtcn:",405713,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 06:26;ueshin;PRed: https://github.com/apache/spark/pull/1432;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compile error of streaming project with 2.0.0-cdh4.6.0,SPARK-2507,12727548,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,gzm55,gzm55,16/Jul/14 03:02,30/Jul/14 23:04,14/Jul/23 06:26,30/Jul/14 23:04,0.9.0,0.9.1,1.0.0,1.0.1,1.0.2,,,,,,,,,DStreams,,,,0,,,,,,"Hi,

When compiling with
{quote}
    ./make-distribution.sh --hadoop 2.0.0-cdh4.6.0 --with-yarn --tgz
{quote}

I have the following errors on streaming java api:

{quote}
Version is 0.9.0-incubating
Making spark-0.9.0-incubating-hadoop_2.0.0-cdh4.6.0-bin.tar.gz
Hadoop version set to 2.0.0-cdh4.6.0
YARN enabled
[info] Loading project definition from /root/spark-source/project/project
[info] Loading project definition from /root/spark-source/project
[info] Set current project to root (in build file:/root/spark-source/)
[info] Compiling 1 Scala source to /root/spark-source/streaming/target/scala-2.10/classes...
[error] /root/spark-source/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala:57: type mismatch;
[error]  found   : org.apache.spark.streaming.dstream.DStream[(K, V)]
[error]  required: org.apache.spark.streaming.api.java.JavaPairDStream[K,V]
[error]  Note: implicit method fromPairDStream is not applicable here because it comes after the application point and it lacks an explicit result type
[error]     dstream.filter((x => f(x).booleanValue()))
[error]                   ^
[error] /root/spark-source/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala:60: type mismatch;
[error]  found   : org.apache.spark.streaming.dstream.DStream[(K, V)]
[error]  required: org.apache.spark.streaming.api.java.JavaPairDStream[K,V]
[error]  Note: implicit method fromPairDStream is not applicable here because it comes after the application point and it lacks an explicit result type
[error]   def cache(): JavaPairDStream[K, V] = dstream.cache()
[error]                                                     ^
[error] /root/spark-source/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala:63: type mismatch;
[error]  found   : org.apache.spark.streaming.dstream.DStream[(K, V)]
[error]  required: org.apache.spark.streaming.api.java.JavaPairDStream[K,V]
[error]  Note: implicit method fromPairDStream is not applicable here because it comes after the application point and it lacks an explicit result type
[error]   def persist(): JavaPairDStream[K, V] = dstream.persist()
[error]                                                         ^

......

[error] /root/spark-source/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala:669: type mismatch;
[error]  found   : org.apache.spark.streaming.dstream.DStream[(K, (com.google.common.base.Optional[V], W))]
[error]  required: org.apache.spark.streaming.api.java.JavaPairDStream[K,(com.google.common.base.Optional[V], W)]
[error]  Note: implicit method fromPairDStream is not applicable here because it comes after the application point and it lacks an explicit result type
[error]     joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}
[error]                         ^
[error] 44 errors found
[error] (streaming/compile:compile) Compilation failed
{quote}

Here is a simple PR fix this problem: https://github.com/apache/spark/pull/153","RedHat 5.3
2.0.0-cdh4.6.0
enable yarn
java version ""1.6.0_45""",gzm55,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405654,,,Wed Jul 30 22:46:36 UTC 2014,,,,,,,,,,"0|i1xt4n:",405677,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"17/Jul/14 00:55;gzm55;Merged;;;","30/Jul/14 22:46;tdas;This was solved in PR https://github.com/apache/spark/pull/153;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullability of Substring expression.,SPARK-2504,12727541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,16/Jul/14 02:35,16/Jul/14 20:02,14/Jul/23 06:26,16/Jul/14 20:02,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"This is a follow-up of [#1359|https://github.com/apache/spark/pull/1359] with nullability narrowing.",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405647,,,Wed Jul 16 02:44:42 UTC 2014,,,,,,,,,,"0|i1xt3j:",405672,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 02:44;ueshin;PRed: https://github.com/apache/spark/pull/1426;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle stage re-submissions properly in the UI,SPARK-2501,12727532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,pwendell,pwendell,pwendell,16/Jul/14 00:48,27/Aug/14 18:45,14/Jul/23 06:26,27/Aug/14 18:45,,,,,,,,1.1.0,,,,,,Web UI,,,,0,,,,,,,,pwendell,shivaram,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2015,,SPARK-2298,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405638,,,Wed Aug 27 18:45:22 UTC 2014,,,,,,,,,,"0|i1xt1j:",405663,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"16/Jul/14 00:52;pwendell;Our handling of stage re-submissions is broken in the UI. For instance, I looked in the JobProgressListener and we index many things on StageId that might better be indexed on StageId, AttemptId. Also, we should probably give the AttemptId when start a task so that we understand which stage attempt to associated it with. I also don't understand exactly what happens when a stage gets re-attempted, do we send a ""stage completed"" event? It might be good to fix the way we deal with stage re-submissions to make this work better overall.;;;","16/Jul/14 23:45;tsudukim;[SPARK-2299] seems to include the problem in JobProgressListener that key of some hashmaps should be stageId + attemptId instead of stageId only.;;;","17/Jul/14 18:12;shivaram;I don't know if this issue will be covered by this, but one major problem right now is that if there was a failure and we resubmit tasks they seem to keep counting up and you see messages like 1010/1000 tasks completed.  I will get a screen shot next time this happens.;;;","17/Jul/14 18:49;tsudukim;Yes, this ticket covers it. I think that problem (like 1010/1000) is caused because we only use stageId (not stageId + attemptId) as the key in JobProgressListener.;;;","19/Jul/14 06:33;tsudukim;[SPARK-2298] is about the problem that the original stage info is overwritten by re-submitted stage in the web UI.;;;","27/Aug/14 18:45;pwendell;Fixed as a result of SPARK-3020 and SPARK-2298
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrent initialization of various DataType objects causes exceptions,SPARK-2498,12727515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ConcreteVitamin,ConcreteVitamin,ConcreteVitamin,15/Jul/14 23:28,16/Jul/14 00:59,14/Jul/23 06:26,16/Jul/14 00:59,1.0.1,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"Reported by Keith Simmons and gorenuru [here|http://apache-spark-user-list.1001560.n3.nabble.com/Error-while-running-Spark-SQL-join-when-using-Spark-1-0-1-td9776.html]. This is probably due to the fact that Scala Reflection API is not thread-safe for 2.10.* versions; see [SI-6240|https://issues.scala-lang.org/browse/SI-6240] for more details.

The purposed fix is to add a lock object that is synchronized on when various data type objects make use of the reflection API (during their initializations or other places in program execution). (Longer-term, we could consider evolving this to a SparkSQL-global lock, and all calls to the reflection APi from Spark SQL should probably synchronize on it.)",,ConcreteVitamin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405621,,,Tue Jul 15 23:28:54 UTC 2014,,,,,,,,,,"0|i1xsxr:",405646,,,,,,,,,,,,,,1.0.2,1.1.0,,,,,,,,,,"15/Jul/14 23:28;ConcreteVitamin;Github PR: https://github.com/apache/spark/pull/1423;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hash of None is different cross machines in CPython,SPARK-2494,12727441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,15/Jul/14 19:18,21/Jul/14 19:04,14/Jul/23 06:26,21/Jul/14 19:04,0.9.0,0.9.1,0.9.2,1.0.0,1.0.1,,,0.9.3,1.0.2,1.1.0,,,,PySpark,,,,0,pyspark,shuffle,,,,"The hash of None, also tuple with None in it, is different cross machines, so the result will be wrong if None appear in the key of partitionBy().

It should use an portable hash function as the default partition function, which generate same hash for all the builtin immutable types, especially tuple.",CPython 2.x ,davies,farrellee,matei,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405547,,,Mon Jul 21 19:04:27 UTC 2014,,,,,,,,,,"0|i1xshj:",405573,,,,,,,,,,,,,,0.9.3,1.0.2,1.1.0,,,,,,,,,"17/Jul/14 15:13;farrellee;[~davies] will you provide an example that demonstrates the issue?;;;","17/Jul/14 17:59;davies;This bug only happen in cluster mode, so it's can not be reproduced in unit tests.

In cluster mode (workers on different machines), it will happen:
 
>>> rdd = sc.parallelize([(None, 1), (None, 2)], 2)
>>> rdd.groupByKey(2).collect()
((None, [1]), (None, [2]))

The same key `None` will be put into different partitions and can not be aggregated 
together.;;;","17/Jul/14 18:12;farrellee;i'm trying to reproduce using the tip of master both in local standalone and cluster standalone (no mesos or yarn). in both cases i get:

{code}
>>> rdd = sc.parallelize([(None, 1), (None, 2)], 2)
>>> result = rdd.groupByKey(2).collect()
>>> print result
[(None, <pyspark.resultiterable.ResultIterable object at 0x17e1710>)]
>>> for x in result[0][1]:
...     print x
....
2
1
{code};;;","17/Jul/14 18:33;davies;The tip version already handle hash of None, but it can not handle hash of tuple with None in it.

Here is the updated test cases, sorry for that:

>>> rdd = sc.parallelize([((None, 1), 1),] *100 , 100)
>>> assert rdd.groupByKey(10).collect() == 1;;;","17/Jul/14 22:12;farrellee;thank you. i've confirmed this:

{code}
>>> rdd.groupByKey(10).collect()
[((None, 1), <pyspark.resultiterable.ResultIterable object at 0x19d4410>), ((None, 1), <pyspark.resultiterable.ResultIterable object at 0x19d4310>), ((None, 1), <pyspark.resultiterable.ResultIterable object at 0x19d7290>)]
{code}

i have 3 workers in my cluster;;;","18/Jul/14 06:06;davies;The PR for this issue: https://github.com/apache/spark/pull/1371;;;","21/Jul/14 19:04;matei;Merged this; thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError when RDD dependencies are too long,SPARK-2490,12727323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,viirya,viirya,15/Jul/14 11:55,01/Aug/14 19:13,14/Jul/23 06:26,01/Aug/14 19:13,1.0.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"When performing some transformations on RDDs after many iterations, the dependencies of RDDs could be very long. It can easily cause StackOverflowError when recursively visiting these dependencies in Spark core. For example:

var rdd = sc.makeRDD(Array(1))
for (i <- 1 to 1000) {
  rdd = rdd.coalesce(1).cache()
  rdd.collect()
}

",,gq,rxin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405429,,,2014-07-15 11:55:10.0,,,,,,,,,,"0|i1xrr3:",405454,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Follow up from SBT build refactor (i.e. SPARK-1776),SPARK-2487,12727260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pwendell,pwendell,15/Jul/14 05:57,29/Jan/15 07:05,14/Jul/23 06:26,29/Jan/15 07:05,,,,,,,,,,,,,,Build,,,,0,,,,,,"This is to track follw up issues relating to SPARK-1776, which was a major re-factoring of the SBT build in Spark.",,pwendell,tgraves,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405366,,,2014-07-15 05:57:14.0,,,,,,,,,,"0|i1xrdb:",405391,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.getCallSite can crash under JVMTI profilers,SPARK-2486,12727240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,willbenton,willbenton,willbenton,15/Jul/14 03:39,15/Jul/14 06:09,14/Jul/23 06:26,15/Jul/14 06:09,1.0.1,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"When running under an instrumenting profiler, Utils.getCallSite sometimes crashes with an NPE while examining stack trace elements.",running under profilers (observed on OS X under YourKit with CPU profiling and/or object allocation site tracking enabled),pwendell,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405346,,,Tue Jul 15 06:09:30 UTC 2014,,,,,,,,,,"0|i1xr9r:",405373,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/14 03:41;willbenton;A (trivial but functional) workaround is here:  https://github.com/apache/spark/pull/1413;;;","15/Jul/14 06:09;pwendell;Issue resolved by pull request 1413
[https://github.com/apache/spark/pull/1413];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage of HiveClient not threadsafe.,SPARK-2485,12727232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,15/Jul/14 02:11,15/Jul/14 16:15,14/Jul/23 06:26,15/Jul/14 16:15,,,,,,,,,,,,,,SQL,,,,0,,,,,,"When making concurrent queries against the hive metastore, sometimes we get an exception that includes the following stack trace: 

{code}
Caused by: java.lang.Throwable: get_table failed: out of sequence response
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:936)
{code}

Likely, we need to synchronize our use of HiveClient.",,ilikerps,llai,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405339,,,Tue Jul 15 16:15:11 UTC 2014,,,,,,,,,,"0|i1xr87:",405366,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Jul/14 16:15;ilikerps;https://github.com/apache/spark/pull/1412;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveQL parses accessing struct fields in an array incorrectly.,SPARK-2483,12727227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,15/Jul/14 01:52,15/Jul/14 21:03,14/Jul/23 06:26,15/Jul/14 21:03,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"Test case:
{code}
case class Data(a: Int, B: Int, n: Nested, nestedArray: Seq[Nested])
case class Nested(a: Int, B: Int)

  test(""nested repeated resolution"") {
    TestHive.sparkContext.parallelize(Data(1, 2, Nested(1,2), Seq(Nested(1,2))) :: Nil)
      .registerAsTable(""nestedRepeatedTest"")
    hql(""SELECT nestedArray[0].a FROM nestedRepeatedTest"").collect()
  }
{code}",,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405334,,,Tue Jul 15 21:03:23 UTC 2014,,,,,,,,,,"0|i1xr73:",405361,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Jul/14 21:03;marmbrus;https://github.com/apache/spark/pull/1411;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve sbt warnings during build,SPARK-2482,12727226,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,15/Jul/14 01:46,12/Sep/14 01:45,14/Jul/23 06:26,12/Sep/14 01:45,,,,,,,,1.2.0,,,,,,,,,,0,,,,,,"At the same time, import the scala.language.postfixOps and org.scalatest.time.SpanSugar._ cause scala.language.postfixOps doesn't work",,andrewor14,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405333,,,Fri Sep 12 01:45:03 UTC 2014,,,,,,,,,,"0|i1xr6v:",405360,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"12/Sep/14 01:45;andrewor14;Fixed in https://github.com/apache/spark/pull/1330;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The environment variables SPARK_HISTORY_OPTS is covered in start-history-server.sh,SPARK-2481,12727225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,gq,gq,15/Jul/14 01:44,26/Aug/14 02:26,14/Jul/23 06:26,26/Aug/14 02:26,1.0.0,1.0.1,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"If we have the following code in the conf/spark-env.sh  
{{export SPARK_HISTORY_OPTS=""-DSpark.history.XX=XX""}}
The environment variables SPARK_HISTORY_OPTS is covered in [start-history-server.sh|https://github.com/apache/spark/blob/master/sbin/start-history-server.sh]
 
{code}
if [ $# != 0 ]; then
  echo ""Using command line arguments for setting the log directory is deprecated. Please ""
  echo ""set the spark.history.fs.logDirectory configuration option instead.""
  export SPARK_HISTORY_OPTS=""$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=$1""
fi
{code}",,andrewor14,gq,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2744,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405332,,,Tue Aug 26 02:26:36 UTC 2014,,,,,,,,,,"0|i1xr6n:",405359,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"15/Jul/14 23:38;tsudukim;spark-env.sh is loaded after these lines are evaluated. (It is loaded in the line spark-daemon.sh is called)
{code:title=sbin/start-history-server.sh|borderStyle=solid}
if [ $# != 0 ]; then
  echo ""Using command line arguments for setting the log directory is deprecated. Please ""
  echo ""set the spark.history.fs.logDirectory configuration option instead.""
  export SPARK_HISTORY_OPTS=""$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=$1""
fi
{code}

If you want to set SPARK_HISTORY_OPTS in spark-env.sh and use the deprecated option like above, you can write like following in spark-env.sh:
{code:title=spark-env.sh|borderStyle=solid}
SPARK_HISTORY_OPTS=""$SPARK_HISTORY_OPTS -Dspark.history.XX=XX""
{code}

This works fine in my environment.;;;","16/Jul/14 02:59;gq;[~tsudukim]
There is only one need to write that is very strange. We should support  {{export SPARK_HISTORY_OPTS=""-DSpark.history.XX=XX""}} kind of writing, like other options.
PR: https://github.com/apache/spark/pull/1341;;;","17/Jul/14 01:36;tsudukim;Ah, I understand what you meant and I agree with you.;;;","26/Aug/14 02:26;andrewor14;Resolved by https://github.com/apache/spark/pull/1341;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For a registered table in OverrideCatalog, the Analyzer failed to resolve references in the format of ""tableName.fieldName""",SPARK-2474,12727117,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,14/Jul/14 16:31,15/Jul/14 21:08,14/Jul/23 06:26,15/Jul/14 21:08,1.0.1,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,"To reproduce the error, execute the following code in hive/console...
{code}
val m = hql(""select key from src"")
m.registerAsTable(""m"")
hql(""select m.key from m"")
{code}
Then, you will see
{code}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: 'm.key, tree:
Project ['m.key]
 LowerCaseSchema 
  Project [key#6]
   LowerCaseSchema 
    MetastoreRelation default, src, None

	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$apply$1.applyOrElse(Analyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$apply$1.applyOrElse(Analyzer.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:156)
...
{code}

However, if you run
{code}
hql(""select tmp.key from m tmp"")
{code}
We are fine.
{code}
SchemaRDD[3] at RDD at SchemaRDD.scala:104
== Query Plan ==
HiveTableScan [key#8], (MetastoreRelation default, src, None), None
{code}",,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2448,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405224,,,Tue Jul 15 21:08:30 UTC 2014,,,,,,,,,,"0|i1xqkv:",405258,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"14/Jul/14 16:37;yhuai;I think the problem is the lookupRelation in OverrideCatalog. 

The current version is 
{code}
abstract override def lookupRelation(
    databaseName: Option[String],
    tableName: String,
    alias: Option[String] = None): LogicalPlan = {
    val (dbName, tblName) = processDatabaseAndTableName(databaseName, tableName)
    val overriddenTable = overrides.get((dbName, tblName))

    // If an alias was specified by the lookup, wrap the plan in a subquery so that attributes are
    // properly qualified with this alias.
    val withAlias =
      overriddenTable.map(r => alias.map(a => Subquery(a, r)).getOrElse(r))

    withAlias.getOrElse(super.lookupRelation(dbName, tblName, alias))
  }
{code}
You can notice that we do not insert a Subquery for the tableName (i.e. Subquery(tableName, logicalPlan)). Seems the SimpleCatalog.lookupRelation does not have this issue because we have 
{code}
val tableWithQualifiers = Subquery(tblName, table)
{code}
;;;","14/Jul/14 20:31;yhuai;[~jerrylam] Can you try the PR (https://github.com/apache/spark/pull/1406) ?;;;","15/Jul/14 21:08;marmbrus;I'm going to mark this as resolved.  Please reopen if issues persist.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL Thrift server sometimes assigns wrong job group name,SPARK-2472,12727055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,lian cheng,lian cheng,14/Jul/14 09:05,19/Feb/15 02:08,14/Jul/23 06:26,19/Feb/15 02:08,1.0.0,,,,,,,1.2.2,1.3.0,,,,,SQL,,,,0,,,,,,"Sample beeline session used to reproduce this issue:

{code}
0: jdbc:hive2://localhost:10000> drop table test;
+---------+
| result  |
+---------+
+---------+
No rows selected (0.614 seconds)
0: jdbc:hive2://localhost:10000> create table hive_table_copy as select * from hive_table;
+------+--------+
| key  | value  |
+------+--------+
+------+--------+
No rows selected (0.493 seconds)
0
{code}

The second statement results in two stages, the first stage is labeled with the first {{drop table}} statement rather than the CTAS statement.",,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405162,,,Thu Feb 19 02:07:55 UTC 2015,,,,,,,,,,"0|i1xq7b:",405197,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,"29/Jul/14 23:01;lian cheng;This bug only affects queries involve insertion, like the CTAS statement in the given example. The reason is that {{InsertIntoHiveTable.execute()}} submits a job to perform the insertion eagerly at the end of the query planning phase. At that time, the job description hasn't been updated, thus shows the previous one.;;;","19/Feb/15 02:07;lian cheng;This ticket is fixed by [#4630|https://github.com/apache/spark/pull/4630] and [#4631|https://github.com/apache/spark/pull/4631].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Twitter Receiver does not stop correctly when streamingContext.stop is called,SPARK-2464,12726972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,12/Jul/14 22:45,18/Sep/14 15:39,14/Jul/23 06:26,25/Jul/14 00:19,1.0.0,1.0.1,,,,,,,,,,,,DStreams,,,,0,,,,,,,,apachespark,nchammas,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2892,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405079,,,Thu Jul 24 18:50:47 UTC 2014,,,,,,,,,,"0|i1xpp3:",405115,,,,,,,,,,,,,,1.0.2,1.1.0,,,,,,,,,,"24/Jul/14 18:50;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/1577;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent description in README about build option,SPARK-2457,12726917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,12/Jul/14 01:19,10/Dec/15 06:54,14/Jul/23 06:26,12/Jul/14 04:11,1.1.0,,,,,,,1.1.0,,,,,,,,,,0,,,,,,"Now, we should use -Pyarn instead of SPARK_YARN when building but README says as follows.

{code}
For Apache Hadoop 2.2.X, 2.1.X, 2.0.X, 0.23.x, Cloudera CDH MRv2, and other Hadoop versions
with YARN, also set `SPARK_YARN=true`:

    # Apache Hadoop 2.0.5-alpha
    $ sbt/sbt -Dhadoop.version=2.0.5-alpha -Pyarn assembly

    # Cloudera CDH 4.2.0 with MapReduce v2
    $ sbt/sbt -Dhadoop.version=2.0.0-cdh4.2.0 -Pyarn assembly

    # Apache Hadoop 2.2.X and newer
    $ sbt/sbt -Dhadoop.version=2.2.0 -Pyarn assembly
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,405024,,,Thu Dec 10 06:54:05 UTC 2015,,,,,,,,,,"0|i1xpdb:",405060,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/14 01:27;sarutak;PR: https://github.com/apache/spark/pull/1382;;;","10/Dec/15 06:54;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1382;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VertexPartition is not serializable,SPARK-2455,12726883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,11/Jul/14 21:55,12/Jul/14 19:06,14/Jul/23 06:26,12/Jul/14 19:06,,,,,,,,1.0.2,1.1.0,,,,,GraphX,,,,0,,,,,,"VertexPartition and ShippableVertexPartition are contained in RDDs but are not marked Serializable, leading to NotSerializableExceptions when using Java serialization.

The fix is simply to mark them as Serializable.",,ankurd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404990,,,Fri Jul 11 22:17:03 UTC 2014,,,,,,,,,,"0|i1xp5r:",405026,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/14 22:17;ankurd;Proposed fix: https://github.com/apache/spark/pull/1376;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Separate driver spark home from executor spark home,SPARK-2454,12726876,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,11/Jul/14 20:38,25/Nov/14 12:08,14/Jul/23 06:26,05/Aug/14 16:05,1.0.1,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"The driver may not always share the same directory structure as the executors. It makes little sense to always re-use the driver's spark home on the executors.

https://github.com/apache/spark/pull/1244/ is an open effort to fix this. However, this still requires us to set SPARK_HOME on all the executor nodes. Really we should separate this out into something like `spark.executor.home` and `spark.driver.home` rather than re-using SPARK_HOME everywhere.",,andrewor,apachespark,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2290,,,SPARK-2290,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404983,,,Sat Aug 02 10:33:40 UTC 2014,,,,,,,,,,"0|i1xp47:",405019,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 11:47;codingcat;this will make sparkHome as an application-specific parameter explicitly, I just thought it will confuse the user since sparkHome is actually a global setup for all application/executors run on the same machine


The good thing here is it can support the user to run the application in different version of spark sharing the same cluster.....(especially when you are doing spark dev work) ;;;","16/Jul/14 20:24;andrewor;There may be multiple installations of Spark on the executor machine, in which case a global SPARK_HOME environment variable is not sufficient. What I am suggesting is that we should still keep the option of allowing the driver to overwrite executor spark homes (spark.executor.home), and only overwrite the executor SPARK_HOME if this is specified.;;;","16/Jul/14 20:58;codingcat;I see, it makes sense to me...;;;","21/Jul/14 23:07;codingcat;there is a related issue and fix 

https://issues.apache.org/jira/browse/SPARK-2404

https://github.com/apache/spark/pull/1331

where we should not overwrite SPARK_HOME in spark-submit and spark-class if the user has set those two values

in our scenario, the remote cluster has the same user  with the login portal, say codingcat, 

so the SPARK_HOME is set to /home/codingcat/spark-1.0, other users in the login portal has a soft link to SPARK_HOME in their own directory, 

however the current scripts overwrite the already-set SPARK_HOME to the pwd running spark-submit, which does not exist in the cluster, causing the exceptions like ""cannot run /home/local_user/spark-class""....


 ;;;","02/Aug/14 05:26;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/1734;;;","02/Aug/14 10:33;srowen;(Sorry if this ends up a double-post). This change is making Jenkins fail tests for ExecutorRunnerTest and SparkSubmitSuite. spark.test.home does not seem to be set.

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-Maven-with-YARN/lastFailedBuild/HADOOP_PROFILE=hadoop-2.3,label=centos/consoleFull;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-statement input to spark repl does not work,SPARK-2452,12726863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,timhunter,timhunter,11/Jul/14 19:17,07/Feb/20 17:23,14/Jul/23 06:26,22/Jul/14 07:39,1.0.1,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"Here is an example:

{code}
scala> val x = 4 ; def f() = x
x: Int = 4
f: ()Int

scala> f()
<console>:11: error: $VAL5 is already defined as value $VAL5
val $VAL5 = INSTANCE;
{code}",,prashant,pwendell,timhunter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2453,,,,,,SPARK-1199,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404970,,,Tue Jul 22 20:31:39 UTC 2014,,,,,,,,,,"0|i1xp1b:",405006,,,,,,,,,,,,,,1.0.2,,,,,,,,,,,"11/Jul/14 19:20;pwendell;This might be related to SPARK-1199 (/cc [~prashant]);;;","11/Jul/14 19:59;pwendell;I think we should narrow the description a bit. This is not a problem with all compound statements, it's only a problem if a statement references a variable within another statement.

{code}
// Reference
scala> val x = 10; val y = x + 10
x: Int = 10
y: Int = 20

scala> y
<console>:11: error: $VAL11 is already defined as value $VAL11
val $VAL11 = INSTANCE;
    ^

// Works fine
scala> val x = 10; val y = 20
x: Int = 10
y: Int = 20

scala> x
res3: Int = 10

scala> y
res4: Int = 20
{code};;;","22/Jul/14 07:39;pwendell;Issue resolved by pull request 1441
[https://github.com/apache/spark/pull/1441];;;","22/Jul/14 20:31;timhunter;Excellent, thanks Patrick.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading from Partitioned Tables is Slow,SPARK-2443,12726720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ConcreteVitamin,marmbrus,marmbrus,11/Jul/14 00:51,16/Jul/14 14:23,14/Jul/23 06:26,14/Jul/14 20:23,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"Here are some numbers, all queries return ~20million:

{code}
SELECT COUNT(*) FROM <non partitioned table>
5.496467726 s

SELECT COUNT(*) FROM <partitioned table stored in parquet>
50.266666947 s

SELECT COUNT(*) FROM <same table as previous but loaded with parquetFile instead of through hive>
2s
{code}",,avignon,cfregly,chutium,ConcreteVitamin,jerrylam,llai,marmbrus,nchammas,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404827,,,Wed Jul 16 14:23:50 UTC 2014,,,,,,,,,,"0|i1xo67:",404865,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"12/Jul/14 09:14;ConcreteVitamin;I am able to reproduce this behavior locally: 2 million rows of key-val pairs stored as a text file (hence this issue is probably not parquet-specific), non-partitioned vs. partitioned into 1 part, and I have seen a more than 10x difference.

In the non-partitioned case, this is the `inputRdd` in HiveTableScan:

MapPartitionsRDD[15] at mapPartitions at TableReader.scala:102 (2 partitions)
  MappedRDD[14] at map at TableReader.scala:222 (2 partitions)
    HadoopRDD[13] at HadoopRDD at TableReader.scala:212 (2 partitions)

In the partitioned case, there's an extra UnionRDD:

UnionRDD[4] at UnionRDD at TableReader.scala:183 (2 partitions)
  MapPartitionsRDD[3] at mapPartitions at TableReader.scala:164 (2 partitions)
    MappedRDD[2] at map at TableReader.scala:222 (2 partitions)
      HadoopRDD[1] at HadoopRDD at TableReader.scala:212 (2 partitions)

The times to make these two RDDs are about the same, and hence the difference in performance lies in the henceforth actual computations of them. Will investigate further.
;;;","12/Jul/14 21:14;rxin;Take a look at this pull request: https://github.com/amplab/shark/pull/329;;;","12/Jul/14 23:53;ConcreteVitamin;Pull request: https://github.com/apache/spark/pull/1390;;;","14/Jul/14 16:01;jerrylam;I wonder if this fix can be easily merged into the current spark release (1.0.1)? We desperately need this fix to perform the benchmark. Thank you!;;;","14/Jul/14 17:27;marmbrus;[~jerrylam] since Spark SQL is an Alpha component we have been pretty aggressive about back porting to the 1.0 branch.  This patch will likely be included in the 1.0.2 release if/when we make one.;;;","14/Jul/14 18:37;ConcreteVitamin;I opened a new PR: https://github.com/apache/spark/pull/1408;;;","16/Jul/14 13:17;chutium;Hi, how can you access parquet table using HiveContext / hql in spark? i tried to add parquet-hive jar or parquet-hive-bundle jar in driver-class-path and spark.executor.extraClassPath, but it doesn't work. CDH5.0.2, hive 0.12.0-cdh5

Thanks;;;","16/Jul/14 14:23;marmbrus;[~chutium], I would recommend using the native parquet interface if possible (explained in the programming guide: http://spark.apache.org/docs/latest/sql-programming-guide.html).  That said, others have reported success using the hive serde for parquet as well.

Please ask future questions on the user mailing list and not JIRA: https://spark.apache.org/community.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add shutdown hook to bin/pyspark,SPARK-2435,12726641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,farrellee,andrewor14,andrewor,10/Jul/14 17:56,05/Nov/14 10:43,14/Jul/23 06:26,04/Sep/14 02:37,1.0.1,,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"We currently never stop the SparkContext cleanly in bin/pyspark unless the user explicitly runs sc.stop(). This behavior is not consistent with bin/spark-shell, in which case Ctrl+D stops the SparkContext before quitting the shell.",,andrewor,apachespark,farrellee,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404748,,,Thu Sep 04 02:37:50 UTC 2014,,,,,,,,,,"0|i1xnon:",404786,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 14:54;farrellee;i couldn't find a PR for this, and it has been a problem for me, so i've created

https://github.com/apache/spark/pull/2183;;;","28/Aug/14 16:14;apachespark;User 'mattf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2183;;;","04/Sep/14 02:37;joshrosen;Issue resolved by pull request 2183
[https://github.com/apache/spark/pull/2183];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In MLlib, implementation for Naive Bayes in Spark 0.9.1 is having an implementation bug.",SPARK-2433,12726625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,rahul1993,rahul1993,10/Jul/14 16:37,17/Jul/14 05:27,14/Jul/23 06:26,17/Jul/14 03:12,0.9.1,,,,,,,0.9.2,,,,,,MLlib,PySpark,,,0,easyfix,test,,,,"Don't have much experience with reporting errors. This is first time. If something is not clear please feel free to contact me (details given below)

In the pyspark mllib library. 
Path : \spark-0.9.1\python\pyspark\mllib\classification.py

Class: NaiveBayesModel

Method:  self.predict

Earlier Implementation:
def predict(self, x):
    """"""Return the most likely class for a data vector x""""""
    return numpy.argmax(self.pi + numpy.log(dot(numpy.exp(self.theta),x)))
        

New Implementation:
No:1
def predict(self, x):
    """"""Return the most likely class for a data vector x""""""
    return numpy.argmax(self.pi + numpy.log(dot(numpy.exp(self.theta),x)))

No:2
def predict(self, x):
    """"""Return the most likely class for a data vector x""""""
    return numpy.argmax(self.pi + dot(x,self.theta.T))

Explanation:
No:1 is correct according to me. Don't know about No:2.

Error one:
The matrix self.theta is of dimension [n_classes , n_features]. 
while the matrix x is of dimension [1 , n_features].

Taking the dot will not work as its [1, n_feature ] x [n_classes,n_features].
It will always give error:  ""ValueError: matrices are not aligned""
In the commented example given in the classification.py, n_classes = n_features = 2. That's why no error.

Both Implementation no.1 and Implementation no. 2 takes care of it.

Error 2:
As basic implementation of naive bayes is: P(class_n | sample) = count_feature_1 * P(feature_1 | class_n ) * count_feature_n * P(feature_n|class_n) * P(class_n)/(THE CONSTANT P(SAMPLE)

and taking the class with max value.
That's what implementation 1 is doing.

In Implementation 2: 
Its basically class with max value :
( exp(count_feature_1) * P(feature_1 | class_n ) * exp(count_feature_n) * P(feature_n|class_n) * P(class_n))

Don't know if it gives the exact result.

Thanks
Rahul Bhojwani
rahulbhojwani2003@gmail.com",Any ,bdechoux,mengxr,rahul1993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404732,,,Thu Jul 17 05:27:04 UTC 2014,,,,,,,,,,"0|i1xnl3:",404770,,,,,,,,,,,,,,0.9.2,,,,,,,,,,,"10/Jul/14 17:59;srowen;Your ""earlier implementation"" is identical to ""new implementation 1"". This does not appear to be the code in master, and I think it's only useful to propose fixes to the current version of code.;;;","10/Jul/14 18:40;bdechoux;A Jira ticket is the first step, the second would have been to provide a diff patch or a github pull request. And you can also write a test to prove your point and make sure that the fix will stay longer.

I will second Sean :
1) work with last version (1.0)
2) you report is not clear, that's why diff patch or pull request are welcomed

And there is a transpose() in the current implementation,  the bug is actually already fixed, see https://github.com/apache/spark/pull/463

You might want to read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark for a next time.;;;","10/Jul/14 19:02;rahul1993;Sorry my mistake.
By earlier implementation I mean how its already implemented.
and that is:
def predict(self, x):
        """"""Return the most likely class for a data vector x""""""
        return numpy.argmax(self.pi + dot(x,self.theta))








-- 
Rahul K Bhojwani
3rd Year B.Tech
Computer Science and Engineering
National Institute of Technology, Karnataka
;;;","10/Jul/14 19:02;rahul1993;Okay fine. I will take care of the proper procedure from next time. Looks like its been already taken care.
 Thank you;;;","16/Jul/14 08:04;mengxr;[~rahul1993] Thanks for reporting this bug! It was fixed in branch-1.0 but not in branch-0.9. So you can send a PR similar to https://github.com/apache/spark/pull/463 to branch-0.9, following https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark . I'll try to cut a release candidate for v0.9.2 tomorrow. So if you don't have time to send the PR, I may have to do that myself.;;;","17/Jul/14 00:45;mengxr;PR for branch-0.9: https://github.com/apache/spark/pull/1453;;;","17/Jul/14 03:12;mengxr;Issue resolved by pull request 1453
[https://github.com/apache/spark/pull/1453];;;","17/Jul/14 05:05;rahul1993;Apologies for no response and not correcting it by myself. Situation was so.
The patch looks perfect.

Thanks,


On Thu, Jul 17, 2014 at 8:44 AM, Xiangrui Meng (JIRA) <jira@apache.org>




-- 

 [image: http://]
Rahul K Bhojwani
[image: http://]about.me/rahul_bhojwani
     <http://about.me/rahul_bhojwani>
;;;","17/Jul/14 05:27;rahul1993;There is another small error in the documentation.

http://spark.apache.org/docs/0.9.1/mllib-guide.html#clustering-2

Have created the Issue


On Thu, Jul 17, 2014 at 10:34 AM, Rahul Bhojwani <




-- 

 [image: http://]
Rahul K Bhojwani
[image: http://]about.me/rahul_bhojwani
     <http://about.me/rahul_bhojwani>
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some of the Scala examples,SPARK-2427,12726519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Artjom,Artjom,10/Jul/14 08:05,10/Jul/14 23:04,14/Jul/23 06:26,10/Jul/14 23:04,1.0.0,,,,,,,1.0.2,1.1.0,,,,,Examples,,,,0,,,,,,"The Scala examples HBaseTest and HdfsTest don't use the correct indexes for the command line arguments. This due to to the fix of JIRA 1565, where these examples were not correctly adapted to the new usage of the submit script.",,Artjom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404626,,,2014-07-10 08:05:07.0,,,,,,,,,,"0|i1xmxr:",404664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone Master is too aggressive in removing Applications,SPARK-2425,12726475,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,markhamstra,markhamstra,markhamstra,09/Jul/14 23:49,09/Sep/14 18:18,14/Jul/23 06:26,09/Sep/14 18:17,1.0.0,,,,,,,1.1.1,1.2.0,,,,,Spark Core,,,,0,,,,,,"When standalone Executors trying to run a particular Application fail a cummulative ApplicationState.MAX_NUM_RETRY times, Master will remove the Application.  This will be true even if there actually are a number of Executors that are successfully running the Application.  This makes long-running standalone-mode Applications in particular unnecessarily vulnerable to limited failures in the cluster -- e.g., a single bad node on which Executors repeatedly fail for any reason can prevent an Application from starting or can result in a running Application being removed even though it could continue to run successfully (just not making use of all potential Workers and Executors.) ",,andrewor14,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2424,,SPARK-3289,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404582,,,Tue Sep 09 18:18:25 UTC 2014,,,,,,,,,,"0|i1xmo7:",404621,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,"09/Sep/14 18:18;andrewor14;Reopened a few times to change the fixed version. There is no net change, so please disregard.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decision tree tests are failing,SPARK-2417,12726299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jsondag,pwendell,pwendell,09/Jul/14 12:10,28/Jul/14 14:09,14/Jul/23 06:26,09/Jul/14 18:07,,,,,,,,1.0.1,1.1.0,,,,,MLlib,,,,0,,,,,,"After SPARK-2152 was merged, these tests started failing in Jenkins:

{code}
- classification stump with all categorical variables *** FAILED ***
  org.scalatest.exceptions.TestFailedException was thrown. (DecisionTreeSuite.scala:257)
- regression stump with all categorical variables *** FAILED ***
  org.scalatest.exceptions.TestFailedException was thrown. (DecisionTreeSuite.scala:284)
{code}

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/97/hadoop.version=1.0.4,label=centos/console",,jsondag,mengxr,Patrickmorton,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2232,,,,SPARK-2152,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404406,,,Wed Jul 09 18:07:20 UTC 2014,,,,,,,,,,"0|i1xllb:",404445,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/14 13:38;jsondag;PR fix here: https://github.com/apache/spark/pull/1343;;;","09/Jul/14 18:07;mengxr;Issue resolved by pull request 1343
[https://github.com/apache/spark/pull/1343];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowWriteSupport should handle empty ArrayType correctly.,SPARK-2415,12726262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,09/Jul/14 08:08,28/Jul/14 14:10,14/Jul/23 06:26,11/Jul/14 02:25,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"{{RowWriteSupport}} doesn't write empty {{ArrayType}} value, so the read value becomes {{null}}.
It should write empty {{ArrayType}} value as it is.",,Patrickmorton,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404369,,,Wed Jul 09 08:16:01 UTC 2014,,,,,,,,,,"0|i1xld3:",404408,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"09/Jul/14 08:16;ueshin;PRed: https://github.com/apache/spark/pull/1339;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoalescedRDD throws exception with certain pref locs,SPARK-2412,12726220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,09/Jul/14 00:41,17/Jul/14 08:01,14/Jul/23 06:26,17/Jul/14 08:01,1.0.0,,,,,,,1.0.2,1.1.0,,,,,Spark Core,,,,0,,,,,,"If the first pass of CoalescedRDD does not find the target number of locations AND the second pass finds new locations, an exception is thrown, as ""groupHash.get(nxt_replica).get"" is not valid.

The fix is just to add an ArrayBuffer to groupHash for that replica if it didn't already exist.",,ilikerps,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404327,,,Thu Jul 17 08:01:42 UTC 2014,,,,,,,,,,"0|i1xl3r:",404366,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 08:01;pwendell;Issue resolved by pull request 1337
[https://github.com/apache/spark/pull/1337];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement SQL SUBSTR() directly in Catalyst,SPARK-2407,12726143,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,willbenton,willbenton,willbenton,08/Jul/14 18:11,25/Jul/14 23:18,14/Jul/23 06:26,15/Jul/14 21:13,1.0.0,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,Currently SQL SUBSTR/SUBSTRING() is delegated to Hive.  It would be nice to implement this directly.,,nchammas,rxin,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404250,,,Fri Jul 25 23:18:58 UTC 2014,,,,,,,,,,"0|i1xkn3:",404290,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Jul/14 18:12;willbenton;I have this on a branch and will submit a PR as soon as I'm done running the test suite locally.;;;","08/Jul/14 19:55;rxin;Great - I'm assigning the ticket to you Will.;;;","10/Jul/14 17:45;willbenton;Here's the PR: https://github.com/apache/spark/pull/1359
;;;","25/Jul/14 23:18;nchammas;Minor point: Shouldn't the issue type for this be ""Improvement"" or ""New Feature"" and not ""Bug""?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initial Partitioned Parquet Support,SPARK-2406,12726138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,08/Jul/14 17:58,18/Aug/14 20:18,14/Jul/23 06:26,18/Aug/14 20:18,,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,apachespark,cheffpj,marmbrus,ravipesala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404245,,,Wed Aug 06 23:43:13 UTC 2014,,,,,,,,,,"0|i1xklz:",404285,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"13/Jul/14 06:38;cheffpj;Just to confirm, this JIRA implies Spark SQL would use the native Parquet api when a Hive table is using the parquet serde (and eventually the Hive 13 native storage format), correct?;;;","14/Jul/14 21:00;marmbrus;I think there are two ways we can achieve this, each with their own pros/cons.  One would just piggyback on the existing hive partitioning API as you propose, but use our (possibly more efficient) parquet reader.  The other would give you the ability to partition parquet tables, without needing to pull in all of hive.;;;","17/Jul/14 01:10;cheffpj;Okay, so it sounds like these are completely different use cases then, which might deserve their own JIRAs:
 * Native support for partitioned Parquet tables (probably what this JIRA is meant to address)
 * Automatic use of Native Parquet readers when using a Hive parquet table (what my previous comment described)

Does that sound correct?;;;","06/Aug/14 23:43;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/1819;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple instances of an InMemoryRelation in a single plan results in recaching,SPARK-2405,12726131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,08/Jul/14 17:35,12/Jul/14 19:14,14/Jul/23 06:26,12/Jul/14 19:14,1.0.0,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,We should instead reuse the cached buffers when they exist.,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404238,,,2014-07-08 17:35:41.0,,,,,,,,,,"0|i1xkkf:",404278,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark stuck when class is not registered with Kryo,SPARK-2403,12726064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,darabos,darabos,08/Jul/14 12:41,10/Nov/14 02:07,14/Jul/23 06:26,08/Jul/14 17:45,1.0.0,,,,,,,1.0.2,1.1.0,,,,,Spark Core,,,,0,,,,,,"We are using Kryo and require registering classes. When trying to serialize something containing an unregistered class, Kryo will raise an exception.

DAGScheduler.submitMissingTasks runs in the scheduler thread and checks if the contents of the task can be serialized by trying to serialize it:

https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L767

It catches NotSerializableException and aborts the task with an error when this happens.

The problem is, Kryo does not raise NotSerializableException for unregistered classes. It raises IllegalArgumentException instead. This exception is not caught and kills the scheduler thread. The application then hangs, waiting indefinitely for the job to finish.

Catching IllegalArgumentException also is a quick fix. I'll send a pull request for it if you agree. Thanks!",,darabos,glenn.strycker@gmail.com,kadiyalakc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404171,,,Mon Nov 10 02:07:22 UTC 2014,,,,,,,,,,"0|i1xk5r:",404211,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/14 12:58;darabos;I think DAGSchedulerActorSupervisor is supposed to kill the system when things like this happen. I am not sure why that does not happen in this case.

I forgot to include the stack trace:

{noformat}
java.lang.IllegalArgumentException: Class is not registered: scala.collection.immutable.Range
Note: To register this class use: kryo.register(scala.collection.immutable.Range.class);
        at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:442) ~[kryo-2.21.jar:na]
        at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:79) ~[kryo-2.21.jar:na]
        at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:472) ~[kryo-2.21.jar:na]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:565) ~[kryo-2.21.jar:na]
        at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:101) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$writeObject$1.apply(ParallelCollectionRDD.scala:65) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$writeObject$1.apply(ParallelCollectionRDD.scala:65) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.util.Utils$.serializeViaNestedStream(Utils.scala:105) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.rdd.ParallelCollectionPartition.writeObject(ParallelCollectionRDD.scala:65) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source) ~[na:na]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_55]
        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_55]
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) ~[na:1.7.0_55]
        at org.apache.spark.scheduler.ShuffleMapTask.writeExternal(ShuffleMapTask.scala:126) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1458) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) ~[na:1.7.0_55]
        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:71) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:767) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:713) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:717) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:716) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at scala.collection.immutable.List.foreach(List.scala:318) ~[scala-library-2.10.4.jar:na]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:716) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:697) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1176) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.actor.ActorCell.invoke(ActorCell.scala:456) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.4.jar:na]
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.4.jar:na]
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.4.jar:na]
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.4.jar:na]
{noformat};;;","10/Nov/14 02:07;kadiyalakc;I still get this error (deploying on YARN cluster). Built from source (date git repo cloned - 11/9/2014). 

mvn -Pyarn -Pdeb -Phadoop-2.4 -Dhadoop.version=2.4.1 -DskipTests clean package install

Here is the error trace:

2014-11-08 14:58:21,134 INFO  [sparkDriver-akka.actor.default-dispatcher-3] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 0 from broadcast at DAGScheduler.scala:838
2014-11-08 14:58:21,158 INFO  [sparkDriver-akka.actor.default-dispatcher-3] cluster.YarnClusterScheduler (Logging.scala:logInfo(59)) - Cancelling stage 0
2014-11-08 14:58:21,163 INFO  [Driver] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 0 failed: reduce at SparkPi.scala:35, took 0.425176 s
2014-11-08 14:58:21,170 INFO  [Driver] yarn.ApplicationMaster (Logging.scala:logInfo(59)) - Final app status: FAILED, exitCode: 15, (reason: User class threw exception: Job aborted due to stage failure: Task serialization failed: java.io.IOException: java.lang.IllegalArgumentException: Class is not registered: scala.collection.immutable.Range
Note: To register this class use: kryo.register(scala.collection.immutable.Range.class);
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:988)
org.apache.spark.rdd.ParallelCollectionPartition.writeObject(ParallelCollectionRDD.scala:51)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:606)
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:876)
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778)
org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:762)
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1381)
akka.actor.Actor$class.aroundReceive(Actor.scala:465)
org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1367)
akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
akka.actor.ActorCell.invoke(ActorCell.scala:487)
akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
akka.dispatch.Mailbox.run(Mailbox.scala:220)
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
)
2014-11-08 14:58:31,722 INFO  [main] ipc.Client (Client.java:handleConnectionFailure(841)) - Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2014-11-08 14:58:32,724 INFO  [main] ipc.Client (Client.java:handleConnectionFailure(841)) - Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2014-11-08 14:58:33,726 INFO  [main] ipc.Client (Client.java:handleConnectionFailure(841)) - Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
config spark.yarn.max.executor.failures is not explained accurately,SPARK-2400,12725944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,crazyjvm,crazyjvm,crazyjvm,08/Jul/14 05:47,08/Jul/14 18:56,14/Jul/23 06:26,08/Jul/14 18:56,,,,,,,,1.1.0,,,,,,YARN,,,,0,,,,,,,,crazyjvm,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,404102,,,Tue Jul 08 05:53:15 UTC 2014,,,,,,,,,,"0|i1xjqf:",404142,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"08/Jul/14 05:52;crazyjvm;it should be ""numExecutors * 2, with minimum of 3"" rather than ""2*numExecutors"".
;;;","08/Jul/14 05:53;crazyjvm;PR @ https://github.com/apache/spark/pull/1282/files;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors should not start their own HTTP servers,SPARK-2392,12725827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,07/Jul/14 19:55,05/Nov/14 10:45,14/Jul/23 06:26,09/Jul/14 00:35,1.0.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"In the long run we should separate out classes used by the driver vs executors  in SparkEnv. For now, we should at least not start an unused HTTP on every executor.",,andrewor,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403985,,,Tue Jul 29 23:24:27 UTC 2014,,,,,,,,,,"0|i1xj1j:",404027,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/14 23:24;andrewor;https://github.com/apache/spark/pull/1335;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT queries ship a whole partition of data for in-memory tables,SPARK-2391,12725805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,07/Jul/14 18:30,08/Jul/14 07:42,14/Jul/23 06:26,08/Jul/14 07:42,1.0.0,,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,Basically the problem here is that Spark's take() runs jobs using allowLocal = true.,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403963,,,2014-07-07 18:30:56.0,,,,,,,,,,"0|i1xiwv:",404006,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Files in .sparkStaging on HDFS cannot be deleted and wastes the space of HDFS,SPARK-2390,12725804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,07/Jul/14 18:30,10/Dec/15 06:54,14/Jul/23 06:26,15/Jul/14 06:55,1.0.0,1.0.1,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"When running jobs with YARN Cluster mode and using HistoryServer, the files in the Staging Directory cannot be deleted.

HistoryServer uses directory where event log is written, and the directory is represented as a instance of o.a.h.f.FileSystem created by using FileSystem.get.

{code:title=FileLogger.scala}
private val fileSystem = Utils.getHadoopFileSystem(new URI(logDir))
{code}
{code:title=utils.getHadoopFileSystem}
def getHadoopFileSystem(path: URI): FileSystem = {
  FileSystem.get(path, SparkHadoopUtil.get.newConfiguration())
}
{code}

On the other hand, ApplicationMaster has a instance named fs, which also created by using FileSystem.get.

{code:title=ApplicationMaster}
private val fs = FileSystem.get(yarnConf)
{code}

FileSystem.get returns cached same instance when URI passed to the method represents same file system and the method is called by same user.

Because of the behavior, when the directory for event log is on HDFS, fs of ApplicationMaster and fileSystem of FileLogger is same instance.

When shutting down ApplicationMaster, fileSystem.close is called in FileLogger#stop, which is invoked by SparkContext#stop indirectly.
{code:title=FileLogger.stop}
def stop() {
  hadoopDataStream.foreach(_.close())
  writer.foreach(_.close())
  fileSystem.close()
}
{code}

And  ApplicationMaster#cleanupStagingDir also called by JVM shutdown hook. In this method, fs.delete(stagingDirPath) is invoked. 

Because fs.delete in ApplicationMaster is called after fileSystem.close in FileLogger, fs.delete fails and results not deleting files in the staging directory.
",,apachespark,mridulm80,pwendell,sarutak,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403962,,,Thu Dec 10 06:54:03 UTC 2015,,,,,,,,,,"0|i1xiwn:",404005,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/14 19:08;sarutak;I modified the way to create a instance of FileSystem in FileLogger as follows, the symptom didn't appear.

{code}
private val fileSystem = FileSystem.newInstance(new URI(logDir), SparkHadoopUtil.get.newConfiguration())  
{code}

;;;","07/Jul/14 20:20;mridulm80;Here, and a bunch of other places, spark currently closes the Filesystem instance : this is incorrect, and should not be done.
The fix would be to remove the fs.close; not force creation of new instances.;;;","07/Jul/14 23:44;sarutak;Thank you for your comment [~mridulm80].

I noticed FileSystem is to be closed by shutdown hook, and the shutdown hook deletes files in the staging directory and then close FileSystem.
So, in this case, I also think there is no problem to remove fs.close.
;;;","08/Jul/14 00:39;sarutak;Pull Requested at https://github.com/apache/spark/pull/1326;;;","15/Jul/14 06:55;pwendell;Issue resolved by pull request 1326
[https://github.com/apache/spark/pull/1326];;;","10/Dec/15 06:54;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/1326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowWriteSupport should use the exact types to cast.,SPARK-2386,12725716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,07/Jul/14 10:34,08/Jul/14 00:04,14/Jul/23 06:26,08/Jul/14 00:04,,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"When execute {{saveAsParquetFile}} with non-primitive type, {{RowWriteSupport}} uses wrong type {{Int}} for {{ByteType}} and {{ShortType}}.",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403874,,,Mon Jul 07 10:41:28 UTC 2014,,,,,,,,,,"0|i1xid3:",403917,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"07/Jul/14 10:41;ueshin;PRed: https://github.com/apache/spark/pull/1315;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"stopReceive in dead loop, cause stackoverflow exception",SPARK-2379,12725615,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joyyoj,joyyoj,06/Jul/14 09:19,01/Aug/14 20:42,14/Jul/23 06:26,01/Aug/14 20:42,1.0.0,,,,,,,1.0.3,1.1.0,,,,,DStreams,,,,0,,,,,,"streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
    stop will call stopReceiver and stopReceiver will call stop if exception occurs, that make a dead loop.

",,adolphdai,apachespark,joyyoj,llai,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403774,,,Thu Jul 31 15:56:03 UTC 2014,,,,,,,,,,"0|i1xhr3:",403817,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/14 22:04;tdas;Can you give us logs and a way to reproduce this problem? Without lots its hard to see what was going wrong.

;;;","22/Jul/14 21:18;tdas;Any information on this? If we have no way to reproduce this, I will close this.;;;","23/Jul/14 01:24;joyyoj;Easily to reproduce this bug. 

java.lang.StackOverflowError
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stop(ReceiverSupervisor.scala:112)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stop(ReceiverSupervisor.scala:112)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stop(ReceiverSupervisor.scala:112)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stop(ReceiverSupervisor.scala:112)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stop(ReceiverSupervisor.scala:112)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stop(ReceiverSupervisor.scala:112)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.stopReceiver(ReceiverSupervisor.scala:141)

--- a/spark/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
+++ b/spark/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
@@ -138,7 +138,7 @@ private[streaming] abstract class ReceiverSupervisor(
       onReceiverStop(message, error)
     } catch {
       case t: Throwable =>
-        stop(""Error stopping receiver "" + streamId, Some(t))
+        logError(""Error stopping receiver "" + streamId + t.getStackTraceString)
     }
   };;;","23/Jul/14 01:25;joyyoj;--- a/spark/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
+++ b/spark/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
@@ -138,7 +138,7 @@ private[streaming] abstract class ReceiverSupervisor(
       onReceiverStop(message, error)
     } catch {
       case t: Throwable =>
        logError(""Error stopping receiver "" + streamId + t.getStackTraceString)
     }
   };;;","29/Jul/14 12:51;adolphdai;me too.;;;","31/Jul/14 15:56;apachespark;User 'joyyoj' has created a pull request for this issue:
https://github.com/apache/spark/pull/1694;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selecting list values inside nested JSON objects raises java.lang.IllegalArgumentException,SPARK-2376,12725595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,nchammas,nchammas,06/Jul/14 00:16,08/Jul/14 02:00,14/Jul/23 06:26,08/Jul/14 02:00,1.0.1,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"Repro script for PySpark, deployed via {{spark-ec2}} at git revision {{9d5ecf8205b924dc8a3c13fed68beb78cc5c7553}}:

{code}
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

raw = sc.parallelize([
    """"""
    {
        ""name"": ""Nick"",
        ""history"": {
            ""countries"": []
        }
    }
    """"""
])

profiles = sqlContext.jsonRDD(raw)
profiles.registerAsTable(""profiles"")
profiles.printSchema()

sqlContext.sql(""SELECT name FROM profiles;"").collect()     # works fine
sqlContext.sql(""SELECT history FROM profiles;"").collect()  # raises exception
{code}

Attempting to select the top-level struct that has a nested list value yields the following error:

{code}
14/07/06 00:10:26 INFO scheduler.TaskSetManager: Loss was due to net.razorvine.pickle.PickleException: couldn't introspect javabean: java.lang.IllegalArgumentException: wrong number of arguments [duplicate 3]
14/07/06 00:10:26 ERROR scheduler.TaskSetManager: Task 26.0:15 failed 4 times; aborting job
14/07/06 00:10:26 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
14/07/06 00:10:26 INFO scheduler.TaskSchedulerImpl: Cancelling stage 26
14/07/06 00:10:26 INFO scheduler.DAGScheduler: Failed to run collect at <stdin>:1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/spark/python/pyspark/rdd.py"", line 649, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File ""/root/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"", line 537, in __call__
  File ""/root/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o286.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 26.0:15 failed 4 times, most recent failure: Exception failure in TID 394 on host ip-10-183-59-125.ec2.internal: net.razorvine.pickle.PickleException: couldn't introspect javabean: java.lang.IllegalArgumentException: wrong number of arguments
        net.razorvine.pickle.Pickler.put_javabean(Pickler.java:603)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:299)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.put_map(Pickler.java:322)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:286)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.put_map(Pickler.java:322)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:286)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:392)
        net.razorvine.pickle.Pickler.dispatch(Pickler.java:195)
        net.razorvine.pickle.Pickler.save(Pickler.java:125)
        net.razorvine.pickle.Pickler.dump(Pickler.java:95)
        net.razorvine.pickle.Pickler.dumps(Pickler.java:80)
        org.apache.spark.sql.SchemaRDD$$anonfun$javaToPython$1$$anonfun$apply$3.apply(SchemaRDD.scala:385)
        org.apache.spark.sql.SchemaRDD$$anonfun$javaToPython$1$$anonfun$apply$3.apply(SchemaRDD.scala:385)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:317)
        org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:203)
        org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:178)
        org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:178)
        org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1213)
        org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:177)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1041)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1025)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1023)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1023)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:631)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:631)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:631)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1226)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

This error persists regardless of whether the list value is empty or consists of base types or structs.",,guoxu1231,nchammas,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403754,,,Sun Jul 06 02:32:50 UTC 2014,,,,,,,,,,"0|i1xhmn:",403797,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"06/Jul/14 02:32;yhuai;Seems there is an issue when we convert java collections to python collections. I will take a look at it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON schema inference may not resolve type conflicts correctly for a field inside an array of structs,SPARK-2375,12725585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,05/Jul/14 20:06,08/Jul/14 00:06,14/Jul/23 06:26,08/Jul/14 00:06,1.0.1,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"For example, for
{code}
{""array"": [{""field"":214748364700}, {""field"":1}]}
{code}
the type of field is resolved as IntType. While, for
{code}
{""array"": [{""field"":1}, {""field"":214748364700}]}
{code}
the type of field is resolved as LongType.",,nchammas,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403744,,,2014-07-05 20:06:40.0,,,,,,,,,,"0|i1xhkf:",403787,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading from a partitioned table results in many metastore queries.,SPARK-2370,12725518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,04/Jul/14 17:24,05/Jul/14 02:19,14/Jul/23 06:26,05/Jul/14 02:18,1.0.0,1.1.0,,,,,,1.1.0,,,,,,SQL,,,,0,,,,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403677,,,2014-07-04 17:24:26.0,,,,,,,,,,"0|i1xh5j:",403720,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
newFilesOnly = true FileInputDStream processes existing files in a directory,SPARK-2362,12725399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tdas,tdas,04/Jul/14 01:25,09/Jul/14 17:47,14/Jul/23 06:26,08/Jul/14 21:31,0.9.0,0.9.1,1.0.0,,,,,0.9.2,1.0.2,1.1.0,,,,DStreams,,,,0,,,,,,,,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403559,,,Fri Jul 04 01:25:35 UTC 2014,,,,,,,,,,"0|i1xgfr:",403603,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/14 01:25;tdas;https://github.com/apache/spark/pull/1077;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in scheduler,SPARK-2353,12725194,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,mridulm80,mridulm80,03/Jul/14 07:46,22/Oct/14 22:18,14/Jul/23 06:26,22/Oct/14 22:18,1.1.0,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"I suspect the recent changes from SPARK-1937 to compute valid locality levels (and ignoring ones which are not applicable) has resulted in this issue.
Specifically, some of the code using currentLocalityIndex (and lastLaunchTime actually) seems to be assuming 
a) constant population of locality levels.
b) probably also immutablility/repeatibility of locality levels

These do not hold any longer.
I do not have the exact values for which this failure was observed (since this is from the logs of a failed job) - but the code path is suspect.

Also note that the line numbers/classes might not exactly match master since we are in the middle of a merge. But the issue should hopefully be evident.

java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.spark.scheduler.TaskSetManager.getAllowedLocalityLevel(TaskSetManager.scala:439)
	at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:388)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$5$$anonfun$apply$2.apply$mcVI$sp(TaskSchedulerImpl.scala:248)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$5.apply(TaskSchedulerImpl.scala:244)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$5.apply(TaskSchedulerImpl.scala:241)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:241)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:241)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.makeOffers(CoarseGrainedSchedulerBackend.scala:133)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:86)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


Unfortunately, we do not have the bandwidth to tackle this issue - would be great if someone could take a look at it ! Thanks.",,codingcat,joshrosen,matei,mridulm80,mubarak.seyed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2931,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403377,,,Wed Oct 22 22:18:14 UTC 2014,,,,,,,,,,"0|i1xfd3:",403428,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/14 22:18;joshrosen;This looks like a duplicate of SPARK-2931, which was fixed prior to the 1.1.0 release, so I'm resolving this as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master throws NPE,SPARK-2350,12725141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adav,andrewor14,andrewor,03/Jul/14 00:03,05/Nov/14 10:43,14/Jul/23 06:26,04/Jul/14 05:32,1.0.0,,,,,,,0.9.2,1.0.1,1.1.0,,,,Spark Core,,,,0,,,,,,"... if we launch a driver and there are more waiting drivers to be launched. This is because we remove from a list while iterating through this.

Here is the culprit from Master.scala (L487 as of the creation of this JIRA, commit bc7041a42dfa84312492ea8cae6fdeaeac4f6d1c).

{code}
for (driver <- waitingDrivers) {
  if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {
    launchDriver(worker, driver)
    waitingDrivers -= driver
  }
}
{code}",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2154,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403324,,,Fri Jul 04 05:32:23 UTC 2014,,,,,,,,,,"0|i1xf1b:",403375,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/14 00:06;andrewor;In general, if Master dies because of an exception, it automatically restarts and the exception message is hidden in the logs. In the mean time, the symptoms are not indicative of a Master having thrown an exception and restarted. It took a while for [~ilikerps] and I to find the exception as we were scrolling through the logs.;;;","03/Jul/14 00:08;andrewor;This is the root cause of SPARK-2154;;;","04/Jul/14 05:32;pwendell;Issue resolved by pull request 1289
[https://github.com/apache/spark/pull/1289];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE in ExternalAppendOnlyMap,SPARK-2349,12725128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,andrewor14,andrewor,02/Jul/14 23:12,05/Nov/14 10:43,14/Jul/23 06:26,03/Jul/14 22:42,1.0.0,,,,,,,1.0.1,1.1.0,,,,,Spark Core,,,,0,,,,,,It throws an NPE on null keys.,,andrewor,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403311,,,Thu Jul 03 22:42:38 UTC 2014,,,,,,,,,,"0|i1xeyn:",403363,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/14 22:42;ilikerps;https://github.com/apache/spark/pull/1288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Graph object can not be set to StorageLevel.MEMORY_ONLY_SER,SPARK-2347,12725093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,bxshi,bxshi,02/Jul/14 20:48,15/Jul/14 22:41,14/Jul/23 06:26,15/Jul/14 22:41,1.0.0,,,,,,,1.1.0,,,,,,GraphX,,,,0,,,,,,"I'm creating Graph object by using 

Graph(vertices, edges, null, StorageLevel.MEMORY_ONLY, StorageLevel.MEMORY_ONLY)

But that will throw out not serializable exception on both workers and driver. 

14/07/02 16:30:26 ERROR BlockManagerWorker: Exception handling buffer message
java.io.NotSerializableException: org.apache.spark.graphx.impl.VertexPartition
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
	at org.apache.spark.serializer.SerializationStream$class.writeAll(Serializer.scala:106)
	at org.apache.spark.serializer.JavaSerializationStream.writeAll(JavaSerializer.scala:30)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:988)
	at org.apache.spark.storage.BlockManager.dataSerialize(BlockManager.scala:997)
	at org.apache.spark.storage.MemoryStore.getBytes(MemoryStore.scala:102)
	at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:392)
	at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:358)
	at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90)
	at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)
	at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
	at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:662)
	at org.apache.spark.network.ConnectionManager$$anon$9.run(ConnectionManager.scala:504)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Even if the driver sometime does not throw this exception, it will throw 

java.io.FileNotFoundException: /tmp/spark-local-20140702151845-9620/2a/shuffle_2_25_3 (No such file or directory)

I know that VertexPartition not supposed to be serializable, so is there any workaround on this?",Spark standalone with 5 workers and 1 driver,ankurd,bxshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2455,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403276,,,Tue Jul 15 22:37:14 UTC 2014,,,,,,,,,,"0|i1xer3:",403329,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/14 21:58;ankurd;VertexPartition is actually supposed to be Serializable; it was an oversight not to mark it as such. A workaround is to use Kryo serialization instead of Java serialization, and I'm submitting a fix as well.;;;","15/Jul/14 22:37;ankurd;SPARK-2455 should have fixed this, so I'm closing the issue. Feel free to reopen if there's still a problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueueInputDStream with oneAtATime=false does not dequeue items,SPARK-2343,12725012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,mlaflamm,mlaflamm,02/Jul/14 16:46,09/Jul/14 22:31,14/Jul/23 06:26,09/Jul/14 22:31,0.9.0,0.9.1,1.0.0,,,,,1.0.2,1.1.0,,,,,DStreams,,,,0,,,,,,QueueInputDStream does not dequeue items when used with the oneAtATime flag disabled. The same items are reprocessed for every batch. ,,mlaflamm,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403195,,,Wed Jul 09 17:52:36 UTC 2014,,,,,,,,,,"0|i1xe9j:",403250,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/14 17:52;tdas;https://github.com/apache/spark/pull/1285;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluation helper's output type doesn't conform to input type,SPARK-2342,12724907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,yijieshen,yijieshen,02/Jul/14 08:14,03/Jul/14 20:29,14/Jul/23 06:26,03/Jul/14 20:29,1.0.0,,,,,,,1.0.1,1.1.0,,,,,SQL,,,,0,easyfix,,,,,"In sql/catalyst/org/apache/spark/sql/catalyst/expressions.scala
{code}protected final def n2 ( i: Row, e1: Expression, e2: Expression, f: ((Numeric[Any], Any, Any) => Any)): Any  {code}
is intended  to do computations for Numeric add/Minus/Multipy.
Just as the comment suggest : {quote}Those expressions are supposed to be in the same data type, and also the return type.{quote}
But in code, function f was casted to function signature:
{code}(Numeric[n.JvmType], n.JvmType, n.JvmType) => Int{code}
I thought it as a typo and the correct should be:
{code}(Numeric[n.JvmType], n.JvmType, n.JvmType) => n.JvmType{code}",,marmbrus,yijieshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403090,,,Thu Jul 03 01:50:40 UTC 2014,,,,,,,,,,"0|i1xdlr:",403145,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/14 17:15;marmbrus;This does look like a typo (though maybe one that doesn't matter due to erasure?).  That said, if you make a PR I'll certainly merge it.  Thanks!;;;","03/Jul/14 01:50;yijieshen;[~marmbrus], I fix the typo in PR: https://github.com/apache/spark/pull/1283.
Please check it, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loadLibSVMFile doesn't handle regression datasets,SPARK-2341,12724894,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,eustache,eustache,02/Jul/14 07:21,15/Jan/15 09:08,14/Jul/23 06:26,31/Jul/14 00:35,1.0.0,,,,,,,1.1.0,,,,,,MLlib,,,,0,easyfix,,,,,"Many datasets exist in LibSVM format for regression tasks [1] but currently the loadLibSVMFile primitive doesn't handle regression datasets.

More precisely, the LabelParser is either a MulticlassLabelParser or a BinaryLabelParser. What happens then is that the file is loaded but in multiclass mode : each target value is interpreted as a class name !

The fix would be to write a RegressionLabelParser which converts target values to Double and plug it into the loadLibSVMFile routine.

[1] http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html ",,apachespark,eustache,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403077,,,Thu Jul 31 00:35:14 UTC 2014,,,,,,,,,,"0|i1xdiv:",403132,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Jul/14 08:23;mengxr;Just set `multiclass = true` to load double values.;;;","02/Jul/14 08:49;eustache;I see that LabelParser with multiclass=true works for the regression
setting.

What I fail to understand is how it is related to multiclass ? Is the
naming proper ?

In any case shouldn't we provide a naming that explicitly mentions
regression ?




;;;","02/Jul/14 09:07;mengxr;It is a little awkward to have both `regression` and `multiclass` as input arguments. I agree that a correct name should be `multiclassOrRegression` or `multiclassOrContinuous`. But it is certainly too long. We tried to make this clear in the doc:

{code}
multiclass: whether the input labels contain more than two classes. If false, any label with value greater than 0.5 will be mapped to 1.0, or 0.0 otherwise. So it works for both +1/-1 and 1/0 cases. If true, the double value parsed directly from the label string will be used as the label value.
{code}

It would be good if we can improve the documentation to make it clearer. But for the API, I don't feel that it is necessary to change.
;;;","02/Jul/14 09:33;eustache;Ok then would you mind that I work on a doc improvement for this ?

Perhaps a simple no-brainer like ""for regression set this to true"" could do
the job...

Personally I think `multiclassOrRegression` is a good option but I let it
to you to decide :)
;;;","02/Jul/14 14:17;srowen;I've been a bit uncomfortable with how the MLlib API conflates categorical values and numbers, since they aren't numbers in general. Treating them as numbers is a convenience in some cases, and common in papers, but feels like suboptimal software design -- should a user have to convert categoricals to some numeric representation? To me it invites confusion, and this is one symptom. So I am not sure ""multiclass"" should mean ""parse target as double"" to begin with?

OK, it's not the issue here. But we're on the subject of an experimental API subject to change with an example of something related that could be improved along the way, and it's my #1 wish for MLlib at the moment. I'd really like to work on a change to try to accommodate classes as, say, strings at least, and not presume doubles. But I am trying to figure out if anyone agrees with that. ;;;","03/Jul/14 07:39;mengxr;[~srowen] Instead of taking string labels directly, we can provide tools to convert them to integer labels (still Double typed). LIBLINEAR/LIBSVM do not support string labels either, but they are still among the top choices for logistic regression and SVM.

[~eustache] Unfortunately, the argument name in Scala is part of the API and loadLibSVMFile is not marked as experimental. So we cannot update the argument name to `multiclassOrRegression`, which is too long anyway. Could you update the doc and change the first sentence from ""multiclass: whether the input labels contain more than two classes"" to ""multiclass: whether the input labels are continuous-valued (for regression) or contain more than two classes""? ;;;","03/Jul/14 08:42;srowen;[~mengxr] For regression, rather than further overloading ""multiclass"" to mean ""regression"", how about modifying the argument to take on three values (as an enum, string, etc.) to distinguish the three modes. The current method would stay, but be deprecated.

multiclass=false is for binary classification. libsvm uses ""0"" and ""1"" (or any ints) for binary classification. But this parses it as a real number, and rounds to 0/1. (Is that was libsvm does?) Maybe it's a convenient semantic overload when you want to transform a continuous value to a 0/1 indicator, but is that implied by libsvm format or just a transformation the caller should make? multiclass=true treats libsvm integer labels as doubles, but not continuous values. It seems like inviting more confusion to have this mode also double as the mode for parsing labels that are continuous values as continuous values.

libsvm is widely used but it's old; I don't think it's file format from long ago should necessarily inform API design now. There are other serializations besides libsvm (plain CSV for instance) and other algorithms (random decision forests).

You can make utilities to convert classes to numbers for benefit of the implementation on the front, and I'll have to in order to use this. Maybe we can start there -- at least if a utility is in the project people aren't all reinventing this in order to use an SVM with actual labels. The caller carries around a dictionary then to do the reverse mapping. The model seems like the place to hold that info, if in fact internally it converts classes to some other representation. Maybe the need would be clearer once the utility is created.

As you say I'm concerned that the API is already locked down early and some of these changes are going to be viewed as infeasible just for that reason.;;;","16/Jul/14 05:04;mengxr;[~srowen] Using enum or string sounds good. As you already knew, using string may be better because of Python.

Rounding is used because people use either +1/-1 or 1/0 for binary classification in LIBSVM and we require 1/0 in MLlib. Actually the +1/-1 is the only corner case I wanted to cover when multiclass=false. We added LIBSVM support because there are many commonly used datasets we can download from LIBSVM/LIBLINEAR website and other places. It is easier for people to test MLlib's algorithms.

It would be nice if you have free cycles to implement a method that convert classes to numbers. For the long term, I'm thinking about for each dataset, we can attach metadata that contains feature names, feature types, number of non-zeros, and for every categorical feature we have a value <-> {0, 1, ...} map.;;;","16/Jul/14 12:12;srowen;OK is it worth a pull request for changing the boolean multiclass argument to a string? I wanted to ask if that was your intent before I do that.

libsvm format support is certainly important. It happens to have to encode non-numeric input as numbers. It need not be that way throughout MLlib, since it isn't that way in other input formats. (In this API method, it's pretty minor, since libsvm does by definition use this encoding.) So yes that would be great if data sets or API objects didn't assume that categorical data was numeric, but encoded type in the data set or even in the object model itself. I think it's mostly a design and type-safety argument -- same reason we have String instead of just byte[] everywhere.

Sure I will have to build this conversion at some point anyway and can share the result then.;;;","18/Jul/14 11:25;srowen;[~mengxr] Here is an example of changing the argument:
https://github.com/srowen/spark/commit/4a584ff9c0ada3d035d4668ecf22ec0e65ed16b6

I won't open a PR yet. I think this is a better API at this point, but the question is more whether the weight of deprecated methods are worth it or not. Another data point to keep in mind regarding how APIs can evolve.;;;","29/Jul/14 17:26;mengxr;[~srowen] For the doc in your version:

{code}
If ""multiclass"", the numeric value parsed directly from the label string will be used as the label value.
If ""continuous"", the double value parsed directly from the string will be used as the label.
{code}

Would user feel confused since the two lines are essentially the same?

Another possible solution is that we parse the labels into doubles and remove the `multiclass` argument. Users can perform a map to transform the labels into binary 0/1 if needed.;;;","29/Jul/14 17:33;srowen;To me, it's less confusing than writing ""multiclass"" for a regression problem. Yes I also think it could be simpler to remove multiclass; the idea I suppose is that binary is merely a special case of that, and the caller can write the required transformation to 0/1 if needed. At least the caller is aware of the transformation and I think that's good. At least, there you just let numbers be numbers and let downstream code figure out whether the number is a continuous value, or the number is a category.;;;","30/Jul/14 05:48;mengxr;That sounds good. Do you mind creating a PR? We can deprecate the existing ones with `multiclass` and add a warning in the doc about the +1/-1 case.;;;","30/Jul/14 18:26;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1663;;;","31/Jul/14 00:35;mengxr;Issue resolved by pull request 1663
[https://github.com/apache/spark/pull/1663];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve paths properly for event logging / history server,SPARK-2340,12724839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor,01/Jul/14 23:06,05/Nov/14 10:45,14/Jul/23 06:26,31/Jul/14 04:58,1.0.1,,,,,,,1.1.0,,,,,,Spark Core,,,,0,,,,,,"We resolved paths for --jars and --files for spark submit when releasing Spark 1.0. The path that the History Server takes in also has the same issue on standalone mode, for example. By default, it tries to read from HDFS, but we should resolve all relative paths to be prefixed by ""file:/"".",,andrewor,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403022,,,Thu Jul 31 04:58:33 UTC 2014,,,,,,,,,,"0|i1xd6v:",403078,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/14 21:42;andrewor;https://github.com/apache/spark/pull/1280;;;","31/Jul/14 04:58;pwendell;Fixed by https://github.com/apache/spark/pull/1280/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL parser in sql-core is case sensitive, but a table alias is converted to lower case when we create Subquery",SPARK-2339,12724818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,01/Jul/14 20:59,12/Oct/15 03:14,14/Jul/23 06:26,08/Jul/14 00:02,1.0.0,,,,,,,1.0.2,1.1.0,,,,,SQL,,,,0,,,,,,"Reported by http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-Join-throws-exception-td8599.html

After we get the table from the catalog, because the table has an alias, we will temporarily insert a Subquery. Then, we convert the table alias to lower case no matter if the parser is case sensitive or not.

To see the issue ...
{code}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.createSchemaRDD

case class Person(name: String, age: Int)

val people = sc.textFile(""examples/src/main/resources/people.txt"").map(_.split("","")).map(p => Person(p(0), p(1).trim.toInt))
people.registerAsTable(""people"")

sqlContext.sql(""select PEOPLE.name from people PEOPLE"")
{code}

The plan is ...
{code}
== Query Plan ==
Project ['PEOPLE.name]
 ExistingRdd [name#0,age#1], MapPartitionsRDD[4] at mapPartitions at basicOperators.scala:176
{code}

You can find that ""PEOPLE.name"" is not resolved.
",,huaxing,nchammas,smolav,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,403001,,,Mon Oct 12 03:14:47 UTC 2015,,,,,,,,,,"0|i1xd27:",403057,,,,,,,,,,,,,,1.1.0,,,,,,,,,,,"02/Jul/14 21:22;yhuai;Also, names of those registered tables are case-sensitive. But, names of Hive tables are case-insensitive. It may cause confusion when a user is using HiveContext. I guess we want to keep registered tables case-sensitive. I will add doc to registerAsTable and registerRDDAaTable.;;;","11/Oct/15 21:03;huaxing;Hi Yin,
I am looking at jira 10754 (https://issues.apache.org/jira/browse/SPARK-10754) and it complains that the table name and column name are case sensitive. 
Did you already add doc to RegisterXXXTable that the table names are case sensitive? If not, I will probably add one. Thanks a lot!!  

;;;","12/Oct/15 03:14;yhuai;[~huaxing] By default, in SQLContext, we will do resolution in a case-sensitive way (you can use spark.sql.caseSensitive to control the behavior). In HiveContext, the default behavior is case-insensitive resolution (so, we will resolve those registered tables in a case-insensitive way). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jenkins Spark-Master-Maven-with-YARN builds failing due to test misconfiguration,SPARK-2338,12724795,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pmackinn,pmackinn,pmackinn,01/Jul/14 19:16,09/Jul/14 16:55,14/Jul/23 06:26,09/Jul/14 11:36,1.0.0,,,,,,,,,,,,,Build,Project Infra,YARN,,0,hadoop2,jenkins,maven,protobuf,yarn,"https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/hadoop.version=2.2.0,label=centos/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/hadoop.version=2.3.0,label=centos/

These builds are currently failing due to the builder configuration being incomplete. After building, they specify the test command as:
{noformat}
/home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn -Dhadoop.version=2.3.0 -Dlabel=centos test -Pyarn -Phive
{noformat}

However, it is not enough to specify the hadoop.version, the tests should instead be run using the hadoop-2.2 and hadoop-2.3 profiles respectively. 

For example:
{noformat}
/home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn -Phadoop2.2 -Dlabel=centos test -Pyarn -Phive
{noformat}

These profiles will not only set the appropriate hadoop.version but also set the version of protobuf-java required by yarn (2.5.0). Without the correct profile set, the test run fails at:

{noformat}
*** RUN ABORTED ***
  java.lang.VerifyError: class org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
{noformat}

since it is getting the default version of protobuf-java (2.4.1) which has the old incompatible version of getUnknownFields.",https://amplab.cs.berkeley.edu/jenkins,pmackinn,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2232,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402978,,,Wed Jul 09 16:55:46 UTC 2014,,,,,,,,,,"0|i1xcxz:",403038,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/14 11:36;pwendell;Thanks a ton for getting to the bottom of this. I was super confused why the tests were so messed up even though this seems totally obvious in retrospect.

I went ahead and updated the build configuration. There are some failing tests in MLLib in maven, I'll try to track those down as well to get this all green.;;;","09/Jul/14 16:55;pmackinn;Patrick, it appears https://github.com/apache/spark/pull/1343 fixes/addresses those mllib DecisionTreeSuite failures. I just got all green in my local tests with it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attribute Error calling PipelinedRDD.id() in pyspark,SPARK-2334,12724762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dcarroll@cloudera.com,dcarroll@cloudera.com,01/Jul/14 16:21,06/Sep/14 23:12,14/Jul/23 06:26,06/Sep/14 23:12,1.0.0,1.1.0,,,,,,1.2.0,,,,,,PySpark,,,,0,,,,,,"calling the id() function of a PipelinedRDD causes an error in PySpark.  (Works fine in Scala.)

The second id() call here fails, the first works:
{code}
r1 = sc.parallelize([1,2,3])
r1.id()
r2=r1.map(lambda i: i+1)
r2.id()
{code}

Error:

{code}
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-31-a0cf66fcf645> in <module>()
----> 1 r2.id()

/usr/lib/spark/python/pyspark/rdd.py in id(self)
    180         A unique ID for this RDD (within its SparkContext).
    181         """"""
--> 182         return self._id
    183 
    184     def __repr__(self):

AttributeError: 'PipelinedRDD' object has no attribute '_id'
{code}",,apachespark,dcarroll@cloudera.com,joshrosen,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3105,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,402945,,,Sat Sep 06 23:12:42 UTC 2014,,,,,,,,,,"0|i1xcqn:",403005,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/14 01:41;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2296;;;","06/Sep/14 23:12;joshrosen;Issue resolved by pull request 2296
[https://github.com/apache/spark/pull/2296];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
