Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Outward issue link (Required),Inward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
StorageTool help specifies user as parameter not name,KAFKA-15114,13541046,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,pprovenzano,pprovenzano,pprovenzano,22/Jun/23 14:41,05/Jul/23 16:45,13/Jul/23 09:17,05/Jul/23 16:45,3.5.0,,,,,,,,,,,,,,,3.5.1,3.6.0,,,,,,kraft,,,,,,0,,,,,,"StorageTool help message current specifies setting a {{user}} parameter when creating a SCRAM record for bootstrap.

The StorageTool parses and only accepts the parameter as {{name}} and so the help message is wrong.

The choice of using {{name}} vs. {{user}} as a parameter is because internally the record uses name, all tests using the StorageTool use name as a parameter, KafkaPrincipals are created with {{name}} and because creating SCRAM credentials is done with {{--entity-name}}

I will change the help to specify {{name}} instead of {{user}}.


 ",,pprovenzano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-22 14:41:12.0,,,,,,,,,,"0|z1ipyw:",9223372036854775807,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISR shrink/expand issues on ZK brokers during migration,KAFKA-15109,13540787,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,mumrah,mumrah,mumrah,20/Jun/23 16:00,27/Jun/23 15:49,13/Jul/23 09:17,22/Jun/23 21:00,3.6.0,,,,,,,,,,,,,,,3.6.0,,,,,,,kraft,replication,,,,,0,,,,,,"KAFKA-15021 introduced a new controller behavior that avoids increasing the leader epoch during the controlled shutdown scenario. This prevents some unnecessary thrashing of metadata and threads on the brokers and clients. 

While a cluster is in a KIP-866 migration and has a KRaft controller with ZK brokers, we cannot employ this leader epoch bump avoidance. The ZK brokers must have the leader epoch bump in order for ReplicaManager to react to the LeaderAndIsrRequest.",,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-20 16:00:09.0,,,,,,,,,,"0|z1iodk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft migration does not proceed and broker dies if authorizer.class.name is set,KAFKA-15098,13540455,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,rndgstn,rndgstn,16/Jun/23 21:18,22/Jun/23 21:00,13/Jul/23 09:17,22/Jun/23 21:00,3.5.0,,,,,,,,,,,,,,,3.5.1,3.6.0,,,,,,kraft,,,,,,1,,,,,,"[ERROR] 2023-06-16 20:14:14,298 [main] kafka.Kafka$ - Exiting Kafka due to fatal exception
java.lang.IllegalArgumentException: requirement failed: ZooKeeper migration does not yet support authorizers. Remove authorizer.class.name before performing a migration.
",,rndgstn,tvainika,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-16 21:18:04.0,,,,,,,,,,"0|z1imcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE 2023-34455 - Vulnerability identified with Apache kafka,KAFKA-15096,13540430,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,manyanda,Sasikumarms,Sasikumarms,16/Jun/23 15:39,07/Jul/23 13:26,13/Jul/23 09:17,19/Jun/23 08:40,3.3.0,3.3.1,3.3.2,3.4.0,3.4.1,3.5.0,,,,,,,,,,3.3.3,3.4.2,3.5.1,3.6.0,,,,,,,,,,0,,,,,,"A new vulnerability CVE-2023-34455 is identified with apache kafka dependency. The vulnerability is coming from snappy-java:1.1.8.4

Version 1.1.10.1 contains a patch for this issue. Please upgrade the snappy-java version to fix this issue

 
snappy-java is a fast compressor/decompressor for Java. Due to use of an unchecked chunk length, an unrecoverable fatal error can occur in versions prior to 1.1.10.1.
The code in the function hasNextChunk in the fileSnappyInputStream.java checks if a given stream has more chunks to read. It does that by attempting to read 4 bytes. If it wasn’t possible to read the 4 bytes, the function returns false. Otherwise, if 4 bytes were available, the code treats them as the length of the next chunk.
In the case that the `compressed` variable is null, a byte array is allocated with the size given by the input data. Since the code doesn’t test the legality of the `chunkSize` variable, it is possible to pass a negative number (such as 0xFFFFFFFF which is -1), which will cause the code to raise a `java.lang.NegativeArraySizeException` exception. A worse case would happen when passing a huge positive value (such as 0x7FFFFFFF), which would raise the fatal `java.lang.OutOfMemoryError` error.",,jlprat,manyanda,Sasikumarms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 10:52:07 UTC 2023,,,,,,,,,,"0|z1im7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 16:02;manyanda;Thank you for reporting the issue [~Sasikumarms] an PR has been opened in 
[https://github.com/apache/kafka/pull/13865]
to bump the version. 
Once merged, I'll let the release managers determine how far the fix can be backported. ;;;","19/Jun/23 10:52;jlprat;I cherry-picked this to 3.3, 3.4 and 3.5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetcher's lag never set when partition is idle,KAFKA-15080,13539612,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,12/Jun/23 07:55,13/Jun/23 13:26,13/Jul/23 09:17,13/Jun/23 13:26,,,,,,,,,,,,,,,,3.5.1,3.6.0,,,,,,,,,,,,0,,,,,,"The PartitionFetchState's lag field is set to None when the state is created and it is updated when bytes are received for a partition. For idle partitions (newly created or not), the lag is never updated because `validBytes > 0` is never true. As a side effect, the partition is considered out-of-sync and could be incorrectly throttled.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-12 07:55:24.0,,,,,,,,,,"0|z1ih5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileTokenRetriever doesn't trim the token before returning it.,KAFKA-15077,13539449,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,smjn,sushmahajn,sushmahajn,09/Jun/23 14:18,11/Jun/23 06:23,13/Jul/23 09:17,11/Jun/23 06:23,,,,,,,,,,,,,,,,3.6.0,,,,,,,clients,,,,,,0,,,,,,"The {{FileTokenRetriever}} class is used to read the access_token from a file on the clients system and then the info is passed along with jaas config to the {{{}OAuthBearerSaslServer{}}}.

The server uses the class {{OAuthBearerClientInitialResponse}} to validate the token format.

In case the token was sent using {{FileTokenRetriever}} on the client side, some EOL character is getting appended to the token, causing authentication to fail with the message (in case to topic create):
 {{ERROR org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed during authentication due to invalid credentials with SASL mechanism OAUTHBEARER}}
 
On the server side the following line [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java#L68] with throw an exception failing the request.",,sushmahajn,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-09 14:18:22.0,,,,,,,,,,"0|z1ig5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exactly-once source tasks fail to start during pending rebalances,KAFKA-15059,13538830,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,05/Jun/23 21:38,11/Jul/23 14:23,13/Jul/23 09:17,21/Jun/23 08:58,3.6.0,,,,,,,,,,,,,,,3.6.0,,,,,,,KafkaConnect,mirrormaker,,,,,1,,,,,,"When asked to perform a round of zombie fencing, the distributed herder will [reject the request|https://github.com/apache/kafka/blob/17fd30e6b457f097f6a524b516eca1a6a74a9144/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1249-L1250] if a rebalance is pending, which can happen if (among other things) a config for a new connector or a new set of task configs has been recently read from the config topic.

Normally this can be alleviated with a simple task restart, which isn't great but isn't terrible.

However, when running MirrorMaker 2 in dedicated mode, there is no API to restart failed tasks, and it can be more common to see this kind of failure on a fresh cluster because three connector configurations are written in rapid succession to the config topic.

 

In order to provide a better experience for users of both vanilla Kafka Connect and dedicated MirrorMaker 2 clusters, we can retry (likely with the same exponential backoff introduced with KAFKA-14732) zombie fencing attempts that fail due to a pending rebalance.",,ChrisEgerton,divijvaidya,viktorsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14718,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 14:23:10 UTC 2023,,,,,,,,,,"0|z1icbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 12:04;ChrisEgerton;On second thought, it may be unnecessary to check for a pending rebalance at all.

 

If the worker that we forward the zombie fencing request to is a zombie leader (i.e., a worker that believes it is the leader but in reality is not), it will fail to finish the round of zombie fencing because it won't be able to write to the config topic with a transactional producer.

If the connector has just been deleted, we'll still fail the request since we force a read-to-end of the config topic and refresh our snapshot of its contents before checking to see if the connector exists.

And regardless, the worker that owns the task will still do a read-to-end of the config topic and verify that (1) no new task configs have been generated for the connector and (2) the worker is still assigned the connector, before allowing the task to process any data.;;;","21/Jun/23 08:58;viktorsomogyi;[~ChrisEgerton] since the PR is merged, I resolve this ticket.;;;","21/Jun/23 13:06;ChrisEgerton;Ah, thanks Viktor!;;;","11/Jul/23 09:11;divijvaidya;[~ChrisEgerton] Is this a backport candidate for 3.5.1?;;;","11/Jul/23 14:23;ChrisEgerton;[~divijvaidya] no; this was introduced by [https://github.com/apache/kafka/pull/13465]. It did not affect anything except trunk and never made it into a release (see the affected/fix versions fields).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression for security.protocol validation starting from 3.3.0,KAFKA-15053,13538657,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dlgaobo,dlgaobo,dlgaobo,02/Jun/23 23:31,29/Jun/23 17:24,13/Jul/23 09:17,29/Jun/23 17:17,3.3.0,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.1,3.6.0,,,,clients,,,,,,0,backport,,,,,"[This|https://issues.apache.org/jira/browse/KAFKA-13793] Jira issue introduced validations on multiple configs. As a consequence, config {{security.protocol}} now only allows upper case values such as PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL. Before this change, lower case values like sasl_ssl, ssl are also supported, there's even a case insensitive logic inside [SecurityProtocol|https://github.com/apache/kafka/blob/146a6976aed0d9f90c70b6f21dca8b887cc34e71/clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProtocol.java#L70-L73] to handle the lower case values.

I think we should treat this as a regression bug since we don't support lower case values anymore since 3.3.0. For versions later than 3.3.0, we are getting error like this when using lower case value sasl_ssl

{{Invalid value sasl_ssl for configuration security.protocol: String must be one of: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL}}",,ChrisEgerton,divijvaidya,dlgaobo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 17:16:54 UTC 2023,,,,,,,,,,"0|z1ib9c:",9223372036854775807,,divijvaidya,,,,,,,,,,,,,,,,,,"05/Jun/23 13:29;ChrisEgerton;Thanks [~dlgaobo]. I agree that this is a regression and we should re-introduce case-insensitive support for this this property.

Are you interested in providing a patch for this?;;;","06/Jun/23 01:23;dlgaobo;Hi [~ChrisEgerton], sure, I can do a fix for that! ;;;","08/Jun/23 23:44;dlgaobo;Hi [~ChrisEgerton] , just created a pull request for the fix [https://github.com/apache/kafka/pull/13831].

I also have a quick question for the release process: imagine this fix is merged and released, to consume this fix, do I need to upgrade to the latest Kafka version? Or is there a way to include this fix in a minor release? Thanks!;;;","21/Jun/23 23:46;dlgaobo;Hi [~ChrisEgerton] , quick question for the fix of this ticket: can we include it into the 3.3.x minor release as well or we have already stopped doing minor release for 3.3.x and the only way is to make it in 3.5.1?;;;","22/Jun/23 14:36;ChrisEgerton;[~dlgaobo] we can probably backport this back to 3.3.x, 3.4.x, and 3.5.x. I'm not certain there will be another 3.3.x release but if there is, we can include this fix in it (assuming it's merged by then, which is looking quite likely).;;;","22/Jun/23 16:46;dlgaobo;Thanks [~ChrisEgerton]! It would be great if we can backport this to 3.3.0! Also, if we do so, is there anything that the client needs to do to consume this change or the change would be there automatically?;;;","23/Jun/23 15:30;ChrisEgerton;[~dlgaobo] I think that may have been a typo, but just to be extra clear–we will never backport this to 3.3.0, which has already been released. We may backport it to the 3.3 branch, which will then cause the change to be included if we do another 3.3.x release (right now, that would be 3.3.3).

 

Assuming you're asking about 3.3.x in general and not 3.3.0 specifically, then yes, I believe that once they've upgraded to the new version (e.g., 3.3.3), there's nothing else users will have to do to benefit from this fix. However, again, I'm not certain there will be another 3.3.x release, so they may have to upgrade to a 3.4.x version to get this fix.;;;","29/Jun/23 17:16;divijvaidya;This has been backported to 3.5.x, 3.4.x and 3.3.x with the following commits:
[https://github.com/apache/kafka/commit/15ce855fc117dbc6ddbc87888be8e91de025c73a]
[https://github.com/apache/kafka/commit/a25cc752a42780a9c042e8a556c393dfc9ab8a21] 
[https://github.com/apache/kafka/commit/041afb73ecb081592cd2f9dcf7964545df7c606b] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy v.1.1.9.1 NoClassDefFound on ARM machines,KAFKA-15044,13538303,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,david.mao,david.mao,david.mao,31/May/23 17:01,01/Jun/23 09:47,13/Jul/23 09:17,31/May/23 21:52,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,We upgraded our snappy dependency but v1.1.9.1 has compatibility issues with arm. We should upgrade to v1.1.10.0 which resolves this issue.,,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-31 17:01:51.0,,,,,,,,,,"0|z1i93c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft controller increases leader epoch when shrinking ISR,KAFKA-15021,13537681,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jsancio,jsancio,jsancio,25/May/23 14:55,09/Jun/23 11:35,13/Jul/23 09:17,09/Jun/23 11:35,,,,,,,,,,,,,,,,3.6.0,,,,,,,controller,kraft,,,,,0,,,,,,"When the KRaft controller shrinks the ISR it also forces the leader epoch to increase. This is unnecessary and cases all of the follower replica fetches to get invalidated.

Here is an example trace of this behavior after replica 8 was shutdown:
{code:java}
kafka-dump-log --cluster-metadata-decoder --files __cluster_metadata-0/00000000000038589501.log | grep Pd7wMb4lSkKI00--SrWNXw
...
| offset: 38655592 CreateTime: 1683849857362 keySize: -1 valueSize: 41 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":7,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[3,1],""leader"":1}}
| offset: 38655593 CreateTime: 1683849857362 keySize: -1 valueSize: 41 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":5,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,4],""leader"":4}}
| offset: 38655594 CreateTime: 1683849857362 keySize: -1 valueSize: 41 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":6,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,1],""leader"":0}}
| offset: 38656159 CreateTime: 1683849974945 keySize: -1 valueSize: 39 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":7,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[3,1,8]}}
| offset: 38656256 CreateTime: 1683849994297 keySize: -1 valueSize: 39 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":5,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,4,8]}}
| offset: 38656299 CreateTime: 1683849997139 keySize: -1 valueSize: 39 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":6,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,1,8]}}
| offset: 38657003 CreateTime: 1683850157379 keySize: -1 valueSize: 30 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":7,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""leader"":8}} {code}
Also, notice how the leader epoch was not increased when the ISR was expanded.",,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-25 14:55:05.0,,,,,,,,,,"0|z1i59k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve handling of broker heartbeat timeouts,KAFKA-15019,13537560,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,24/May/23 23:03,05/Jun/23 21:08,13/Jul/23 09:17,02/Jun/23 14:59,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,Improve handling of overload situations in the KRaft controller,,cmccabe,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 21:08:49 UTC 2023,,,,,,,,,,"0|z1i4iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 14:59;mimaison;Since the PR is merged I'm marking this as resolved in 3.5.0

If there is follow up work left to do, feel free to reopen (and adjust Fix Version/s) or create a new ticket.;;;","05/Jun/23 21:08;cmccabe;Thanks, [~mimaison].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New ClientQuotas are not written to ZK from snapshot ,KAFKA-15017,13537354,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,pprovenzano,mumrah,mumrah,23/May/23 14:39,01/Jun/23 14:52,13/Jul/23 09:17,01/Jun/23 14:51,3.5.0,,,,,,,,,,,,,,,,,,,,,,kraft,,,,,,0,,,,,,Similar issue to KAFKA-15009,,mumrah,pprovenzano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 14:52:03 UTC 2023,,,,,,,,,,"0|z1i394:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 15:02;pprovenzano;Client Quotas and SCRAM both suffer from this issue as they are intermixed on ZK.;;;","01/Jun/23 14:52;pprovenzano;Fix is on 3.5 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LICENSE-binary file contains dependencies not included anymore,KAFKA-15016,13537198,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,mimaison,mimaison,22/May/23 16:00,24/May/23 15:34,13/Jul/23 09:17,24/May/23 15:34,,,,,,,,,,,,,,,,3.6.0,,,,,,,,,,,,,0,,,,,,While adjusting LICENSE-binary for 3.5.0 I noticed a few entries are not dependencies anymore. We should resync the file properly,,divijvaidya,mimaison,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-22 16:00:12.0,,,,,,,,,,"0|z1i2ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Binaries contain 2 versions of reload4j,KAFKA-15015,13537196,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,atusharm,mimaison,mimaison,22/May/23 15:56,24/May/23 01:34,13/Jul/23 09:17,24/May/23 01:34,3.4.1,3.5.0,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,,,,,,"These releases ship 2 versions of reload4j:
- reload4j-1.2.19.jar
- reload4j-1.2.25.jar",,atusharm,mimaison,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 12:27:50 UTC 2023,,,,,,,,,,"0|z1i2a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 04:59;atusharm;Hi, [~mimaison] can i take this?;;;","23/May/23 06:13;showuon;[~atusharm] , yes, you can take it. But since we're going to release v3.4.1/v3.5.0, it would be great if you can fix it soon (maybe today?). Let us know if you found anything. Thank you.;;;","23/May/23 07:52;mimaison;I don't think it's a blocker for 3.4.1 or 3.5.0 so we don't necessarily need the fix immediately.;;;","23/May/23 12:27;atusharm;[~mimaison] [~showuon] raised PR [GitHub Pull Request #13745|https://github.com/apache/kafka/pull/13745];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonConverter fails when there are leading Zeros in a field,KAFKA-15012,13537176,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yash.mayya,ranjanrao,ranjanrao,22/May/23 14:23,02/Jun/23 15:20,13/Jul/23 09:17,02/Jun/23 15:20,3.3.2,3.4.0,,,,,,,,,,,,,,3.6.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"When there are leading zeros in a field in the Kakfa Record, a sink connector using JsonConverter fails with the below exception

 
{code:java}
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:206)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:474)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error: 
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:324)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertKey(WorkerSinkTask.java:531)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	... 13 more
Caused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Invalid numeric value: Leading zeroes not allowed
 at [Source: (byte[])""00080153032837""; line: 1, column: 2]
Caused by: com.fasterxml.jackson.core.JsonParseException: Invalid numeric value: Leading zeroes not allowed
 at [Source: (byte[])""00080153032837""; line: 1, column: 2]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1840)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:712)
	at com.fasterxml.jackson.core.base.ParserMinimalBase.reportInvalidNumber(ParserMinimalBase.java:551)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._verifyNoLeadingZeroes(UTF8StreamJsonParser.java:1520)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parsePosNumber(UTF8StreamJsonParser.java:1372)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:855)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:754)
	at com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4247)
	at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:2734)
	at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:64)
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:322)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertKey(WorkerSinkTask.java:531)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:474)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}
 

 

To resolve the issue, we need to add the below line when in the JsonSerializer.java and JsonDeserializer.java classes.
{code:java}
objectMapper.enable(JsonReadFeature.ALLOWLEADINGZEROSFORNUMBERS.mappedFeature()); {code}
Attaching a patch file showing the changes here. 

 ",,ChrisEgerton,ranjanrao,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-15008,,,,,,"22/May/23 14:23;ranjanrao;enable_ALLOW_LEADING_ZEROS_FOR_NUMBERS_in_jackson_object_mapper_.patch;https://issues.apache.org/jira/secure/attachment/13058414/enable_ALLOW_LEADING_ZEROS_FOR_NUMBERS_in_jackson_object_mapper_.patch",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 18:33:30 UTC 2023,,,,,,,,,,"0|z1i25k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 06:12;yash.mayya;Thanks for filing this Jira [~ranjanrao]. Simply enabling the _ALLOWLEADINGZEROSFORNUMBERS_ feature would likely be a backward incompatible change since there could potentially be users relying on the existing behavior to send bad data (i.e. some numeric field has leading zeroes when it isn't expected to) to a DLQ topic. This might need a small KIP adding a new config (maybe {_}allow.leading.zeroes.for.numbers{_}) to the JsonConverter which defaults to false in order to be backward compatible. Alternatively, we could design a way to allow users to configure the various [JsonReadFeature|https://fasterxml.github.io/jackson-core/javadoc/2.10/com/fasterxml/jackson/core/json/JsonReadFeature.html]s and [JsonWriteFeature|https://fasterxml.github.io/jackson-core/javadoc/2.10/com/fasterxml/jackson/core/json/JsonWriteFeature.html]s for the JsonSerializer / JsonDeserializer used in the JsonConverter.;;;","30/May/23 17:17;ChrisEgerton;[~yash.mayya] I'm wondering if compatibility is necessarily a concern here. The logic this ticket is concerned with is deserialization of data already present in Kafka, and converting it to the Kafka Connect format for use by a sink connector. There will be nothing different about the data given to those connectors depending on whether the JSON in the upstream Kafka record's key/value contains a leading zero or not.

As a result, it's hard to think of a reason for those records in Kafka to qualify as ""invalid"" and for there to be any reason for them to land in the DLQ besides there being a bug in the converter.

If we agree that this is a bug in the converter, then even if it causes records to be sent to the DLQ, that is a result of the DLQ mechanism being a catch-all for errors–expected or unexpected–that occur in the connector's data pipeline, and fixing those errors should not be considered a breaking change as long as they do not lead to unexpected behavior in the connector (which, in this case, should be fine).

 

[~ranjanrao] Thanks for filing this ticket. We don't accept patch requests over Jira, but if you'd like to submit a pull request on GitHub (preferably with a unit test added to verify behavior and prevent regression), I'd be happy to review (as long as the discussion on compatibility brought up by Yash can be addressed).

 

I'll also note that the idea of a general-purpose KIP to allow users to configure arbitrary features for the JsonConverter's underlying (de)serializers is fascinating and may be worth pursuing if there are other valuable use cases (perhaps skipping over comments could be useful, for example?).;;;","30/May/23 18:33;yash.mayya;Thanks [~ChrisEgerton], that's a fair point. I was comparing this to https://issues.apache.org/jira/browse/KAFKA-8713 but your point regarding there being nothing different about the data given to the sink connectors post conversion here makes sense and causes this case to be different from [KIP-581|https://cwiki.apache.org/confluence/display/KAFKA/KIP-581%3A+Value+of+optional+null+field+which+has+default+value]. I agree that this qualifies as a bug in the converter so I think we should go ahead with the proposed fix here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft Controller doesn't reconcile with Zookeeper metadata upon becoming new controller while in dual write mode.,KAFKA-15010,13536920,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,davidarthur,akhileshchg,akhileshchg,19/May/23 23:19,02/Jun/23 14:47,13/Jul/23 09:17,02/Jun/23 14:47,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,kraft,,,,,,0,,,,,,"When a KRaft controller fails over, the existing migration driver (in dual write mode) can fail in between Zookeeper writes and may leave Zookeeper with incomplete and inconsistent data. So when a new controller becomes active (and by extension new migration driver becomes active), this first thing we should do is load the in-memory snapshot and use it to write metadata to Zookeeper to have a steady state. We currently do not do this and it may leave Zookeeper in inconsistent state.",,akhileshchg,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 14:47:19 UTC 2023,,,,,,,,,,"0|z1i0ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 14:47;mimaison;Since the PR is merged I'm marking this as resolved.

If there is follow up work left to do, feel free to reopen (and adjust Fix Version/s) or create a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New ACLs are not written to ZK during migration,KAFKA-15009,13536918,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,19/May/23 22:52,24/May/23 22:06,13/Jul/23 09:17,24/May/23 22:06,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,kraft,,,,,,0,kraft,migration,,,,"While handling snapshots in dual-write mode, we are missing the logic to detect new ACLs created in KRaft. This means we will not write these new ACLs back to ZK and they would be missing if a user rolled back their cluster to ZK mode. ",,akhileshchg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-19 22:52:21.0,,,,,,,,,,"0|z1i0k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MV is not set correctly in the MetadataPropagator in migration.,KAFKA-15007,13536769,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,18/May/23 17:20,24/May/23 22:07,13/Jul/23 09:17,24/May/23 22:07,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"MV changes are not set in propagator unless we're in DUAL_WRITE mode. But we do this, we'll skip any known MV changes. The propagator should always know the correct MV so that it sends correct UMR and LISR during DUAL_WRITE.",,akhileshchg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 18:42:22 UTC 2023,,,,,,,,,,"0|z1hznc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 18:42;akhileshchg;PR: https://github.com/apache/kafka/pull/13732;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic config changes are not synced during zk to kraft migration (dual-write),KAFKA-15004,13536598,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,17/May/23 16:32,02/Jun/23 14:59,13/Jul/23 09:17,02/Jun/23 14:59,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,,,akhileshchg,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 14:58:28 UTC 2023,,,,,,,,,,"0|z1hylc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 14:58;mimaison;Since the PR is merged I'm marking this as resolved in 3.5.0

If there is follow up work left to do, feel free to reopen (and adjust Fix Version/s) or create a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicIdReplicaAssignment is not updated in migration (dual-write) when partitions are changed for topic,KAFKA-15003,13536594,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,17/May/23 16:31,02/Jun/23 14:58,13/Jul/23 09:17,02/Jun/23 14:58,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,kraft,,,,,,0,,,,,,,,akhileshchg,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 14:58:03 UTC 2023,,,,,,,,,,"0|z1hykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 12:54;mimaison;[~akhileshchg] Can you share more details? It would help me triage whether this is indeed a blocker for 3.5. Are you working on a fix?;;;","19/May/23 18:34;akhileshchg;Hi, I raised a PR for the issue here: https://github.com/apache/kafka/pull/13735

In DUAL_WRITE, the current topic metadata replication to Zookeeper is incorrect. When we add new partitions to the topic, we don't update them in Zookeeper. Also, we do not account for partial failures of the Zookeeper writes during Controller failovers. So fixed that as well.;;;","19/May/23 18:34;akhileshchg;[~mimaison] I gave more details. Please see if it makes sense to you.;;;","22/May/23 14:11;mimaison;Removing this from 3.5.0 (at least for now) so I can build 3.5.0 RC0;;;","02/Jun/23 14:58;mimaison;Since the PR is merged I'm marking this as resolved in 3.5.0

If there is follow up work left to do, feel free to reopen (and adjust Fix Version/s) or create a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JmxToolTest failing with initializationError,KAFKA-14997,13536074,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fvaleri,sagarrao,sagarrao,13/May/23 12:59,16/May/23 02:31,13/Jul/23 09:17,16/May/23 02:31,,,,,,,,,,,,,,,,3.6.0,,,,,,,,,,,,,0,flaky-test,,,,,"Noticed that JmxToolTest fails with 

```
h4. Error
java.io.IOException: Cannot bind to URL [rmi://:44743/jmxrmi]: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)]
h4. Stacktrace
java.io.IOException: Cannot bind to URL [rmi://:44743/jmxrmi]: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)]
 at javax.management.remote.rmi.RMIConnectorServer.newIOException(RMIConnectorServer.java:827)
 at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:432)
 at org.apache.kafka.tools.JmxToolTest.startJmxAgent(JmxToolTest.java:337)
 at org.apache.kafka.tools.JmxToolTest.beforeAll(JmxToolTest.java:55)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeAllMethod(TimeoutExtension.java:70)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllMethods$13(ClassBasedTestDescriptor.java:411)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllMethods(ClassBasedTestDescriptor.java:409)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:215)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:84)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
 at java.util.ArrayList.forEach(ArrayList.java:1259)
 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
 at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:110)
 at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:90)
 at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:85)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at com.sun.proxy.$Proxy2.stop(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
 at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)
 at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
 at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
 Suppressed: java.lang.NullPointerException
 at org.apache.kafka.tools.JmxToolTest.afterAll(JmxToolTest.java:61)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptAfterAllMethod(TimeoutExtension.java:118)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeAfterAllMethods$15(ClassBasedTestDescriptor.java:439)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeAfterAllMethods$16(ClassBasedTestDescriptor.java:437)
 at java.util.ArrayList.forEach(ArrayList.java:1259)
 at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1082)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeAfterAllMethods(ClassBasedTestDescriptor.java:437)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.after(ClassBasedTestDescriptor.java:231)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.after(ClassBasedTestDescriptor.java:84)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:161)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:161)
 ... 48 more
Caused by: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)]
 at com.sun.jndi.rmi.registry.RegistryContext.bind(RegistryContext.java:161)
 at com.sun.jndi.toolkit.url.GenericURLContext.bind(GenericURLContext.java:241)
 at javax.naming.InitialContext.bind(InitialContext.java:425)
 at javax.management.remote.rmi.RMIConnectorServer.bind(RMIConnectorServer.java:644)
 at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:427)
 ... 76 more
Caused by: java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)
 at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:623)
 at sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:216)
 at sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:202)
 at sun.rmi.server.UnicastRef.newCall(UnicastRef.java:343)
 at sun.rmi.registry.RegistryImpl_Stub.bind(RegistryImpl_Stub.java:65)
 at com.sun.jndi.rmi.registry.RegistryContext.bind(RegistryContext.java:155)
 ... 80 more
Caused by: java.net.ConnectException: Connection timed out (Connection timed out)
 at java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
 at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
 at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
 at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at java.net.Socket.connect(Socket.java:607)
 at java.net.Socket.connect(Socket.java:556)
 at java.net.Socket.<init>(Socket.java:452)
 at java.net.Socket.<init>(Socket.java:229)
 at sun.rmi.transport.proxy.RMIDirectSocketFactory.createSocket(RMIDirectSocketFactory.java:40)
 at sun.rmi.transport.proxy.RMIMasterSocketFactory.createSocket(RMIMasterSocketFactory.java:148)
 at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:617)
 ... 85 more
```
One such build [https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-13453/6/tests]
 
[~fvaleri] i am assigning it to you. ",,fvaleri,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 07:46:05 UTC 2023,,,,,,,,,,"0|z1hvjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 07:46;fvaleri;Hi [~sagarrao], thanks for letting me know. I'll open a PR to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The KRaft controller should properly handle overly large user operations,KAFKA-14996,13536016,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,ecomar,ecomar,12/May/23 18:30,02/Jun/23 14:57,13/Jul/23 09:17,02/Jun/23 14:57,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,controller,,,,,,0,,,,,,"If an attempt is made to create a topic with

num partitions >= QuorumController.MAX_RECORDS_PER_BATCH  (10000)

the client receives an UnknownServerException - it could rather receive a better error.

The controller logs

{{2023-05-12 19:25:10,018] WARN [QuorumController id=1] createTopics: failed with unknown server exception IllegalStateException at epoch 2 in 21956 us.  Renouncing leadership and reverting to the last committed offset 174. (org.apache.kafka.controller.QuorumController)}}
{{java.lang.IllegalStateException: Attempted to atomically commit 10001 records, but maxRecordsPerBatch is 10000}}
{{    at org.apache.kafka.controller.QuorumController.appendRecords(QuorumController.java:812)}}
{{    at org.apache.kafka.controller.QuorumController$ControllerWriteEvent.run(QuorumController.java:719)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:127)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:210)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:181)}}
{{    at java.base/java.lang.Thread.run(Thread.java:829)}}
{{[}}",,ecomar,karbonitekream,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 14:57:21 UTC 2023,,,,,,,,,,"0|z1hv6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 14:45;ecomar;Similar error is encounter if creating partitions > QuorumController.MAX_RECORDS_PER_BATCH on an existing topic.

More worrying is that the cluster looks like it can be unstable after the error occurs.

Seen in a cluster with 6 nodes 0,1,2=broker,controller 3,4,5=broker

e.g. server.log for node 1 :

 

{{[2023-05-19 15:43:32,640] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=300, retries=86, voteStates=\{0=UNRECORDED, 1=GRANTED, 2=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1145) from CandidateState(localId=1, epoch=299, retries=85, voteStates=\{0=UNRECORDED, 1=GRANTED, 2=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1817) (org.apache.kafka.raft.QuorumState)}}
{{[2023-05-19 15:43:32,649] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 0 when making an ApiVersionsRequest with correlation id 4646. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:32,650] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 2 when making an ApiVersionsRequest with correlation id 4647. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:33,095] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 0 when making an ApiVersionsRequest with correlation id 4652. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:33,147] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 2 when making an ApiVersionsRequest with correlation id 4656. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:33,594] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 0 when making an ApiVersionsRequest with correlation id 4678. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:33,696] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 2 when making an ApiVersionsRequest with correlation id 4684. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:33,773] INFO [RaftManager id=1] Election has timed out, backing off for 1000ms before becoming a candidate again (org.apache.kafka.raft.KafkaRaftClient)}}
{{[2023-05-19 15:43:34,774] INFO [RaftManager id=1] Re-elect as candidate after election backoff has completed (org.apache.kafka.raft.KafkaRaftClient)}}
{{[2023-05-19 15:43:34,784] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=301, retries=87, voteStates=\{0=UNRECORDED, 1=GRANTED, 2=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1022) from CandidateState(localId=1, epoch=300, retries=86, voteStates=\{0=UNRECORDED, 1=GRANTED, 2=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1145) (org.apache.kafka.raft.QuorumState)}}
{{[2023-05-19 15:43:34,802] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 0 when making an ApiVersionsRequest with correlation id 4691. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}
{{[2023-05-19 15:43:34,825] WARN [RaftManager id=1] Received error UNKNOWN_SERVER_ERROR from node 2 when making an ApiVersionsRequest with correlation id 4692. Disconnecting. (org.apache.kafka.clients.NetworkClient)}}



In this state, client requests that should mutate the metadata (eg delete a topic) always timeout

{{% bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic edotest1}}
{{Error while executing topic command : Call(callName=deleteTopics, deadlineMs=1684507597582, tries=1, nextAllowedTryMs=1684507597698) timed out at 1684507597598 after 1 attempt(s)}}
{{[2023-05-19 15:46:37,602] ERROR org.apache.kafka.common.errors.TimeoutException: Call(callName=deleteTopics, deadlineMs=1684507597582, tries=1, nextAllowedTryMs=1684507597698) timed out at 1684507597598 after 1 attempt(s)}}
{{Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled deleteTopics request with correlation id 5 due to node 5 being disconnected}}
{{ (kafka.admin.TopicCommand$)}}

 

 ;;;","19/May/23 14:49;ecomar;given that this means a client request can cause a cluster to become unavailable, I'd raise the Priority to critical

 

this is a potential denial of service attack?

cc

[~mimaison] [~ijuma] [~rajinisivaram@gmail.com] ;;;","19/May/23 14:51;ecomar;The controller.log s are full of 

{{[2023-05-19 15:50:18,834] WARN [QuorumController id=0] getFinalizedFeatures: failed with unknown server exception RuntimeException in 28 us.  The controller is already in standby mode. (org.apache.kafka.controller.QuorumController)}}
{{java.lang.RuntimeException: No in-memory snapshot for epoch 84310. Snapshot epochs are: 61900}}
{{    at org.apache.kafka.timeline.SnapshotRegistry.getSnapshot(SnapshotRegistry.java:173)}}
{{    at org.apache.kafka.timeline.SnapshotRegistry.iterator(SnapshotRegistry.java:131)}}
{{    at org.apache.kafka.timeline.TimelineObject.get(TimelineObject.java:69)}}
{{    at org.apache.kafka.controller.FeatureControlManager.finalizedFeatures(FeatureControlManager.java:303)}}
{{    at org.apache.kafka.controller.QuorumController.lambda$finalizedFeatures$16(QuorumController.java:2016)}}
{{    at org.apache.kafka.controller.QuorumController$ControllerReadEvent.run(QuorumController.java:546)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:127)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:210)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:181)}}
{{    at java.base/java.lang.Thread.run(Thread.java:829)}};;;","22/May/23 12:59;ecomar;The controller instability is not reproducible with 3.4 (at the git commit `2f13471181` so it must be a regression)

Also 3.5 `10189d6159` does not exhibit the controller bug;;;","22/May/23 13:20;ecomar;is the state the controller gets in similar to

https://issues.apache.org/jira/browse/KAFKA-14644

?;;;","24/May/23 09:18;karbonitekream;The {{getFinalizedFeatures}} regression (and cluster instability) was probably introduced with this PR:

[https://github.com/apache/kafka/pull/13679#issuecomment-1559681643];;;","24/May/23 15:30;ecomar;I found another way to get into the error state.

3 broker/controller cluster, all 3 voters. If I shut down the 2 non-active quorum members, the remaining acive controller enters the state where it logs 
`[2023-05-24 16:29:45,129] WARN [BrokerToControllerChannelManager id=1 name=heartbeat] Received error UNKNOWN_SERVER_ERROR from node 1 when making an ApiVersionsRequest with correlation id 3945. Disconnecting. (org.apache.kafka.clients.NetworkClient)`

and correspondingly 
```
[2023-05-24 16:29:45,128] WARN [QuorumController id=1] getFinalizedFeatures: failed with unknown server exception RuntimeException in 222 us.  The controller is already in standby mode. (org.apache.kafka.controller.QuorumController)
java.lang.RuntimeException: No in-memory snapshot for epoch 159730. Snapshot epochs are:
    at org.apache.kafka.timeline.SnapshotRegistry.getSnapshot(SnapshotRegistry.java:173)
    at org.apache.kafka.timeline.SnapshotRegistry.iterator(SnapshotRegistry.java:131)
    at org.apache.kafka.timeline.TimelineObject.get(TimelineObject.java:69)
    at org.apache.kafka.controller.FeatureControlManager.finalizedFeatures(FeatureControlManager.java:303)
    at org.apache.kafka.controller.QuorumController.lambda$finalizedFeatures$16(QuorumController.java:2016)
    at org.apache.kafka.controller.QuorumController$ControllerReadEvent.run(QuorumController.java:546)
```
in controller.log;;;","26/May/23 17:09;ecomar;Opened a PR to allow responding gracefully with INVALID_PARTITION error to the clients

https://github.com/apache/kafka/pull/13766;;;","31/May/23 13:16;mimaison;[https://github.com/apache/kafka/pull/13742] has been merged, can we resolve this ticket?;;;","02/Jun/23 14:57;mimaison;Since the PR is merged I'm marking this as resolved in 3.5.0

If there is follow up work left to do, feel free to reopen (and adjust Fix Version/s) or create a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 jose4j is vulnerable to CVE- Improper Cryptographic Algorithm,KAFKA-14994,13535990,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,atusharm,jetlyg,jetlyg,12/May/23 13:34,13/May/23 11:40,13/Jul/23 09:17,13/May/23 10:21,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,Security,,,,,"Jose4j has the following vulnerability with high score of 7.1. 
jose4j is vulnerable to Improper Cryptographic Algorithm. The vulnerability exists due to the way `RSA1_5` and `RSA_OAEP` is implemented, allowing an attacker to decrypt `RSA1_5` or `RSA_OAEP` encrypted ciphertexts, and in addition, it may be feasible to sign with affected keys.

Please help upgrade the library to latest version
Current version in use: 0.7.9
Latest version with the fix: 0.9.3
CVE-
- Improper Cryptographic Algorithm
- Severity: HIGH
- CVSS: 7.1
- Disclosure Date: 07 Feb 2023 19:00PM EST
- Vulnerability Info: https://sca.analysiscenter.veracode.com/vulnerability-database/vulnerabilities/40398",,atusharm,jetlyg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,Sat May 13 02:46:56 UTC 2023,,,,,,,,,,"0|z1hv0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/23 02:46;atusharm;PR: https://github.com/apache/kafka/pull/13717;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker consumers don't get configs prefixed with source.cluster,KAFKA-14980,13535540,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,mimaison,mimaison,09/May/23 16:34,22/May/23 14:04,13/Jul/23 09:17,19/May/23 14:04,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,mirrormaker,,,,,,0,,,,,,"As part of KAFKA-14021, we made a change to MirrorConnectorConfig.sourceConsumerConfig() to grab all configs that start with ""source."". Previously it was grabbing configs prefixed with ""source.cluster."". 

This means existing connector configuration stop working, as configurations such as bootstrap.servers are not passed to source consumers.

For example, the following connector configuration was valid in 3.4 and now makes the connector tasks fail:

{code:json}
{
    ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
    ""name"": ""source"",
    ""topics"": ""test"",
    ""tasks.max"": ""30"",
    ""source.cluster.alias"": ""one"",
    ""target.cluster.alias"": ""two"",
    ""source.cluster.bootstrap.servers"": ""localhost:9092"",
   ""target.cluster.bootstrap.servers"": ""localhost:29092""
}
{code}


The connector attempts to start source consumers with bootstrap.servers = [] and the task crash with 


{noformat}
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:837)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:671)
	at org.apache.kafka.connect.mirror.MirrorUtils.newConsumer(MirrorUtils.java:59)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:103)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.initializeAndStart(AbstractWorkerSourceTask.java:274)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:202)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:259)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:75)
	at 
org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:181)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
{noformat}


",,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 09 16:34:37 UTC 2023,,,,,,,,,,"0|z1hs8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/23 16:34;mimaison;cc [~ChrisEgerton] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExactlyOnceWorkerSourceTask does not remove parent metrics,KAFKA-14978,13535466,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,durban,durban,durban,09/May/23 06:03,19/May/23 12:50,13/Jul/23 09:17,11/May/23 07:32,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,3.6.0,,,,KafkaConnect,,,,,,0,,,,,,"ExactlyOnceWorkerSourceTask removeMetrics does not invoke super.removeMetrics, meaning that only the transactional metrics are removed, and common source task metrics are not.",,durban,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-09 06:03:58.0,,,,,,,,,,"0|z1hrs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect partition count metrics for kraft controllers,KAFKA-14963,13534868,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jsancio,jsancio,jsancio,03/May/23 17:41,05/May/23 07:55,13/Jul/23 09:17,05/May/23 07:55,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,,,,controller,kraft,,,,,0,,,,,,"It is possible for the KRaft controller to report more partitions than are available in the cluster. This is because the following test fail against 3.4.0:
{code:java}
       @Test
      public void testPartitionCountDecreased() {
          ControllerMetrics metrics = new MockControllerMetrics();
          ControllerMetricsManager manager = new ControllerMetricsManager(metrics);          Uuid createTopicId = Uuid.randomUuid();
          Uuid createPartitionTopicId = new Uuid(
              createTopicId.getMostSignificantBits(),
              createTopicId.getLeastSignificantBits()
          );
          Uuid removeTopicId = new Uuid(createTopicId.getMostSignificantBits(), createTopicId.getLeastSignificantBits());
          manager.replay(topicRecord(""test"", createTopicId));
          manager.replay(partitionRecord(createPartitionTopicId, 0, 0, Arrays.asList(0, 1, 2)));
          manager.replay(partitionRecord(createPartitionTopicId, 1, 0, Arrays.asList(0, 1, 2)));
          manager.replay(removeTopicRecord(removeTopicId));
          assertEquals(0, metrics.globalPartitionCount());
      }
{code}",,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-03 17:41:00.0,,,,,,,,,,"0|z1ho3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whitespace in ACL configuration causes Kafka startup to fail,KAFKA-14962,13534858,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,divijvaidya,divijvaidya,divijvaidya,03/May/23 16:49,15/May/23 12:32,13/Jul/23 09:17,15/May/23 12:32,,,,,,,,,,,,,,,,3.6.0,,,,,,,,,,,,,0,,,,,,"Kafka's startup can fail if there is a trailing or leading whitespace for a configuration value. This fix makes it more tolerant towards cases where a user might accidentally add a trailing or leading whitespace in ACL configuration.
{code:java}
ERROR [KafkaServer id=3] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)

java.lang.IllegalArgumentException: For input string: ""true ""

    at scala.collection.StringOps$.toBooleanImpl$extension(StringOps.scala:943)

    at kafka.security.authorizer.AclAuthorizer.$anonfun$configure$4(AclAuthorizer.scala:153) {code}",,divijvaidya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-03 16:49:06.0,,,,,,,,,,"0|z1ho1c:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test org.apache.kafka.connect.integration.OffsetsApiIntegrationTest#testGetSinkConnectorOffsetsDifferentKafkaClusterTargeted,KAFKA-14956,13534707,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yash.mayya,sagarrao,sagarrao,02/May/23 15:38,29/May/23 07:01,13/Jul/23 09:17,29/May/23 07:01,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,,0,flaky-test,,,,,"```
h4. Error
org.opentest4j.AssertionFailedError: Condition not met within timeout 15000. Sink connector consumer group offsets should catch up to the topic end offsets ==> expected: <true> but was: <false>
h4. Stacktrace
org.opentest4j.AssertionFailedError: Condition not met within timeout 15000. Sink connector consumer group offsets should catch up to the topic end offsets ==> expected: <true> but was: <false>
 at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
 at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
 at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
 at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
 at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
 at app//org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:337)
 at app//org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385)
 at app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:334)
 at app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:318)
 at app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:291)
 at app//org.apache.kafka.connect.integration.OffsetsApiIntegrationTest.getAndVerifySinkConnectorOffsets(OffsetsApiIntegrationTest.java:150)
 at app//org.apache.kafka.connect.integration.OffsetsApiIntegrationTest.testGetSinkConnectorOffsetsDifferentKafkaClusterTargeted(OffsetsApiIntegrationTest.java:131)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base@17.0.7/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base@17.0.7/java.lang.reflect.Method.invoke(Method.java:568)
 at app//org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
 at app//org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at app//org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
 at app//org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at app//org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at app//org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at app//org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
 at app//org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
 at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
 at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
 at app//org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
 at app//org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
 at app//org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
 at app//org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
 at app//org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
 at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at app//org.junit.runners.ParentRunner.run(ParentRunner.java:413)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:108)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:40)
 at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:60)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:52)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base@17.0.7/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base@17.0.7/java.lang.reflect.Method.invoke(Method.java:568)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at jdk.proxy1/jdk.proxy1.$Proxy2.processTestClass(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
 at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)
 at app//worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
 at app//worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
```
[https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-13594/5/tests/]
 
The test failure seems unrelated to the changes in the PR. [~yash.mayya] , can you plz take a look?",,sagarrao,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 29 07:01:18 UTC 2023,,,,,,,,,,"0|z1hn40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/23 07:01;yash.mayya;This test seems to be consistently passing after [https://github.com/apache/kafka/pull/13465] was merged (timeout value for the read offsets operation in *OffsetsApiIntegrationTest* was bumped up) so I'm gonna mark this ticket as resolved. In case a failure re-occurs, this ticket can be re-opened for investigation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft controller node shutting down while renouncing leadership,KAFKA-14946,13534272,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,showuon,akshaykumar001,akshaykumar001,27/Apr/23 08:09,04/May/23 01:45,13/Jul/23 09:17,04/May/23 01:45,3.3.1,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,controller,kraft,,,,,0,,,,,,"* We are using the zookeeper less Kafka (kafka Kraft).
 * The cluster is having 3 nodes.
 * One of the nodes gets automatically shut down randomly.
 * Checked the logs but didn't get the exact reason.
 * Kafka version - 3.3.1
 * Attaching the log files. 
 * Time - 2023-04-21 16:28:23

*state-change.log -*
[https://drive.google.com/file/d/1eS-ShKlhGPsIJoybHndlhahJnucU8RWA/view?usp=share_link]
 
*server.log -*
[https://drive.google.com/file/d/1Ov5wrQIqx2AS4J7ppFeHJaDySsfsK588/view?usp=share_link]",,akshaykumar001,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14035,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 08:11:32 UTC 2023,,,,,,,,,,"0|z1hkg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 08:11;akshaykumar001;[2023-04-13 01:49:17,411] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37110, but in the new epoch 37111, the leader is (none). Reverting to last committed offset 28291464. (org.apache.kafka.controller.QuorumController)
[2023-04-13 01:49:17,531] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=37112, voters=[1, 2, 3], electionTimeoutMs=982) (org.apache.kafka.raft.QuorumState)

[2023-04-13 02:00:33,902] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37116, but in the new epoch 37117, the leader is (none). Reverting to last committed offset 28292807. (org.apache.kafka.controller.QuorumController)
[2023-04-13 02:00:33,936] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=37118, voters=[1, 2, 3], electionTimeoutMs=1497) (org.apache.kafka.raft.QuorumState)

[2023-04-13 02:00:35,014] ERROR [Controller 1] processBrokerHeartbeat: unable to start processing because of NotControllerException. (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:12:21,883] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37129, but in the new epoch 37131, the leader is (none). Reverting to last committed offset 28294206. (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37141, but in the new epoch 37142, the leader is (none). Reverting to last committed offset 28294325. (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] INFO [Controller 1] writeNoOpRecord: failed with NotControllerException in 16561838 us (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] INFO [Controller 1] maybeFenceReplicas: failed with NotControllerException in 8520846 us (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2023-04-13 02:13:41,329] INFO [BrokerLifecycleManager id=1] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2023-04-13 02:13:41,351] ERROR Encountered fatal fault: exception while renouncing leadership (org.apache.kafka.server.fault.ProcessExitingFaultHandler)
java.lang.NullPointerException
        at org.apache.kafka.timeline.SnapshottableHashTable$HashTier.mergeFrom(SnapshottableHashTable.java:125)
        at org.apache.kafka.timeline.Snapshot.mergeFrom(Snapshot.java:68)
        at org.apache.kafka.timeline.SnapshotRegistry.deleteSnapshot(SnapshotRegistry.java:236)
        at org.apache.kafka.timeline.SnapshotRegistry$SnapshotIterator.remove(SnapshotRegistry.java:67)
        at org.apache.kafka.timeline.SnapshotRegistry.revertToSnapshot(SnapshotRegistry.java:214)
        at org.apache.kafka.controller.QuorumController.renounce(QuorumController.java:1232)
        at org.apache.kafka.controller.QuorumController.access$3300(QuorumController.java:150)
        at org.apache.kafka.controller.QuorumController$QuorumMetaLogListener.lambda$handleLeaderChange$3(QuorumController.java:1076)
        at org.apache.kafka.controller.QuorumController$QuorumMetaLogListener.lambda$appendRaftEvent$4(QuorumController.java:1101)
        at org.apache.kafka.controller.QuorumController$ControlEvent.run(QuorumController.java:496)
        at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
        at java.lang.Thread.run(Thread.java:750)
[2023-04-13 02:13:41,385] INFO [BrokerServer id=1] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ClientQuotaControlManager validation,KAFKA-14943,13534235,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,26/Apr/23 23:10,24/May/23 23:00,13/Jul/23 09:17,24/May/23 23:00,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-26 23:10:33.0,,,,,,,,,,"0|z1hk80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest#testConnectorBoundary,KAFKA-14938,13534138,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sajain16,sagarrao,sagarrao,26/Apr/23 12:02,12/Jul/23 15:59,13/Jul/23 09:17,12/Jul/23 15:46,,,,,,,,,,,,,,,,3.4.2,3.5.2,3.6.0,,,,,KafkaConnect,,,,,,0,,,,,,"Test seems to be failing with 

```
ava.lang.AssertionError: Not enough records produced by source connector. Expected at least: 100 + but got 72
h4. Stacktrace
java.lang.AssertionError: Not enough records produced by source connector. Expected at least: 100 + but got 72
 at org.junit.Assert.fail(Assert.java:89)
 at org.junit.Assert.assertTrue(Assert.java:42)
 at org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testConnectorBoundary(ExactlyOnceSourceIntegrationTest.java:421)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:108)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:40)
 at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:60)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:52)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
 at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)
 at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
```",,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-26 12:02:47.0,,,,,,,,,,"0|z1hjmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sporadic ""Address already in use"" when starting kafka cluster embedded within tests",KAFKA-14908,13532654,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,kwall,kwall,14/Apr/23 11:53,27/Apr/23 09:38,13/Jul/23 09:17,19/Apr/23 02:59,,,,,,,,,,,,,,,,3.6.0,,,,,,,,,,,,,0,,,,,,"We have an integration test suite that starts/stops a kafka cluster before/after each test.   Kafka is being started programmatically within the same JVM that is running the tests.

Sometimes we get sporadic failures from with Kafka as it tries to bind the server socket.
{code:java}
org.apache.kafka.common.KafkaException: Socket server failed to bind to 0.0.0.0:9092: Address already in use.
    at kafka.network.Acceptor.openServerSocket(SocketServer.scala:684)
    at kafka.network.Acceptor.<init>(SocketServer.scala:576)
    at kafka.network.DataPlaneAcceptor.<init>(SocketServer.scala:433)
    at kafka.network.SocketServer.createDataPlaneAcceptor(SocketServer.scala:247)
    at kafka.network.SocketServer.createDataPlaneAcceptorAndProcessors(SocketServer.scala:226)
    at kafka.network.SocketServer.$anonfun$new$31(SocketServer.scala:173)
    at kafka.network.SocketServer.$anonfun$new$31$adapted(SocketServer.scala:173)
    at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575)
    at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:933)
    at kafka.network.SocketServer.<init>(SocketServer.scala:173)
    at kafka.server.KafkaServer.startup(KafkaServer.scala:331) {code}
Investigation has shown that the socket is in the timed_wait state from a previous test.

I know Kafka supports ephemeral ports, but this isn't convenient to our use-case.  

I'd like to suggest that Kafka is changed to set the SO_REUSEADDR on the server socket.  I believe this is standard practice for server applications that run on well known ports .

I don't believe this change would introduce a backward compatibility concerns. 

 

I will open a PR so that can be considered. Thank you.",,kwall,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 12:14:31 UTC 2023,,,,,,,,,,"0|z1haiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 12:14;kwall;Opened PR: https://github.com/apache/kafka/pull/13572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaBasedLog infinite retries can lead to StackOverflowError,KAFKA-14902,13532461,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,durban,durban,durban,13/Apr/23 08:20,24/Apr/23 07:50,13/Jul/23 09:17,18/Apr/23 07:45,,,,,,,,,,,,,,,,3.5.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"KafkaBasedLog subclasses use an infinite retry on producer sends, using a callback. Sometimes, when specific errors are encountered, the callback is invoked in the send call, on the calling thread. If this happens enough times, a stack overflow happens.

Example stacktrace from 2.5 (but the newest code can also encounter the same):
{code:java}
2023-01-14 12:48:23,487 ERROR org.apache.kafka.connect.runtime.WorkerTask: WorkerSourceTask{id=MirrorSourceConnector-1} Task threw an uncaught and unrecoverable exception java.lang.StackOverflowError: null at org.apache.kafka.common.metrics.stats.SampledStat.record(SampledStat.java:50) at org.apache.kafka.common.metrics.stats.Rate.record(Rate.java:60) at org.apache.kafka.common.metrics.stats.Meter.record(Meter.java:80) at org.apache.kafka.common.metrics.Sensor.record(Sensor.java:188) at org.apache.kafka.common.metrics.Sensor.record(Sensor.java:178) at org.apache.kafka.clients.producer.internals.BufferPool.recordWaitTime(BufferPool.java:202) at org.apache.kafka.clients.producer.internals.BufferPool.allocate(BufferPool.java:147) at org.apache.kafka.clients.producer.internals.RecordAccumulator.append(RecordAccumulator.java:221) at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:941) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:862) at org.apache.kafka.connect.util.KafkaBasedLog.send(KafkaBasedLog.java:238) at org.apache.kafka.connect.storage.KafkaStatusBackingStore$4.onCompletion(KafkaStatusBackingStore.java:298) ... at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:959) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:862) at org.apache.kafka.connect.util.KafkaBasedLog.send(KafkaBasedLog.java:238) at org.apache.kafka.connect.storage.KafkaStatusBackingStore$4.onCompletion(KafkaStatusBackingStore.java:298){code}
Note the repeated KafkaProducer.send -> KafkaProducer.doSend -> KafkaStatusBackingStore$4.onCompletion calls, causing the issue.",,durban,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14929,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-13 08:20:26.0,,,,,,,,,,"0|z1h9c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[ MirrorMaker ] sync.topic.configs.enabled not working as expected,KAFKA-14898,13532391,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,bseenu,bseenu,12/Apr/23 18:05,12/Apr/23 20:41,13/Jul/23 09:17,12/Apr/23 20:41,3.4.0,,,,,,,,,,,,,,,,,,,,,,mirrormaker,,,,,,0,mirrormaker,,,,,"Hello,

In my replication set up , i do not want to sync the topic configs, the use case is to have different retention time for the topic on the target cluster, I am passing the config
{code:java}
 sync.topic.configs.enabled = false{code}
but this is not working as expected the topic retention time is being set to whatever is being set in the source cluster, looking at the mirrormaker logs i can see that MirrorSourceConnector is still setting the above config as true
{code:java}
[2023-04-12 17:04:55,184] INFO [MirrorSourceConnector|task-8] ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.include.jmx.reporter = true
        auto.offset.reset = earliest
        bootstrap.servers = [sourcecluster.com:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-null-2
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = null
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 45000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        source.cluster.alias = prod-240
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        sync.topic.acls.enabled = true
        sync.topic.acls.interval.seconds = 600
        sync.topic.configs.enabled = true
        sync.topic.configs.interval.seconds = 600
        target.cluster.alias = dev-240
        task.assigned.partitions = [test1-8, test1-18, test1-28, test2-6, test2-16, test2-26]
        tasks.max = 10
        topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
        topics = [test1, test2]
        topics.blacklist = null
        topics.exclude = [.*[\-\.]internal, .*\.replica, __.*]
        transforms = []
        value.converter = null {code}
 

Can you please let me know if i am missing any other config

I am using kafka version - 3.4

Thanks,

-srini",,bseenu,gharris1727,,,,,,,,,,,,,,,,,,,,KAFKA-10586,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 20:34:19 UTC 2023,,,,,,,,,,"0|z1h8wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 18:22;gharris1727;[~bseenu] Are you running MM2 with the MirrorMaker dedicated launcher? There's a known issue where a multi-node cluster is unable to persist configuration changes: https://issues.apache.org/jira/browse/KAFKA-10586 which has a fix to be released in 3.5.0.
You can verify that the above is affecting you if you see these log messages: [https://github.com/apache/kafka/blob/9c0caca6600cd18c161f7be48a7745a98ff091dd/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMaker.java#L236-L241] indicating that the existing connector configuration is being used (meaning that your new connector configuration with enabled=false is being dropped)
In the meantime, you can cold-start your cluster (shut down all nodes, then bring them all back) and that will ensure that at least one of the nodes takes your on-disk configuration and applies it to the connectors. ;;;","12/Apr/23 20:34;bseenu;[~gharris1727] Thanks for your help on this

 

Yes i could see that log message which you pointed out, i was running mirrormaker in distributed mode having 3 nodes and 10 tasks on each

I have verified by running only on one node and it is working as expected

 

Thanks,

-srini;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetadataLoader must call finishSnapshot after loading a snapshot,KAFKA-14894,13532222,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,cmccabe,cmccabe,cmccabe,11/Apr/23 17:24,17/Apr/23 17:50,13/Jul/23 09:17,12/Apr/23 18:06,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-11 17:24:28.0,,,,,,,,,,"0|z1h7v4:",9223372036854775807,,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix rack-aware range assignor to improve rack-awareness with co-partitioning,KAFKA-14891,13532182,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,11/Apr/23 12:42,12/Apr/23 07:36,13/Jul/23 09:17,12/Apr/23 07:36,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,We currently check all states for rack-aware assignment with co-partitioning ([https://github.com/apache/kafka/blob/396536bb5aa1ba78c71ea824d736640b615bda8a/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java#L176).] We should check each group of co-partitioned states separately so that we can use rack-aware assignment with co-partitioning for subsets of topics.,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-11 12:42:56.0,,,,,,,,,,"0|z1h7m8:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TransactionMetadata with producer epoch -1 should be expirable ,KAFKA-14880,13531546,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,05/Apr/23 08:11,06/Apr/23 07:18,13/Jul/23 09:17,06/Apr/23 07:18,3.0.0,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,3.0.3,3.1.3,3.2.4,3.3.3,3.4.1,,,,,,,,,0,,,,,,"We have seen the following error in logs:
{noformat}
""Mar 22, 2019 @ 21:57:56.655"",Error,""kafka-0-0"",""transaction-log-manager-0"",""Uncaught exception in scheduled task 'transactionalId-expiration'"",""java.lang.IllegalArgumentException: Illegal new producer epoch -1
{noformat}

Investigations showed that it is actually possible for a transaction metadata object to still have -1 as producer epoch when it transitions to Dead.

When a transaction metadata is created for the first time (in handleInitProducerId), it has -1 as its producer epoch. Then a producer epoch is attributed and the transaction coordinator tries to persist the change. If the write fail for instance because there is an under min isr, the transaction metadata remains with its epoch as -1 forever or until the init producer id is retried.

This means that it is possible for transaction metadata to remain with -1 as producer epoch until it gets expired. At the moment, this is not allowed because we enforce a producer epoch greater or equals to 0 in prepareTransitionTo.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-05 08:11:21.0,,,,,,,,,,"0|z1h3pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in KStreamWindowAggregate with ON_WINDOW_CLOSE emit strategy,KAFKA-14864,13530517,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vcrfxia,vcrfxia,vcrfxia,28/Mar/23 23:16,04/Apr/23 05:09,13/Jul/23 09:17,04/Apr/23 05:09,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,streams,,,,,,0,,,,,,"The Streams DSL processor implementation for the ON_WINDOW_CLOSE emit strategy during KStream windowed aggregations opens a key-value iterator but does not call `close()` on it ([link|https://github.com/apache/kafka/blob/5afedd9ac37c4d740f47867cfd31eaed15dc542f/streams/src/main/java/org/apache/kafka/streams/kstream/internals/AbstractKStreamTimeWindowAggregateProcessor.java#L203]), despite the Javadocs for the iterator making clear that users must do so in order to release resources ([link|https://github.com/apache/kafka/blob/5afedd9ac37c4d740f47867cfd31eaed15dc542f/streams/src/main/java/org/apache/kafka/streams/state/KeyValueIterator.java#L27]).  

I discovered this bug while running load testing benchmarks and noticed that some runs were sporadically hitting OOMs, so it is definitely possible to hit this in practice.",,mjsax,vcrfxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-28 23:16:25.0,,,,,,,,,,"0|z1gxdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Plugins which do not have a valid no-args constructor are visible in the REST API,KAFKA-14863,13530487,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gharris1727,gharris1727,gharris1727,28/Mar/23 19:11,03/Jun/23 16:55,13/Jul/23 09:17,02/Jun/23 15:13,,,,,,,,,,,,,,,,3.6.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"Currently, the Connect plugin discovery mechanisms only assert that a no-args constructor is present when necessary. In particular, this assertion happens for Connectors when the framework needs to evaluate the connector's version method.
It also happens for ConnectorConfigOverridePolicy, ConnectRestExtension, and ConfigProvider plugins, which are loaded via the ServiceLoader. The ServiceLoader constructs instances of plugins with their no-args constructor during discovery, so these plugins are discovered even if they are not Versioned.

This has the effect that these unusable plugins which are missing a default constructor appear in the REST API, but are not able to be instantiated or used. To make the ServiceLoader and Reflections discovery mechanisms behave more similar, this assertion should be applied to all plugins, and a log message emitted when plugins do not follow the constructor requirements.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14654,KAFKA-14627,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-28 19:11:36.0,,,,,,,,,,"0|z1gx6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outer stream-stream join does not output all results with multiple input partitions,KAFKA-14862,13530446,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,cadonna,cadonna,28/Mar/23 14:16,22/May/23 12:28,13/Jul/23 09:17,24/Apr/23 20:04,3.1.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,streams,,,,,,1,,,,,,"If I execute the following Streams app once with two input topics each with 1 partition and then with input topics each with two partitions, I get different results.
  
{code:java}
final KStream<String, String> leftSide = builder.stream(leftSideTopic);
final KStream<String, String> rightSide = builder.stream(rightSideTopic);

final KStream<String, String> leftAndRight = leftSide.outerJoin(
    rightSide,
    (leftValue, rightValue) ->
        (rightValue == null) ? leftValue + ""/NOTPRESENT"": leftValue + ""/"" + rightValue,
    JoinWindows.ofTimeDifferenceAndGrace(
        Duration.ofSeconds(20), 
        Duration.ofSeconds(10)),
    StreamJoined.with(
        Serdes.String(), /* key */
        Serdes.String(), /* left value */
        Serdes.String()  /* right value */
    ));
    leftAndRight.print(Printed.toSysOut());
{code}

To reproduce, produce twice the following batch of records with an interval greater than window + grace period (i.e. > 30 seconds) in between the two batches:
{code}
(0, 0)
(1, 1)
(2, 2)
(3, 3)
(4, 4)
(5, 5)
(6, 6)
(7, 7)
(8, 8)
(9, 9)
{code}

With input topics with 1 partition I get:
{code}
[KSTREAM-PROCESSVALUES-0000000008]: 0, 0/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 1, 1/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 2, 2/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 3, 3/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 4, 4/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 5, 5/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 6, 6/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 7, 7/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 8, 8/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 9, 9/NOTPRESENT
{code}

With input topics with 2 partitions I get:
{code}
[KSTREAM-PROCESSVALUES-0000000008]: 1, 1/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 3, 3/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 4, 4/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 7, 7/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 8, 8/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 9, 9/NOTPRESENT
{code}

I would expect to get the same set of records, maybe in a different order due to the partitioning.",,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-28 14:16:08.0,,,,,,,,,,"0|z1gwxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some MetadataLoader bugs,KAFKA-14857,13530293,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,27/Mar/23 22:21,24/May/23 23:00,13/Jul/23 09:17,24/May/23 23:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-27 22:21:18.0,,,,,,,,,,"0|z1gw00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the serializer/deserialize which extends ClusterResourceListener is not added to Metadata,KAFKA-14853,13530269,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,27/Mar/23 17:07,29/Mar/23 08:02,13/Jul/23 09:17,29/Mar/23 08:02,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,I noticed this issue when reviewing  KAFKA-14848 ,,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-27 17:07:43.0,,,,,,,,,,"0|z1gvuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumer incorrectly passes locally-scoped deserializers to FetchConfig,KAFKA-14848,13530088,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,25/Mar/23 23:32,29/Jun/23 21:08,13/Jul/23 09:17,25/Apr/23 22:08,3.5.0,,,,,,,,,,,,,,,,,,,,,,clients,consumer,,,,,0,kip-945,,,,,"[~rayokota] found some {{{}NullPointerException{}}}s that originate because of a recently introduced error in the {{KafkaConsumer}} constructor. The code was changed to pass the deserializer variables into the {{FetchConfig}} constructor. However, this code change incorrectly used the locally-scoped variables, not the instance-scoped variables. Since the locally-scoped variables could be {{{}null{}}}, this results in the {{FetchConfig}} storing {{null}} references, leading to downstream breakage.

Suggested change:
{noformat}
- FetchConfig<K, V> fetchConfig = new FetchConfig<>(config, keyDeserializer, valueDeserializer, isolationLevel);
+ FetchConfig<K, V> fetchConfig = new FetchConfig<>(config, this.keyDeserializer, this.valueDeserializer, isolationLevel);
{noformat}",,kirktrue,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,KAFKA-14365,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-25 23:32:03.0,,,,,,,,,,"0|z1guqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector plugins config endpoint does not include Common configs,KAFKA-14843,13529914,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jeqo,jeqo,jeqo,24/Mar/23 06:28,28/Mar/23 16:44,13/Jul/23 09:17,28/Mar/23 15:26,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,3.2.4,3.3.3,3.4.1,3.5.0,,,,KafkaConnect,,,,,,0,,,,,,"Connector plugins GET config endpoint introduced in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-769%3A+Connect+APIs+to+list+all+connector+plugins+and+retrieve+their+configuration+definitions]  allows to get plugin configuration from the rest endpoint.

This configuration only includes the plugin configuration, but not the base configuration of the Sink/Source Connector.

For instance, when validating the configuration of a plugin, _all_ configs are returned:

```

curl -s $CONNECT_URL/connector-plugins/io.aiven.kafka.connect.http.HttpSinkConnector/config | jq -r '.[].name' | sort -u | wc -l     
21

curl -s $CONNECT_URL/connector-plugins/io.aiven.kafka.connect.http.HttpSinkConnector/config/validate -XPUT -H 'Content-type: application/json' --data ""\{\""connector.class\"": \""io.aiven.kafka.connect.http.HttpSinkConnector\"", \""topics\"": \""example-topic-name\""}"" | jq -r '.configs[].definition.name' | sort -u | wc -l
39

```

and the missing configs are all from base config:

```

diff validate.txt config.txt                                                                                                    
6,14d5
< config.action.reload
< connector.class
< errors.deadletterqueue.context.headers.enable
< errors.deadletterqueue.topic.name
< errors.deadletterqueue.topic.replication.factor
< errors.log.enable
< errors.log.include.messages
< errors.retry.delay.max.ms
< errors.retry.timeout
16d6
< header.converter
24d13
< key.converter
26d14
< name
33d20
< predicates
35,39d21
< tasks.max
< topics
< topics.regex
< transforms
< value.converter

```

Would be great to get the base configs from the same endpoint as well, so we could rely on it instead of using the validate endpoint to get all configs.",,jeqo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-24 06:28:23.0,,,,,,,,,,"0|z1gtns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude protected variable from JavaDocs,KAFKA-14839,13529835,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,atusharm,mjsax,mjsax,23/Mar/23 16:56,13/May/23 01:34,13/Jul/23 09:17,13/May/23 01:34,,,,,,,,,,,,,,,,3.5.0,,,,,,,documentation,streams,,,,,0,,,,,,"Cf [https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/kstream/JoinWindows.html#enableSpuriousResultFix]

The variable `enableSpuriousResultFix` is protected, and it's not public API, and thus should not show up in the JavaDocs.",,atusharm,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 10:58:04 UTC 2023,,,,,,,,,,"0|z1gt68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 09:03;atusharm;Hi, [~mjsax] can i work in this?;;;","26/Apr/23 02:53;mjsax;Sure!;;;","12/May/23 10:58;atusharm;[~mjsax] edited PR to apply this configuration in whole project;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix UtilsTest#testToLogDateTimeFormat failure in some cases,KAFKA-14836,13529771,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,egyed.t,egyed.t,egyed.t,23/Mar/23 13:07,24/May/23 01:41,13/Jul/23 09:17,27/Mar/23 09:57,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,,,,,," 
{code:java}
org.apache.kafka.common.utils.UtilsTest.testToLogDateTimeFormat {code}
test is failing in some cases. It uses hard coded datetime ({*}2020-11-09 12:34:05{*}), while the assertation expected part contains the offset of the current time ({*}Instant.now(){*}). This can lead to problems if there is a summer and winter time for the specific location. We should use the same datetime in the offset query instead of the current time.
{noformat}
org.opentest4j.AssertionFailedError: expected: <2020-11-09 12:34:05,123 -07:00> but was: <2020-11-09 12:34:05,123 -08:00>{noformat}
 ",,egyed.t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-23 13:07:29.0,,,,,,,,,,"0|z1gss0:",9223372036854775807,,viktorsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect loading SSL configs when contacting non-HTTPS URLs,KAFKA-14816,13528877,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,imcdo,imcdo,17/Mar/23 00:15,20/Mar/23 16:40,13/Jul/23 09:17,20/Mar/23 16:40,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,KafkaConnect,,,,,,0,,,,,,"Due to changes made here: [https://github.com/apache/kafka/pull/12828]
Connect now unconditionally loads SSL configs from the worker into rest clients it uses for cross-worker communication and uses them even when issuing requests to HTTP (i.e., non-HTTPS) URLs. Previously, it would only attempt to load (and validate) SSL properties when issuing requests to HTTPS URLs. This can cause issues when a Connect cluster has stopped securing its REST API with SSL but its worker configs still contain the old (and now-invalid) SSL properties. When this happens, REST requests that hit a follower worker but need to be forwarded to the leader will fail, and connectors that perform dynamic reconfigurations via [ConnectorContext::requestTaskReconfiguration|https://kafka.apache.org/34/javadoc/org/apache/kafka/connect/connector/ConnectorContext.html#requestTaskReconfiguration()] will fail to trigger that reconfiguration if they are not running on the leader.

In our testing environments - older versions without the linked changes pass with the following configuration, and newer versions with the changes fail:

{{ssl.keystore.location = /mnt/security/test.keystore.jks}}
{{ssl.keystore.password = [hidden]}}
{{ssl.keystore.type = JKS}}
{{ssl.protocol = TLSv1.2}}

It's important to note that the file {{/mnt/security/test.keystore.jks}} isn't generated for our non-SSL tests, however these configs are still included in our worker config file.

This leads to a 500 response when hitting the create connector REST endpoint with the following error:

bq. { ""error_code"":500,   ""message"":""Failed to start RestClient:   /mnt/security/test.keystore.jks is not a valid keystore"" }",,ChrisEgerton,imcdo,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14346,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 17:28:42 UTC 2023,,,,,,,,,,"0|z1gn9k:",9223372036854775807,,jolshan,,,,,,,,,,,,,,,,,,"17/Mar/23 17:28;ChrisEgerton;Thanks [~imcdo] for filing this. I believe the root cause is slightly different than what you initially described on the ticket, so I've updated the description with something more plausible given the changes in the PR where this was originally discussed.

This is definitely worth addressing as it is a regression in behavior and can cause clusters to fail after upgrades. Hopefully people aren't leaving invalid SSL properties in their worker configs when using HTTP for their REST API, but there's no reason that their cluster should break if they are.

I'll file a PR to fix this sometime today.

CC [~gharris1727] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerPerformance still counting successful sending in console when sending failed,KAFKA-14812,13528758,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hudeqi,hudeqi,hudeqi,16/Mar/23 08:52,21/Mar/23 09:01,13/Jul/23 09:17,21/Mar/23 09:01,3.3.2,,,,,,,,,,,,,,,3.5.0,,,,,,,tools,,,,,,0,kafka-producer-perf-test,tools,,,,"When using ProducerPerformance, I found that when the sending fails, it is still counted as successfully sent by stat and the metrics are printed in console. For example, when there is no write permission and cannot be written in, the sending success rate is still magically displayed.",,hudeqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 08:59;hudeqi;WechatIMG27.jpeg;https://issues.apache.org/jira/secure/attachment/13056401/WechatIMG27.jpeg",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 14:05:46 UTC 2023,,,,,,,,,,"0|z1gmj4:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,"16/Mar/23 14:05;hudeqi;https://github.com/apache/kafka/pull/13404;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect incorrectly logs that no records were produced by source tasks,KAFKA-14809,13528498,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,hgeraldino,hgeraldino,hgeraldino,14/Mar/23 20:05,16/Mar/23 12:46,13/Jul/23 09:17,16/Mar/23 12:46,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,3.1.2,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,3.3.2,3.4.0,,3.0.3,3.1.3,3.2.4,3.3.3,3.4.1,3.5.0,,KafkaConnect,,,,,,0,,,,,,"There's an *{{if}}* condition when [committing offsets|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L219] that is referencing the wrong variable, so the statement always evaluates to {*}true{*}.

This causes log statements like the following to be spuriously emitted:
{quote}[2023-03-14 16:18:04,675] DEBUG WorkerSourceTask\{id=job-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:220)
{quote}",,ChrisEgerton,hgeraldino,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13669,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 12:38:51 UTC 2023,,,,,,,,,,"0|z1gkxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 20:20;ChrisEgerton;[~hgeraldino] I wanted a Jira ticket for this so that users could easily find the cause of the problem if they were confused by the incorrect log messages, so I've taken a stab at changing the title and description to describe not just the specific bug in the code, but the user-facing effects that it has. Feel free to alter anything you'd like; I just want this to be discoverable by users who may be searching for, e.g., log messages to understand what's going wrong.;;;","14/Mar/23 20:43;hgeraldino;No that's perfect. Thanks [~ChrisEgerton]!;;;","16/Mar/23 12:38;ChrisEgerton;This is arguably the root cause of KAFKA-13669, since it's likely that users would not have been so alarmed by these log messages if they were being emitted correctly.

It's probably not worth it to revert the downgrade from {{INFO}} to {{DEBUG}} level at this point, but it's at least worth noting the connection between these two issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect docs fail to build with Gradle Swagger plugin 2.2.8,KAFKA-14804,13528276,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mimaison,mumrah,mumrah,13/Mar/23 17:11,14/Mar/23 11:06,13/Jul/23 09:17,14/Mar/23 11:06,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"There is an incompatibility somewhere between versions 2.2.0 and 2.2.8 that cause the following error when building the connect docs:

{code}
Caused by: org.gradle.api.GradleException: io.swagger.v3.jaxrs2.integration.SwaggerLoader.setOpenAPI31(java.lang.Boolean)
        at io.swagger.v3.plugins.gradle.tasks.ResolveTask.resolve(ResolveTask.java:458)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:125)
        at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:58)
        at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:51)
        at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:29)
        at org.gradle.api.internal.tasks.execution.TaskExecution$3.run(TaskExecution.java:242)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$1.execute(DefaultBuildOperationRunner.java:29)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$1.execute(DefaultBuildOperationRunner.java:26)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.run(DefaultBuildOperationRunner.java:47)
        at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:68)
        at org.gradle.api.internal.tasks.execution.TaskExecution.executeAction(TaskExecution.java:227)
        at org.gradle.api.internal.tasks.execution.TaskExecution.executeActions(TaskExecution.java:210)
        at org.gradle.api.internal.tasks.execution.TaskExecution.executeWithPreviousOutputFiles(TaskExecution.java:193)
        at org.gradle.api.internal.tasks.execution.TaskExecution.execute(TaskExecution.java:166)
        at org.gradle.internal.execution.steps.ExecuteStep.executeInternal(ExecuteStep.java:93)
        at org.gradle.internal.execution.steps.ExecuteStep.access$000(ExecuteStep.java:44)
        at org.gradle.internal.execution.steps.ExecuteStep$1.call(ExecuteStep.java:57)
        at org.gradle.internal.execution.steps.ExecuteStep$1.call(ExecuteStep.java:54)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
        at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
        at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:54)
        at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:44)
        at org.gradle.internal.execution.steps.RemovePreviousOutputsStep.execute(RemovePreviousOutputsStep.java:67)
        at org.gradle.internal.execution.steps.RemovePreviousOutputsStep.execute(RemovePreviousOutputsStep.java:37)
        at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:41)
        at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:74)
        at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:55)
        at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:50)
        at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:28)
        at org.gradle.internal.execution.steps.CaptureStateAfterExecutionStep.executeDelegateBroadcastingChanges(CaptureStateAfterExecutionStep.java:100)
        at org.gradle.internal.execution.steps.CaptureStateAfterExecutionStep.execute(CaptureStateAfterExecutionStep.java:72)
        at org.gradle.internal.execution.steps.CaptureStateAfterExecutionStep.execute(CaptureStateAfterExecutionStep.java:50)
        at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:40)
        at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:29)
        at org.gradle.internal.execution.steps.BuildCacheStep.executeWithoutCache(BuildCacheStep.java:166)
        at org.gradle.internal.execution.steps.BuildCacheStep.lambda$execute$1(BuildCacheStep.java:70)
        at org.gradle.internal.Either$Right.fold(Either.java:175)
        at org.gradle.internal.execution.caching.CachingState.fold(CachingState.java:59)
        at org.gradle.internal.execution.steps.BuildCacheStep.execute(BuildCacheStep.java:68)
        at org.gradle.internal.execution.steps.BuildCacheStep.execute(BuildCacheStep.java:46)
        at org.gradle.internal.execution.steps.StoreExecutionStateStep.execute(StoreExecutionStateStep.java:36)
        at org.gradle.internal.execution.steps.StoreExecutionStateStep.execute(StoreExecutionStateStep.java:25)
        at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:36)
        at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:22)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:91)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$2(SkipUpToDateStep.java:55)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:55)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:37)
        at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:65)
        at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:36)
        at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:37)
        at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:27)
        at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:76)
        at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:37)
        at org.gradle.internal.execution.steps.ValidateStep.execute(ValidateStep.java:94)
        at org.gradle.internal.execution.steps.ValidateStep.execute(ValidateStep.java:49)
        at org.gradle.internal.execution.steps.CaptureStateBeforeExecutionStep.execute(CaptureStateBeforeExecutionStep.java:71)
        at org.gradle.internal.execution.steps.CaptureStateBeforeExecutionStep.execute(CaptureStateBeforeExecutionStep.java:45)
        at org.gradle.internal.execution.steps.SkipEmptyWorkStep.executeWithNonEmptySources(SkipEmptyWorkStep.java:177)
        at org.gradle.internal.execution.steps.SkipEmptyWorkStep.execute(SkipEmptyWorkStep.java:81)
        at org.gradle.internal.execution.steps.SkipEmptyWorkStep.execute(SkipEmptyWorkStep.java:53)
        at org.gradle.internal.execution.steps.RemoveUntrackedExecutionStateStep.execute(RemoveUntrackedExecutionStateStep.java:32)
        at org.gradle.internal.execution.steps.RemoveUntrackedExecutionStateStep.execute(RemoveUntrackedExecutionStateStep.java:21)
        at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsStartedStep.execute(MarkSnapshottingInputsStartedStep.java:38)
        at org.gradle.internal.execution.steps.LoadPreviousExecutionStateStep.execute(LoadPreviousExecutionStateStep.java:36)
        at org.gradle.internal.execution.steps.LoadPreviousExecutionStateStep.execute(LoadPreviousExecutionStateStep.java:23)
        at org.gradle.internal.execution.steps.CleanupStaleOutputsStep.execute(CleanupStaleOutputsStep.java:75)
        at org.gradle.internal.execution.steps.CleanupStaleOutputsStep.execute(CleanupStaleOutputsStep.java:41)
        at org.gradle.internal.execution.steps.AssignWorkspaceStep.lambda$execute$0(AssignWorkspaceStep.java:32)
        at org.gradle.api.internal.tasks.execution.TaskExecution$4.withWorkspace(TaskExecution.java:287)
        at org.gradle.internal.execution.steps.AssignWorkspaceStep.execute(AssignWorkspaceStep.java:30)
        at org.gradle.internal.execution.steps.AssignWorkspaceStep.execute(AssignWorkspaceStep.java:21)
        at org.gradle.internal.execution.steps.IdentityCacheStep.execute(IdentityCacheStep.java:37)
        at org.gradle.internal.execution.steps.IdentityCacheStep.execute(IdentityCacheStep.java:27)
        at org.gradle.internal.execution.steps.IdentifyStep.execute(IdentifyStep.java:42)
        at org.gradle.internal.execution.steps.IdentifyStep.execute(IdentifyStep.java:31)
        at org.gradle.internal.execution.impl.DefaultExecutionEngine$1.execute(DefaultExecutionEngine.java:64)
        at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeIfValid(ExecuteActionsTaskExecuter.java:146)
        at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:135)
        at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46)
        at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:51)
        at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57)
        at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:74)
        at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:77)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:55)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
        at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:52)
        at org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:42)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:338)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:325)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:318)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:304)
        at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:463)
        at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:380)
        at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
        at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:49)
Caused by: java.lang.NoSuchMethodException: io.swagger.v3.jaxrs2.integration.SwaggerLoader.setOpenAPI31(java.lang.Boolean)
        at io.swagger.v3.plugins.gradle.tasks.ResolveTask.resolve(ResolveTask.java:441)
        ... 117 more
{code}

This can be reproduced by setting the plugin version to 2.2.8 and running the ""releaseTarGz"" Gradle task",,chia7712,mumrah,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 04:47:09 UTC 2023,,,,,,,,,,"0|z1gjk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 04:47;yash.mayya;[https://github.com/swagger-api/swagger-core/compare/v2.2.0...v2.2.1#diff-6dd7e9f5703cec610d73a0b72b757860dfd0373a65f9f9120b8ab96dcac5f735] 

 

[https://github.com/swagger-api/swagger-core/compare/v2.2.0...v2.2.1#diff-57de48456c464b5d8ec53c48ca8fba6d6b09f85385aec86139a121b61c319fee] 

 

I'm not sure why there was a breaking change in a patch release but we simply need to bump [these two|https://github.com/apache/kafka/blob/df86228aaffb315f22ca3f19511b344b0e0bb4b1/gradle/dependencies.gradle#L125-L126] up to 2.2.8 as well to fix the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encoded sensitive configs are not decoded before migration,KAFKA-14801,13528184,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,13/Mar/23 07:10,16/Mar/23 01:22,13/Jul/23 09:17,16/Mar/23 01:22,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,,,,kraft,,,,,,0,,,,,,,,akhileshchg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 05:35:31 UTC 2023,,,,,,,,,,"0|z1gizk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 05:35;akhileshchg;https://github.com/apache/kafka/pull/13384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade snappy-java Version to 1.1.9.1,KAFKA-14800,13528020,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjramos,jjramos,jjramos,10/Mar/23 17:50,24/Mar/23 15:28,13/Jul/23 09:17,24/Mar/23 15:27,3.3.3,3.4.1,3.5.0,,,,,,,,,,,,,3.5.0,,,,,,,clients,,,,,,0,,,,,,"The current {{kafka-clients}} latest released versions depend on {{snappy-java}} {{{}1.1.8.4{}}}. This particular version is affected by [bug 302|https://github.com/xerial/snappy-java/issues/302], which causes problems when using the OSGI  bundle on ARM based architectures. The bug itself has been fixed in version {{{}1.1.9.0{}}}, could the {{kafka-clients}} bundle be updated to transitively include the fixed version of {{{}snappy-java{}}}?.

—

Example {{build.gradle}} file:
{noformat}
plugins {
    id 'java'
}

group 'org.example'
version '1.0.0SNAPSHOT'

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.apache.kafka:kafka-clients:3.3.2'
}
{noformat}
 

Dependency Insight:
{noformat}
./gradlew -q dependencyInsight --dependency snappy-java --configuration runtimeClasspath
org.xerial.snappy:snappy-java:1.1.8.4
  Variant runtime:
    | Attribute Name                 | Provided     | Requested    |
    |--------------------------------|--------------|--------------|
    | org.gradle.status              | release      |              |
    | org.gradle.category            | library      | library      |
    | org.gradle.libraryelements     | jar          | jar          |
    | org.gradle.usage               | java-runtime | java-runtime |
    | org.gradle.dependency.bundling |              | external     |
    | org.gradle.jvm.environment     |              | standard-jvm |
    | org.gradle.jvm.version         |              | 11           |

org.xerial.snappy:snappy-java:1.1.8.4
\--- org.apache.kafka:kafka-clients:3.3.2
     \--- runtimeClasspath


{noformat}",ARM,jjramos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-10 17:50:53.0,,,,,,,,,,"0|z1ghz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source tasks fail if connector attempts to abort empty transaction,KAFKA-14799,13527847,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,09/Mar/23 15:46,16/Mar/23 12:34,13/Jul/23 09:17,14/Mar/23 19:10,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,KafkaConnect,,,,,,0,,,,,,"If a source task invokes [TransactionContext::abortTransaction|https://kafka.apache.org/34/javadoc/org/apache/kafka/connect/source/TransactionContext.html#abortTransaction()] while the current transaction is empty, and then returns an empty batch of records from the next (or current) invocation of {{{}SourceTask::poll{}}}, the task will fail.

This is because the Connect framework will honor the transaction abort request by invoking [KafkaProducer::abortTransaction|https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#abortTransaction()], but without having first invoked [KafkaProducer::beginTransaction|https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#beginTransaction()] (since no records had been received from the task), which leads to an {{{}IllegalStateException{}}}.

An example stack trace for this scenario:
{quote}[2023-03-09 10:41:25,053] ERROR [exactlyOnceQuestionMark|task-0] ExactlyOnceWorkerSourceTask\{id=exactlyOnceQuestionMark-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:210)
java.lang.IllegalStateException: TransactionalId exactly-once-source-integration-test-exactlyOnceQuestionMark-0: Invalid transition attempted from state READY to state ABORTING_TRANSACTION
    at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:974)
    at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:967)
    at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$beginAbort$3(TransactionManager.java:269)
    at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1116)
    at org.apache.kafka.clients.producer.internals.TransactionManager.beginAbort(TransactionManager.java:266)
    at org.apache.kafka.clients.producer.KafkaProducer.abortTransaction(KafkaProducer.java:835)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask$3.abortTransaction(ExactlyOnceWorkerSourceTask.java:495)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask$3.shouldCommitTransactionForBatch(ExactlyOnceWorkerSourceTask.java:473)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask$TransactionBoundaryManager.maybeCommitTransactionForBatch(ExactlyOnceWorkerSourceTask.java:398)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask.batchDispatched(ExactlyOnceWorkerSourceTask.java:186)
    at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:362)
    at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:202)
    at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:257)
    at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:75)
    at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:181)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
{quote}
 

As far as a fix goes, we have a few options:
 # Gracefully handle this case by translating the call to {{TransactionContext::abortTransaction}} into a no-op
 # Throw an exception (probably an {{{}IllegalStateException{}}}) from {{{}TransactionContext::abortTransaction{}}}, which may fail the task, but would give it the option to swallow the exception and continue processing if it would like
 # Forcibly fail the task without giving it the chance to swallow an exception, using a similar strategy to how we fail tasks that request that a transaction be committed and aborted for the same record (see [here|https://github.com/apache/kafka/blob/c5240c0390892fe9ecbe5285185c370e7be8b2aa/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTransactionContext.java#L78-L86])",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 15:49:34 UTC 2023,,,,,,,,,,"0|z1ggwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 15:49;ChrisEgerton;Right now I'm leaning toward option 1, probably with a {{{}WARN{}}}-level log message.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MM2 does not emit offset syncs when conservative translation logic exceeds positive max.offset.lag,KAFKA-14797,13527709,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,gharris1727,gharris1727,gharris1727,08/Mar/23 23:05,21/Mar/23 13:51,13/Jul/23 09:17,21/Mar/23 13:51,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,mirrormaker,,,,,,0,,,,,,"This is a regression in MirrorMaker 2 introduced by KAFKA-12468.

Reproduction steps:
1. Set max.offset.lag to a non-zero value.
2. Set up a 1-1 replication flow which does not skip upstream offsets or have a concurrent producer to the target topic.
3. Produce more than max.offset.lag records to the source topic and allow replication to proceed.
4. Examine end offsets, checkpoints and/or target consumer group lag

Expected behavior:
Consumer group lag should be at most max.offset.lag.

Actual behavior:
Consumer group lag is significantly larger than max.offset.lag.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12468,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-08 23:05:15.0,,,,,,,,,,"0|z1gg20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to deserialize base64 JSON strings ,KAFKA-14794,13527672,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jsancio,jsancio,jsancio,08/Mar/23 17:14,14/Mar/23 19:22,13/Jul/23 09:17,14/Mar/23 19:22,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"h1. Problem

The following test fails:
{code:java}
@Test
public void testBinaryNode() throws IOException {
    byte[] expected = new byte[] {5, 2, 9, 4, 1, 8, 7, 0, 3, 6};
    StringWriter writer = new StringWriter();
    ObjectMapper mapper = new ObjectMapper();

    mapper.writeTree(mapper.createGenerator(writer), new BinaryNode(expected));

    JsonNode binaryNode = mapper.readTree(writer.toString());

    assertTrue(binaryNode.isTextual(), binaryNode.toString());
    byte[] actual = MessageUtil.jsonNodeToBinary(binaryNode, ""Test base64 JSON string"");
    assertEquals(expected, actual);
}
{code}
with the following error:
{code:java}
 Gradle Test Run :clients:test > Gradle Test Executor 20 > MessageUtilTest > testBinaryNode() FAILED
    java.lang.RuntimeException: Test base64 JSON string: expected Base64-encoded binary data.
        at org.apache.kafka.common.protocol.MessageUtil.jsonNodeToBinary(MessageUtil.java:165)
        at org.apache.kafka.common.protocol.MessageUtilTest.testBinaryNode(MessageUtilTest.java:102)
{code}
The reason for the failure is because FasterXML Jackson deserializes base64 JSON strings to a TextNode not to a BinaryNode.
h1. Solution

The method {{MessageUtil::jsonNodeToBinary}} should not assume that the input {{JsonNode}} is always a {{{}BinaryNode{}}}. It should also support decoding {{{}TextNode{}}}.

{{JsonNode::binaryValue}} is supported by both {{BinaryNode}} and {{{}TextNode{}}}.",,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-08 17:14:36.0,,,,,,,,,,"0|z1gfts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in LazyIndex.get(),KAFKA-14792,13527513,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ijuma,ijuma,ijuma,07/Mar/23 20:52,07/Mar/23 23:56,13/Jul/23 09:17,07/Mar/23 23:56,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"`LazyIndex.get()` has a race condition that can result in a ClassCastException being thrown in some cases.

This was introduced when it was rewritten from Scala to Java.",,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-07 20:52:25.0,,,,,,,,,,"0|z1geug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MM2 logs misleading error during topic ACL sync when broker does not have authorizer configured,KAFKA-14781,13527302,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,06/Mar/23 16:02,08/Mar/23 15:36,13/Jul/23 09:17,08/Mar/23 15:26,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,mirrormaker,,,,,,0,,,,,,"When there is no broker-side authorizer configured on a Kafka cluster targeted by MirrorMaker 2, users see error-level log messages like this one:{{{}{}}}
{quote}[2023-03-06 10:53:57,488] ERROR [MirrorSourceConnector|worker] Scheduler for MirrorSourceConnector caught exception in scheduled task: syncing topic ACLs (org.apache.kafka.connect.mirror.Scheduler:102)
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SecurityDisabledException: No Authorizer is configured on the broker
    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
    at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
    at org.apache.kafka.connect.mirror.MirrorSourceConnector.listTopicAclBindings(MirrorSourceConnector.java:456)
    at org.apache.kafka.connect.mirror.MirrorSourceConnector.syncTopicAcls(MirrorSourceConnector.java:342)
    at org.apache.kafka.connect.mirror.Scheduler.run(Scheduler.java:93)
    at org.apache.kafka.connect.mirror.Scheduler.executeThread(Scheduler.java:112)
    at org.apache.kafka.connect.mirror.Scheduler.lambda$scheduleRepeating$0(Scheduler.java:50)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.errors.SecurityDisabledException: No Authorizer is configured on the broker
{quote}
This can be misleading as it looks like something is wrong with MM2 or the Kafka cluster. In reality, it's usually fine, since topic ACL syncing is enabled by default and it's reasonable for Kafka clusters (especially in testing/dev environments) to not have authorizers enabled.

We should try to catch this specific case and downgrade the severity of the log message from {{ERROR}} to either {{INFO}} or {{{}DEBUG{}}}. We may also consider suggesting to users that they disable topic ACL syncing if their Kafka cluster doesn't have authorization set up, but this should probably only be emitted once over the lifetime of the connector in order to avoid generating log spam.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-06 16:02:18.0,,,,,,,,,,"0|z1gdjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the removed listeners should not be reconfigurable,KAFKA-14774,13526874,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,02/Mar/23 15:09,29/Mar/23 14:08,13/Jul/23 09:17,27/Mar/23 10:48,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,,,,,,"Users can alter broker configuration to remove specify listeners. However, the removed listeners are NOT removed from `reconfigurables` list. It can result in the idle processors if users increases the network threads subsequently.",,chia7712,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-02 15:09:31.0,,,,,,,,,,"0|z1gaww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE while converting OffsetFetch from version < 8 to version >= 8,KAFKA-14744,13525894,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,23/Feb/23 12:21,23/Feb/23 17:32,13/Jul/23 09:17,23/Feb/23 17:32,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"While refactoring the OffsetFetch handling in KafkaApis, we introduced a NullPointerException (NPE). The NPE arises when the FetchOffset API is called with a client using a version older than version 8 and using null for the topics to signal that all topic-partition offsets must be returned. This means that this bug mainly impacts admin tools. The consumer does not use null.

This NPE is here: https://github.com/apache/kafka/commit/24a86423e9907b751d98fddc7196332feea2b48d#diff-0f2f19fd03e2fc5aa9618c607b432ea72e5aaa53866f07444269f38cb537f3feR237.

We missed this during the refactor because we had no tests in place to test this mode.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-23 12:21:30.0,,,,,,,,,,"0|z1g4vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MessageConversionsTimeMs for fetch request metric is not updated,KAFKA-14743,13525860,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,23/Feb/23 10:11,13/Jul/23 06:58,13/Jul/23 09:17,26/Feb/23 07:17,3.3.1,3.4.0,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,core,,,,,,0,,,,,,"During message conversion, there are 2 metrics we should update as doc written:

!image-2023-02-23-18-09-24-916.png|width=652,height=121!

 

In KAFKA-14295, it is addressing the issue in FetchMessageConversionsPerSec metric. This ticket will address the issue in *kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request=fetch.*

 

 

 ",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14295,,,,"23/Feb/23 10:09;showuon;image-2023-02-23-18-09-24-916.png;https://issues.apache.org/jira/secure/attachment/13055757/image-2023-02-23-18-09-24-916.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 06:58:10 UTC 2023,,,,,,,,,,"0|z1g4o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/23 06:58;showuon;PR: https://github.com/apache/kafka/pull/13297;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The kafakConsumer pollForFetches(timer) method takes up a lot of cpu due to the abnormal exit of the heartbeat thread,KAFKA-14729,13525134,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,RivenSun,RivenSun,RivenSun,17/Feb/23 05:16,02/Mar/23 03:37,13/Jul/23 09:17,02/Mar/23 03:37,3.3.0,,,,,,,,,,,,,,,3.5.0,,,,,,,consumer,,,,,,0,,,,,,"h2. case situation:

1. The business program occupies a large amount of memory, causing the `run()` method of HeartbeatThread of kafkaConsumer to exit abnormally.
{code:java}
2023-02-14 06:55:57.771[][ERROR][AbstractCoordinator][kafka-coor][Consumer clientId=consumer-5, groupId=*****_dev_VA] Heartbeat thread failed due to unexpected error java.lang.OutOfMemoryError: Java heap space {code}
2. The finally module of the heartbeat thread ` run()` method only prints the log, but does not update the value of `AbstractCoordinator.state`.
3. For kafkaConsumer with the groupRebalance mechanism enabled, in the `kafkaConsumer#pollForFetches(timer)` method, pollTimeout may eventually take the value `timeToNextHeartbeat(now)`.
4. Since the heartbeat thread has exited, `heartbeatTimer.deadlineMs` will never be updated again.
And the `AbstractCoordinator.state` field value will always be {*}STABLE{*},
So the `timeToNextHeartbeat(long now)` method will return {color:#ff0000}0{color}.
0 will be passed to the underlying `networkClient#poll` method.

 

In the end, the user calls the `poll(duration)` method in an endless loop, and the `kafkaConsumer#pollForFetches(timer)` method will always return very quickly, taking up a lot of cpu.

 
h2. solution:

1. Refer to the note of `MemberState.STABLE` :
{code:java}
the client has joined and is sending heartbeats.{code}
When the heartbeat thread exits, in `finally` module, we should add code:
{code:java}
state = MemberState.UNJOINED;
closed = true;{code}
2. In the `AbstractCoordinator#timeToNextHeartbeat(now)` method, add a new judgment condition: `heartbeatThread.hasFailed()`
{code:java}
if (state.hasNotJoinedGroup() || heartbeatThread.hasFailed())
    return Long.MAX_VALUE;
return heartbeat.timeToNextHeartbeat(now);{code}
 ",,adupriez,kirktrue,pnee,RivenSun,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/23 05:15;RivenSun;image-2023-02-17-13-15-50-362.png;https://issues.apache.org/jira/secure/attachment/13055540/image-2023-02-17-13-15-50-362.png","17/Feb/23 05:16;RivenSun;jstack_highCpu.txt;https://issues.apache.org/jira/secure/attachment/13055539/jstack_highCpu.txt",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 08:08:24 UTC 2023,,,,,,,,,,"0|z1g074:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 05:29;RivenSun;Hi [~guozhang]   , [~showuon]
Could you give some suggestions for this issue?
Thanks.;;;","17/Feb/23 06:05;RivenSun;1. After careful consideration, in the case of KafkaConsumer's normal close, in the `finally` module of heartbeatThread `run()` method:
{code:java}
finally {
    log.debug(""Heartbeat thread has closed"");
    closed = true;
}{code}

2.The `AbstractCoordinator#timeToNextHeartbeat(now)` method is modified as follows:
{code:java}
protected synchronized long timeToNextHeartbeat(long now) {
    // if we have not joined the group or we are preparing rebalance,
    // we don't need to send heartbeats
    if (state.hasNotJoinedGroup() ||
            (heartbeatThread != null && (heartbeatThread.hasFailed() || heartbeatThread.closed)))
        return Long.MAX_VALUE;
    return heartbeat.timeToNextHeartbeat(now);
}{code}
The case of `heartbeatThread==null` is not considered. Because `heartbeatThread` will be updated to *null* only after KafkaConsumer calls the `close()` method.;;;","17/Feb/23 07:56;showuon;The change makes sense to me. But I'm more interested in knowing why the heartbeat thread will be OOM? resource leak? Do you have any clue?

 ;;;","17/Feb/23 08:08;RivenSun;In fact, this is due to the abnormal code of the business side, which applied for a large amount of memory in a short period of time, reaching the maximum heap size requested by the java program.
In turn, it may cause the heartbeat thread to throw an OOM exception when creating HeartbeatRequest.

And OOM is just one of the abnormal exit conditions of heartbeatThread.

In short, when the heartbeat thread exits normally or abnormally, the closed field should be updated to true.


[~showuon] Thanks for your reply.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect EOS mode should periodically call task commit,KAFKA-14727,13525066,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,16/Feb/23 17:11,17/Feb/23 17:22,13/Jul/23 09:17,16/Feb/23 23:52,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,KafkaConnect,,,,,,1,,,,,,"In non-EOS mode, there is a background thread which periodically commits offsets for a task. If this thread does not have resources to flush on the framework side (records, or offsets) it still calls the task's commit() method to update the internal state of the task.

In EOS mode, there is no background thread, and all offset commits are performed on the main task thread in response to sending records to Kafka. This has the effect of only triggering the task's commit() method when there are records to send to Kafka, which is different than non-EOS mode.

In order to bring the two modes into better alignment, and allow tasks reliant on the non-EOS empty commit() behavior to work in EOS mode out-of-the-box, EOS mode should provide offset commits periodically for tasks which do not produce records.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12468,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-16 17:11:29.0,,,,,,,,,,"0|z1fzs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaStreams can' get running if the rebalance happens before StreamThread gets shutdown completely,KAFKA-14717,13524711,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,14/Feb/23 19:37,24/Feb/23 19:46,13/Jul/23 09:17,24/Feb/23 19:46,,,,,,,,,,,,,,,,3.5.0,,,,,,,streams,,,,,,0,,,,,,"I noticed this issue when tracing KAFKA-7109

StreamThread closes the consumer before changing state to DEAD. If the partition rebalance happens quickly, the other StreamThreads can't change KafkaStream state from REBALANCING to RUNNING since there is a PENDING_SHUTDOWN StreamThread",,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-14 19:37:16.0,,,,,,,,,,"0|z1fxlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams global table startup takes too long,KAFKA-14713,13524663,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,tamasszekeres,tamasszekeres,14/Feb/23 12:29,17/Feb/23 19:56,13/Jul/23 09:17,17/Feb/23 19:56,3.0.2,,,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,,,,,,"*Some context first*

We have a spring based kafka streams application. This application is listening to two topics. Let's call them apartment and visitor. The apartments are stored in a global table, while the visitors are in the stream we are processing, and at one point we are joining the visitor stream together with the apartment table. In our test environment, both topics contain 10 partitions.

*Issue*

At first deployment, everything goes fine, the global table is built and all entries in the stream are processed.

After everything is finished, we shut down the application, restart it and send out a new set of visitors. The application seemingly does not respond.

After some more debugging it turned out that it simply takes 5 minutes to start up, because the global table takes 30 seconds (default value for the global request timeout) to accept that there are no messages in the apartment topics, for each and every partition. If we send out the list of apartments as new messages, the application starts up immediately.

To make matters worse, we have clients with 96 partitions, where the startup time would be 48 minutes. Not having messages in the topics between application shutdown and restart is a valid use case, so this is quite a big problem.

*Possible workarounds*

We could reduce the request timeout, but since this value is not specific for the global table initialization, but a global request timeout for a lot of things, we do not know what else it will affect, so we are not very keen on doing that. Even then, it would mean a 1.5 minute delay for this particular client (more if we will have other use cases in the future where we will need to use more global tables), which is far too much, considering that the application would be able to otherwise start in about 20 seconds.

*Potential solutions we see*
 # Introduce a specific global table initialization timeout in GlobalStateManagerImpl. Then we would be able to safely modify that value without fear of making some other part of kafka unstable.
 # Parallelize the initialization of the global table partitions in GlobalStateManagerImpl: knowing that the delay at startup is constant instead of linear with the number of partitions would be a huge help.
 # As long as we receive a response, accept the empty map in the KafkaConsumer, and continue instead of going into a busy-waiting loop.",,mjsax,tamasszekeres,,,,,,,,,,,,,,,,,,,,KAFKA-12980,,,,,,,,,,KAFKA-10315,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 19:55:38 UTC 2023,,,,,,,,,,"0|z1fxao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 19:23;mjsax;Sounds like a duplicate to https://issues.apache.org/jira/browse/KAFKA-14442 ? Can we close this ticket?;;;","15/Feb/23 08:51;tamasszekeres;Hi [~mjsax] looks similar, but not exactly. They see this issue with exactly_once_beta processing guarantee, while we have it with the default at_least_once. For us the important part is that is issue is fixed (or at least a safe workaround is provided) as soon as possible, because right now I am between a rock and a hard place because of it.;;;","15/Feb/23 21:43;mjsax;What version are you using? – Also, can you point me to the code where it actually waits/hangs (as you did already looked into it, it would be quicker this way). – I am not sure yet, if both issues are actually the same though or not. (Maybe the ""eos"" config on the other ticket is a red herring.) But I guess we can dig into it a little bit.;;;","16/Feb/23 05:35;tamasszekeres;Entry point would be the [GlobalStateManagerImpl|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java], where the [pollMsPlusRequestTimeout|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L115] is defined as POLL_MS_CONFIG (0.1 sec, which is fine), plus [REQUEST_TIMEOUT_MS_CONFIG|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L113] (30 sec, which is the part causing the problem). Then in [restoreState|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L240] we start [polling the globalConsumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L274] with this inflated poll timeout.

After that we jump into the [KafkaConsumer|https://github.com/apache/kafka/blob/3.0.2/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java], and start the [polling|https://github.com/apache/kafka/blob/3.0.2/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1238] for new records, which will give us back an empty map, since there are no new entries coming in, and this loop will get stuck for 30 seconds.

Now the first set of links are pointing to the trunk, while the second set are pointing to the 3.0.2 tag, because the trunk has changed significantly since what I have, and the changes in 3.4.0 might actually solve my problem, so before continuing, let me spend some time trying to verify this (I have to admit previously I only checked the latest version of the GlobalStateManagerImpl, and there there were no significant changes yet) :)

Edit: okay, 3.4.0 does not have this issue anymore, which means that KAFKA-14442 might also be resolved with it. Anyway I'm closing this ticket. Thanks for the help :);;;","16/Feb/23 23:51;mjsax;Thanks for getting back. Glad it's resolved. I am not sure why though. Comparing 3.4 and 3.0 code, it seems they do the same thing.

In the end, if you have valid checkpoint on restart, you should not even hit `poll(pollMsPlusRequestTimeout)` during restore, because it should hold that `offset == highWatermark` and we should not enter the while-loop...

For K14442, we know that `offset == highWatermark - 1` (because we write the ""incorrect"" watermark into the checkpoint file), and thus `poll()` is executed and hangs because there is no data – the last ""record"" is just a commit marker.;;;","17/Feb/23 07:52;tamasszekeres;The difference (at least for my use case) is on the KafkaConsumer side, where the [pollForFetches|https://github.com/apache/kafka/blob/3.4.0/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1243] returns a Fetch instead of just a Map. The Fetch object is able to differentiate between ""no new message"" and ""could not get data"", so it does not wait until the request timeout passes anymore for a new record to arrive, but will return immediately.;;;","17/Feb/23 19:55;mjsax;Ah. Thanks. That makes sense. Did not look into the consumer code, only streams. So it's fixed via https://issues.apache.org/jira/browse/KAFKA-12980 in 3.2.0 – updated the ticket accordingly. Thanks for getting back. It bugged my that I did not understand it :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafaka-metadata-quorum.sh does not honor --command-config,KAFKA-14711,13524513,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rndgstn,rndgstn,rndgstn,13/Feb/23 17:32,13/Feb/23 23:43,13/Jul/23 09:17,13/Feb/23 23:43,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,,,,kraft,,,,,,0,,,,,,https://github.com/apache/kafka/pull/12951 accidentally eliminated support for the `--command-config` option in the `kafka-metadata-quorum.sh` command.  This was an undetected regression in the 3.4.0 release.,,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-13 17:32:44.0,,,,,,,,,,"0|z1fwdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Follower should truncate before incrementing high watermark,KAFKA-14704,13524170,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,10/Feb/23 07:41,15/Feb/23 08:55,13/Jul/23 09:17,14/Feb/23 09:03,,,,,,,,,,,,,,,,2.8.3,3.0.3,3.1.3,3.2.4,3.3.3,3.4.1,3.5.0,,,,,,,0,,,,,,"When a leader becomes a follower, it is likely that it has uncommitted records in its log. When it reaches out to the leader, the leader will detect that they have diverged and it will return the diverging epoch and offset. The follower truncates it log based on this.

There is a small caveat in this process. When the leader return the diverging epoch and offset, it also includes its high watermark, low watermark, start offset and end offset. The current code in the `AbstractFetcherThread` works as follow. First it process the partition data and then it checks whether there is a diverging epoch/offset. The former may accidentally expose uncommitted records as this step updates the local watermark to whatever is received from the leader. As the follower, or the former leader, may have uncommitted records, it will be able to updated the high watermark to a larger offset if the leader has a higher watermark than the current local one. This result in exposing uncommitted records until the log is finally truncated. The time window is short but a fetch requests coming at the right time to the follower could read those records. This is especially true for clients out there which uses recent versions of the fetch request but without implementing KIP-320.

When this happens, the follower logs the following message: `Non-monotonic update of high watermark from (offset=21437 segment=[20998:98390]) to (offset=21434 segment=[20998:97843])`.

This patch proposes to mitigate the issue by starting by checking on whether a diverging epoch/offset is provided by the leader and skip processing the partition data if it is. This basically means that the first fetch request will result in truncating the log and a subsequent fetch request will update the log/high watermarks.",,dajac,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 09:57:00 UTC 2023,,,,,,,,,,"0|z1fu94:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"10/Feb/23 09:40;showuon;[~dajac] , could you add some bug description? Thanks.

 ;;;","10/Feb/23 09:53;dajac;[~showuon] Done. Sorry for that.;;;","10/Feb/23 09:57;showuon;No worries! Nice catch!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2023-25194: Apache Kafka: Possible RCE/Denial of service attack via SASL JAAS JndiLoginModule configuration using Kafka Connect,KAFKA-14696,13523934,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,millie,millie,09/Feb/23 09:29,14/Feb/23 14:26,13/Jul/23 09:17,14/Feb/23 14:26,2.8.1,2.8.2,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"CVE Reference: [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-34917]

 

Will Kafka 2.8.X provide a patch to fix this vulnerability?

If yes, when will the patch be provided?

 

Thanks",,millie,omkreddy,qingqingqingtian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 13:58:23 UTC 2023,,,,,,,,,,"0|z1fssw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 18:34;omkreddy;There are no plans to provide a patch for older versions (< 3.40)

We have given some advice as part CVE announcement: https://lists.apache.org/thread/vy1c7fqcdqvq5grcqp6q5jyyb302khyz;;;","10/Feb/23 08:06;millie;As you said, I'm going to fix this by turning the commit ae22ec1a0ea005664439c3f45111aa34390ecaa1 of 3.4 into the 2.8 branch. Are there any other suggestions for repair this on 2.8 ?;;;","10/Feb/23 13:58;omkreddy;Yes, you can fix cherry picking the commit ae22ec1a0ea005664439c3f45111aa34390ecaa1 2.8 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPCProducerIdManager should not wait for a new block,KAFKA-14694,13523844,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,08/Feb/23 22:31,12/Jul/23 03:28,13/Jul/23 09:17,22/Jun/23 17:30,,,,,,,,,,,,,,,,3.6.0,,,,,,,,,,,,,0,,,,,,"RPCProducerIdManager initiates an async request to the controller to grab a block of producer IDs and then blocks waiting for a response from the controller.

This is done in the request handler threads while holding a global lock. This means that if many producers are requesting producer IDs and the controller is slow to respond, many threads can get stuck waiting for the lock.

This may also be a deadlock concern under the following scenario:

if the controller has 1 request handler thread (1 chosen for simplicity) and receives an InitProducerId request, it may deadlock.
basically any time the controller has N InitProducerId requests where N >= # of request handler threads has the potential to deadlock.

consider this:
1. the request handler thread tries to handle an InitProducerId request to the controller by forwarding an AllocateProducerIds request.
2. the request handler thread then waits on the controller response (timed poll on nextProducerIdBlock)
3. the controller's request handler threads need to pick this request up, and handle it, but the controller's request handler threads are blocked waiting for the forwarded AllocateProducerIds response.

 

We should not block while waiting for a new block and instead return immediately to free the request handler threads.",,jeffkbkim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 22:31:01.0,,,,,,,,,,"0|z1fs8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft Controller and ProcessExitingFaultHandler can deadlock shutdown,KAFKA-14693,13523824,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jsancio,jsancio,jsancio,08/Feb/23 19:10,14/Feb/23 18:07,13/Jul/23 09:17,14/Feb/23 18:07,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,controller,,,,,,0,,,,,,"h1. Problem

When the kraft controller encounters an error that it cannot handle it calls {{ProcessExitingFaultHandler}} which calls {{Exit.exit}} which calls {{{}Runtime.exit{}}}.

Based on the Runtime.exit documentation:
{quote}All registered [shutdown hooks|https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#addShutdownHook-java.lang.Thread-], if any, are started in some unspecified order and allowed to run concurrently until they finish. Once this is done the virtual machine [halts|https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#halt-int-].
{quote}
One of the shutdown hooks registered by Kafka is {{{}Server.shutdown(){}}}. This shutdown hook eventually calls {{{}KafkaEventQueue.close{}}}. This last close method joins on the controller thread. Unfortunately, the controller thread also joined waiting for the shutdown hook thread to finish.

Here are an sample thread stacks:
{code:java}
   ""QuorumControllerEventHandler"" #45 prio=5 os_prio=0 cpu=429352.87ms elapsed=620807.49s allocated=38544M defined_classes=353 tid=0x00007f5aeb31f800 nid=0x80c in Object.wait()  [0x00007f5a658fb000]
     java.lang.Thread.State: WAITING (on object monitor)                                                                                                                                                                                                            at java.lang.Object.wait(java.base@17.0.5/Native Method)
          - waiting on <no object reference available>
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1304)
          - locked <0x00000000a29241f8> (a org.apache.kafka.common.utils.KafkaThread)
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1372)
          at java.lang.ApplicationShutdownHooks.runHooks(java.base@17.0.5/ApplicationShutdownHooks.java:107)
          at java.lang.ApplicationShutdownHooks$1.run(java.base@17.0.5/ApplicationShutdownHooks.java:46)
          at java.lang.Shutdown.runHooks(java.base@17.0.5/Shutdown.java:130)
          at java.lang.Shutdown.exit(java.base@17.0.5/Shutdown.java:173)
          - locked <0x00000000ffe020b8> (a java.lang.Class for java.lang.Shutdown)
          at java.lang.Runtime.exit(java.base@17.0.5/Runtime.java:115)
          at java.lang.System.exit(java.base@17.0.5/System.java:1860)
          at org.apache.kafka.common.utils.Exit$2.execute(Exit.java:43)
          at org.apache.kafka.common.utils.Exit.exit(Exit.java:66)
          at org.apache.kafka.common.utils.Exit.exit(Exit.java:62)
          at org.apache.kafka.server.fault.ProcessExitingFaultHandler.handleFault(ProcessExitingFaultHandler.java:54)
          at org.apache.kafka.controller.QuorumController$ControllerWriteEvent$1.apply(QuorumController.java:891)
          at org.apache.kafka.controller.QuorumController$ControllerWriteEvent$1.apply(QuorumController.java:874)
          at org.apache.kafka.controller.QuorumController.appendRecords(QuorumController.java:969){code}
and
{code:java}
  ""kafka-shutdown-hook"" #35 prio=5 os_prio=0 cpu=43.42ms elapsed=378593.04s allocated=4732K defined_classes=74 tid=0x00007f5a7c09d800 nid=0x4f37 in Object.wait()  [0x00007f5a47afd000]
     java.lang.Thread.State: WAITING (on object monitor)
          at java.lang.Object.wait(java.base@17.0.5/Native Method)
          - waiting on <no object reference available>
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1304)
          - locked <0x00000000a272bcb0> (a org.apache.kafka.common.utils.KafkaThread)
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1372)
          at org.apache.kafka.queue.KafkaEventQueue.close(KafkaEventQueue.java:509)
          at org.apache.kafka.controller.QuorumController.close(QuorumController.java:2553)
          at kafka.server.ControllerServer.shutdown(ControllerServer.scala:521)
          at kafka.server.KafkaRaftServer.shutdown(KafkaRaftServer.scala:184)
          at kafka.Kafka$.$anonfun$main$3(Kafka.scala:99)
          at kafka.Kafka$$$Lambda$406/0x0000000800fb9730.apply$mcV$sp(Unknown Source)
          at kafka.utils.Exit$.$anonfun$addShutdownHook$1(Exit.scala:38)
          at kafka.Kafka$$$Lambda$407/0x0000000800fb9a10.run(Unknown Source)
          at java.lang.Thread.run(java.base@17.0.5/Thread.java:833)
          at org.apache.kafka.common.utils.KafkaThread.run(KafkaThread.java:64) {code}
h1. Possible Solution

A possible solution is to have the controller's unhandled fault handler call {{Runtime.halt}} instead of {{{}Runtime.exit{}}}.",,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 19:10:55.0,,,,,,,,,,"0|z1fs4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible issue on documentation : producerConfig -> retries,KAFKA-14687,13523482,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,2me,2me,07/Feb/23 11:36,07/Feb/23 23:33,13/Jul/23 09:17,07/Feb/23 23:33,3.3.2,,,,,,,,,,,,,,,,,,,,,,documentation,,,,,,0,,,,,,"Since Kafka 3, The documentation said [here|https://kafka.apache.org/documentation/#producerconfigs_retries] that
{code:java}
Allowing retries while setting enable.idempotence to false and max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first. {code}
 

 

Or I think that a ""without"" is missing

Allowing retries while setting enable.idempotence to false and max.in.flight.requests.per.connection to 1 will potentially change the ordering of records

Should be

 

Allowing retries while setting enable.idempotence to false and +*without setting*+ max.in.flight.requests.per.connection to 1 will potentially change the ordering of records",,2me,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 23:33:20 UTC 2023,,,,,,,,,,"0|z1fq0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 23:33;2me;I see that was fixed at this revision number : d2556e02a25af28cf4cc093a190cc0154145121e

[https://github.com/apache/kafka/commit/d2556e02a25af28cf4cc093a190cc0154145121e]

But not yet update on the website

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token endpoint URL used for OIDC cannot be set on the JAAS config,KAFKA-14676,13523203,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,06/Feb/23 09:57,12/Feb/23 20:04,13/Jul/23 09:17,12/Feb/23 20:04,3.1.2,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,security,,,,,,0,,,,,,"Kafka allows multiple clients within a JVM to use different SASL configurations by configuring the JAAS configuration in `sasl.jaas.config` instead of the JVM-wide system property. For SASL login, we reuse logins within a JVM by caching logins indexed by their sasl.jaas.config. This relies on login configs being overridable using `sasl.jaas.config`. 

KIP-768 ([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186877575)] added support for OIDC for SASL/OAUTHBEARER. The token endpoint used to acquire tokens can currently only be configured using the Kafka config `sasl.oauthbearer.token.endpoint.url`. This prevents different clients within a JVM from using different URLs. We need to either provide a way to override the URL within `sasl.jaas.config` or include more of the client configs in the LoginMetadata used as key for cached logins.",,kirktrue,rsivaram,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-06 09:57:42.0,,,,,,,,,,"0|z1foag:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Raft idle ratio is inaccurate,KAFKA-14664,13522148,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,31/Jan/23 03:09,15/Feb/23 22:40,13/Jul/23 09:17,15/Feb/23 22:40,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"The `poll-idle-ratio-avg` metric is intended to track how idle the raft IO thread is. When completely idle, it should measure 1. When saturated, it should measure 0. The problem with the current measurements is that they are treated equally with respect to time. For example, say we poll twice with the following durations:

Poll 1: 2s

Poll 2: 0s

Assume that the busy time is negligible, so 2s passes overall.

In the first measurement, 2s is spent waiting, so we compute and record a ratio of 1.0. In the second measurement, no time passes, and we record 0.0. The idle ratio is then computed as the average of these two values (1.0 + 0.0 / 2 = 0.5), which suggests that the process was busy for 1s, which overestimates the true busy time.

Instead, we should sum up the time waiting over the full interval. 2s passes total here and 2s is idle, so we should compute 1.0.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 03:09:17.0,,,,,,,,,,"0|z1fhu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ACL listings in documentation are out of date,KAFKA-14662,13522101,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tinaselenge,mimaison,mimaison,30/Jan/23 17:34,10/May/23 08:23,13/Jul/23 09:17,08/May/23 06:36,,,,,,,,,,,,,,,,3.6.0,,,,,,,core,docs,,,,,0,,,,,,"ACLs listed in https://kafka.apache.org/documentation/#operations_resources_and_protocols are out of date. They only cover API keys up to 47 (OffsetDelete) and don't include DescribeClientQuotas, AlterClientQuotas, DescribeUserScramCredentials, AlterUserScramCredentials, DescribeQuorum, AlterPartition, UpdateFeatures, DescribeCluster, DescribeProducers, UnregisterBroker, DescribeTransactions, ListTransactions, AllocateProducerIds.

This is hard to keep up to date so we should consider whether this could be generated automatically.",,jeqo,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-30 17:34:00.0,,,,,,,,,,"0|z1fhk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Divide by zero security vulnerability (sonatype-2019-0422),KAFKA-14660,13522063,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mjsax,BigAndy,BigAndy,30/Jan/23 12:30,08/Feb/23 05:40,13/Jul/23 09:17,04/Feb/23 08:49,3.3.2,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,streams,,,,,,0,,,,,,"Looks like SonaType has picked up a ""Divide by Zero"" issue reported in a PR and, because the PR was never merged, is now reporting it as a security vulnerability in the latest Kafka Streams library.

 

See:
 * [Vulnerability: sonatype-2019-0422]([https://ossindex.sonatype.org/vulnerability/sonatype-2019-0422?component-type=maven&component-name=org.apache.kafka%2Fkafka-streams&utm_source=ossindex-client&utm_medium=integration&utm_content=1.7.0)]

 * [Original PR]([https://github.com/apache/kafka/pull/7414])

 

While it looks from the comments made by [~mjsax] and [~bbejeck] that the divide-by-zero is not really an issue, the fact that its now being reported as a vulnerability is, especially with regulators.

PITA, but we should consider either getting this vulnerability removed (Google wasn't very helpful in providing info on how to do this), or fixed (Again, not sure how to tag the fix as fixing this issue).  One option may just be to reopen the PR and merge (and then fix forward by switching it to throw an exception).",,BigAndy,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 22:04:28 UTC 2023,,,,,,,,,,"0|z1fhbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 20:07;BigAndy;Hey Matthias ;);;;","30/Jan/23 21:49;mjsax;Hey Andy – thanks for the ticket. We would have accepted a PR, too. :D;;;","30/Jan/23 23:25;BigAndy;As per description, I think the best 'fix' might be to re-open the PR and merge it, as it looks to me that the vulnerability report it tied to the PR.  

If you know how I can tag a new PR to resolve the vulnerability, then I'm all ears.;;;","31/Jan/23 16:36;mjsax;The original PR did not make sense, as if totalCapacity would really be zero, there is a bug and just setting it to 1 does not sound right. I did already merge a new PR that just raises an exception for this case, and thus avoid divide-by-zero. This should resolve the issue.;;;","03/Feb/23 00:06;BigAndy;The issue here is more the SonaType security vulnerability report than any impossible to reach divide by zero issue. Unfortunately, I'm struggling to find information on _how_ to mark the vulnerability resolved in SonaType.  This was why I was suggesting opening and merging the PR, as it seems the PR is the cause of the report.

I realise the PR's solution wasn't ideal. Hence I was suggesting to merge and put in a second change after to fix the fix, so to speak.

If you've already summited a fix for the DBZ, then I see two potential ways forward:
 # work out how to inform SonaType the issue is fixed:
 ## There is a [Report correction|https://ossindex.sonatype.org/doc/report-vulnerability] link on the bug report.  May you, or I if you let me know the PR you fixed the DBZ in, can use this to raise the fact its been fixed?
 ## Maybe just tagging the [SonaType issue|https://ossindex.sonatype.org/vulnerability/sonatype-2019-0422?component-type=maven&component-name=org.apache.kafka%2Fkafka-streams&utm_source=ossindex-client&utm_medium=integration&utm_content=1.7.0] in your PR would be enough?
 ## Does someone in Confluent know about this stuff that you can talk to?
 ## ????
 # reopen, 'adjust' and merge the original PR... hopefully triggering SonaType to mark the issue resolved.;;;","03/Feb/23 00:09;BigAndy;[~mjsax] can you provide a link to the PR that fixed this DBZ issue?  I'm going to email sonatype and link to this issue...;;;","03/Feb/23 16:50;mjsax;Not sure why the PR was not auto-linked... Fixed.

[https://github.com/apache/kafka/pull/13175]

Thanks for your follow up. Can we close this ticket? Let me know if there is anything else I can do.;;;","04/Feb/23 08:50;BigAndy;[~mjsax] Just want to check this won't be released until 3.5.0.  Next public release is 3.4.0.  I'm assuming this missed the cut off, but just want to check its not a typo...;;;","06/Feb/23 22:04;mjsax;Correct. 3.4.0 is already voted and should be released soon. I plan to cherry-pick for 3.4.1 after 3.4.0 is out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"source-record-write-[rate|total] metrics include filtered records",KAFKA-14659,13521821,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,hgeraldino,cbeard,cbeard,28/Jan/23 01:42,28/Feb/23 14:50,13/Jul/23 09:17,28/Feb/23 14:41,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,KafkaConnect,,,,,,0,,,,,,"Source tasks in Kafka connect offer two sets of metrics (documented in [ConnectMetricsRegistry.java|https://github.com/apache/kafka/blob/72cfc994f5675be349d4494ece3528efed290651/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectMetricsRegistry.java#L173-L191]):
||Metric||Description||
|source-record-poll-rate|The average per-second number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.|
|source-record-write-rate|The average per-second number of records output from the transformations and written to Kafka for this task belonging to the named source connector in this worker. This is after transformations are applied and excludes any records filtered out by the transformations.|

There are also corresponding ""-total"" metrics that capture the total number of records polled and written for the metrics above, respectively.

In short, the ""poll"" metrics capture the number of messages sourced pre-transformation/filtering, and the ""write"" metrics should capture the number of messages ultimately written to Kafka post-transformation/filtering. However, the implementation of the {{source-record-write-*}}  metrics _includes_ records filtered out by transformations (and also records that result in produce failures with the config {{{}errors.tolerance=all{}}}).
h3. Details

In [AbstractWorkerSourceTask.java|https://github.com/apache/kafka/blob/a382acd31d1b53cd8695ff9488977566083540b1/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java#L389-L397], each source record is passed through the transformation chain where it is potentially filtered out, checked to see if it was in fact filtered out, and if so it is accounted for in the internal metrics via {{{}counter.skipRecord(){}}}.
{code:java}
for (final SourceRecord preTransformRecord : toSend) {         
    retryWithToleranceOperator.sourceRecord(preTransformRecord);
    final SourceRecord record = transformationChain.apply(preTransformRecord);            
    final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(record);
    if (producerRecord == null || retryWithToleranceOperator.failed()) {                
        counter.skipRecord();
        recordDropped(preTransformRecord);
        continue;
    }
    ...
{code}
{{SourceRecordWriteCounter.skipRecord()}} is implemented as follows:
{code:java}
    ....
    public SourceRecordWriteCounter(int batchSize, SourceTaskMetricsGroup metricsGroup) {
        assert batchSize > 0;
        assert metricsGroup != null;
        this.batchSize = batchSize;
        counter = batchSize;
        this.metricsGroup = metricsGroup;
    }
    public void skipRecord() {
        if (counter > 0 && --counter == 0) {
            finishedAllWrites();
        }
    }
    ....
    private void finishedAllWrites() {
        if (!completed) {
            metricsGroup.recordWrite(batchSize - counter);
            completed = true;
        }
    }
{code}
For example: If a batch starts with 100 records, {{batchSize}} and {{counter}} will both be initialized to 100. If all 100 records get filtered out, {{counter}} will be decremented 100 times, and {{{}finishedAllWrites(){}}}will record the value 100 to the underlying {{source-record-write-*}}  metrics rather than 0, the correct value according to the documentation for these metrics.
h3. Solutions

Assuming the documentation correctly captures the intent of the {{source-record-write-*}}  metrics, it seems reasonable to fix these metrics such that filtered records do not get counted.

It may also be useful to add additional metrics to capture the rate and total number of records filtered out by transformations, which would require a KIP.

I'm not sure what the best way of accounting for produce failures in the case of {{errors.tolerance=all}} is yet. Maybe these failures deserve their own new metrics?",,cbeard,ChrisEgerton,hgeraldino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-28 01:42:56.0,,,,,,,,,,"0|z1ffuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When listening on fixed ports, defer port opening until we're ready",KAFKA-14658,13521799,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,27/Jan/23 18:40,24/May/23 23:01,13/Jul/23 09:17,24/May/23 23:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"When we are listening on fixed ports, we should defer opening ports until we're ready to accept traffic. If we open the broker port too early, it can confuse monitoring and deployment systems. This is a particular concern when in KRaft mode, since in that mode, we create the SocketServer object earlier in the startup process than when in ZK mode.

The approach taken in this PR is to defer opening the acceptor port until Acceptor.start is called. Note that when we are listening on a random port, we continue to open the port ""early,"" in the SocketServer constructor. The reason for doing this is that there is no other way to find the random port number the kernel has selected. Since random port assignment is not used in production deployments, this should be reasonable.",,cmccabe,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-27 18:40:45.0,,,,,,,,,,"0|z1ffq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brokers rejecting LISR during ZK migration,KAFKA-14656,13521629,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,mumrah,mumrah,26/Jan/23 15:43,31/Jan/23 14:50,13/Jul/23 09:17,31/Jan/23 14:50,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,0,,,,,,"During the ZK migration, the KRaft controller sends controller RPCs to the ZK brokers (LISR, UMR, etc). Since the migration can begin immediately after a ZK broker starts up with migration enabled, it is possible that this broker is not seen as alive by the rest of the brokers. This is due to the KRaft controller taking over before the ZK controller can send out UMR with the restarted broker.

 

The result is that the parts of the LISR sent by KRaft immediately after the metadata migration is rejected by brokers due the leader being offline. 

 

The fix for this is to send an UMR to all brokers after the migration with the set of alive brokers.",,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-26 15:43:18.0,,,,,,,,,,"0|z1feog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connectors have incorrect Thread Context Classloader during initialization,KAFKA-14654,13521543,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,25/Jan/23 17:54,25/May/23 14:23,13/Jul/23 09:17,25/May/23 14:23,3.0.0,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,3.6.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"This is the same underlying issue as https://issues.apache.org/jira/browse/KAFKA-8340 , which fixed the thread context classloader for service loaded plugins, and for plugins which are first loaded _after_ plugin path scanning.
However, that correction PR failed to fix the context classloader for Connector classes, which are first instantiated and initialized during plugin path scanning, via the versionFor method.",,ChrisEgerton,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14670,KAFKA-8340,KAFKA-14863,,KAFKA-14627,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-25 17:54:47.0,,,,,,,,,,"0|z1fe5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQv2 can throw ConcurrentModificationException when accessing Tasks ,KAFKA-14650,13521189,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,ableegoldman,ableegoldman,24/Jan/23 23:54,09/Feb/23 18:35,13/Jul/23 09:17,09/Feb/23 18:35,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,,,,streams,,,,,,0,,,,,,"From failure in *[PositionRestartIntegrationTest.verifyStore[cache=false, log=true, supplier=IN_MEMORY_WINDOW, kind=PAPI]|https://ci-builds.apache.org/job/Kafka/job/kafka/job/3.4/63/testReport/junit/org.apache.kafka.streams.integration/PositionRestartIntegrationTest/Build___JDK_11_and_Scala_2_13___verifyStore_cache_false__log_true__supplier_IN_MEMORY_WINDOW__kind_PAPI_/]*
java.util.ConcurrentModificationException
	at java.base/java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1208)
	at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1244)
	at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1239)
	at java.base/java.util.HashMap.putMapEntries(HashMap.java:508)
	at java.base/java.util.HashMap.putAll(HashMap.java:781)
	at org.apache.kafka.streams.processor.internals.Tasks.allTasksPerId(Tasks.java:361)
	at org.apache.kafka.streams.processor.internals.TaskManager.allTasks(TaskManager.java:1537)
	at org.apache.kafka.streams.processor.internals.StreamThread.allTasks(StreamThread.java:1278)
	at org.apache.kafka.streams.KafkaStreams.query(KafkaStreams.java:1921)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.iqv2WaitForResult(IntegrationTestUtils.java:168)
	at org.apache.kafka.streams.integration.PositionRestartIntegrationTest.shouldReachExpectedPosition(PositionRestartIntegrationTest.java:438)
	at org.apache.kafka.streams.integration.PositionRestartIntegrationTest.verifyStore(PositionRestartIntegrationTest.java:423)",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-24 23:54:53.0,,,,,,,,,,"0|z1fbyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failures instantiating Connect plugins hides other plugins from REST API, or crash worker",KAFKA-14649,13521182,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gharris1727,gharris1727,gharris1727,24/Jan/23 22:33,03/Mar/23 14:55,13/Jul/23 09:17,02/Mar/23 20:11,3.0.0,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,3.4.1,3.5.0,,,,,,KafkaConnect,,,,,,0,,,,,,"Connect plugin path scanning evaluates the version() method of plugins to determine which version of a plugin to load, and what version to advertise as part of the REST API. This process involves reflectively constructing an instance of the class and calling the version method, which can fail in the following scenarios:

1. If a plugin throws an exception from a static initialization block
2. If a plugin does not have a default constructor (such as a non-static inner class)
3. If a plugin has a default constructor is not public
4. If a plugin throws an exception from the default constructor
5. If a plugin's version method throws an exception

If any of the above is true for any single connector or rest extension on the classpath or plugin.path, the plugin path scanning will exit early, and potentially hide other unrelated plugins. This is primarily an issue in development and test environments, because they are easy-to-make code mistakes that would generally not make it to a release. Exceptions from the version method, however, can cause the worker to fail to start up as they are uncaught.

It is desirable for the worker to instead log these exceptions and continue. This will prevent one mis-implemented plugin from affecting other plugins, while still causing integration tests to fail against the plugin itself. We can augment logging to make it clear how to correct these failures, where before it was rather opaque and difficult to debug.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14627,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-24 22:33:06.0,,,,,,,,,,"0|z1fbxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SubscriptionWrapper is of an incompatible version (Kafka Streams 3.2.3 -> 3.3.2),KAFKA-14646,13521024,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,joschi,joschi,23/Jan/23 18:12,14/Feb/23 14:31,13/Jul/23 09:17,26/Jan/23 02:03,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,3.3.3,3.4.0,,,,,,streams,,,,,,0,,,,,,"Hey folks,
 
we've just updated an application from *_Kafka Streams 3.2.3 to 3.3.2_* and started getting the following exceptions:
{code:java}
org.apache.kafka.common.errors.UnsupportedVersionException: SubscriptionWrapper is of an incompatible version. {code}
After swiftly looking through the code, this exception is potentially thrown in two places:
 * [https://github.com/apache/kafka/blob/3.3.2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionJoinForeignProcessorSupplier.java#L73-L78]
 ** Here the check was changed in Kafka 3.3.x: [https://github.com/apache/kafka/commit/9dd25ecd9ce17e608c6aba98e0422b26ed133c12]

 * [https://github.com/apache/kafka/blob/3.3.2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionStoreReceiveProcessorSupplier.java#L94-L99]
 ** Here the check wasn't changed.

 
Is it possible that the second check in {{SubscriptionStoreReceiveProcessorSupplier<K, KO>}} was forgotten?
 
Any hints how to resolve this issue without a downgrade?
Since this only affects 2 of 15 topologies in the application, I'm hesitant to just downgrade to Kafka 3.2.3 again since the internal topics might already have been updated to use the ""new"" version of {{{}SubscriptionWrapper{}}}.
 
Related discussion in the Confluent Community Slack: [https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1674497054507119]
h2. Stack trace
{code:java}
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=1_8, processor=XXX-joined-changed-fk-subscription-registration-source, topic=topic.rev7-XXX-joined-changed-fk-subscription-registration-topic, partition=8, offset=12297976, stacktrace=org.apache.kafka.common.errors.UnsupportedVersionException: SubscriptionWrapper is of an incompatible version.

    at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:750)
    at org.apache.kafka.streams.processor.internals.TaskExecutor.processTask(TaskExecutor.java:100)
    at org.apache.kafka.streams.processor.internals.TaskExecutor.process(TaskExecutor.java:81)
    at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1182)
    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:770)
    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:588)
    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:550) {code}","Kafka Streams 3.2.3 (before update)
Kafka Streams 3.3.2 (after update)
Java 17 (Eclipse Temurin 17.0.5), Linux x86_64",joschi,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Tue Jan 31 15:36:46 UTC 2023,,,,,,,,,,"0|z1fayw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 19:18;mjsax;Did you upgrade with two rolling bounced leveraging `upgrad_from` config?

I assume is related to https://issues.apache.org/jira/browse/KAFKA-13769

Of course, K13769 could have introduced some bug, but we actually to test rolling upgrades and would hope it would have caught it (otherwise, we need to improve our testing...)

We unfortunately, lag some docs on the web-page about the required two rolling bounce upgrade path – it unfortunately slipped. We have it in the backlog to add the missing docs asap.;;;","23/Jan/23 20:38;joschi;{quote}Did you upgrade with two rolling bounced leveraging `upgrad_from` config?
{quote}
[~mjsax] Ah, in fact we didn't. I assumed (incorrectly) that this would only be necessary when updating across major versions.

I found [https://kafka.apache.org/33/documentation/streams/upgrade-guide] which clearly states it to be necessary for upgrading from Kafka Streams 3.2.x as well.

We're running our application in AWS ECS Fargate and thus a rolling upgrade would be necessary.

I will try this tomorrow and update this ticket. Thanks for the hint!;;;","23/Jan/23 22:15;mjsax;Thanks for following up – glad to hear that it's in the docs... And I hope it resolved the problem.;;;","24/Jan/23 08:42;joschi;Unfortunately the two rolling updates (with {{upgrade.from=""3.2""}} and then removing the setting again) didn't help.

We still see the same exception:
{code:java}
org.apache.kafka.common.errors.UnsupportedVersionException: SubscriptionWrapper is of an incompatible version. {code}
 

[~mjsax] Do you have any hints how to resolve this issue? We see it in only 2 topologies out of 15 and I'm afraid that downgrading to Kafka Streams 3.2.3 will break something else now.

 ;;;","24/Jan/23 11:12;joschi;Could it still be that the check in [https://github.com/apache/kafka/blob/3.3.2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionStoreReceiveProcessorSupplier.java#L94-L99] is too restrictive and should read *{{record.value().getVersion() > SubscriptionWrapper.CURRENT_VERSION}}* instead?;;;","26/Jan/23 00:24;mjsax;Could be – let me sync with Alex who worked on the ticket I linked above – need to think about it a little bit.;;;","26/Jan/23 02:02;mjsax;Ok. Talked to Alex and he quickly figured it out – it's quite embarrassing... The last two PRs for https://issues.apache.org/jira/browse/KAFKA-13769 did not land in 3.3 release branch... (seems we forgot to cherry-pick after merging to `trunk`) So we indeed have a bug in 3.3.2 (and 3.3.1 and 3.3.0), and it's only fixed in 3.4.0... Bottom line: K13769 does not really fix the issue in 3.3.2 or earlier, but only in 3.4.0 which should be available soon.

I just cherry-picked both commits to 3.3 branch, so in case there will be a 3.3.3 release, it would get picked up.

Thanks for reporting this! Will close this ticket now. Feel free to follow up in the comments.;;;","31/Jan/23 06:05;mjsax;I was thinking about this issue, and I think the only way to upgrade is to ""drain"" your topology. Ie, you would need to stop your upstream producers and not send any new input data. Afterwards, you let KS finish processing of all input data (including processing of all data from internal topics, ie, repartition, fk-subscription, and fk-response topics), to really ""drain"" the topology completely. Next, do a two round rolling bounce using `upgrade.from`, and finally resume your upstream producers.

Would you be willing to try this out (and report back)?;;;","31/Jan/23 15:36;joschi;[~mjsax] Unfortunately we already ""solved"" the issue by recreating the respective topologies with new internal topic names, so we cannot try out your suggestion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Plugin classloader not used when retrieving connector plugin config defs via REST API,KAFKA-14645,13521008,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,23/Jan/23 15:58,28/Mar/23 16:43,13/Jul/23 09:17,30/Jan/23 17:06,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,3.2.4,3.3.3,3.4.1,3.5.0,,,,KafkaConnect,,,,,,0,,,,,,"We don't switch to the plugin classloader when servicing requests to the {{GET /connector-plugins/<type>/config}} endpoint, which can result in classloading failures for, e.g., properties that accept the name of a pluggable class to load.

Reported externally in [https://github.com/kcctl/kcctl/issues/266]",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14670,KAFKA-13510,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/kcctl/kcctl/issues/266,,,,,,,,,,,9223372036854775807,,,,2023-01-23 15:58:29.0,,,,,,,,,,"0|z1favc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Process should stop after failure in raft IO thread,KAFKA-14644,13520862,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,20/Jan/23 22:38,04/May/23 07:23,13/Jul/23 09:17,25/Jan/23 17:45,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,,,,,,"We have seen a few cases where an unexpected error in the Raft IO thread causes the process to enter a zombie state where it is no longer participating in the raft quorum. In this state, a controller can no longer become leader or help in elections, and brokers can no longer update metadata. It may be better to stop the process in this case since there is no way to recover.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-20 22:38:56.0,,,,,,,,,,"0|z1f9zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka CooperativeStickyAssignor revokes/assigns partition in one rebalance cycle,KAFKA-14639,13520189,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,bojanblagojevic,bojanblagojevic,19/Jan/23 11:51,02/May/23 11:48,13/Jul/23 09:17,28/Apr/23 09:17,3.2.1,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,clients,consumer,,,,,0,,,,,,"I have an application that runs 6 consumers in parallel. I am getting some unexpected results when I use {{{}CooperativeStickyAssignor{}}}. If I understand the mechanism correctly, if the consumer looses partition in one rebalance cycle, the partition should be assigned in the next rebalance cycle.

This assumption is based on the [RebalanceProtocol|https://kafka.apache.org/31/javadoc/org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.RebalanceProtocol.html] documentation and few blog posts that describe the protocol, like [this one|https://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/] on Confluent blog.
{quote}The assignor should not reassign any owned partitions immediately, but instead may indicate consumers the need for partition revocation so that the revoked partitions can be reassigned to other consumers in the next rebalance event. This is designed for sticky assignment logic which attempts to minimize partition reassignment with cooperative adjustments.
{quote}
{quote}Any member that revoked partitions then rejoins the group, triggering a second rebalance so that its revoked partitions can be assigned. Until then, these partitions are unowned and unassigned.
{quote}
These are the logs from the application that uses {{{}protocol='cooperative-sticky'{}}}. In the same rebalance cycle ({{{}generationId=640{}}}) {{partition 74}} moves from {{consumer-3}} to {{{}consumer-4{}}}. I omitted the lines that are logged by the other 4 consumers.

Mind that the log is in reverse(bottom to top)
{code:java}
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : New partition assignment: partition-59, seek to min common offset: 85120524
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler2 : Partitions [partition-59] assigned successfully
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : Partitions assigned: [partition-59]
2022-12-14 11:18:24 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Adding newly assigned partitions: partition-59
2022-12-14 11:18:24 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the new Assignment(partitions=[partition-59])
2022-12-14 11:18:24 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Request joining group due to: need to revoke partitions [partition-26, partition-74] as indicated by the current assignment and re-join
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler2 : Partitions [partition-26, partition-74] revoked successfully
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : Finished removing partition data
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] (Re-)joining group
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : New partition assignment: partition-74, seek to min common offset: 107317730
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler2 : Partitions [partition-74] assigned successfully
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : Partitions assigned: [partition-74]
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Adding newly assigned partitions: partition-74
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the new Assignment(partitions=[partition-74])
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Request joining group due to: need to revoke partitions [partition-57] as indicated by the current assignment and re-join
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler2 : Partitions [partition-57] revoked successfully
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : Finished removing partition data
2022-12-14 11:18:22 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : Partitions revoked: [partition-26, partition-74]
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Revoke previously assigned partitions partition-26, partition-74
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-59]\n\tCurrent owned partitions: [partition-26, partition-74]\n\tAdded partitions (assigned - owned): [partition-59]\n\tRevoked partitions (owned - assigned): [partition-26, partition-74]
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Successfully synced group in generation Generation{generationId=640, memberId='partition-3-my-client-id-my-group-id-c31afd19-3f22-43cb-ad07-9088aa98d3af', protocol='cooperative-sticky'}
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Successfully joined group with generation Generation{generationId=640, memberId='partition-3-my-client-id-my-group-id-c31afd19-3f22-43cb-ad07-9088aa98d3af', protocol='cooperative-sticky'}
2022-12-14 11:18:22 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : Partitions revoked: [partition-57]
2022-12-14 11:18:22 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Revoke previously assigned partitions partition-57
2022-12-14 11:18:22 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-74]\n\tCurrent owned partitions: [partition-57]\n\tAdded partitions (assigned - owned): [partition-74]\n\tRevoked partitions (owned - assigned): [partition-57]
2022-12-14 11:18:21 1 — [id-1] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Successfully synced group in generation Generation{generationId=640, memberId='partition-4-my-client-id-my-group-id-ae2af665-edc9-4a8e-b658-98372d142477', protocol='cooperative-sticky'}
2022-12-14 11:18:21 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Successfully joined group with generation Generation{generationId=640, memberId='partition-4-my-client-id-my-group-id-ae2af665-edc9-4a8e-b658-98372d142477', protocol='cooperative-sticky'} {code}
Is this expected?

Kafka client version is 3.2.1.",,ableegoldman,bojanblagojevic,guozhang,pnee,showuon,tmancill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 16:27;bojanblagojevic;consumers-jira.log;https://issues.apache.org/jira/secure/attachment/13054790/consumers-jira.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 22:13:21 UTC 2023,,,,,,,,,,"0|z1f5u0:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"20/Jan/23 04:27;ableegoldman;There's definitely something off, it almost looks like consumer-3 actually missed the gen 640 rebalance altogether – which would definitely explain the symptom you describe, as partition 74 would be free to assign to consumer-4 if consumer-3 didn't join the group in time to report partition-74 among its ""ownedPartitions"". 

Reading from the bottom up, the first 3 lines show consumer-4 going through the entire rebalance and getting partition-74 assigned, all before consumer-3 even joins the group in gen 640. So that all adds up so far. What seems odd to me is that we then see consumer-3 appear to ""successfully join the group"" AND ""successfully sync the group"" for that same generation, which should not be the case if it missed the rebalance entirely – it should fail on JoinGroup if it truly did not manage to join the gen 640 rebalance in time, and certainly should fail the SyncGroup. So that does seem weird to me

If you're able to reproduce this, could you try doing so with DEBUG logging enabled? In the meantime, I might be able to make more sense of what's going on if you can upload the full logs over this period of time;;;","23/Jan/23 17:09;bojanblagojevic;Thank you [~ableegoldman] for the quick response.
 
I just noticed that I forgot to add some details about the problem. 6 consumers that were mentioned were running on one of the 15 pods in Kubernetes cluster. Each pod spawns 6 consumers. Topic that is being consumed has 96 partitions.
 
I only saw this behaviour once, on our production environment and I am not able to reproduce it. After the issue happened the consumers were reconfigured back to the previously used `{{{}RoundRobinAssignor{}}}`.
  
I have the logs from the `{{{}ConsumerCoordinator{}}}` that did the assignment of the partitions. Log contains problematic generation 640 and three more that came before it. As in the first log excerpt I filtered out other consumers:
{code:java}
""2022-12-14 11:18:11 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment for group at generation 638: {
consumer-4=Assignment(partitions=[partition-57]),
consumer-0=Assignment(partitions=[partition-2, partition-93]),
consumer-1=Assignment(partitions=[partition-10, partition-58]),
consumer-3=Assignment(partitions=[partition-26, partition-74]),
consumer-5=Assignment(partitions=[partition-65]),
consumer-2=Assignment(partitions=[partition-34]),
}""
""2022-12-14 11:18:11 1 --- [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-26, partition-74]\n\tCurrent owned partitions: [partition-26, partition-74]\n\tAdded partitions (assigned - owned): []\n\tRevoked partitions (owned - assigned): []""
""2022-12-14 11:18:11 1 --- [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the new Assignment(partitions=[partition-26, partition-74])""
""2022-12-14 11:18:16 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment for group at generation 639: {
...
consumer-4=Assignment(partitions=[partition-57]),
consumer-0=Assignment(partitions=[partition-2, partition-93]),
consumer-1=Assignment(partitions=[partition-24]),
consumer-3=Assignment(partitions=[partition-26, partition-74]),
consumer-5=Assignment(partitions=[partition-88]),
consumer-2=Assignment(partitions=[partition-34]),
...
}""
""2022-12-14 11:18:21 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment for group at generation 640: {
...
consumer-4=Assignment(partitions=[partition-74]),
consumer-0=Assignment(partitions=[partition-2, partition-93]),
consumer-1=Assignment(partitions=[partition-21]),
consumer-3=Assignment(partitions=[partition-59]),
consumer-5=Assignment(partitions=[partition-82]),
consumer-2=Assignment(partitions=[partition-38]),
...
}"" 
""2022-12-14 11:18:28   1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=partition-5--my-client-id-my-group-id-random_hash, groupId=my-group-id] Finished assignment for group at generation 641: {
...
consumer-4=Assignment(partitions=[partition-74]), 
consumer-0=Assignment(partitions=[partition-2, partition-93]), 
consumer-1=Assignment(partitions=[partition-21]), 
consumer-3=Assignment(partitions=[partition-59]), 
consumer-5=Assignment(partitions=[partition-82]), 
consumer-2=Assignment(partitions=[partition-38]), 
...
}""{code}
It seems that in the same generation partition-74 changed ownership from consumer-3 to consumer-4.
 
I would expect:
- generation 640 -> consumer-3 looses partition-74
- generation 641 -> consumer-4 acquires partition-74;;;","24/Jan/23 08:41;ableegoldman;Thanks for the additional logs, that does indeed verify that both consumers participated in the same rebalance. My next guess would be that for some reason, consumer-3 had ""lost"" its partitions prior to joining the group in gen 640, in which case they would be allowed to be freely given away to another consumer in that same generation. Can you check and/or provide all the logs from consumer-3 between gen 639 and gen 640? Is there anything in there about resetting the generation, dropping out of the group, resetting the member id, anything at all like that? 

I also notice that the assignment changes drastically between gen 639 and 640, it's not ""sticky"" at all which should have been easy for the assignor to do if the previous assignment was something relatively simple like each consumer claiming exactly one or two partitions (only) and all from the same topic. Something fishy is definitely going on.

The only other thing off the top of my head to check would be that every single consumer was configured with (only) the CooperativeStickyAssignor over the full period from gen 639 through the end of gen 640, or at least check the group leader (consumer-5 I think?) and consumers 3 & 4.

I'll take a look at the assignor logic and see if anything jumps out at me on my end, but I have to say the complete lack of stickiness in the assignment from 639 to 640 is fairly perplexing and something I have never seen before with the CooperativeStickyAssignor. There have been some recents bugs related to rebalancing edge cases that have been fixed over the past few versions, so I may go back over those and see if anything was messed up by them. I did in fact discover one bug affecting rebalancing/assignment in the past few months which had been introduced by that series of fixes, so perhaps there is another.;;;","24/Jan/23 16:28;bojanblagojevic;Thank you again for the quick response. I will try to answer to your questions.
h5. Answers:
??1. Can you check and/or provide all the logs from consumer-3 between gen 639 and gen 640? Is there anything in there about resetting the generation, dropping out of the group, resetting the member id, anything at all like that???
I don't see nothing like that happening between gen 639 and gen 640. I attached the log excerpt related to few surrounding generations [^consumers-jira.log].

??2. The only other thing off the top of my head to check would be that every single consumer was configured with (only) the CooperativeStickyAssignor over the full period from gen 639 through the end of gen 640, or at least check the group leader (consumer-5 I think?) and consumers 3 & 4.??
The full logs are unfortunately expired but I am pretty sure that all the consumers were configured with only `CooperativeStickyAssignor`. They are part of Kuberenetes deployment in which all the pods share the configuration. I observed correct group leader behaviour when changing ownership of other partitions. I followed ownership changes when partition *partition-68* is moved.
It belonged to the consumer partition-2-6b9db8686f-hswvn... in generation 639.
{code:java}
Final assignment of partitions to consumers:
partition-2-6b9db8686f-hswvn-bbcfa7e4-7d5b-4227-ad62-99e8cc6e176f=[partition-20, partition-68]
...
Finished assignment for group at generation 639:
partition-2-6b9db8686f-hswvn-bbcfa7e4-7d5b-4227-ad62-99e8cc6e176f=Assignment(partitions=[partition-20, partition-68]) {code}
Between generation 639 and generation 640 new pod joins, pod-6b9db8686f-p87m9. One of the Kafka consumers that belongs to this pod, partition-3-6b9db8686f-p87m9..., in generation 640 gets the partition-68 as assigned and it is logged in AbstractStickyAssignor.constrainedAssign.
{code:java}
Final assignment of partitions to consumers:
partition-3-6b9db8686f-p87m9-737d9359-daa5-4d89-9e4c-40a433aa8c6c=[partition-68]{code}
Since this partition is changing ownership, it does not show up in the log of ConsumerCoordinator, which is expected.
{code:java}
Finished assignment for group at generation 640:
partition-3-6b9db8686f-p87m9-737d9359-daa5-4d89-9e4c-40a433aa8c6c=Assignment(partitions=[]){code}
And it gets assigned in generation 641:
 
{code:java}
Final assignment of partitions to consumers: partition-3-6b9db8686f-psfx4-5dd12c98-e698-44aa-9131-56281e798369=[partition-68] 
... 
Finished assignment for group at generation 641: partition-3-6b9db8686f-psfx4-5dd12c98-e698-44aa-9131-56281e798369=Assignment(partitions=[partition-68]){code}
It gets assigned to a consumer which is different than the one determined in generation 640 but this does not break the rebalance barrier.
h5. Additional notes
Not sure if it matters. I saw a several consumers logging:
{code:java}
... org.apache.kafka.clients.Metadata ... Resetting the last seen epoch of partition partition-74 to 149 since the associated topicId changed from null to nixqUZnpQYWjY0RreaCczA"" {code}
 
I think that the HB thread(I am assuming this based on the [code I've read|https://github.com/apache/kafka/blob/3.2.1/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1193]) is requesting join on behalf of consumers. Again, not sure if it matters:
{code:java}
""2022-12-14 11:17:48 1 --- [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group""
""2022-12-14 11:17:48 1 --- [consumer-2] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-2-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group""
""2022-12-14 11:17:48 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing""
""2022-12-14 11:17:48 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-2-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing""
""2022-12-14 11:17:47 1 --- [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group""
""2022-12-14 11:17:47 1 --- [consumer-1] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-1-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group""
""2022-12-14 11:17:47 1 --- [consumer-0] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-0-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group""
""2022-12-14 11:17:46 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-0-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing""
""2022-12-14 11:17:46 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing""
""2022-12-14 11:17:45 1 --- [consumer-5] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5-my-client-id-my-group-id-random_hash, groupId=my-group-id] (Re-)joining group""
""2022-12-14 11:17:45 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-1-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing""
""2022-12-14 11:17:45 1 --- [my-group-id] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-5-my-client-id-my-group-id-random_hash, groupId=my-group-id] Request joining group due to: group is already rebalancing"" {code}
 ;;;","25/Mar/23 03:48;pnee;A few issues I see...
 # why did consumer-4 revoke partition-57? Aren't partitions supposed to be sticky?
 # There are a lot of sync group failures, which makes me wonder if it is an exact problem described in KAFKA-13891 - the pod might be a slightly slower in sending out the sync group, and therefore result in out-of-date owned partitions.
 # The timing seems off, consumer-3 completed to join/sync after consumer-4 completed the revocation. Not sure if this is an issue with the logging.

It seems like a lot of folks agree on making assignor better at handling partition ownership, across multiple generations, but no one seems to be implementing the fix.;;;","27/Mar/23 23:29;guozhang;I looked at both JIRA tickets and I believe the root cause is KAFKA-13891, and this ticket is just a symptom of that issue. More specifically, I think there are two common scenarios here, and just to use the example's names for illustration purposes only:

Scenario 1:

* in generation 639, partition-74 is ""decided"" to go to consumer-3 in the join-group. Note here I used the word ""decided"" since the assignment only completes after the Sync-group round trip. So this assignment is only logged on the leader, but consumer-3 has not learned about this new assignment in the sync-group yet.
* then before the sync-group finishes, a new member joins (or more generally, some other event happens, such as what KAFKA-13891 describes: some member received the sync-group earlier than others, and immediately sends another re-join group to trigger a new rebalance). The server-side group coordinator bumps up the generation to 640 for the new rebalance.
* consumer-3 sends the sync-group for generation 639 and gets a REBALANCE_IN_PROGRESS, and hence it needs to re-join the group without knowing partition-74 was given to it in generation 639. In this case consumer-3 has never officially ""owned"" partition-74.
* in generation 640, partition-74 is decided to consumer-4.

From the callback's perspective, consumer-3 would not trigger the `onPartitionAssigned`, nor `onPartitionRevoked` for partition-74, since the assignment in generation 639 got obsoleted before it even gets notified and hence it does not ever get that partition ever. But this should still be okay since whoever previously owned partition-74 prior to generation 639, should still invoke the `onPartitionRevoked` for partition-74.

Scenario 2 (pretty much what KAFKA-13891's description well stated):

* prior to generation 639, partition-74 is already owned by consumer-3 and consumer-3 knows about it. The rebalance of generation 639 did not change that ownership, i.e. it's still assigned to consumer-3.
* then before the sync-group finishes, a new member joins (or more generally, some other event happens, such as what KAFKA-13891 describes: some member received the sync-group earlier than others, and immediately sends another re-join group to trigger a new rebalance). The server-side group coordinator bumps up the generation to 640 for the new rebalance.
* consumer-3 sends the sync-group for generation 639 and gets a REBALANCE_IN_PROGRESS, and hence it needs to re-join the group with generation reset. In this case, its owned partitions are discarded by the broker-side coordinator since its generation is literally ""-1"".
* in generation 640, partition-74 is decided to consumer-4 with the perception that no one owns this partition in that generation.

In this case, the `onPartitionRevoked` would be triggered on consumer 3 and `onPartitionAssigned` on consumer 4 at the same time, which is a bad case.;;;","27/Mar/23 23:48;guozhang;As for the fix.. it's quite tough, since the ultimate and best solution is the new rebalance protocol KIP, and if we want to have a near-term fix, it has to be only touching on the client side, not the server side. That means the suggestions on the other ticket cannot apply.

The trickiness is that, when a sync-group request got a REBALANCE_IN_PROGRESS error for generation X, it means that (let's take the conversion case of COORDINATOR_LOAD_IN_PROGRESS out for the moment) the consumer did not actually miss a rebalance, it's just that the next generation X+1 has started; HOWEVER, its current owned partitions is not from generation X, but from an older version, say X-1 (since it did not get the actual assignment for generation X), and hence when it sends out that owned partitions of generation X-1, it is likely to be discarded by the coordinators. Without fixing the broker-side, this trickiness is hard to resolve.

I'd suggest we coordinate with the folks working on the KIP (a.k.a. on the broker-side) for the fix at the near term. If everyone agrees this is a good issue to be remedied before the KIP is out, I'd suggest we took the broker-change, a.k.a. the description in KAFKA-14016 by Shawn. 

If that's considered a too short fix given the KIP, another fix could be, that on the assignor side, do not discard the old generation's owned partitions if those partitions are NOT claimed by anyone else --- admittedly this is a risky (think about a partition owned by host A in generation X, and then assigned to B in generation X+1 which host A missed, and then host B missed generation X+2 in which case it's given back to A, in which case we may see that partitions consuming offsets ""going backwards"") fix but may give us the runway until the new rebalance protocol is out. Personally I do not like it and would only consider as a last resort.;;;","28/Mar/23 22:13;guozhang;Had some chat with [~pnee] today regarding the client-side fix, I think a more conservative fix may work, which is to only honor the old generation's owned partitions if 1) it's the only participant claiming to own this partition, 2) it's generation is no smaller than the current generation X minus 1. If its generation is X-2 or less, then the above mentioned risk would materialize.

That being said, I still feel that the ultimate fix should touch on the broker side, and hence would better be for the new rebalance protocol KIP.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to 3.4 from old versions (< 0.10) are failing due to incompatible meta.properties check,KAFKA-14637,13520105,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,18/Jan/23 19:40,19/Jan/23 18:17,13/Jul/23 09:17,19/Jan/23 18:17,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,0,,,,,,3.4 has a check in broker startup to ensure cluster.id is provided in `metadata.properties`. This is not always the case if the previous version of Kafka is < 0.10.,,akhileshchg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-18 19:40:33.0,,,,,,,,,,"0|z1f5c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OAuth's HttpAccessTokenRetriever potentially leaks secrets in logging  ,KAFKA-14623,13518370,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,13/Jan/23 21:31,29/Jun/23 21:07,13/Jul/23 09:17,17/Feb/23 19:32,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,3.3.3,3.4.0,,,,,,clients,security,,,,,0,OAuth,,,,,"The OAuth code that communicates via HTTP with the IdP (HttpAccessTokenRetriever.java) includes logging that outputs the request and response payloads. Among them are:
 * [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/HttpAccessTokenRetriever.java#L265]
 * [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/HttpAccessTokenRetriever.java#L274]
 * [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/HttpAccessTokenRetriever.java#L320]

It should be determined if there are other places sensitive information might be inadvertently exposed.",,kirktrue,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 19:14:02 UTC 2023,,,,,,,,,,"0|z1eun4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 22:11;kirktrue;cc [~smjn] [~omkreddy] ;;;","17/Feb/23 19:14;kirktrue;-Reopening to fix in 3.1.x and 3.2.x branches.-

Update: not fixing in those older versions after all.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Off by one error in generated snapshot IDs causes misaligned fetching,KAFKA-14618,13517929,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jsancio,hachikuji,hachikuji,12/Jan/23 22:51,13/Jan/23 23:19,13/Jul/23 09:17,13/Jan/23 21:25,,,,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,0,,,,,,"We implemented new snapshot generation logic here: [https://github.com/apache/kafka/pull/12983]. A few days prior to this patch getting merged, we had changed the `RaftClient` API to pass the _exclusive_ offset when generating snapshots instead of the inclusive offset: [https://github.com/apache/kafka/pull/12981]. Unfortunately, the new snapshot generation logic was not updated accordingly. The consequence of this is that the state on replicas can get out of sync. In the best case, the followers fail replication because the offset after loading a snapshot is no longer aligned on a batch boundary.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-12 22:51:19.0,,,,,,,,,,"0|z1erxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic config records written to log even when topic creation fails,KAFKA-14612,13517497,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,andyg2,hachikuji,hachikuji,10/Jan/23 20:47,13/Jan/23 00:27,13/Jul/23 09:17,13/Jan/23 00:27,,,,,,,,,,,,,,,,3.4.0,,,,,,,kraft,,,,,,0,,,,,,"Config records are added when handling a `CreateTopics` request here: [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java#L549.] If the subsequent validations fail and the topic is not created, these records will still be written to the log.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-10 20:47:47.0,,,,,,,,,,"0|z1epao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams Processor API cannot use state stores,KAFKA-14609,13517252,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,philipp94831,philipp94831,09/Jan/23 14:30,09/Jan/23 17:48,13/Jul/23 09:17,09/Jan/23 15:53,3.3.0,,,,,,,,,,,,,,,,,,,,,,streams,,,,,,0,,,,,,"The recently introduced Kafka Streams Processor API (since 3.3, https://issues.apache.org/jira/browse/KAFKA-13654) likely has a bug with regards to using state stores. The [getStateStore|https://javadoc.io/static/org.apache.kafka/kafka-streams/3.3.1/org/apache/kafka/streams/processor/api/ProcessingContext.html#getStateStore-java.lang.String-] method returns null, even though the store has been registered according to the docs. The old transformer API still works. I created a small project that demonstrates the behavior. It uses both methods to register a store for the transformer, as well as the processor API: https://github.com/bakdata/kafka-streams-state-store-demo/blob/main/src/test/java/com/bakdata/kafka/StreamsStateStoreTest.java",,bbejeck,mjsax,philipp94831,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 17:48:25 UTC 2023,,,,,,,,,,"0|z1enuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 15:52;bbejeck;Hi [~philipp94831] 

Thanks for reporting this issue.  I believe it's been fixed with https://issues.apache.org/jira/browse/KAFKA-14388.

You could pull down either the 3.4.0 branch or 3.3.2 and build from source and test it.  For now, I'm going to mark this as fixed.  ;;;","09/Jan/23 15:53;bbejeck;Fixed by https://issues.apache.org/jira/browse/KAFKA-14388;;;","09/Jan/23 16:02;philipp94831;Thanks, I didn't find that issue. Do you know when 3.3.2 will be released?;;;","09/Jan/23 17:48;bbejeck;It just received approval on the dev list, so I'd say within a week.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZkMetadataCache.getClusterMetadata is missing rack information in aliveNodes,KAFKA-14571,13516488,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ecomar,ecomar,ecomar,04/Jan/23 14:41,04/Jan/23 23:42,13/Jul/23 09:17,04/Jan/23 23:41,3.3,,,,,,,,,,,,,,,3.3.3,3.4.0,,,,,,core,,,,,,0,,,,,,"ZkMetadataCache.getClusterMetadata returns a Cluster object where the aliveNodes are missing their rack info.

when ZkMetadataCache updates the metadataSnapshot, includes the rack in `aliveBrokers` but not in `aliveNodes` ",,ecomar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 22:12:12 UTC 2023,,,,,,,,,,"0|z1ej5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 22:12;ecomar;classified as minor because the the Cluster object returned from MetadataCache.getClusterMetadata(...)
is passed to ClientQuotaCallback.updateClusterMetadata(...)
and not by the describeCluster client API;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Netty to 4.1.86.Final to fix CVEs,KAFKA-14564,13516379,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bribera,bribera,bribera,03/Jan/23 21:31,08/Apr/23 07:58,13/Jul/23 09:17,04/Jan/23 08:55,3.3.1,,,,,,,,,,,,,,,3.5.0,,,,,,,core,,,,,,0,,,,,,"4.1.86 fixes two CVEs:
 * [https://nvd.nist.gov/vuln/detail/CVE-2022-41881]
 * [https://nvd.nist.gov/vuln/detail/CVE-2022-41915]

 ",,bribera,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 08:56:26 UTC 2023,,,,,,,,,,"0|z1eih4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 00:29;bribera;https://github.com/apache/kafka/pull/13070;;;","04/Jan/23 08:56;showuon;[~ableegoldman] , do we want to include this into v3.4.0?

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing .lock file when using metadata.log.dir,KAFKA-14557,13515961,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jsancio,jsancio,jsancio,29/Dec/22 19:45,10/Jan/23 20:25,13/Jul/23 09:17,10/Jan/23 20:25,,,,,,,,,,,,,,,,3.3.3,3.4.0,,,,,,kraft,,,,,,0,,,,,,"If the Kafka node is configured to use a metadata.log.dir that is different from the one specified in log.dir or log.dirs, Kafka doesn't create and grab a file lock for the metadata.lor.dir. The log dir lock file is named .lock.

This makes is possible for multiple Kafka node to use the same metadata log dir at the same time. This is not supported and should not be allowed.",,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-29 19:45:05.0,,,,,,,,,,"0|z1efwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorCheckpointTask throws NullPointerException when group hasn't consumed from some partitions,KAFKA-14545,13515142,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,csolidum,csolidum,csolidum,21/Dec/22 21:48,23/Feb/23 14:26,13/Jul/23 09:17,04/Jan/23 11:04,3.3.0,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,mirrormaker,,,,,,0,,,,,,"MirrorTaskConnector looks like it's throwing a NullPointerException when a consumer group hasn't consumed from all topics from a partition. This blocks the syncing of consumer group offsets to the target cluster. The stacktrace and error message is as follows:
{code:java}
WARN Failure polling consumer state for checkpoints. (org.apache.kafka.connect.mirror.MirrorCheckpointTask)
at java.base/java.lang.Thread.run(Thread.java:829)Dec 20
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:72)
at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:244)
at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:189)
at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:346)
at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.poll(AbstractWorkerSourceTask.java:452)
at org.apache.kafka.connect.mirror.MirrorCheckpointTask.poll(MirrorCheckpointTask.java:142)
at org.apache.kafka.connect.mirror.MirrorCheckpointTask.sourceRecordsForGroup(MirrorCheckpointTask.java:160)
at org.apache.kafka.connect.mirror.MirrorCheckpointTask.checkpointsForGroup(MirrorCheckpointTask.java:177)
at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
at java.base/java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1764)
at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
at org.apache.kafka.connect.mirror.MirrorCheckpointTask.lambda$checkpointsForGroup$2(MirrorCheckpointTask.java:174)
at org.apache.kafka.connect.mirror.MirrorCheckpointTask.checkpoint(MirrorCheckpointTask.java:191)
java.lang.NullPointerException
 {code}
This seems to happen if the OffsetFetch call returns a OffsetFetchPartitionResponsePartition with a negative commitedOffset. Mirrormaker should handle this case more gracefully and still be sync over consumer offsets for non negative partitions.
{code:java}
TRACE [AdminClient clientId=adminclient-55] Call(callName=offsetFetch(api=OFFSET_FETCH), deadlineMs=1671657869539, tries=0, nextAllowedTryMs=0) got response OffsetFetchResponseData(throttleTimeMs=0, topics=[OffsetFetchResponseTopic(name='XXX', partitions=[OffsetFetchResponsePartition(partitionIndex=1, committedOffset=866, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=0, committedOffset=865, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=9, committedOffset=868, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=14, committedOffset=870, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=5, committedOffset=803, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=8, committedOffset=881, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=11, committedOffset=-1, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=4, committedOffset=872, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=7, committedOffset=863, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=10, committedOffset=835, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=13, committedOffset=860, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=12, committedOffset=885, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=3, committedOffset=771, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=6, committedOffset=859, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=2, committedOffset=820, committedLeaderEpoch=-1, metadata='', errorCode=0), OffsetFetchResponsePartition(partitionIndex=15, committedOffset=826, committedLeaderEpoch=-1, metadata='', errorCode=0)])], errorCode=0, groups=[]) (org.apache.kafka.clients.admin.KafkaAdminClient) {code}",,csolidum,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14072,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 28 23:07:58 UTC 2022,,,,,,,,,,"0|z1eavs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 22:03;csolidum;Adding a check for null values in [MirrorCheckpointTask.checkpointsForGroups |https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java#L172]seems like the simplest fix for this issue.;;;","28/Dec/22 23:07;csolidum;ended up making the change in checkpoint instead of checkpointsForGroups since it was easier to test there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The ""is-future"" should be removed from metrics tags after future log becomes current log",KAFKA-14544,13515127,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,21/Dec/22 18:58,26/Dec/22 07:44,13/Jul/23 09:17,26/Dec/22 07:44,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"we don't remove ""is-future=true"" tag from future log after the future log becomes ""current"" log. It causes two potential issues:
 # the metrics monitors can't get metrics of Log if they don't trace the property ""is-future=true"".
 # all Log metrics of specify partition get removed if the partition is moved to another folder again.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-21 18:58:34.0,,,,,,,,,,"0|z1easw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataOutputStreamWritable#writeByteBuffer writes the wrong portion of the parameterized buffer,KAFKA-14540,13515025,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mmarshall,mmarshall,21/Dec/22 05:57,24/Feb/23 21:58,13/Jul/23 09:17,24/Feb/23 21:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"The method DataOutputStreamWritable#writeByteBuffer uses the buffer's position instead of its arrayOffset when writing the buffer to the output stream. As a result, the resulting buffer is corrupted.",,mmarshall,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 21:58:39 UTC 2023,,,,,,,,,,"0|z1ea6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 21:58;mmarshall;Fixed by https://github.com/apache/kafka/pull/13032;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly handle failed fetch when partitions unassigned,KAFKA-14532,13514773,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,lbrutschy,lbrutschy,lbrutschy,19/Dec/22 21:09,21/Dec/22 08:34,13/Jul/23 09:17,21/Dec/22 08:34,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,clients,,,,,,0,,,,,,"On master, all our long-running test jobs are running into this exception: 
{code:java}
java.lang.IllegalStateException: No current assignment for partition stream-soak-test-KSTREAM-OUTERTHIS-0000000086-store-changelog-1 2 at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:370) 3 at org.apache.kafka.clients.consumer.internals.SubscriptionState.clearPreferredReadReplica(SubscriptionState.java:623) 4 at java.util.LinkedHashMap$LinkedKeySet.forEach(LinkedHashMap.java:559) 5 at org.apache.kafka.clients.consumer.internals.Fetcher$1.onFailure(Fetcher.java:349) 6 at org.apache.kafka.clients.consumer.internals.RequestFuture.fireFailure(RequestFuture.java:179) 7 at org.apache.kafka.clients.consumer.internals.RequestFuture.raise(RequestFuture.java:149) 8 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:613) 9 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:427) 10 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) 11 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251) 12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1307) 13 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1243) 14 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) 15 at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:450) 16 at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:910) 17 at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:773) 18 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:613) 19 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:575) 20[2022-12-13 04:01:59,024] ERROR [i-016cf5d2c1889c316-StreamThread-1] stream-client [i-016cf5d2c1889c316] Encountered the following exception during processing and sent shutdown request for the entire application. (org.apache.kafka.streams.KafkaStreams) 21org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: No current assignment for partition stream-soak-test-KSTREAM-OUTERTHIS-0000000086-store-changelog-1 22 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:653) 23 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:575) 24Caused by: java.lang.IllegalStateException: No current assignment for partition stream-soak-test-KSTREAM-OUTERTHIS-0000000086-store-changelog-1 25 at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:370) 26 at org.apache.kafka.clients.consumer.internals.SubscriptionState.clearPreferredReadReplica(SubscriptionState.java:623) 27 at java.util.LinkedHashMap$LinkedKeySet.forEach(LinkedHashMap.java:559) 28 at org.apache.kafka.clients.consumer.internals.Fetcher$1.onFailure(Fetcher.java:349) 29 at org.apache.kafka.clients.consumer.internals.RequestFuture.fireFailure(RequestFuture.java:179) 30 at org.apache.kafka.clients.consumer.internals.RequestFuture.raise(RequestFuture.java:149) 31 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:613) 32 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:427) 33 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) 34 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251) 35 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1307) 36 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1243) 37 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) 38 at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:450) 39 at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:910) 40 at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:773) 41 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:613) 42 ... 1 more
 
{code}
 ",,ableegoldman,dajac,lbrutschy,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 12:49:36 UTC 2022,,,,,,,,,,"0|z1e8qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 12:49;dajac;cc [~ableegoldman];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft controller time-based snapshots are too frequent,KAFKA-14531,13514761,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,mumrah,mumrah,19/Dec/22 17:28,20/Dec/22 20:31,13/Jul/23 09:17,20/Dec/22 20:31,3.4.0,,,,,,,,,,,,,,,3.4.0,3.5.0,,,,,,controller,kraft,,,,,0,,,,,,The KRaft controller is generating snapshots too frequently by a factor of 1000000. This is due to a missed conversion from milliseconds to nanoseconds.,,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-19 17:28:07.0,,,,,,,,,,"0|z1e8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Base64 encoder used by OIDC OAuthBearerLoginCallbackHandler,KAFKA-14496,13514039,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,kirktrue,vendre,vendre,15/Dec/22 12:04,16/Dec/22 14:31,13/Jul/23 09:17,16/Dec/22 14:31,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,clients,,,,,,0,,,,,,"Currently our team is setting up a blueprint for our Kafka consumers/producers to provide guidelines on how to connect to our broker using the OIDC security mechanism. The blueprint is written in Java using the latest 3.3.1 Kafka library dependencies managed by Spring Boot 3.0.0.

While trying to use the new built-in {{org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler}} introduced by [KIP-768: Extend SASL/OAUTHBEARER with Support for OIDC|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186877575], we've noticed that some calls to retrieve the token work out well, while some of them (seemingly randomly) are failing with 401 Unauthorized.

After some debugging we've got to the conclusion that the faulty behavior is caused by {{org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever#formatAuthorizationHeader:}}
{code:java}
static String formatAuthorizationHeader(String clientId, String clientSecret) {
    clientId = sanitizeString(""the token endpoint request client ID parameter"", clientId);
    clientSecret = sanitizeString(""the token endpoint request client secret parameter"", clientSecret);
    
    String s = String.format(""%s:%s"", clientId, clientSecret);
    String encoded = Base64.getUrlEncoder().encodeToString(Utils.utf8(s));
    return String.format(""Basic %s"", encoded);
} {code}
The above code is using {{java.util.Base64#getUrlEncoder}} on line 311 to encode the authorization header value, which is using the alphabet described in [section 5 of the RFC|https://www.rfc-editor.org/rfc/rfc4648#section-5] during the encoding algorithm. As stated by the Basic Authentication Scheme [definition|https://www.rfc-editor.org/rfc/rfc7617#section-2] however, [section 4 of the RFC|https://www.rfc-editor.org/rfc/rfc4648#section-4] should be used:

??4. and obtains the basic-credentials by encoding this octet sequence using Base64 ([RFC4648], Section 4) into a sequence of US-ASCII characters ([RFC0020]).??

The difference between the 2 alphabets are only on two characters (62: '+' vs. '-' and 63: '/' vs. '_'), that's why the 401 Unauthorized response arises only for certain credential values.

Here's a concrete example use case:

 
{code:java}
String s = String.format(""%s:%s"", ""SOME_RANDOM_LONG_USER_01234"", ""9Q|0`8i~ute-n9ksjLWb\\50\""AX@UUED5E"");
System.out.println(Base64.getUrlEncoder().encodeToString(Utils.utf8(s))); {code}
would print out:
{code:java}
U09NRV9SQU5ET01fTE9OR19VU0VSXzAxMjM0OjlRfDBgOGl-dXRlLW45a3NqTFdiXDUwIkFYQFVVRUQ1RQ== {code}
while
{code:java}
String s = String.format(""%s:%s"", ""SOME_RANDOM_LONG_USER_01234"", ""9Q|0`8i~ute-n9ksjLWb\\50\""AX@UUED5E"");
System.out.println(Base64.getEncoder().encodeToString(Utils.utf8(s))); {code}
would give:
{code:java}
U09NRV9SQU5ET01fTE9OR19VU0VSXzAxMjM0OjlRfDBgOGl+dXRlLW45a3NqTFdiXDUwIkFYQFVVRUQ1RQ== {code}
Please notice the '-' vs. '+' characters.

 

The 2 code snippets above would not behave differently for other credentials, where the encoded result doesn't use the 62nd character of the alphabet:
{code:java}
String s = String.format(""%s:%s"", ""SHORT_USER_01234"", ""9Q|0`8i~ute-n9ksjLWb\\50\""AX@UUED5E"");
System.out.println(Base64.getEncoder().encodeToString(Utils.utf8(s))); {code}
{code:java}
U0hPUlRfVVNFUl8wMTIzNDo5UXwwYDhpfnV0ZS1uOWtzakxXYlw1MCJBWEBVVUVENUU=

{code}
 

As a *conclusion* I would suggest that line 311 of {{HttpAccessTokenRetriever}} should be modified to use {{Base64.getEncoder().encodeToString(...)}} instead of {{Base64.getUrlEncoder().encodeToString(...).}} 

 

I'm attaching a short sample application with tests proving that the above encoding method is rejected by the standard Spring Security HTTP basic authentication as well.",,kirktrue,omkreddy,showuon,vendre,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/22 12:05;vendre;base64test.zip;https://issues.apache.org/jira/secure/attachment/13053903/base64test.zip",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 06:40:58 UTC 2022,,,,,,,,,,"0|z1e488:",9223372036854775807,,manikumar,,,,,,,,,,,,,,,,,,"15/Dec/22 16:25;omkreddy;[~vendre] Thanks for reporting the issue. Would you like to submit a fix for this?

 

cc [~kirktrue];;;","15/Dec/22 18:54;vendre;[~omkreddy] I would be very glad to submit a fix.;;;","15/Dec/22 20:12;kirktrue;Thanks for catching this, Endre!;;;","15/Dec/22 20:34;kirktrue;[~vendre] - has been deemed a release blocker for AK 3.3.2 and 3.4. We really need a fix ASAP. Are you in a position to submit a patch in the next day or two? I want to make sure you get the credit for finding/fixing the issue. Thanks!;;;","15/Dec/22 20:55;vendre;[~kirktrue], there would be no technical issue for me to submit a fix.

Two things to consider however that could incur some delay:
 # would be the first time for me to contribute (I already have gone through the guide, but you never know, I might miss something)
 # I'm from Europe time zone, today I cannot stay for a long time

Please feel free to decide based on the above info, and if necessary, just assign the ticket to someone else, I wouldn't mind.

Thanks a lot!;;;","15/Dec/22 22:56;kirktrue;[~vendre] - thanks for the timely response!

I appreciate your candor and certainly don't want to rush you due to the tight scheduling of the AK release. I'll reassign the ticket to myself and use your fix and test case.

Thanks!;;;","15/Dec/22 23:48;kirktrue;[~vendre] - I've made a pull request ([https://github.com/apache/kafka/pull/13000)] based off your suggestions.

I'm happy to give you co-authorship attribution if you are comfortable sharing your email address.

Thanks!;;;","16/Dec/22 06:40;vendre;[~kirktrue], thanks for acting so fast upon the ticket and quickly applying the fix. We'll be very happy to have it available within the next release.

I'm definitely comfortable sharing my GitHub email address: [vendre@gmail.com.|mailto:vendre@gmail.com.] Thanks for the offering!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectorClientConfigOverridePolicy is not closed at worker shutdown,KAFKA-14463,13513233,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,nizhikov,gharris1727,gharris1727,12/Dec/22 17:08,24/Jan/23 02:38,13/Jul/23 09:17,24/Jan/23 02:38,2.3.0,,,,,,,,,,,,,,,3.5.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"The ConnectorClientConfigOverridePolicy is marked AutoCloseable, but is never closed by the worker on shutdown.

This is currently not a critical issue, as all known implementations of the policy have a no-op close. But a possible implementation which does instantiate background resources that must be closed in close() would leak those resources in a test environment.",,gharris1727,nizhikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 21 11:15:42 UTC 2023,,,,,,,,,,"0|z1dz9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/23 11:15;nizhikov;Hello, [~gharris1727] , [~ChrisEgerton]. Can, you, please, take a look at my changes?

 

https://github.com/apache/kafka/pull/13144;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreQueryIntegrationTest#shouldQuerySpecificActivePartitionStores logic to check for active partitions seems brittle.,KAFKA-14461,13512220,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sagarrao,sagarrao,sagarrao,11/Dec/22 16:21,21/Dec/22 12:34,13/Jul/23 09:17,21/Dec/22 12:34,,,,,,,,,,,,,,,,,,,,,,,streams,unit tests,,,,,0,,,,,,"{noformat}
StoreQueryIntegrationTest#shouldQuerySpecificActivePartitionStores {noformat}
has a logic to figure out active partitions:

 

 
{code:java}
final boolean kafkaStreams1IsActive = (keyQueryMetadata.activeHost().port() % 2) == 1;{code}
 

 

This is very brittle as when a new test gets added, this check would need to be changed to `==0`. It's a hassle to change it everytime with a new test added. Should look to improve this.

Also, this test relies on junit4 annotations which can be migrated to Junit 5 so that we can use @BeforeAll to set up and @AfterAll to shutdown the cluster instead of the current way where it's being done before/after every test.

 ",,mjsax,sagarrao,,,,,,,,,,,,,,,,,,,KAFKA-14454,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-11 16:21:42.0,,,,,,,,,,"0|z1dt08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent in quorum controller fenced broker metric,KAFKA-14457,13511407,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jsancio,jsancio,jsancio,10/Dec/22 00:27,27/Dec/22 20:46,13/Jul/23 09:17,20/Dec/22 19:04,3.1.0,3.1.1,3.1.2,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"It is possible for controllers to replay a record twice. This happens when the active controller replays an uncommitted record, resigns and replays the same record when it becomes committed.

The controller handles these transition changes by using timeline data structures and reverting to previous in-memory snapshots.

This functionality is not used when computing the fenced and unfenced metrics. Specifically, the metric can over count when executing this code:
{code:java}
           } else if (prevRegistration == null) {
              if (registration.fenced()) {
                  controllerMetrics.setFencedBrokerCount(controllerMetrics.fencedBrokerCount() + 1);
                  log.info(""Added new fenced broker: {}"", registration.id());
              } else {
                  controllerMetrics.setActiveBrokerCount(controllerMetrics.activeBrokerCount() + 1);
                  log.info(""Added new unfenced broker: {}"", registration.id());
              }{code}
From {{ClusterControlManager::updateMetrics.}}",,jsancio,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-10 00:27:25.0,,,,,,,,,,"0|z1dnzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Connect create and update REST APIs should surface failures while writing to the config topic,KAFKA-14455,13510786,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yash.mayya,yash.mayya,yash.mayya,08/Dec/22 13:25,08/May/23 15:19,13/Jul/23 09:17,02/Feb/23 16:03,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,KafkaConnect,,,,,,0,,,,,,"Kafka Connect's `POST /connectors` and `PUT /connectors/\{connector}/config` REST APIs internally simply write a message to the Connect cluster's internal config topic (which is then processed asynchronously by the herder). However, no callback is passed to the producer's send method and there is no error handling in place for producer send failures (see [here|https://github.com/apache/kafka/blob/c1a54671e8fc6c7daec5f5ec3d8c934be96b4989/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java#L716] / [here|https://github.com/apache/kafka/blob/c1a54671e8fc6c7daec5f5ec3d8c934be96b4989/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java#L726]).

Consider one such case where the Connect worker's principal doesn't have a WRITE ACL on the cluster's config topic. Now suppose the user submits a connector's configs via one of the above two APIs. The producer send [here|https://github.com/apache/kafka/blob/c1a54671e8fc6c7daec5f5ec3d8c934be96b4989/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java#L716] / [here|https://github.com/apache/kafka/blob/c1a54671e8fc6c7daec5f5ec3d8c934be96b4989/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java#L726] won't succeed (due to a TopicAuthorizationException) but the API responses will be `201 Created` success responses anyway. This is a very poor UX because the connector will actually never be created but the API response indicated success. Furthermore, this failure would only be detectable if TRACE logs are enabled (via [this log)|https://github.com/apache/kafka/blob/df29b17fc40f7c15460988d58bc652c3d66b60f8/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java] making it near impossible for users to debug. Producer callbacks should be used to surface write failures back to the user via the API response.",,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14674,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 07:43:57 UTC 2022,,,,,,,,,,"0|z1dk5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/22 07:43;yash.mayya;https://github.com/apache/kafka/pull/12984;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest#shouldThrowIllegalArgumentExceptionWhenCustomPartionerReturnsMultiplePartitions passes when run individually but not when is run as part of the IT,KAFKA-14454,13510684,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sagarrao,sagarrao,sagarrao,08/Dec/22 02:23,11/Dec/22 16:21,13/Jul/23 09:17,10/Dec/22 01:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Newly added test KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest#shouldThrowIllegalArgumentExceptionWhenCustomPartionerReturnsMultiplePartitions as part of KIP-837 passes when run individually but fails when is part of IT class and hence is marked as Ignored. 

 ",,ableegoldman,mjsax,sagarrao,,,,,,,,,,,,,,,,KAFKA-14461,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-08 02:23:15.0,,,,,,,,,,"0|z1djiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirror Maker Connectors leak admin clients used for topic creation,KAFKA-14443,13509950,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,gharris1727,gharris1727,gharris1727,05/Dec/22 18:33,08/Dec/22 03:02,13/Jul/23 09:17,07/Dec/22 22:01,,,,,,,,,,,,,,,,3.4.0,,,,,,,mirrormaker,,,,,,0,,,,,,"the MirrorMaker connectors are each responsible for creating internal topics.

For example, the Checkpoint connector creates a forwarding admin and passes it to a method to create the topic, but never closes the ForwardingAdmin or delegate objects: [https://github.com/apache/kafka/blob/13c9c78a1f4ad92023e8354069c6817b44c89ce6/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointConnector.java#L161-L164]

Instead, this object should be intentionally closed when it is no longer needed, to prevent consuming resources in a running MM2 application.",,ChrisEgerton,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13401,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 22:01:18 UTC 2022,,,,,,,,,,"0|z1df00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 22:01;ChrisEgerton;Merged and backported the fix to the 3.4 release branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlobalKTable restoration waits requestTimeout during application restart,KAFKA-14442,13509845,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,gergolep,gergolep,05/Dec/22 10:19,24/Feb/23 19:34,13/Jul/23 09:17,24/Feb/23 19:34,3.0.0,,,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,,,,,,"Using ""exactly_once_beta"" the highWatermark ""skips"" an offset after a transaction but in this case the global .checkpoint file contains different value (smaller by 1) than the highWatermark.
During restoration because of the difference between the checkpoint and highWatermark a poll will be attempted but sometimes there is no new record on the partition and the GlobalStreamThread has to wait for the requestTimeout to continue.
If there is any new record on the partition the problem does not occure.",,ableegoldman,gergolep,mjsax,,,,,,,,,,,,,,,,,,,KAFKA-12980,,,,,,,,,KAFKA-6607,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 07:04:14 UTC 2023,,,,,,,,,,"0|z1deco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 19:59;mjsax;Just learned about https://issues.apache.org/jira/browse/KAFKA-12980 – we should verify it it would actually fix this issue, similar to https://issues.apache.org/jira/browse/KAFKA-14713 ;;;","20/Feb/23 07:04;gergolep;Ahh, thanks! I tried with the 3.2.0 version and it works like a charm. Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kraft: StandardAuthorizer allowing a non-authorized user when `allow.everyone.if.no.acl.found` is enabled,KAFKA-14435,13509237,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,emissionnebula,emissionnebula,emissionnebula,02/Dec/22 13:13,21/Feb/23 07:30,13/Jul/23 09:17,21/Feb/23 07:30,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,,,,,,,,,,3.3.2,3.4.0,,,,,,kraft,,,,,,0,,,,,,"When `allow.everyone.if.no.acl.found` is enabled, the authorizer should allow everyone only if there is no ACL present for a particular resource. But if there are ACL present for the resource, then it shouldn't be allowing everyone.

StandardAuthorizer is allowing the principals for which no ACLs are defined even when the resource has other ACLs.

 

This behavior can be validated with the following test case:

 
{code:java}
@Test
public void testAllowEveryoneConfig() throws Exception {
    StandardAuthorizer authorizer = new StandardAuthorizer();
    HashMap<String, Object> configs = new HashMap<>();
    configs.put(SUPER_USERS_CONFIG, ""User:alice;User:chris"");
    configs.put(ALLOW_EVERYONE_IF_NO_ACL_IS_FOUND_CONFIG, ""true"");
    authorizer.configure(configs);
    authorizer.start(new AuthorizerTestServerInfo(Collections.singletonList(PLAINTEXT)));
    authorizer.completeInitialLoad();


    // Allow User:Alice to read topic ""foobar""
    List<StandardAclWithId> acls = asList(
        withId(new StandardAcl(TOPIC, ""foobar"", LITERAL, ""User:Alice"", WILDCARD, READ, ALLOW))
    );
    acls.forEach(acl -> authorizer.addAcl(acl.id(), acl.acl()));

    // User:Bob shouldn't be allowed to read topic ""foobar""
    assertEquals(singletonList(DENIED),
        authorizer.authorize(new MockAuthorizableRequestContext.Builder().
                setPrincipal(new KafkaPrincipal(USER_TYPE, ""Bob"")).build(),
            singletonList(newAction(READ, TOPIC, ""foobar""))));

}
 {code}
 

In the above test, `User:Bob` should be DENIED but the above test case fails.",,ChrisEgerton,emissionnebula,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 13:19:41 UTC 2022,,,,,,,,,,"0|z1dals:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:19;emissionnebula;This can be fixed by adding a flag `noResourceAcls` in `MatchingAclBuilder` class. We can set this flag inside the `if` block [here|https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java#L523].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBStore relies on finalizers to not leak memory,KAFKA-14432,13508113,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,lbrutschy,lbrutschy,lbrutschy,01/Dec/22 15:58,10/Dec/22 02:22,13/Jul/23 09:17,08/Dec/22 02:30,,,,,,,,,,,,,,,,3.4.0,,,,,,,streams,,,,,,0,,,,,,"Relying on finalizers in RocksDB has been deprecated for a long time, and starting with rocksdb 7, finalizers are removed completely (see [https://github.com/facebook/rocksdb/pull/9523]). 

Kafka Streams currently relies on finalizers in parts to not leak memory. This needs to be resolved before we can upgrade to RocksDB 7.

See  [https://github.com/apache/kafka/pull/12809] .

This is a native heap profile after running Kafka Streams without finalizers for a few hours:
{code:java}
Total: 13547.5 MB
12936.3 95.5% 95.5% 12936.3 95.5% rocksdb::port::cacheline_aligned_alloc
438.5 3.2% 98.7% 438.5 3.2% rocksdb::BlockFetcher::ReadBlockContents
84.0 0.6% 99.3% 84.2 0.6% rocksdb::Arena::AllocateNewBlock
45.9 0.3% 99.7% 45.9 0.3% prof_backtrace_impl
8.1 0.1% 99.7% 14.6 0.1% rocksdb::BlockBasedTable::PutDataBlockToCache
6.4 0.0% 99.8% 12941.4 95.5% Java_org_rocksdb_Statistics_newStatistics___3BJ
6.1 0.0% 99.8% 6.9 0.1% rocksdb::LRUCacheShard::Insert@2d8b20
5.1 0.0% 99.9% 6.5 0.0% rocksdb::VersionSet::ProcessManifestWrites
3.9 0.0% 99.9% 3.9 0.0% rocksdb::WritableFileWriter::WritableFileWriter
3.2 0.0% 99.9% 3.2 0.0% std::string::_Rep::_S_create{code}",,ableegoldman,lbrutschy,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-01 15:58:43.0,,,,,,,,,,"0|z1d3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopologyTestDriver instantiation raised NPE in JUnit5,KAFKA-14431,13507783,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mehrdad.karami,mehrdad.karami,01/Dec/22 09:06,01/Dec/22 09:17,13/Jul/23 09:17,01/Dec/22 09:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,," 

Library: *kafka-streams*
Version: *3.3.1*
Framework: *spring-boot:2.7.6*

Defining the following code in JUnit5 test method:

 
{code:java}
try (final TopologyTestDriver testDriver = new TopologyTestDriver(sb.build(), streamProps)) {


}
 {code}

The test *passes* with following {*}{color:#FF0000}errors{color}{*}:

 

 
{code:java}
java.lang.NullPointerException: Cannot invoke ""org.apache.kafka.streams.processor.api.ProcessorSupplier.get()"" because ""this.processorSupplier"" is null
    at org.apache.kafka.streams.kstream.internals.graph.ProcessorParameters.toString(ProcessorParameters.java:133)
    at java.base/java.lang.String.valueOf(String.java:3365)
    at java.base/java.lang.StringBuilder.append(StringBuilder.java:169)
    at org.apache.kafka.streams.kstream.internals.graph.ProcessorGraphNode.toString(ProcessorGraphNode.java:52)
    at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:277)
    at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:249)
    at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:211)
    at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:161)
    at ch.qos.logback.classic.spi.LoggingEvent.getFormattedMessage(LoggingEvent.java:293)
    at ch.qos.logback.classic.spi.LoggingEvent.prepareForDeferredProcessing(LoggingEvent.java:206)
    at ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:223)
    at ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:102)
    at ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:84)
    at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51)
    at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270)
    at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257)
    at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421)
    at ch.qos.logback.classic.Logger.filterAndLog_2(Logger.java:414)
    at ch.qos.logback.classic.Logger.debug(Logger.java:490)
    at org.apache.kafka.streams.kstream.internals.InternalStreamsBuilder.buildAndOptimizeTopology(InternalStreamsBuilder.java:293)
    at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:628)
    at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:613)
    at com.metao.book.order.kafka.OrderStreamTest.shouldAggregateRecord(OrderStreamTest.java:35)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:567)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54){code}

*Acceptance Criteria:*

Manage to check for nullability 

 ",,mehrdad.karami,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Dec 01 09:17:04 UTC 2022,,,,,,,,,,"0|z1d1mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 09:17;mehrdad.karami;Saw already fixed in truck branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer rebalance stuck after new static member joins a group with members not supporting static members,KAFKA-14422,13506203,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,28/Nov/22 08:15,28/Nov/22 19:29,13/Jul/23 09:17,28/Nov/22 19:29,2.3.0,2.4.0,2.5.0,2.6.0,2.7.0,2.8.0,3.0.0,3.1.0,3.2.0,3.3.0,,,,,,3.2.4,3.3.2,3.4.0,,,,,,,,,,,0,,,,,,"When a consumer group on a version prior to 2.3 is rolled upgraded to a newer version and static membership is enabled in the meantime, the consumer group remains stuck, iff the leader is still on the old version.

The issue is that setting `GroupInstanceId` in the response to the leader is only supported from JoinGroup version >= 5 and that `GroupInstanceId` is not ignorable nor handled anywhere else. Hence is there is at least one static member in the group, sending the JoinGroup response to the leader fails with a serialization error.

> org.apache.kafka.common.errors.UnsupportedVersionException: Attempted to write a non-default groupInstanceId at version 2

When this happens, the member stays around until the group coordinator is bounced because a member with a non-null `awaitingJoinCallback` is never expired.

We should do two things here:
1) `GroupInstanceId` should be ignorable;
2) We should handle errors while calling callbacks.

",,dajac,philbour,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-28 08:15:22.0,,,,,,,,,,"0|z1crwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThreadCache is getting slower with every additional state store,KAFKA-14415,13505373,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lbrutschy,lbrutschy,lbrutschy,22/Nov/22 15:05,07/Dec/22 01:23,13/Jul/23 09:17,07/Dec/22 01:23,,,,,,,,,,,,,,,,3.4.0,,,,,,,streams,,,,,,0,,,,,,"There are a few lines in `ThreadCache` that I think should be optimized. `sizeBytes` is called at least once, and potentially many times in every `put` and is linear in the number of caches (= number of state stores, so typically proportional to number of tasks). That means, with every additional task, every put gets a little slower.Compare the throughput of TIME_ROCKS on trunk (green graph):

[http://kstreams-benchmark-results.s3-website-us-west-2.amazonaws.com/experiments/stateheavy-3-5-3-4-0-51b7eb7937-jenkins-20221113214104-streamsbench/]

This is the throughput of TIME_ROCKS is 20% higher when a constant time `sizeBytes` implementation is used:

[http://kstreams-benchmark-results.s3-website-us-west-2.amazonaws.com/experiments/stateheavy-3-5-LUCASCOMPARE-lucas-20221122140846-streamsbench/]

The same seems to apply for the MEM backend (initial throughput >8000 instead of 6000), however, I cannot run the same benchmark here because the memory is filled too quickly.

[http://kstreams-benchmark-results.s3-website-us-west-2.amazonaws.com/experiments/stateheavy-3-5-LUCASSTATE-lucas-20221121231632-streamsbench/]

 ",,ableegoldman,lbrutschy,mehrdad.karami,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 21:08:35 UTC 2022,,,,,,,,,,"0|z1cms0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 06:27;ableegoldman;Nice find, I bet the assumption when this was first implemented was that the number of named caches/actual state stores would be pretty low, but the reality is many apps can easily grow to the point of this many sizeBytes() invocations having nontrivial overhead...so yeah, good catch :) ;;;","28/Nov/22 21:48;mehrdad.karami;Hi [~lbrutschy]  can you tell me which program for benchmark is used please?;;;","29/Nov/22 21:08;lbrutschy;I fear the tool for generating this is a company-internal;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Double iteration of records in batches to be restored,KAFKA-14406,13503714,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nicktelford,ableegoldman,ableegoldman,19/Nov/22 01:30,21/Nov/22 09:54,13/Jul/23 09:17,19/Nov/22 01:31,,,,,,,,,,,,,,,,3.4.0,,,,,,,streams,,,,,,0,,,,,,"While restoring a batch of records, {{RocksDBStore}} was iterating the {{{}ConsumerRecord{}}}s, building a list of {{{}KeyValue{}}}s, and then iterating _that_ list of {{{}KeyValue{}}}s to add them to the RocksDB batch.

Simply adding the key and value directly to the RocksDB batch prevents this unnecessary second iteration, and the creation of itermediate {{KeyValue}} objects, improving the performance of state restoration, and reducing unnecessary object allocation.

(thanks to Nick Telford for finding this)",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-19 01:30:49.0,,,,,,,,,,"0|z1ccjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CVE-2022-34917 | Fixed in 3.3.1",KAFKA-14389,13502630,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,masood31,masood31,15/Nov/22 06:48,15/Nov/22 14:29,13/Jul/23 09:17,15/Nov/22 14:29,3.3.1,,,,,,,,,,,,,,,,,,,,,,documentation,,,,,,0,,,,,,"the following link has not been updated with v3.3.1. 

[https://kafka.apache.org/cve-list#CVE-2022-34917]

Its says the fixed is available until 3.2.3, so does it mean this CVE is re-introduced in the v3.3.1.",,masood31,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 14:29:46 UTC 2022,,,,,,,,,,"0|z1c5vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 09:26;mimaison;Later versions are not affected by this issue. You can see that 3.3.1 is not listed in the affected releases.;;;","15/Nov/22 14:29;masood31;Thanks for clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE When Retrieving StateStore with new Processor API,KAFKA-14388,13502404,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbejeck,bbejeck,bbejeck,14/Nov/22 20:18,16/Nov/22 22:52,13/Jul/23 09:17,16/Nov/22 22:07,3.3.0,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,streams,,,,,,0,,,,,,"Using the new Processor API introduced with KIP-820 when adding a state store to the Processor when executing `context().getStore(""store-name"")` always returns `null` as the store is not in the `stores` `HashMap` in the `ProcessorStateManager`.  This occurs even when using the `ConnectedStoreProvider.stores()` method

I've confirmed the store is associated with the processor by viewing the `Topology` description.   

From some initial triage, it looks like the store is never registered.",,ableegoldman,bbejeck,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 22:51:47 UTC 2022,,,,,,,,,,"0|z1c4hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 21:12;bbejeck;PR with details of the bug https://github.com/apache/kafka/pull/12861;;;","16/Nov/22 22:07;bbejeck;Merged PR to trunk;;;","16/Nov/22 22:51;bbejeck;Cherry-picked to 3.3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamThreads can miss rebalance events when processing records during a rebalance,KAFKA-14382,13501023,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,ableegoldman,ableegoldman,11/Nov/22 04:09,20/Jan/23 22:51,13/Jul/23 09:17,19/Nov/22 06:52,,,,,,,,,,,,,,,,3.0.3,3.1.3,3.2.4,3.3.2,3.4.0,,,streams,,,,,,0,new-consumer-threading-should-fix,rebalancing,,,,"One of the main improvements introduced by the cooperative protocol was the ability to continue processing records during a rebalance. In Streams, we take advantage of this by polling with a timeout of 0 when a rebalance is/has been in progress, so it can return immediately and continue on through the main loop to process new records. The main poll loop uses an algorithm based on the max.poll.interval.ms to ensure the StreamThread returns to call #poll in time to stay in the consumer group.

 

Generally speaking, it should exit the processing loop and invoke poll within a few minutes at most based on the poll interval, though typically it will break out much sooner once it's used up all the records from the last poll (based on the max.poll.records config which Streams sets to 1,000 by default). However, if doing heavy processing or setting a higher max.poll.records, the thread may continue processing for more than a few seconds. If it had sent out a JoinGroup request before going on to process and was waiting for its JoinGroup response, then once it does return to invoke #poll it will process this response and send out a SyncGroup – but if the processing took too long, this SyncGroup may immediately fail with the REBALANCE_IN_PROGRESS error.

 

Essentially, while the thread was processing the group leader will itself be processing the JoinGroup subscriptions of all members and generating an assignment, then sending this back in its SyncGroup. This may take only a few seconds or less, and the group coordinator will not yet have noticed (or care) that one of the consumers hasn't sent a SyncGroup – it will just return the assigned partitions in the SyncGroup request of the members who have responded in time, and ""complete"" the rebalance in their eyes. But if the assignment involved moving any partitions from one consumer to another, then it will need to trigger a followup rebalance right away to finish assigning those partitions which were revoked in the previous rebalance. This is what causes a new rebalance to be kicked off just seconds after the first one began.

 

If the consumer that was stuck processing was among those who needed to revoke partitions, this can lead to repeating rebalances – since it fails the SyncGroup of the 1st rebalance it never receives the assignment for it and never knows to revoke those partitions, meaning it will rejoin for the new rebalance still claiming them among its ownedPartitions. When the assignor generates the same assignment for the 2nd rebalance, it will again see that some partitions need to be revoked and will therefore trigger yet another new rebalance after finishing the 2nd. This can go on for as long as the StreamThreads are struggling to finish the JoinGroup phase in time due to processing.

 

Note that the best workaround at the moment is probably to just set a lower max.poll.records to reduce the processing loop duration",,ableegoldman,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 20 22:51:28 UTC 2023,,,,,,,,,,"0|z1bvz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/22 09:50;ableegoldman;Ok while working out a fix I realized the true root cause of this bug, the above is all correct but the fundamental issue is how quickly we trigger a new rebalance, not the thread's inability to call poll within just a few seconds – it shouldn't have to. The problem is the assignor might schedule an immediate followup rebalance for itself and then leave the poll() call after sending the assignment/SyncGroup request, but before the rebalance is actually finished, and then trigger a new rebalance when it checks and sees that one has been scheduled. 

Basically we should just make sure the thread waits for any ongoing rebalance to finish before triggering a new one;;;","20/Jan/23 22:51;guozhang;Thanks for catching this bug [~ableegoldman]! I'm late reviewing the PR but I agree with your general case description still. And I think we already have the tools to solve the fundamentals as well:

1) When we have the consumer thread refactoring done (hence that's why I also add the corresponding label), rebalance would be done by the background thread completely and not relying on Streams to call `poll` in time at all. To validate that the caller thread is still alive, we still need to call it within the max.poll.call, but nothing else like the rebalance related timeous would matter.

2) When we have restoration (a heavy IO operation) to the separate thread, we should also see the likelihood that the stream thread stuck and not being able to call `poll` in time much less as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consumer should refresh preferred read replica on update metadata,KAFKA-14379,13500994,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,11/Nov/22 00:17,12/Dec/22 09:06,13/Jul/23 09:17,12/Dec/22 09:06,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"The consumer (fetcher) refreshes the preferred read replica only on three conditions:
 # the consumer receives an OFFSET_OUT_OF_RANGE error
 # the follower does not exist in the client's metadata (i.e., offline)
 # after metadata.max.age.ms (5 min default)

For other errors, it will continue to reach to the possibly unavailable follower and only after 5 minutes will it refresh the preferred read replica and go back to the leader.

A specific example is when a partition is reassigned. the consumer will get NOT_LEADER_OR_FOLLOWER which triggers a metadata update but the preferred read replica will not be refreshed as the follower is still online. it will continue to reach out to the old follower until the preferred read replica expires.

the consumer can instead refresh its preferred read replica whenever it makes a metadata update request. so when the consumer receives i.e. NOT_LEADER_OR_FOLLOWER it can find the new preferred read replica without waiting for the expiration.

 ",,dengziming,jeffkbkim,showuon,yding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 22:06:47 UTC 2022,,,,,,,,,,"0|z1bvso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 02:28;dengziming;Hello [~jeffkbkim] , you created another duplicates of this issue, I have closed them, plz not create identical issues.;;;","15/Nov/22 23:01;jeffkbkim;sorry for that. got an error when i clicked on create;;;","24/Nov/22 08:30;showuon;[~jeffkbkim] , we recently also encountered this issue, do you plan to submit PR to fix this issue before v3.4.0/v3.3.2?

 ;;;","24/Nov/22 08:31;showuon;Oh, sorry, I saw you have a WIP PR opened. Thanks.

 ;;;","28/Nov/22 17:41;jeffkbkim;[~showuon] yes, that is the current plan. can you share what error the client faced and the result?;;;","28/Nov/22 22:06;jeffkbkim;updated from draft to open PR. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RackAwareReplicaSelector should choose a replica from the isr,KAFKA-14372,13500778,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,09/Nov/22 21:19,14/Dec/22 09:56,13/Jul/23 09:17,24/Nov/22 14:11,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"The default replica selector chooses a replica on whether the broker.rack matches the client.rack in the fetch request and whether the offset exists in the follower. If the follower is not in the ISR, we know it's lagging behind which will also lag the consumer behind. Let's consider two cases:
 # the follower recovers and joins the isr. the consumer will no longer lag
 # the follower continues to lag behind. after 5 minutes, the consumer will refresh the preferred read replica and it returns the same lagging follower since the offset the consumer will fetch from is capped by the follower's HWM. this can go on indefinitely

If the replica selector chooses a broker in the ISR then we can ensure that at least every 5 minutes the consumer will consume from an up-to-date replica. 

 

 

 

 ",,jeffkbkim,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-09 21:19:05.0,,,,,,,,,,"0|z1bugw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation: Streams Security page has broken links,KAFKA-14360,13496560,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lqxshay,CatchDepthHeightLight,CatchDepthHeightLight,05/Nov/22 14:34,15/Nov/22 21:31,13/Jul/23 09:17,15/Nov/22 21:31,,,,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,0,,,,,,"A number of links on the 'Streams Security' page are 404-ing

https://kafka.apache.org/documentation/streams/developer-guide/security.html

* Kafka’s security features https://kafka.apache.org/documentation/documentation.html#security
* Java Producer and Consumer API https://kafka.apache.org/documentation/clients/index.html#kafka-clients",,ableegoldman,CatchDepthHeightLight,lqxshay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 21:17:10 UTC 2022,,,,,,,,,,"0|z1b4g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 02:35;ableegoldman;Weird, thanks for finding this – I'm guessing the first one is broken due to an extra '/documentation' in the link, it should presumably be directing to [https://kafka.apache.org/documentation/#security]

 

As for the 2nd one, it's a little less clear what it should be pointing to – oddly there does not seem to be a dedicated ""clients"" page on the actual AK docs site, if you go into the drop-down menu under ""Docs"" for example and click on ""Clients"" there, it actually sends you to a wiki page on the non-java clients. I don't think that's what was intended here. Since it mentions APIs my best guess is that it should be linking to this: [https://kafka.apache.org/documentation/#api] ;;;","15/Nov/22 21:17;lqxshay;Thanks [~ableegoldman]. Here is an PR to fix this broken link [https://github.com/apache/kafka/pull/12857.] Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Users should not be able to create a regular topic name __cluster_metadata,KAFKA-14358,13495956,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jsancio,jsancio,jsancio,04/Nov/22 21:43,02/Dec/22 16:27,13/Jul/23 09:17,02/Dec/22 16:27,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,controller,,,,,,0,,,,,,"The following test passes and it should not:
{code:java}
 $ git diff                           
diff --git a/core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala b/core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala
index 57834234cc..14b1435d00 100644
--- a/core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala
+++ b/core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala
@@ -102,6 +102,12 @@ class CreateTopicsRequestTest extends AbstractCreateTopicsRequestTest {
     validateTopicExists(""partial-none"")
   }
  
+  @ParameterizedTest(name = TestInfoUtils.TestWithParameterizedQuorumName)
+  @ValueSource(strings = Array(""zk"", ""kraft""))
+  def testClusterMetadataTopicFails(quorum: String): Unit = {
+    createTopic(""__cluster_metadata"", 1, 1)
+  }
+
   @ParameterizedTest(name = TestInfoUtils.TestWithParameterizedQuorumName)
   @ValueSource(strings = Array(""zk""))
   def testCreateTopicsWithVeryShortTimeouts(quorum: String): Unit = {{code}
Result of this test:
{code:java}
 $ ./gradlew core:test --tests CreateTopicsRequestTest.testClusterMetadataTopicFails
> Configure project :
Starting build with version 3.4.0-SNAPSHOT (commit id bc780c7c) using Gradle 7.5.1, Java 1.8 and Scala 2.13.8
Build properties: maxParallelForks=12, maxScalacThreads=8, maxTestRetries=0

> Task :core:test
Gradle Test Run :core:test > Gradle Test Executor 8 > CreateTopicsRequestTest > testClusterMetadataTopicFails(String) > kafka.server.CreateTopicsRequestTest.testClusterMetadataTopicFails(String)[1] PASSED
Gradle Test Run :core:test > Gradle Test Executor 8 > CreateTopicsRequestTest > testClusterMetadataTopicFails(String) > kafka.server.CreateTopicsRequestTest.testClusterMetadataTopicFails(String)[2] PASSED

BUILD SUCCESSFUL in 44s
44 actionable tasks: 3 executed, 41 up-to-date
{code}
I think that this test should fail in both KRaft and ZK. We want this to fail in ZK so that it can be migrated to KRaft.",,ChrisEgerton,dengziming,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 21:43:34.0,,,,,,,,,,"0|z1b0q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflow occurs in kafka-producer-perf-test.sh (class ProducerProfrmance ).,KAFKA-14355,13495541,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mintomio,mintomio,mintomio,04/Nov/22 12:26,05/Nov/22 12:19,13/Jul/23 09:17,05/Nov/22 12:19,3.3.1,,,,,,,,,,,,,,,3.4.0,,,,,,,tools,,,,,,0,,,,,," 

An integer overflow occurs if a fairly large value is passed to {{{}--num-records{}}}.  It causes a {{{}NegativeArraySizeException error{}}}.
{code:java}
./bin/kafka-producer-perf-test.sh --topic=test --producer-props bootstrap.servers=localhost:9092 --num-records 2000000000000000 --throughput 1 --record-size 10
Exception in thread ""main"" java.lang.NegativeArraySizeException
    at org.apache.kafka.tools.ProducerPerformance$Stats.<init>(ProducerPerformance.java:354)
    at org.apache.kafka.tools.ProducerPerformance.start(ProducerPerformance.java:97)
    at org.apache.kafka.tools.ProducerPerformance.main(ProducerPerformance.java:52){code}",,mintomio,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 12:26:20.0,,,,,,,,,,"0|z1ay5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source task producers commit transactions even if offsets cannot be serialized,KAFKA-14339,13492520,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,swasnik@confluent.io,swasnik@confluent.io,27/Oct/22 12:47,30/Nov/22 00:59,13/Jul/23 09:17,30/Nov/22 00:59,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,KafkaConnect,,,,,,0,exactly-once,,,,,"In ExactlyOnceWorkerSourceTask, producer.commitTransaction is performed even if offsetWriter faces a serialization error. 

This leads to no offsets being sent to the producer, but still trying to commit the transaction. ",,kirktrue,swasnik@confluent.io,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2022-10-27 12:47:19.0,,,,,,,,,,"0|z1afk0:",9223372036854775807,,cegerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"topic name with ""."" cannot be created after deletion",KAFKA-14337,13491397,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,showuon,thanhnd96,thanhnd96,26/Oct/22 03:17,12/Dec/22 23:59,13/Jul/23 09:17,28/Oct/22 17:21,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,kraft,,,,,,0,,,,,,"Hi admin,

My issue is after i create topic like topic.AAA or Topic.AAA.01 then delete 1 of the other 2 topics.

Then i can't create 1 of the 2 topics.

But i create topic test123 then delete and recreate fine.

This is log i tried to create topic.AAA

WARN [Controller 1] createTopics: failed with unknown server exception NoSuchElementException at epoch 14 in 193 us.  Renouncing leadership and reverting to the last committed offset 28. (org.apache.kafka.controller.QuorumController)
java.util.NoSuchElementException
        at org.apache.kafka.timeline.SnapshottableHashTable$CurrentIterator.next(SnapshottableHashTable.java:167)
        at org.apache.kafka.timeline.SnapshottableHashTable$CurrentIterator.next(SnapshottableHashTable.java:139)
        at org.apache.kafka.timeline.TimelineHashSet$ValueIterator.next(TimelineHashSet.java:120)
        at org.apache.kafka.controller.ReplicationControlManager.validateNewTopicNames(ReplicationControlManager.java:799)
        at org.apache.kafka.controller.ReplicationControlManager.createTopics(ReplicationControlManager.java:567)
        at org.apache.kafka.controller.QuorumController.lambda$createTopics$7(QuorumController.java:1832)
        at org.apache.kafka.controller.QuorumController$ControllerWriteEvent.run(QuorumController.java:767)
        at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
        at java.base/java.lang.Thread.run(Thread.java:829)

ERROR [Controller 1] processBrokerHeartbeat: unable to start processing because of NotControllerException. (org.apache.kafka.controller.QuorumController)

 

I'm run kafka mode Kraft !!!

Tks admin.",,showuon,thanhnd96,,,,,,,,,,,,,,,,,,,,,,KAFKA-14271,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,Tue Nov 01 06:16:58 UTC 2022,,,,,,,,,,"0|z1a8mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 05:24;showuon;Nice find! Investigating!

 ;;;","27/Oct/22 07:49;thanhnd96;Hi admin,

Tks you for fixed , after i change code to this link [GitHub Pull Request #12790|https://github.com/apache/kafka/pull/12790] , it's worked on kafka Source download: [kafka-3.3.1-src.tgz.|https://downloads.apache.org/kafka/3.3.1/kafka-3.3.1-src.tgz]

But can I fix on Binary downloads: Scala 2.13  - [kafka_2.13-3.3.1.tgz|https://downloads.apache.org/kafka/3.3.1/kafka_2.13-3.3.1.tgz] ([asc|https://downloads.apache.org/kafka/3.3.1/kafka_2.13-3.3.1.tgz.asc], [sha512|https://downloads.apache.org/kafka/3.3.1/kafka_2.13-3.3.1.tgz.sha512]) on my kafka.

I deploy to 3 broker kafka running kraft mode .

Best regards !!!;;;","31/Oct/22 07:10;showuon;[~thanhnd96] , this bug fix will be in Kafka v3.3.2 and v3.4.0 and later. The release date is not confirmed, yet, but it should be happen by the end of the year, or the beginning of 2023. FYI

 ;;;","01/Nov/22 06:16;thanhnd96;thank you [~showuon] !!!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelayedFetch purgatory not completed when appending as follower,KAFKA-14334,13490636,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,24/Oct/22 15:33,18/Nov/22 15:26,13/Jul/23 09:17,16/Nov/22 14:04,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"Currently, the ReplicaManager.delayedFetchPurgatory is only completed when appending as leader. With [https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica] enabled, followers will also have to complete delayed fetch requests after successfully replicating. Otherwise, consumer fetches to closest followers will hit fetch.max.wait.ms",,jeffkbkim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-24 15:33:39.0,,,,,,,,,,"0|z1a3y0:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointer in ProcessorParameters.toString,KAFKA-14325,13487231,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jeqo,mgh,mgh,20/Oct/22 09:07,21/Nov/22 02:28,13/Jul/23 09:17,21/Nov/22 02:25,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,streams,,,,,,0,,,,,,"After switching from {{Transformer}} to using {{FixedKeyProcessor}} I get some NullPointer exceptions logged. It seems to be due to {{ProcessorParameters::toString}} using it's field {{processorSupplier}} [without a null check|https://github.com/apache/kafka/blob/0a045d4ef7d67dbe35b8fd2e1c51df87af19a0ab/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/ProcessorParameters.java#L133]. After KAFKA-13654: Extend KStream process with new Processor API ([#11993|https://github.com/apache/kafka/pull/11993]) this field can be null and instead the field {{fixedKeyProcessorSupplier}} is set.

 

Stack trace
{code:java}
java.lang.NullPointerException: Cannot invoke ""org.apache.kafka.streams.processor.api.ProcessorSupplier.get()"" because ""this.processorSupplier"" is null
    at org.apache.kafka.streams.kstream.internals.graph.ProcessorParameters.toString(ProcessorParameters.java:133)
    at java.base/java.lang.String.valueOf(String.java:4218)
    at java.base/java.lang.StringBuilder.append(StringBuilder.java:173)
    at org.apache.kafka.streams.kstream.internals.graph.ProcessorGraphNode.toString(ProcessorGraphNode.java:52)
    at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:277)
    at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:249)
    at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:211)
    at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:161)
    at ch.qos.logback.classic.spi.LoggingEvent.getFormattedMessage(LoggingEvent.java:293)
    at ch.qos.logback.classic.spi.LoggingEvent.prepareForDeferredProcessing(LoggingEvent.java:206)
    at ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:223)
    at ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:102)
    at ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:84)
    at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51)
    at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270)
    at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257)
    at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421)
    at ch.qos.logback.classic.Logger.filterAndLog_2(Logger.java:414)
    at ch.qos.logback.classic.Logger.debug(Logger.java:490)
    at org.apache.kafka.streams.kstream.internals.InternalStreamsBuilder.buildAndOptimizeTopology(InternalStreamsBuilder.java:293)
    at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:628) {code}",,ableegoldman,Cerchie,jeqo,mgh,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 21:21:32 UTC 2022,,,,,,,,,,"0|z19iyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 08:11;ableegoldman;Hm, nice catch, we should definitely fix this up.

 

Hey [~jeqo] , can you give this class a quick glance to see if there is anything else that's still accessing/using the old processorSupplier field instead of the new/fixed key one?;;;","15/Nov/22 21:21;jeqo;Hi there. Of course, looking at this now.

Looking at why this wasn't triggered before:
 * Logging is on debug level  [https://github.com/apache/kafka/blob/21a15c6b1f1ee80f163633ba617ad381f5edc0c1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java#L292-L294]
 ** but logging in tests is only at INFO level: [https://github.com/apache/kafka/blob/46bee5bcf3e6877079111cbfd91a2fbaf3975a98/streams/src/test/resources/log4j.properties#L30] 

Turning this config to DEBUG is causing a bunch of tests to fail: 6216 tests completed, 256 failed, 1 skipped (locally)

MockApiProcessorSupplier is causing some strange issues, e.g. org.apache.kafka.streams.StreamsBuilderTest#shouldMergeStreams: java.lang.AssertionError: expected:<1> but was:<5>

Will have to debug further.

Also, added a couple of additional castings to this class: https://github.com/apache/kafka/pull/12859/files;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[CVE-2018-25032] introduced by rocksdbjni:6.29.4.1,KAFKA-14324,13487184,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,christo_lolov,vinsonZhang,vinsonZhang,20/Oct/22 01:21,09/May/23 06:41,13/Jul/23 09:17,15/Nov/22 11:51,3.1.2,3.2.3,3.3.1,,,,,,,,,,,,,3.0.3,3.1.3,3.2.4,3.3.2,3.4.0,,,streams,,,,,,0,,,,,,"Hi, Team
There is an old CVE introduced by rocksdbjni-6.29.4.1, which has already been fixed by [https://github.com/facebook/rocksdb/commit/5dbdb197f19644d3f53f75781a3ef56e4387134b]

[https://nvd.nist.gov/vuln/detail/cve-2018-25032]

*Current Description:* 

zlib before 1.2.12 allows memory corruption when deflating (i.e., when compressing) if the input has many distant matches.

CVE-2018-25032 - CVSS Score:{*}7.5{*} (v3.0) (zlib-1.2.11)

Please help to upgrade the rocksdb.
Thanks",,ableegoldman,christo_lolov,mjsax,vinsonZhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/22 06:38;vinsonZhang;6.29.4.1_to_7.1.2_compat_report.html;https://issues.apache.org/jira/secure/attachment/13051403/6.29.4.1_to_7.1.2_compat_report.html","26/Oct/22 06:38;vinsonZhang;6.29.4.1_to_7.7.3_compat_report.html;https://issues.apache.org/jira/secure/attachment/13051404/6.29.4.1_to_7.7.3_compat_report.html",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 13:14:08 UTC 2022,,,,,,,,,,"0|z19io8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 13:14;christo_lolov;Hello [~vinsonZhang]! I prepared a pull request for this and I would be grateful if you could review it :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Jackson for CVE fix,KAFKA-14320,13486931,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tomncooper,jlisam,jlisam,18/Oct/22 22:51,18/Nov/22 18:10,13/Jul/23 09:17,18/Nov/22 18:10,3.2.0,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,core,,,,,,0,security,,,,,"There is a CVE for Jackson:

Jackson: [CVE-2020-36518|https://nvd.nist.gov/vuln/detail/CVE-2020-36518] - Fixed by upgrading to 2.14.0+",,jlisam,,,,,,,,,,,,,,,,,,,,KAFKA-14044,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-18 22:51:36.0,,,,,,,,,,"0|z19h48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProduceRequest timeouts are logged as network exceptions,KAFKA-14317,13486898,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,18/Oct/22 19:08,14/Mar/23 15:26,13/Jul/23 09:17,09/Mar/23 09:04,3.3.0,,,,,,,,,,,,,,,3.5.0,,,,,,,clients,logging,producer ,,,,0,,,,,,"In {{{}NetworkClient.handleTimedOutRequests{}}}, we disconnect the broker connection:
{code:java}
private void handleTimedOutRequests(List<ClientResponse> responses, long now) {
    List<String> nodeIds = this.inFlightRequests.nodesWithTimedOutRequests(now);
    for (String nodeId : nodeIds) {
        // close connection to the node
        this.selector.close(nodeId);
        log.info(""Disconnecting from node {} due to request timeout."", nodeId);
        processDisconnection(responses, nodeId, now, ChannelState.LOCAL_CLOSE);
    }
}
{code}
This calls {{processDisconnection}} which calls {{{}cancelInFlightRequests{}}}:
{code:java}
for (InFlightRequest request : inFlightRequests) {
    if (log.isDebugEnabled()) {
        log.debug(""Cancelled in-flight {} request with correlation id {} due to node {} being disconnected "" +
                ""(elapsed time since creation: {}ms, elapsed time since send: {}ms, request timeout: {}ms): {}"",
            request.header.apiKey(), request.header.correlationId(), nodeId,
            request.timeElapsedSinceCreateMs(now), request.timeElapsedSinceSendMs(now),
            request.requestTimeoutMs, request.request);
    } else {
        log.info(""Cancelled in-flight {} request with correlation id {} due to node {} being disconnected "" +
                ""(elapsed time since creation: {}ms, elapsed time since send: {}ms, request timeout: {}ms)"",
            request.header.apiKey(), request.header.correlationId(), nodeId,
            request.timeElapsedSinceCreateMs(now), request.timeElapsedSinceSendMs(now),
            request.requestTimeoutMs);
    }

    if (!request.isInternalRequest) {
        if (responses != null)
            responses.add(request.disconnected(now, null));
    } else if (request.header.apiKey() == ApiKeys.METADATA) {
        metadataUpdater.handleFailedRequest(now, Optional.empty());
    }
}
{code}
We create a new {{ClientResponse}} in which the {{disconnected}} flag is set.

We then complete the record batch In {{Sender.handleProduceResponse}} with:
{code:java}
if (response.wasDisconnected()) {
    log.trace(""Cancelled request with header {} due to node {} being disconnected"",
    requestHeader, response.destination());
    for (ProducerBatch batch : batches.values())
        completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION, String.format(""Disconnected from node %s"", response.destination())),
    correlationId, now);
}
{code}
This seems like it could be confusing for customers that they would see network exceptions on a request timeout instead of a timeout error.

One implication of completing the batch with a network exception is that the producer will try to refresh metadata after a request timeout. I can see arguments for why this is necessary.",,kirktrue,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,KAFKA-10228,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 15:35:57 UTC 2023,,,,,,,,,,"0|z19gzc:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"24/Oct/22 20:56;kirktrue;This looks related to KAFKA-10228, but that Jira is still open and seems to suggest only a logging change.

I _believe_ we want to change the behavior to complete the batch using a different {{Errors}} type.;;;","07/Feb/23 15:35;kirktrue;[~dajac] could you take another look at the PR? Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElementException in feature control iterator,KAFKA-14316,13486891,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,18/Oct/22 18:08,18/Oct/22 23:35,13/Jul/23 09:17,18/Oct/22 23:35,3.3.0,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"We noticed this exception during testing:
{code:java}
java.util.NoSuchElementException
2	at org.apache.kafka.timeline.SnapshottableHashTable$HistoricalIterator.next(SnapshottableHashTable.java:276)
3	at org.apache.kafka.timeline.SnapshottableHashTable$HistoricalIterator.next(SnapshottableHashTable.java:189)
4	at org.apache.kafka.timeline.TimelineHashMap$EntryIterator.next(TimelineHashMap.java:360)
5	at org.apache.kafka.timeline.TimelineHashMap$EntryIterator.next(TimelineHashMap.java:346)
6	at org.apache.kafka.controller.FeatureControlManager$FeatureControlIterator.next(FeatureControlManager.java:375)
7	at org.apache.kafka.controller.FeatureControlManager$FeatureControlIterator.next(FeatureControlManager.java:347)
8	at org.apache.kafka.controller.SnapshotGenerator.generateBatch(SnapshotGenerator.java:109)
9	at org.apache.kafka.controller.SnapshotGenerator.generateBatches(SnapshotGenerator.java:126)
10	at org.apache.kafka.controller.QuorumController$SnapshotGeneratorManager.run(QuorumController.java:637)
11	at org.apache.kafka.controller.QuorumController$ControlEvent.run(QuorumController.java:539)
12	at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
13	at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
14	at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
15	at java.base/java.lang.Thread.run(Thread.java:833)
16	at org.apache.kafka.common.utils.KafkaThread.run(KafkaThread.java:64) {code}
The iterator `FeatureControlIterator.hasNext()` checks two conditions: 1) whether we have already written the metadata version, and 2) whether the underlying iterator has additional records. However, in `next()`, we also check that the metadata version is at least high enough to include it in the log. When this fails, then we can see an unexpected `NoSuchElementException` if the underlying iterator is empty.

 ",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-18 18:08:55.0,,,,,,,,,,"0|z19gxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorSourceConnector throwing NPE during `isCycle` check,KAFKA-14314,13486824,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,johnk,johnk,johnk,18/Oct/22 11:20,28/Oct/22 15:28,13/Jul/23 09:17,28/Oct/22 15:28,3.3.1,,,,,,,,,,,,,,,3.4.0,,,,,,,mirrormaker,,,,,,0,,,,,,"We are using MirrorMaker to replicate topics across clusters in AWS. As the process is starting up, we are getting a NullPointerException when MirrorSourceConnector is calling `isCycle`.

Retrieving the `upstreamTopic` on [this line of code|https://github.com/apache/kafka/blob/cc582897bfb237572131369a598f7869220b43dc/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java#L497] is returning null, which causes the NPE on the next line. ",,johnk,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 14:23:37 UTC 2022,,,,,,,,,,"0|z19giw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 12:26;mimaison;Which replication policy are you using?;;;","18/Oct/22 13:17;johnk;We're using a custom replication policy. Here are the contents of the mm2-msc.json file. Is this what you're asking for? I'm new to this so I'm unsure exactly what all constitutes the policy.

 
{code:java}
{
  ""name"": ""mm2-msc"",
  ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
  ""replication.policy.class"": ""com.amazonaws.kafka.samples.CustomMM2ReplicationPolicy"",
  ""clusters"": ""prim, sec"",
  ""source.cluster.alias"": ""prim"",
  ""source.cluster.bootstrap.servers"": ""PRIMARY_BOOTSTRAP_URL"",
  ""source.cluster.security.protocol"": ""SSL"",
  ""source.cluster.ssl.truststore.location"": ""/tmp/kafka.client.truststore.jks"",
  ""source.cluster.ssl.truststore.password"": ""changeit"",
  ""target.cluster.alias"": ""sec"",
  ""target.cluster.bootstrap.servers"": ""SECONDARY_BOOTSTRAP_URL"",
  ""target.cluster.security.protocol"": ""SSL"",
  ""target.cluster.ssl.truststore.location"": ""/tmp/kafka.client.truststore.jks"",
  ""target.cluster.ssl.truststore.password"": ""changeit"",
  ""topics"": "".*"",
  ""tasks.max"": ""4"",
  ""key.converter"": "" org.apache.kafka.connect.converters.ByteArrayConverter"",
  ""value.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter"",
  ""replication.factor"": ""3"",
  ""offset-syncs.topic.replication.factor"": ""1"",
  ""sync.topic.acls.interval.seconds"": ""10"",
  ""sync.topic.configs.interval.seconds"": ""10"",
  ""refresh.topics.interval.seconds"": ""10"",
  ""refresh.groups.interval.seconds"": ""10"",
  ""consumer.group.id"": ""mm2-msc"",
  ""producer.enable.idempotence"": ""true""
}{code}
 ;;;","18/Oct/22 13:29;mimaison;Thanks for the details.

The ReplicationPolicy [javadoc|https://kafka.apache.org/33/javadoc/org/apache/kafka/connect/mirror/ReplicationPolicy.html#upstreamTopic-java.lang.String-] states that upstreamTopic can return null to indicate a topic is not remote. So this is a bug in MirrorSourceConnector.isCycle(), null should be handled correctly.

Are you interested in submitting a pull request to fix this small issue?

 ;;;","18/Oct/22 13:35;johnk;Thanks, Mickael. Sure I can do that. I thought it was a bug because it seems that `null` is a legitimate value here. We might have something misconfigured but even in that case it shouldn't blow up so I'll submit a PR a bit later today.;;;","18/Oct/22 13:48;mimaison;Great, I'll assign this ticket to you then. Thanks!;;;","19/Oct/22 14:23;johnk;I'm not sure exactly how your project manages these tickets but I submitted [PR 12769|https://github.com/apache/kafka/pull/12769] this morning and mentioned this ticket number in the comment. It's still building.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect Worker clean shutdown does not cleanly stop connectors/tasks,KAFKA-14311,13486724,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sagarrao,gharris1727,gharris1727,17/Oct/22 23:51,23/Jan/23 19:16,13/Jul/23 09:17,23/Jan/23 19:16,3.3.1,,,,,,,,,,,,,,,3.5.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"When the DistributedHerder::stop() method called, it triggers asynchronous shutdown of the background herder thread, and continues with synchronous shutdown of some other resources, including the stopAndStartExecutor.

This executor is responsible for cleanly stopping connectors and tasks, which it  the DistributedHerder::halt() method. There is a race condition between the halt() method asynchronously submitting these connector/task stop jobs and the stop() method terminating the executor. If the executor is terminated first, this exception appears:
{noformat}
[2022-10-17 16:29:23,396] ERROR [Worker clientId=connect-2, groupId=connect-integration-test-connect-cluster-1] Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:366)
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@62878e25[Not completed, task = org.apache.kafka.connect.runtime.distributed.DistributedHerder$$Lambda$2285/0x00000008015046a8@58deade3] rejected from java.util.concurrent.ThreadPoolExecutor@10351ac3[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1]
    at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)
    at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)
    at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)
    at java.base/java.util.concurrent.AbstractExecutorService.invokeAll(AbstractExecutorService.java:247)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startAndStop(DistributedHerder.java:1667)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.halt(DistributedHerder.java:765)
    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:361)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:833){noformat}",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-17 23:51:57.0,,,,,,,,,,"0|z19fwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams upgrade tests do not cover for FK-joins,KAFKA-14309,13486693,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lbrutschy,lbrutschy,lbrutschy,17/Oct/22 19:58,02/Dec/22 10:18,13/Jul/23 09:17,15/Nov/22 19:29,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,,0,,,,,,"The current streams upgrade system test for FK joins inserts the production of foreign key data and an actual foreign key join in every version of SmokeTestDriver except for the latest. The effect was that FK join upgrades are not tested at all, since no FK join code is executed after the bounce in the system test.

We should enable the FK-join code in the system test.",,lbrutschy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-17 19:58:10.0,,,,,,,,,,"0|z19fps:",9223372036854775807,,Gerrrr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer.send without record key and batch.size=0 goes into infinite loop,KAFKA-14303,13486307,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,soarez,soarez,soarez,14/Oct/22 10:24,27/Dec/22 01:32,13/Jul/23 09:17,28/Nov/22 19:25,3.3.0,3.3.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,clients,producer ,,,,,0,bug,client,Partitioner,producer,regresion,"3.3 has broken previous producer behavior.

A call to {{producer.send(record)}} with a record without a key and configured with {{batch.size=0}} never returns.

Reproducer:
{code:java}
class ProducerIssueTest extends IntegrationTestHarness {
  override protected def brokerCount = 1
  @Test
  def testProduceWithBatchSizeZeroAndNoRecordKey(): Unit = {
    val topicName = ""foo""
    createTopic(topicName)
    val overrides = new Properties
    overrides.put(ProducerConfig.BATCH_SIZE_CONFIG, 0)
    val producer = createProducer(keySerializer = new StringSerializer, valueSerializer = new StringSerializer, overrides)
    val record = new ProducerRecord[String, String](topicName, null, ""hello"")
    val future = producer.send(record) // goes into infinite loop here
    future.get(10, TimeUnit.SECONDS)
  }
} {code}
 

[Documentation for producer configuration|https://kafka.apache.org/documentation/#producerconfigs_batch.size] states {{batch.size=0}} as a valid value:
{quote}Valid Values: [0,...]
{quote}
and recommends its use directly:
{quote}A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely).
{quote}",,jeqo,junrao,kirktrue,showuon,soarez,,,,,,,,,,,,,,,,,,,,,,KAFKA-14553,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 28 19:25:14 UTC 2022,,,,,,,,,,"0|z19dc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 11:54;soarez;The issue can be more specifically replicated with a unit test on {{{}RecordAccumulator{}}}, adding this test to {{RecordAccumulatorTest}}
{code:java}
@Test
public void testBatchSizeZero() throws Exception {
    int batchSize = 0;
    long totalSize = 1024 * 1024;
    RecordAccumulator accum = createTestRecordAccumulator(batchSize, totalSize, CompressionType.NONE, 10);
    accum.append(topic, RecordMetadata.UNKNOWN_PARTITION, 0, key, new byte[32], Record.EMPTY_HEADERS,
            null, maxBlockTimeMs, false, time.milliseconds(), cluster);
}
{code};;;","14/Oct/22 12:31;showuon;[~soarez] , thanks for reporting the issue. Looks like you've done some investigation, are you willing to submit a PR to fix it?

 ;;;","14/Oct/22 12:51;soarez; [~showuon] Yes, I'm currently investigating and hoping to come up with a PR;;;","28/Nov/22 19:25;junrao;Merged the PR to 3.3 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft controller snapshot not trigger after resign,KAFKA-14300,13486198,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jsancio,jsancio,jsancio,13/Oct/22 19:52,18/Oct/22 22:35,13/Jul/23 09:17,18/Oct/22 22:35,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"When the active KRaft controller resigns it resets the newBytesSinceLastSnapshot field to zero. This is not accurate since it is not guarantee that there is an on disk snapshot at the committed offset.

 

Since when the active controller resign it always reverts to the last committed state then it doesn't reset the newBytesSinceLastSnapshot. newBytesSinceLastSnapshot always represent committed bytes read since the last snapshot.",,jsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-13 19:52:44.0,,,,,,,,,,"0|z19cns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition leaders are not demoted during kraft controlled shutdown,KAFKA-14296,13486059,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,13/Oct/22 07:40,14/Oct/22 00:10,13/Jul/23 09:17,14/Oct/22 00:10,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"When the BrokerServer starts its shutting down process, it transitions to SHUTTING_DOWN and sets isShuttingDown to true. With this state change, the follower state changes are short-cutted. This means that a broker which was serving as leader would remain acting as a leader until controlled shutdown completes. Instead, we want the leader and ISR state to be updated so that requests will return NOT_LEADER and the client can find the new leader.",,dajac,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-13 07:40:33.0,,,,,,,,,,"0|z19bsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchMessageConversionsPerSec meter not recorded,KAFKA-14295,13485969,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,david.mao,david.mao,12/Oct/22 21:08,26/Feb/23 07:23,13/Jul/23 09:17,23/Feb/23 17:51,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,,,,,,,0,,,,,,"The broker topic metric FetchMessageConversionsPerSec doesn't get recorded on a fetch message conversion.

The bug is that we pass in a callback that expects a MultiRecordsSend in KafkaApis:
{code:java}
def updateConversionStats(send: Send): Unit = {
  send match {
    case send: MultiRecordsSend if send.recordConversionStats != null =>
      send.recordConversionStats.asScala.toMap.foreach {
        case (tp, stats) => updateRecordConversionStats(request, tp, stats)
      }
    case _ =>
  }
} {code}
But we call this callback with a NetworkSend in the SocketServer:
{code:java}
selector.completedSends.forEach { send =>
  try {
    val response = inflightResponses.remove(send.destinationId).getOrElse {
      throw new IllegalStateException(s""Send for ${send.destinationId} completed, but not in `inflightResponses`"")
    }
    updateRequestMetrics(response)

    // Invoke send completion callback
    response.onComplete.foreach(onComplete => onComplete(send))
...{code}
Note that Selector.completedSends returns a collection of NetworkSend",,chia7712,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14743,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 18 20:58:08 UTC 2023,,,,,,,,,,"0|z19b8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/23 20:58;chia7712;I encountered this issue too. [~david.mao] Are you going to file PR?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft broker controlled shutdown can be delayed indefinitely,KAFKA-14292,13485766,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,alyssahuang,hachikuji,hachikuji,12/Oct/22 00:16,13/Oct/22 20:33,13/Jul/23 09:17,13/Oct/22 20:33,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,,,,,,,0,,,,,,"We noticed when rolling a kraft cluster that it took an unexpectedly long time for one of the brokers to shutdown. In the logs, we saw the following:
{code:java}
Oct 11, 2022 @ 17:53:38.277	[Controller 1] The request from broker 8 to shut down can not yet be granted because the lowest active offset 2283357 is not greater than the broker's shutdown offset 2283358.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
2Oct 11, 2022 @ 17:53:38.277	[Controller 1] Updated the controlled shutdown offset for broker 8 to 2283362.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
3Oct 11, 2022 @ 17:53:40.278	[Controller 1] Updated the controlled shutdown offset for broker 8 to 2283366.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
4Oct 11, 2022 @ 17:53:40.278	[Controller 1] The request from broker 8 to shut down can not yet be granted because the lowest active offset 2283361 is not greater than the broker's shutdown offset 2283362.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
5Oct 11, 2022 @ 17:53:42.279	[Controller 1] The request from broker 8 to shut down can not yet be granted because the lowest active offset 2283365 is not greater than the broker's shutdown offset 2283366.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
6Oct 11, 2022 @ 17:53:42.279	[Controller 1] Updated the controlled shutdown offset for broker 8 to 2283370.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
7Oct 11, 2022 @ 17:53:44.280	[Controller 1] The request from broker 8 to shut down can not yet be granted because the lowest active offset 2283369 is not greater than the broker's shutdown offset 2283370.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	
8Oct 11, 2022 @ 17:53:44.281	[Controller 1] Updated the controlled shutdown offset for broker 8 to 2283374.	org.apache.kafka.controller.BrokerHeartbeatManager	DEBUG	 {code}
From what I can tell, it looks like the controller waits until all brokers have caught up to the {{controlledShutdownOffset}} of the broker that is shutting down before allowing it to proceed. Probably the intent is to make sure they have all the leader and ISR state.

The problem is that the {{controlledShutdownOffset}} seems to be updated after every heartbeat that the controller receives: https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java#L1996. Unless all other brokers can catch up to that offset before the next heartbeat from the shutting down broker is received, then the broker remains in the shutting down state indefinitely.

In this case, it took more than 40 minutes before the broker completed shutdown:
{code:java}
1Oct 11, 2022 @ 18:36:36.105	[Controller 1] The request from broker 8 to shut down has been granted since the lowest active offset 2288510 is now greater than the broker's controlled shutdown offset 2288510.	org.apache.kafka.controller.BrokerHeartbeatManager	INFO	
2Oct 11, 2022 @ 18:40:35.197	[Controller 1] The request from broker 8 to unfence has been granted because it has caught up with the offset of it's register broker record 2288906.	org.apache.kafka.controller.BrokerHeartbeatManager	INFO{code}
It seems like the bug here is that we should not keep updating {{controlledShutdownOffset}} if it has already been set.",,dengziming,hachikuji,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-12 00:16:04.0,,,,,,,,,,"0|z19a08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft: ApiVersionsResponse doesn't have finalizedFeatures and finalizedFeatureEpoch in KRaft mode,KAFKA-14291,13485765,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dengziming,akhileshchg,akhileshchg,12/Oct/22 00:14,08/Jun/23 21:18,13/Jul/23 09:17,12/May/23 05:48,,,,,,,,,,,,,,,,,,,,,,,kraft,,,,,,0,,,,,,"https://github.com/apache/kafka/blob/d834947ae7abc8a9421d741e742200bb36f51fb3/core/src/main/scala/kafka/server/ApiVersionManager.scala#L53
```
class SimpleApiVersionManager(
  val listenerType: ListenerType,
  val enabledApis: collection.Set[ApiKeys],
  brokerFeatures: Features[SupportedVersionRange]
) extends ApiVersionManager {

  def this(listenerType: ListenerType) = {
    this(listenerType, ApiKeys.apisForListener(listenerType).asScala, BrokerFeatures.defaultSupportedFeatures())
  }

  private val apiVersions = ApiVersionsResponse.collectApis(enabledApis.asJava)

  override def apiVersionResponse(requestThrottleMs: Int): ApiVersionsResponse = {
    ApiVersionsResponse.createApiVersionsResponse(requestThrottleMs, apiVersions, brokerFeatures)
  }
}

```

ApiVersionManager for KRaft doesn't add the finalizedFeatures and finalizedFeatureEpoch to the ApiVersionsResponse.",,ableegoldman,akhileshchg,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 02:46:12 UTC 2022,,,,,,,,,,"0|z19a00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 02:21;dengziming;Thank you [~akhileshchg] for reporting this.

This problem is similar to KAFKA-13990, I already found this problem, I also found it only occur in the kraft controllers and the brokers are not infected by this, so this problem doesn't have any impact currently since the finalizedFeatures is not used in kraft controller. ;;;","15/Dec/22 02:46;ableegoldman;Marking this as a duplicate of KAFKA-13990 as described above;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix connector creation authorization tests not doing anything,KAFKA-14283,13484863,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mdedetrich,mdedetrich,,06/Oct/22 20:31,05/Feb/23 02:45,13/Jul/23 09:17,07/Oct/22 17:18,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,,Currently the testCreateConnectorWithoutHeaderAuthorization and testCreateConnectorWithHeaderAuthorization tests within ConnectorsResourceTest aren't actually doing anything. This is because in reality the requests should be forwarded to a leader and tests aren't actually testing that a leader RestClient request is made,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-06 20:31:32.0,,,,,,,,,,"0|z194hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecordCollector throws exception on message processing,KAFKA-14282,13484725,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,SebastianBruckner,SebastianBruckner,06/Oct/22 09:21,12/Nov/22 02:22,13/Jul/23 09:17,12/Nov/22 01:59,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,streams,,,,,,1,,,,,,"Since we upgrade from version 3.2.0 to 3.3.1 we see a lot of exceptions thrown by the RecordCollector
{code:java}
stream-thread [XXX-StreamThread-1] task [2_8] Unable to records bytes produced to topic XXX by sink node KSTREAM-SINK-0000000033 as the node is not recognized.
Known sink nodes are [KSTREAM-SINK-0000000057, XXX-joined-fk-subscription-registration-sink]. 
{code}
Restarting the application did not help.

I think this is related to [KIP-846|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=211886093] which was introduced in 3.3.0 with the ticket https://issues.apache.org/jira/browse/KAFKA-13945 .",,ableegoldman,andkir,DannyNullZwo,jeqo,joschi,mjsax,ramiz.mehran,SebastianBruckner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 02:57:13 UTC 2022,,,,,,,,,,"0|z193n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 10:29;DannyNullZwo;We are seeing the same problem. We are disabling the *org.apache.kafka.streams.processor.internals.RecordCollectorImpl* logger for the time beeing.;;;","07/Nov/22 08:24;ableegoldman;Thanks for the report. I'll try to look into this, do you have any application logs from around the time of these exceptions that you could upload here?;;;","08/Nov/22 05:21;ramiz.mehran;I am also facing the same. Adding logs below:



<app_name> <server-ip> 07-11-2022 19:00:31.602 [,] ERROR org.apache.kafka.streams.processor.internals.RecordCollectorImpl: -  stream-thread [<client-id>-StreamThread-8] task [0_136] Unable to records bytes produced to topic <topic-name> by sink node KSTREAM-SINK-0000000009 as the node is not recognized.

Known sink nodes are [].

<app_name> <server-ip> 07-11-2022 19:00:31.602 [,] ERROR org.apache.kafka.streams.processor.internals.RecordCollectorImpl: -  stream-thread [<client-id>-StreamThread-8] task [0_136] Unable to records bytes produced to topic <topic-name> by sink node KSTREAM-SINK-0000000009 as the node is not recognized.

Known sink nodes are [].

<app_name> <server-ip> 07-11-2022 19:00:31.602 [,] ERROR org.apache.kafka.streams.processor.internals.RecordCollectorImpl: -  stream-thread [<client-id>-StreamThread-8] task [0_136] Unable to records bytes produced to topic <topic-name> by sink node KSTREAM-SINK-0000000009 as the node is not recognized.

Known sink nodes are [].;;;","09/Nov/22 02:44;ableegoldman;Oh wait I think I know what's going on here. Super dumb bug, I'll try to put out a fix shortly. Sorry for the inconvenience everyone!;;;","09/Nov/22 02:57;ableegoldman;Here we go: [https://github.com/apache/kafka/pull/12836];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft Controllers should crash after failing to apply any metadata record ,KAFKA-14275,13484330,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Niket Goel,Niket Goel,Niket Goel,03/Oct/22 21:36,13/Oct/22 20:34,13/Jul/23 09:17,11/Oct/22 16:56,3.3.1,,,,,,,,,,,,,,,3.3,,,,,,,kraft,,,,,,0,,,,,,"When replaying records on a standby controller, any error encountered will halt further processing of that batch. Currently we log an error and allow the controller to continue normal operation. In contrast a similar error on the active controller causes it to halt and exit the jvm. This is inconsistent behavior as nothing prevents a standby from eventually becoming the active controller (even when it had skipped over a record batch). We should halt the process in the case of a standby controller as well.",,Niket Goel,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 03 22:30:26 UTC 2022,,,,,,,,,,"0|z1917c:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"03/Oct/22 22:30;Niket Goel;PR – https://github.com/apache/kafka/pull/12709;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams logs exception on startup,KAFKA-14270,13483992,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,eikemeier,eikemeier,eikemeier,30/Sep/22 09:54,04/Oct/22 10:03,13/Jul/23 09:17,04/Oct/22 10:00,3.3.0,,,,,,,,,,,,,,,3.4.0,,,,,,,streams,,,,,,0,,,,,,"Kafka Streams expects a version resource at /kafka/kafka-streams-version.properties. It is read by {{{}ClientMetrics{}}}, initialised by

[https://github.com/apache/kafka/blob/3.3.0/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L894]

When the resource is not found,

[https://github.com/apache/kafka/blob/3.3.0/streams/src/main/java/org/apache/kafka/streams/internals/metrics/ClientMetrics.java#L55]

logs a warning at startup:
org.apache.kafka.streams.internals.metrics.ClientMetrics <clinit> WARN: Error while loading kafka-streams-version.properties
java.lang.NullPointerException: inStream parameter is null
	at java.base/java.util.Objects.requireNonNull(Objects.java:233)
	at java.base/java.util.Properties.load(Properties.java:407)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:894)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:856)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:826)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:738)",,cadonna,eikemeier,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/apache/kafka/pull/12700,,,,,,,,,,,9223372036854775807,,,groovy,Tue Oct 04 10:03:54 UTC 2022,,,,,,,,,,"0|z18z4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/22 03:54;guozhang;Thanks for filing the bug [~eikemeier], will take a look.

From your description, it seems whenever Kafka Streams is started, with whatever integration tooling besides groovy, it will always log a warning?;;;","04/Oct/22 09:09;eikemeier;Yes. Sorry about the “Groovy” tag, the Gradle build script is written in Groovy, so this fix is in Groovy code.;;;","04/Oct/22 10:03;cadonna;[~eikemeier] Thanks for the fix!
I tested your code manually and merged your PR.

I also added you o the contributors group in Jira so that I could assign this ticket to you.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prefix ACLs may shadow other prefix ACLs,KAFKA-14265,13483736,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,29/Sep/22 00:01,30/Sep/22 14:02,13/Jul/23 09:17,29/Sep/22 18:20,3.3.0,,,,,,,,,,,,,,,3.3.1,,,,,,,,,,,,,0,,,,,,"Prefix ACLs may shadow other prefix ACLs. Consider the case where we have prefix ACLs for foobar, fooa, and f. If we were matching a resource named ""foobar"", we'd start scanning at the foobar ACL, hit the fooa ACL, and stop -- missing the f ACL.

To fix this, we should re-scan for ACLs at the first divergence point (in this case, f) whenever we hit a mismatch of this kind.",,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-29 00:01:53.0,,,,,,,,,,"0|z18xkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryKeyValueStore iterator still throws ConcurrentModificationException,KAFKA-14260,13482990,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Cerchie,aviperksy,aviperksy,24/Sep/22 00:12,10/Dec/22 03:35,13/Jul/23 09:17,07/Dec/22 03:42,2.3.1,3.2.3,,,,,,,,,,,,,,3.4.0,,,,,,,streams,,,,,,0,,,,,,"This is the same bug as KAFKA-7912 which was then re-introduced by KAFKA-8802.

Any iterator returned from {{InMemoryKeyValueStore}} may end up throwing a ConcurrentModificationException because the backing map is not concurrent safe. I expect that this only happens when the store is retrieved from {{KafkaStreams.store()}} from outside of the topology since any usage of the store from inside of the topology should be naturally single-threaded.

To start off, a reminder that this behaviour explicitly violates the interface contract for {{ReadOnlyKeyValueStore}} which states
{quote}The returned iterator must be safe from java.util.ConcurrentModificationExceptions
{quote}
It is often complicated to make code to demonstrate concurrency bugs, but thankfully it is trivial to reason through the source code in {{InMemoryKeyValueStore.java}} to show why this happens:
 * All of the InMemoryKeyValueStore methods that return iterators do so by passing a keySet based on the backing TreeMap to the InMemoryKeyValueIterator constructor.
 * These keySets are all VIEWS of the backing map, not copies.
 * The InMemoryKeyValueIterator then makes a private copy of the keySet by passing the original keySet into the constructor for TreeSet. This copying was implemented in KAFKA-8802, incorrectly intending it to fix the concurrency problem.
 * TreeSet then iterates over the keySet to make a copy. If the original backing TreeMap in InMemoryKeyValueStore is changed while this copy is being created it will fail-fast a ConcurrentModificationException.

This bug should be able to be trivially fixed by replacing the backing TreeMap with a ConcurrentSkipListMap but here's the rub:

This bug has already been found in KAFKA-7912 and the TreeMap was replaced with a ConcurrentSkipListMap. It was then reverted back to a TreeMap in KAFKA-8802 because of the performance regression. I can [see from one of the PRs|https://github.com/apache/kafka/pull/7212/commits/384c12e40f3a59591f897d916f92253e126820ed] that it was believed the concurrency problem with the TreeMap implementation was fixed by copying the keyset when the iterator is created but the problem remains, plus the fix creates an extra copy of the iterated portion of the set in memory.

For what it's worth, the performance difference between TreeMap and ConcurrentSkipListMap do not extend into complexity. TreeMap enjoys a similar ~2x speed through all operations with any size of data, but at the cost of what turned out to be an easy-to-encounter bug.

This is all unfortunate since the only time the state stores ever get accessed concurrently is through the `KafkaStreams.store()` mechanism, but I would imagine that ""correct and slightly slower) is better than ""incorrect and faster"".

Too bad BoilerBay's AirConcurrentMap is closed-source and patented.",,ableegoldman,aviperksy,Cerchie,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 10 03:35:28 UTC 2022,,,,,,,,,,"0|z18t0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 22:32;guozhang;Hello [~aviperksy] just checking is https://github.com/apache/kafka/pull/11367 related?;;;","26/Sep/22 22:43;aviperksy;[~guozhang] as far as I can see, your PR with the ""copyOnRange"" flag set does not manage to avoid the concurrency problem. InMemoryKeyValueStore.java line 125 still creates the iterator using the original map, and if that original map is a TreeMap then the iteration over the TreeMap when you're copying it/(or parts of it) will eventually result in a ConcurrentModificationException if the map is modified during the iteration. When the flag is not set, your PR uses a ConcurrentSkipListMap which is immune to ConcurrentModificationExceptions.;;;","03/Oct/22 03:26;guozhang;Hello [~aviperksy] sorry for the late reply! I looked at the code again and I think I agree with you --- we are probably looking at different versions of the source code since in latest trunk, line 125 seems irrelevant (https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java#L125) --- but the thing is that at the time when this line was called:

{code}
if (forward) {
                this.iter = new TreeSet<>(keySet).iterator();
            } else {
                this.iter = new TreeSet<>(keySet).descendingIterator();
            }
{code}

in which the constructor of {{TreeSet}} loops over the {{keySet}}, if that {{keySet}}'s underlying map is modified then we will still have an issue. As for the fix, I think in the near term we'd have to bite the bullet of performance can turn back to ConcurrentSkipListMap (we may tune some initial params e.g. using concurrencyLevel of 1). In the long term, I think we could leverage on similar ideas we are pursuing for transactional state stores, where we keep two in-memory maps, and the first map is read-only for IQ, and second is used to maintain deltas within a commit interval and during processing, we'd need to read both maps, and upon committing we lock the first one to apply the deltas.

cc [~ableegoldman] what do you think?;;;","11/Nov/22 06:07;ableegoldman;Seems like we should just make sure to synchronize when making an iterator/copying the key set, right? Actually it looks like most of the methods already are but I notice that the new #prefixScan for example is not. [~aviperksy]  were you using that API when you saw this ConcurrentModificationException?

It would really be a bummer if we had to go back to the ConcurrentSkipList, it just could not keep up with the basic TreeSet especially for larger stores :/ Of course if we have to then we have to, but we should make sure to exhaust the alternatives before we settle on that;;;","14/Nov/22 14:57;Cerchie;Getting to work adding the missing `sychronized`. ;;;","07/Dec/22 04:13;ableegoldman;Ok I did merge a patch to fix where we forgot to synchronize, which is certainly a bug leading to potential CME, but I realize that's not what this ticket was about so I want to explain why I resolved it: ie that synchronization is sufficient for avoiding CMEs. I do think you pointed out something of note here, though, which is worth following up on though perhaps tracking separately.

In the IMKVIterator constructor from [~guozhang]'s snippet above, it's true we get an iterator based on the original map, but it's still just a copy of that map: so this iterator doesn't pin any part of the original map and just happily returns the set of keys that were in the original map when the range API was invoked. There's no way to modify the contents of this copy as it's internal to the (also internal) iterator, and even if you delete a record with a given key in that store, the actual key object itself still exists (and can/will still be returned by that iterator)

So I really don't see how a CME is possible if we properly synchronize the APIs to enforce single-threaded access while that copy is being made. Which we do (now, since merging [~Cerchie] 's PR)

That said, it still feels a bit awkward because the keyset-copy iterator can return keys that no longer exist in the actual store. In this case when we issue a get on that key it'll return null, and the range read will have an entry with a null value. Technically Streams makes no guarantees about whether a range scan will reflect only the original state store contents or only the latest contents or anything in between, and I'm not sure there's even a ""right"" answer there.

Still, returning a KeyValue(""key1"", null) is still pretty awkward and likely unexpected by most users, so I _can_ this resulting in an NPE. Fortunately that's a much easier fix, as we can just toss out that result and return whatever is next. I think it's worth filing a separate ticket for that one, though

[~aviperksy] thoughts? Did I miss something obvious here? Also note that the code has changed a lot over the years, so it's possible what you described does affect some older branch(es);;;","10/Dec/22 03:35;ableegoldman;Well, actually, it turns out we are breaking a public contract here because I just happened to notice that we do in fact assert that null values shouldn't be returned. So I filed https://issues.apache.org/jira/browse/KAFKA-14460;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BrokerRegistration#toString throws an exception, terminating metadata replay",KAFKA-14259,13482982,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,23/Sep/22 19:29,29/Sep/22 18:17,13/Jul/23 09:17,28/Sep/22 20:40,3.3.0,,,,,,,,,,,,,,,3.3.1,,,,,,,,,,,,,0,,,,,,"BrokerRegistration#toString throws an exception, terminating metadata replay, because the sorted() method is used on an entry set rather than a key set.


{noformat}
        Caused by:                                                                                                                                                                           
        java.util.concurrent.ExecutionException: java.lang.ClassCastException: class java.util.HashMap$Node cannot be cast to class java.lang.Comparable (java.util.HashMap$Node and java.lan
g.Comparable are in module java.base of loader 'bootstrap')                                                                                                                                  
            at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)                                                                                        
            at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)                                                                                             
            at kafka.server.BrokerServer.startup(BrokerServer.scala:846)                                                                                                                     
            ... 147 more                                                                                                                                                                     
                                                                                                                                                                                             
            Caused by:                                                                                                                                                                       
            java.lang.ClassCastException: class java.util.HashMap$Node cannot be cast to class java.lang.Comparable (java.util.HashMap$Node and java.lang.Comparable are in module java.base 
of loader 'bootstrap')                                                                                                                                                                       
                at java.base/java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47)                                                                                       
                at java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)                                                                                                    
                at java.base/java.util.TimSort.sort(TimSort.java:220)                                                                                                                        
                at java.base/java.util.Arrays.sort(Arrays.java:1307)                                                                                                                         
                at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:353)                                                                                          
                at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)                                                                                           
                at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)                                                                                    
                at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921)                                                                                      
                at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)                                                                                           
                at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)                                                                                          
                at org.apache.kafka.metadata.BrokerRegistration.toString(BrokerRegistration.java:228)                                                                                        
                at java.base/java.util.Formatter$FormatSpecifier.printString(Formatter.java:3056)                                                                                            
                at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2933)                                                                                                  
                at java.base/java.util.Formatter.format(Formatter.java:2689)                                                                                                                 
                at java.base/java.util.Formatter.format(Formatter.java:2625)                                                                                                                 
                at java.base/java.lang.String.format(String.java:4143)
                at java.base/java.util.Optional.toString(Optional.java:457)
                at java.base/java.lang.String.valueOf(String.java:4218)
                at java.base/java.lang.StringBuilder.append(StringBuilder.java:173)
                at java.base/java.util.AbstractMap.toString(AbstractMap.java:555)
                at java.base/java.lang.String.valueOf(String.java:4218)
                at java.base/java.lang.StringBuilder.append(StringBuilder.java:173)
                at org.apache.kafka.image.ClusterDelta.toString(ClusterDelta.java:225)
{noformat}
",,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-23 19:29:48.0,,,,,,,,,,"0|z18syw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temporarily disable unsafe downgrade,KAFKA-14243,13482247,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,19/Sep/22 23:55,20/Sep/22 21:36,13/Jul/23 09:17,20/Sep/22 21:36,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,Temporarily disable unsafe downgrade since we haven't implemented reloading snapshots on unsafe downgrade,,cmccabe,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-19 23:55:12.0,,,,,,,,,,"0|z18oi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft replicas can delete segments not included in a snapshot,KAFKA-14238,13481909,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,16/Sep/22 15:59,17/Sep/22 13:44,13/Jul/23 09:17,17/Sep/22 13:44,,,,,,,,,,,,,,,,3.3.0,,,,,,,core,kraft,,,,,0,,,,,,"We see this in the log
{code:java}
Deleting segment LogSegment(baseOffset=243864, size=9269150, lastModifiedTime=1662486784182, largestRecordTimestamp=Some(1662486784160)) due to retention time 604800000ms breach based on the largest record timestamp in the segment {code}
This then cause {{KafkaRaftClient}} to throw an exception when sending batches to the listener:
{code:java}
 java.lang.IllegalStateException: Snapshot expected since next offset of org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@195461949 is 0, log start offset is 369668 and high-watermark is 547379
	at org.apache.kafka.raft.KafkaRaftClient.lambda$updateListenersProgress$4(KafkaRaftClient.java:312)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at org.apache.kafka.raft.KafkaRaftClient.lambda$updateListenersProgress$5(KafkaRaftClient.java:311)
	at java.base/java.util.OptionalLong.ifPresent(OptionalLong.java:165)
	at org.apache.kafka.raft.KafkaRaftClient.updateListenersProgress(KafkaRaftClient.java:309){code}
The on disk state for the cluster metadata partition confirms this:
{code:java}
 ls __cluster_metadata-0/
00000000000000369668.index
00000000000000369668.log
00000000000000369668.timeindex
00000000000000503411.index
00000000000000503411.log
00000000000000503411.snapshot
00000000000000503411.timeindex
00000000000000548746.snapshot
leader-epoch-checkpoint
partition.metadata
quorum-state{code}
Noticed that there are no {{checkpoint}} files and the log doesn't have a segment at base offset 0.

This is happening because the {{LogConfig}} used for KRaft sets the retention policy to {{delete}} which causes the method {{deleteOldSegments}} to delete old segments even if there are no snaspshot for it. For KRaft, Kafka should only delete segment that breach the log start offset.

Log configuration for KRaft:
{code:java}
      val props = new Properties()
      props.put(LogConfig.MaxMessageBytesProp, config.maxBatchSizeInBytes.toString)
      props.put(LogConfig.SegmentBytesProp, Int.box(config.logSegmentBytes))
      props.put(LogConfig.SegmentMsProp, Long.box(config.logSegmentMillis))
      props.put(LogConfig.FileDeleteDelayMsProp, Int.box(Defaults.FileDeleteDelayMs))
      LogConfig.validateValues(props)
      val defaultLogConfig = LogConfig(props){code}
Segment deletion code:
{code:java}
     def deleteOldSegments(): Int = {
      if (config.delete) {
        deleteLogStartOffsetBreachedSegments() +
          deleteRetentionSizeBreachedSegments() +
          deleteRetentionMsBreachedSegments()
      } else {
        deleteLogStartOffsetBreachedSegments()
      }
    }{code}",,ijuma,jagsancio,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14241,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 22:50:38 UTC 2022,,,,,,,,,,"0|z18mfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 16:21;ijuma;Thanks for the JIRA. What's the proposed fix?;;;","16/Sep/22 16:53;jagsancio;It looks like delete policy needs to be set to either delete or compact:
{code:java}
        .define(CleanupPolicyProp, LIST, Defaults.CleanupPolicy, ValidList.in(LogConfig.Compact, LogConfig.Delete), MEDIUM, CompactDoc,
          KafkaConfig.LogCleanupPolicyProp)
{code}
Neither is correct for KRaft topics. KIP-630 talks about adding a third policy called snapshot:
{code:java}
The __cluster_metadata topic will have snapshot as the cleanup.policy. {code}
https://cwiki.apache.org/confluence/display/KAFKA/KIP-630%3A+Kafka+Raft+Snapshot#KIP630:KafkaRaftSnapshot-ProposedChanges;;;","16/Sep/22 21:43;jagsancio;Was able to write a test that fails with the current implementation:
{code:java}
> Task :core:test FAILED
kafka.raft.KafkaMetadataLogTest.testSegmentLessThanLatestSnapshot() failed, log available in /home/jsancio/work/kafka/core/build/reports/testOutput/kafka.raft.KafkaMetadataLogTest.testSegmentLessThanLatestSnapshot().test.stdoutKafkaMetadataLogTest > testSegmentNotDeleteWithoutSnapshot() FAILED
    org.opentest4j.AssertionFailedError: latest snapshot offset (1440) must be >= log start offset (20010) ==> expected: <true> but was: <false>
        at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
        at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
        at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
        at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
        at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:210)
        at kafka.raft.KafkaMetadataLogTest.testSegmentLessThanLatestSnapshot(KafkaMetadataLogTest.scala:921)
 {code};;;","16/Sep/22 22:50;jagsancio;PR: [https://github.com/apache/kafka/pull/12655]

Created an issue to implement the snapshot cleanup policy as a followup after 3.3.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ListGroups request produces too much Denied logs in authorizer,KAFKA-14236,13481861,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,agriffaut,agriffaut,agriffaut,16/Sep/22 10:12,14/Feb/23 14:28,13/Jul/23 09:17,22/Sep/22 00:43,2.0.0,,,,,,,,,,,,,,,3.4.0,,,,,,,core,,,,,,0,patch-available,,,,,"Context

On a multi-tenant secured cluster, with many consumers, a call to ListGroups api will log an authorization error for each consumer group of other tenant.

Reason

The handleListGroupsRequest function first tries to authorize a DESCRIBE CLUSTER, and if it fails it will then try to authorize a DESCRIBE GROUP on each consumer group.

Fix

In that case neither the DESCRIBE CLUSTER, nor the DESCRIBE GROUP of other tenant were intended, and should be specified in the Action using logIfDenied: false",,agriffaut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,2022-09-16 10:12:18.0,,,,,,,,,,"0|z18m4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lazy val exemptSensor Could Cause Deadlock,KAFKA-14225,13481361,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hshi,hshi,hshi,13/Sep/22 18:09,14/Feb/23 14:27,13/Jul/23 09:17,11/Oct/22 17:49,2.5.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,core,,,,,,0,,,,,,"There is a chance to cause deadlock when multiple threads access ClientRequestQuotaManager.

In the version of Scala 2.12, the lazy val initialization is under the object lock. The deadlock could happen in the following condition:

In thread a, when ClientRequestQuotaManager.exemptSensor is being initialized, it has acquired the object lock and enters the the actual initialization block. The initialization of 'exemptSensor' requires another lock private val lock = new ReentrantReadWriteLock() and it is waiting for this lock.

In thread b, at the same time, ClientQuotaManager.updateQuota() is called and it has already acquired ReentrantReadWriteLock lock by calling lock.writeLock().lock(). And then it executes info(). If this is the first time accessing Logging.logger, which is also a lazy val, it need to wait for the object lock.

Deadlock happens.

Since the lazy val initialization is under the object lock, we should avoid using lazy val if the initialization function holds another lock to prevent holding two locks at the same time which is prone for deadlock. ",,hshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 18:09:41.0,,,,,,,,,,"0|z18j2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exhausted BatchMemoryPool,KAFKA-14222,13481167,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,12/Sep/22 18:09,13/Sep/22 15:12,13/Jul/23 09:17,13/Sep/22 15:12,,,,,,,,,,,,,,,,3.3.0,,,,,,,kraft,,,,,,0,,,,,,"For a large number of topics and partition the broker can encounter this issue:
{code:java}
[2022-09-12 14:14:42,114] ERROR [BrokerMetadataSnapshotter id=4] Unexpected error handling CreateSnapshotEvent (kafka.server.metadata.BrokerMetadataSnapshotter)
org.apache.kafka.raft.errors.BufferAllocationException: Append failed because we failed to allocate memory to write the batch
        at org.apache.kafka.raft.internals.BatchAccumulator.append(BatchAccumulator.java:161)
        at org.apache.kafka.raft.internals.BatchAccumulator.append(BatchAccumulator.java:112)
        at org.apache.kafka.snapshot.RecordsSnapshotWriter.append(RecordsSnapshotWriter.java:167)
        at kafka.server.metadata.RecordListConsumer.accept(BrokerMetadataSnapshotter.scala:49)
        at kafka.server.metadata.RecordListConsumer.accept(BrokerMetadataSnapshotter.scala:42)
        at org.apache.kafka.image.TopicImage.write(TopicImage.java:78)
        at org.apache.kafka.image.TopicsImage.write(TopicsImage.java:79)
        at org.apache.kafka.image.MetadataImage.write(MetadataImage.java:129)
        at kafka.server.metadata.BrokerMetadataSnapshotter$CreateSnapshotEvent.run(BrokerMetadataSnapshotter.scala:116)
        at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
        at java.base/java.lang.Thread.run(Thread.java:829) {code}
This can happen because the snapshot is larger than {{{}5 * 8MB{}}}.",,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10652,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-12 18:09:37.0,,,,,,,,,,"0|z18hw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
app-reset-tool.html should remove reference to --zookeeper flag that no longer exists,KAFKA-14217,13480949,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,09/Sep/22 23:38,12/Sep/22 15:34,13/Jul/23 09:17,12/Sep/22 15:34,3.3,3.3.0,,,,,,,,,,,,,,3.3,,,,,,,docs,documentation,,,,,0,,,,,,app-reset-tool.html should remove reference to --zookeeper flag that no longer exists,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 23:38:17.0,,,,,,,,,,"0|z18gjs:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove ZK reference from org.apache.kafka.server.quota.ClientQuotaCallback javadoc,KAFKA-14216,13480948,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,09/Sep/22 23:36,12/Sep/22 15:35,13/Jul/23 09:17,12/Sep/22 15:35,3.3,3.3.0,,,,,,,,,,,,,,3.3,,,,,,,docs,documentation,,,,,0,,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 23:36:06.0,,,,,,,,,,"0|z18gjk:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft forwarded requests have no quota enforcement,KAFKA-14215,13480943,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,09/Sep/22 22:35,13/Sep/22 03:53,13/Jul/23 09:17,13/Sep/22 03:53,3.3,3.3.0,,,,,,,,,,,,,,3.3,3.3.0,,,,,,,,,,,,0,,,,,,"On the broker, the `BrokerMetadataPublisher` is responsible for propagating quota changes from `ClientQuota` records to `ClientQuotaManager`. On the controller, there is no similar logic, so no client quotas are enforced on the controller.

On the broker side, there is no enforcement as well since the broker assumes that the controller will be the one to do it. Basically it looks at the throttle time returned in the response from the controller. If it is 0, then the response is sent immediately without any throttling. 

So the consequence of both of these issues is that controller-bound requests have no throttling today.

 ",,dengziming,hachikuji,soarez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 22:35:08.0,,,,,,,,,,"0|z18gig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandardAuthorizer may transiently process ACLs out of write order,KAFKA-14214,13480936,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,09/Sep/22 21:41,16/Jun/23 14:00,13/Jul/23 09:17,20/Sep/22 21:36,3.3,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"The issue with StandardAuthorizer#authorize is, that it looks up aclsByResources (which is of type ConcurrentSkipListMap)twice for every authorize call and uses Iterator with weak consistency guarantees on top of aclsByResources. This can cause the authorize function call to process the concurrent writes out of order.
*Issue 1:*
When StandardAuthorizer calls into a simple authorize function, we check the ACLs for literal/prefix matches for the resource and then make one more call to check the ACLs for matching wildcard entries. Between the two (checkSection) calls, let’s assume we add a DENY for resource literal and add an ALLOW ALL wildcard. The first call to check literal/prefix rules will SKIP DENY ACL since the writes are not yet processed and the second call would find ALLOW wildcard entry which results in ALLOW authorization for the resource when it is actually DENY.

*Issue: 2*

For authorization, StandardAuthorizer depends on an iterator that iterates through the ordered set of ACLs. The iterator has weak consistency guarantees. So when writes for two ACLs occur, one of the ACLs might be still visible to the iterator while the other is not. 

Let’s say below two ACLS are getting added in the following order to the set.
Acl1 = StandardAcl(TOPIC, foobar, LITERAL, DENY, READ, user1)
Acl2 = StandardAcl(TOPIC, foo, PREFIX, ALLOW, READ, user1)
Depending on the position of the iterator on the ordered set during the write call, the iterator might just see Acl2 which prompts it to ALLOW the topic to be READ even though the DENY rule was written before.",,akhileshchg,showuon,soarez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 21:41:39.0,,,,,,,,,,"0|z18ggw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams log message has partition and offset transposed,KAFKA-14211,13480852,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mallwoodrbi,mallwoodrbi,09/Sep/22 11:52,07/Nov/22 08:57,13/Jul/23 09:17,07/Nov/22 07:28,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,,,,,,"The log warning message for out-of-order KTable update has partition and offset the wrong way around.

For example:
{noformat}
[...-StreamThread-1] WARN org.apache.kafka.streams.kstream.internals.KTableSource - Detected out-of-order KTable update for KTABLE-FK-JOIN-OUTPUT-STATE-STORE-0000000274, old timestamp=[1649245600022] new timestamp=[1642429126882]. topic=[...-KTABLE-FK-JOIN-SUBSCRIPTION-RESPONSE-0000000269-topic] partition=[2813] offset=[0].{noformat}",,ableegoldman,cadonna,mallwoodrbi,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 07:28:54 UTC 2022,,,,,,,,,,"0|z18fy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 12:14;cadonna;[~mallwoodrbi] Thank you for filing a ticket.
This is fixed via https://github.com/apache/kafka/pull/11905/files;;;","07/Nov/22 07:28;ableegoldman;Resolving this since it's apparently fixed by PR (see Bruno's comment) – [~cadonna]  can you fill out the ""Fix Version"" for this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumer#commitAsync throws unexpected WakeupException,KAFKA-14208,13480622,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,renqs,renqs,08/Sep/22 10:22,19/Sep/22 08:53,13/Jul/23 09:17,13/Sep/22 07:44,3.2.1,,,,,,,,,,,,,,,3.2.3,3.3.0,,,,,,clients,,,,,,0,,,,,,"We recently encountered a bug after upgrading Kafka client to 3.2.1 in Flink Kafka connector (FLINK-29153). Here's the exception:
{code:java}
org.apache.kafka.common.errors.WakeupException
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:514)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:278)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:252)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.coordinatorUnknownAndUnready(ConsumerCoordinator.java:493)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsAsync(ConsumerCoordinator.java:1055)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1573)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:226) {code}
As {{WakeupException}} is not listed in the JavaDoc of {{{}KafkaConsumer#commitAsync{}}}, Flink Kafka connector doesn't catch the exception thrown directly from KafkaConsumer#commitAsync but handles all exceptions in the callback.

I checked the source code and suspect this is caused by KAFKA-13563. Also we never had this exception in commitAsync when we used Kafka client 2.4.1 & 2.8.1. 

I'm wondering if this is kind of breaking the public API as the WakeupException is not listed in JavaDoc, and maybe it's better to invoke the callback to handle the {{WakeupException}} instead of throwing it directly from the method itself. ",,guozhang,jagsancio,renqs,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 22:43:17 UTC 2022,,,,,,,,,,"0|z18ejs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 23:53;guozhang;Hello Qingsheng, thanks for reporting this issue, and I looked at the source code and agree with you that it was introduced as part of KAFKA-13563. I will try to fix this with a follow-up PR.;;;","09/Sep/22 22:35;guozhang;I marked it as a blocker for 3.3.0 since it was a newly introduced regression. [~jsancio] Let me know what do you think?;;;","09/Sep/22 22:43;jagsancio;Okay. Thanks for the update. We have a few other blockers for 3.3 so we have time to get this merged and cherry picked.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QuorumController must correctly handle overly large batches,KAFKA-14204,13480344,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,06/Sep/22 19:35,08/Sep/22 21:40,13/Jul/23 09:17,08/Sep/22 21:40,,,,,,,,,,,,,,,,3.3.0,,,,,,,controller,kraft,,,,,0,,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14197,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-06 19:35:48.0,,,,,,,,,,"0|z18cv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft broker should disable snapshot generation after error replaying the metadata log,KAFKA-14203,13480343,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,jagsancio,jagsancio,06/Sep/22 19:33,12/Sep/22 21:59,13/Jul/23 09:17,12/Sep/22 21:59,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,core,,,,,,0,,,,,,The broker skips records for which there was an error when replaying the log. This means that the MetadataImage has diverged from the state persistent in the log. The broker should disable snapshot generation else the next time a snapshot gets generated it will result in inconsistent data getting persisted.,,jagsancio,showuon,soarez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-06 19:33:25.0,,,,,,,,,,"0|z18cuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer should not send group instance ID if committing with empty member ID,KAFKA-14201,13479945,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,hachikuji,hachikuji,03/Sep/22 00:25,08/Sep/22 22:12,13/Jul/23 09:17,08/Sep/22 22:12,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"The consumer group instance ID is used to support a notion of ""static"" consumer groups. The idea is to be able to identify the same group instance across restarts so that a rebalance is not needed. However, if the user sets `group.instance.id` in the consumer configuration, but uses ""simple"" assignment with `assign()`, then the instance ID nevertheless is sent in the OffsetCommit request to the coordinator. This may result in a surprising UNKNOWN_MEMBER_ID error. The consumer should probably be smart enough to only send the instance ID when committing as part of a consumer group.",,dajac,hachikuji,ijuma,jagsancio,MarkC0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 16:26:56 UTC 2022,,,,,,,,,,"0|z18ag0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/22 00:27;hachikuji;Worth mentioning that the alternative is to make the server more permissive and just ignore instance ID if it is set in OffsetCommit while memberId is empty.;;;","03/Sep/22 11:36;dajac;The advantage of being more permissive on the server side is that it would fix the issue for existing consumers. We should perhaps do both.;;;","06/Sep/22 14:06;ijuma;If it's reasonably simple, it does make sense to do it on both sides since upgrades are done independently for clients and brokers.;;;","06/Sep/22 16:26;jagsancio;[~hachikuji] [~dajac] Is this a blocker for 3.3.0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-features.sh must exit with non-zero error code on error,KAFKA-14200,13479921,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,02/Sep/22 17:04,07/Sep/22 16:06,13/Jul/23 09:17,07/Sep/22 16:06,3.3,3.3.0,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,kafka-features.sh must exit with a non-zero error code on error. We must do this in order to catch regressions like KAFKA-13990.,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-02 17:04:57.0,,,,,,,,,,"0|z18aao:",9223372036854775807,,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Installed kafka in ubuntu and not able to access in browser.  org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600),KAFKA-14199,13479876,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,9397,9397,02/Sep/22 11:20,12/Jan/23 11:04,13/Jul/23 09:17,12/Jan/23 11:04,,,,,,,,,,,,,,,,,,,,,,,admin,,,,,,0,,,,,,"I am new to kafka. I have installed the zookeeper and kafka in my local ubuntu machine. When i try to access the kafka in my browser [http://ip:9092|http://ip:9092/]  ia m facing this error.

+++

[SocketServer listenerType=ZK_BROKER, nodeId=0] Unexpected error from /127.0.0.1; closing connection (org.apache.kafka.common.network.Selector)
org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)
    at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:105)
    at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
    at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
    at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
    at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
    at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
    at kafka.network.Processor.poll(SocketServer.scala:989)
    at kafka.network.Processor.run(SocketServer.scala:892)
    at java.base/java.lang.Thread.run(Thread.java:829)

+++

Also I have checked by updating the socket.request.max.bytes=500000000 in ~/kafka/config/server.properties file still getting same error

 

pls figure it out. Thanks",,9397,akouame,christo_lolov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 11:04:46 UTC 2023,,,,,,,,,,"0|z18a0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 13:17;akouame;you could not communicate with the kafka broker via the http protocol, the broker speak kafka protocol.This link allows you to start : https://www.youtube.com/watch?v=oI7VAS9KSS4;;;","12/Jan/23 11:04;christo_lolov;There has been no further request for help on this ticket, so I will close it as resolved. If any further help is needed feel free to reachout on dev@kafka.apache.org!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Duplicated consumption during rebalance, causing OffsetValidationTest to act flaky",KAFKA-14196,13479762,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,pnee,pnee,pnee,02/Sep/22 02:28,19/Sep/22 08:53,13/Jul/23 09:17,13/Sep/22 08:11,3.2.1,,,,,,,,,,,,,,,3.2.3,3.3.0,,,,,,clients,consumer,,,,,0,new-consumer-threading-should-fix,,,,,"Several flaky tests under OffsetValidationTest are indicating potential consumer duplication issue, when autocommit is enabled.  I believe this is affecting *3.2* and onward.  Below shows the failure message:

 
{code:java}
Total consumed records 3366 did not match consumed position 3331 {code}
 

After investigating the log, I discovered that the data consumed between the start of a rebalance event and the async commit was lost for those failing tests.  In the example below, the rebalance event kicks in at around 1662054846995 (first record), and the async commit of the offset 3739 is completed at around 1662054847015 (right before partitions_revoked).

 
{code:java}
{""timestamp"":1662054846995,""name"":""records_consumed"",""count"":3,""partitions"":[{""topic"":""test_topic"",""partition"":0,""count"":3,""minOffset"":3739,""maxOffset"":3741}]}
{""timestamp"":1662054846998,""name"":""records_consumed"",""count"":2,""partitions"":[{""topic"":""test_topic"",""partition"":0,""count"":2,""minOffset"":3742,""maxOffset"":3743}]}
{""timestamp"":1662054847008,""name"":""records_consumed"",""count"":2,""partitions"":[{""topic"":""test_topic"",""partition"":0,""count"":2,""minOffset"":3744,""maxOffset"":3745}]}
{""timestamp"":1662054847016,""name"":""partitions_revoked"",""partitions"":[{""topic"":""test_topic"",""partition"":0}]}
{""timestamp"":1662054847031,""name"":""partitions_assigned"",""partitions"":[{""topic"":""test_topic"",""partition"":0}]}
{""timestamp"":1662054847038,""name"":""records_consumed"",""count"":23,""partitions"":[{""topic"":""test_topic"",""partition"":0,""count"":23,""minOffset"":3739,""maxOffset"":3761}]} {code}
A few things to note here:
 # Manually calling commitSync in the onPartitionsRevoke cb seems to alleviate the issue
 # Setting includeMetadataInTimeout to false also seems to alleviate the issue.

The above tries seems to suggest that contract between poll() and asyncCommit() is broken.  AFAIK, we implicitly uses poll() to ack the previously fetched data, and the consumer would (try to) commit these offsets in the current poll() loop.  However, it seems like as the poll continues to loop, the ""acked"" data isn't being committed.

 

I believe this could be introduced in  KAFKA-14024, which originated from KAFKA-13310.

More specifically, (see the comments below), the ConsumerCoordinator will alway return before async commit, due to the previous incomplete commit.  However, this is a bit contradictory here because:
 # I think we want to commit asynchronously while the poll continues, and if we do that, we are back to KAFKA-14024, that the consumer will get rebalance timeout and get kicked out of the group.
 # But we also need to commit all the ""acked"" offsets before revoking the partition, and this has to be blocked.

*Steps to Reproduce the Issue:*
 # Check out AK 3.2
 # Run this several times: (Recommend to only run runs with autocommit enabled in consumer_test.py to save time)
{code:java}
_DUCKTAPE_OPTIONS=""--debug"" TC_PATHS=""tests/kafkatest/tests/client/consumer_test.py::OffsetValidationTest.test_consumer_failure"" bash tests/docker/run_tests.sh {code}
 

*Steps to Diagnose the Issue:*
 # Open the test results in *results/*
 # Go to the consumer log.  It might look like this

 
{code:java}
results/2022-09-03--005/OffsetValidationTest/test_consumer_failure/clean_shutdown=True.enable_autocommit=True.metadata_quorum=ZK/2/VerifiableConsumer-0-xxxxxxxxxx/dockerYY {code}
3. Find the docker instance that has partition getting revoked and rejoined.  Observed the offset before and after.

*Propose Fixes:*

 TBD

 

https://github.com/apache/kafka/pull/12603",,guozhang,hachikuji,ijuma,pnee,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 04:14:52 UTC 2022,,,,,,,,,,"0|z189bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 03:31;pnee;If I understand this correctly: Seems like this is introduced in https://issues.apache.org/jira/browse/KAFKA-14024, which originated from https://issues.apache.org/jira/browse/KAFKA-13310.  I think the cause of the flakiness/duplication is, the consumer is busy waiting for the prior async commit to complete (in order to complete the rebalance process), while fetching new data.  After the async complete finished, the partition gets revoked, and the fetch progress will be lost, and eventually causes duplicated consumption.

A few comments:
 # Do we want to continue to fetch, while waiting for the async commit to complete? I believe this is the expectation of the new poll API.
 # If we don't want to block consumer from fetching, then we will need to continue to commit asynchronously.  I see this could be problematic, as the consumer could stuck in the poll loop while busy catching up with committing the fetched data, and never complete the rebalance process.;;;","06/Sep/22 02:31;pnee;Kind of originated from this commit: https://github.com/apache/kafka/pull/12349/files;;;","06/Sep/22 08:50;showuon;[~pnee] , thanks for the analysis. Yes, we forgot about during the following poll, the offset might advance while we're waiting for the old async offset commit completion.

Actually, while checking the code, even if we don't do the change for KAFKA-14024,and KAFKA-13310, (that is, changing sync commit to async commit) the issue will still happen, just not that easily. The issue is, in the consumer#poll process, we do onJoinPrepare (i.e. commit the offset), and then fetch new records. I'm thinking we should have a way to terminate poll process to avoid it keep fetching new records and return.

 

Maybe in `KafkaConsumer#updateAssignmentMetadataIfNeeded`, we passed in a parameter to allow the `onJoinPrepare` method to change the flag to notify if we need to terminate the poll and not to fetch records. WDYT?

cc [~guozhang] [~dajac]  [~aiquestion];;;","06/Sep/22 17:00;pnee;Thanks Luke, per your suggestion, could you elaborate more about the reason to terminate the poll?

I've got a few questions to clarify here:
 # I don't think we need to pause the fetch if the previous async commit (autocommit) hasn't yet go through, for the normal situation (not rebalancing)? Because as long as we are sending out the commit, I think we could tentatively assume the acked data has been committed. Am I right?
 # I think we only need to pause the fetch, if there's a rebalance process taking place, because it only waits for the current in-flight commit, then revoke the partition.  Once the partition is revoked, I don't think we can do anything about the uncommitted data.

And because this regression was caused by the ""rebalancing internal state"" (pardon me if the words use is confusing), do you think it might be worth exposing the rebalance internal states? and perhaps adding a state to represent the current rebalancing progress, to prevent more fetching from happening during onJoinPrepare?;;;","06/Sep/22 21:03;guozhang;[~pnee] Thanks for reporting this. While reviewing KAFKA-13310 I have realized this, but as Luke said this is not a new regression (we would potentially have duplicates even before this, since as we commit sync, and if the commit fails, we still log a warning and move forward with the revocation, in which case we would also have duplicates), I suggested we add a TODO there indicating it's sub-optimal but is allowed under at least once semantics.

I think in the long run, as we move the rebalancing related procedure all to the background thread, this would no longer be an issue since between the time background thread received an response telling it to start rebalancing (of which, the first step is to potentially revoking partitions in `onJoinPrepare`), and the time after the auto commit has been completed, the background thread could simply mark those revoking partitions as ""not retrievable"" so that calling thread's `poll` calls would not return any more data for those partitions. Right?

If that's the case, then we only need to consider before that comes, what we should do with this. Like I said, the behaviors before are 1) we commit sync, and even if it fails we still move forward, which would cause duplicates, or 2) we commit async so that `poll` timeout could be respected, but we would still potentially return data for those revoking partitions. I'm thinking what about just taking the middle ground: we still commit async, while at the same time mark those revoking partitions as ""not retrievable"" to not return any more data, note this would still not forbid duplicates completely, but would basically take us to where we were in the likelihood of the duplicates. And then we rely on the threading remodeling (there's a WIP page that Philip would be sending out soon) to completely resolve this issue.;;;","07/Sep/22 01:48;showuon;[~pnee] 
 # I don't think we need to pause the fetch if the previous async commit (autocommit) hasn't yet go through, for the normal situation (not rebalancing)? Because as long as we are sending out the commit, I think we could tentatively assume the acked data has been committed. Am I right?

 --> correct. for normal situation (not rebalancing), we don't pause anything
 # I think we only need to pause the fetch, if there's a rebalance process taking place, because it only waits for the current in-flight commit, then revoke the partition.  Once the partition is revoked, I don't think we can do anything about the uncommitted data.

--> correct.

 

[~guozhang] , thanks for the suggestion.

> I suggested we add a TODO there indicating it's sub-optimal but is allowed under at least once semantics.

Agree!

> we still commit async, while at the same time mark those revoking partitions as ""not retrievable"" to not return any more data

Sounds good to me!

 

From Philip:

> And because this regression was caused by the ""rebalancing internal state"" (pardon me if the words use is confusing), do you think it might be worth exposing the rebalance internal states? and perhaps adding a state to represent the current rebalancing progress, to prevent more fetching from happening during onJoinPrepare?

I think we can just `pause` the SubscriptionState of the partitions that we're going to revoked. From the javadoc:
{code:java}
/**
 * Suspend fetching from the requested partitions. Future calls to {@link #poll(Duration)} will not return
 * any records from these partitions until they have been resumed using {@link #resume(Collection)}.
 * Note that this method does not affect partition subscription. In particular, it does not cause a group
 * rebalance when automatic assignment is used.
 *
 * Note: Rebalance will not preserve the pause/resume state.
 * @param partitions The partitions which should be paused
 * @throws IllegalStateException if any of the provided partitions are not currently assigned to this consumer
 */
@Override
public void pause(Collection<TopicPartition> partitions) {{code}
 

I think that's what we want, right?;;;","07/Sep/22 16:10;pnee;Thanks Luke and GW, it looks like we could just pause it, but I'll test it out to see if that does what we want... I'll get back to you guys soon. :);;;","07/Sep/22 21:59;pnee;[~showuon] and [~guozhang] - I think pausing should probably work, and it's also kind of convenient because the partition revocation will unpausing these partition automatically.  Let me know if you think the draft is ok, I'll add tests later on: [https://github.com/apache/kafka/pull/12603]

 

Though a few questions here:
 # Should we consider the difference between cooperative and eager protocol.  Because, cooperative doesn't revoke all partitions.  However, I worry that the subscription might change during the onJoinPrepare, so I meant there could be edge cases we need to handle here.
 # I believe this only applies to autocommit enabled.  I think for non-autocommit case, user should handle the offset during the revocation, so we are good there?;;;","08/Sep/22 03:29;showuon;# Should we consider the difference between cooperative and eager protocol.  Because, cooperative doesn't revoke all partitions.  However, I worry that the subscription might change during the onJoinPrepare, so I meant there could be edge cases we need to handle here.

--> I think we should consider the difference between cooperative and eager protocol, because one of the purpose for cooperative rebalance is to allow ""non-revoking"" partitions can keep processing during rebalance. About the edge case, I think that's fine because in the your PR, we'll check and pause the partitions each time we enter onJoinPrepare, right? So, even if there's subscription change while we're waiting commitAsync, we can pause the updated subscription partitions in onJoinPrepare each time. Besides, that's really rare. WDYT?
 # I believe this only applies to autocommit enabled.  I think for non-autocommit case, user should handle the offset during the revocation, so we are good there?

--> Yes, we only need to worry about autocmmit enabled case;;;","08/Sep/22 23:59;guozhang;Thanks Philip, and regarding your two questions above I agree with [~showuon]'s thoughts as well. Especially for 1), I think even if subscriptions changed in between consecutive onJoinPrepare, as long as they will not change the assigned partitions (i.e. as long as `assignFromSubscribed()` has not called) I think we are fine, since the returned records depend on that assigned partitions.;;;","11/Sep/22 19:35;ijuma;To clarify, this was introduced in 3.2.1 (not 3.2.0), correct?

Also, this is currently marked as a blocker. Is there a crisp description of the regression?;;;","11/Sep/22 21:38;pnee;[~ijuma] - I think that's right, according to the [release notes|https://downloads.apache.org/kafka/3.2.1/RELEASE_NOTES.html] (I see 10424 there).  I can add the description but I don't really know where, do you mean by updating the description/title of this ticket?;;;","13/Sep/22 04:14;hachikuji;>  Also, this is currently marked as a blocker. Is there a crisp description of the regression?

Prior to revocation, eager rebalance strategies will attempt to auto-commit offsets before revoking partitions and joining the rebalance. Originally this logic was synchronous, which meant there was no opportunity for additional data to be returned before the revocation completed. This changed when we introduced asynchronous offset commit logic. Any progress made between the time the asynchronous offset commit was sent and the revocation completed would be lost. This results in duplicate consumption.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix KRaft AlterConfig policy usage for Legacy/Full case,KAFKA-14195,13479719,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rndgstn,rndgstn,rndgstn,01/Sep/22 18:08,02/Sep/22 07:09,13/Jul/23 09:17,02/Sep/22 07:09,3.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"The fix for https://issues.apache.org/jira/browse/KAFKA-14039 adjusted the invocation of the alter configs policy check in KRaft to match the behavior in ZooKeeper, which is to only provide the configs that were explicitly sent in the request. While the code was correct for the incremental alter configs case, the code actually included the implicit deletions for the legacy/non-incremental alter configs case, and those implicit deletions are not included in the ZooKeeper-based invocation. The implicit deletions should not be passed in the legacy case.",,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 18:08:00.0,,,,,,,,,,"0|z1891s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Cluster.nodeIfOnline,KAFKA-14194,13479711,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ndrwdn,ndrwdn,ndrwdn,01/Sep/22 16:48,09/Sep/22 15:08,13/Jul/23 09:17,05/Sep/22 07:57,3.2.1,,,,,,,,,,,,,,,3.3.0,3.4.0,,,,,,clients,,,,,,0,,,,,,When utilizing rack-aware Kafka consumers and the Kafka broker cluster is restarted an NPE can occur during transient metadata updates.,,ndrwdn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 16:48:31.0,,,,,,,,,,"0|z18900:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect system test ConnectRestApiTest is failing,KAFKA-14193,13479446,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yash.mayya,yash.mayya,yash.mayya,31/Aug/22 08:26,08/Sep/22 21:48,13/Jul/23 09:17,08/Sep/22 19:29,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"[ConnectRestApiTest|https://github.com/apache/kafka/blob/trunk/tests/kafkatest/tests/connect/connect_rest_test.py] is currently failing on `trunk` and `3.3` with the following assertion error:

 

 
{code:java}
AssertionError()
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/dist-packages/ducktape/tests/runner_client.py"", line 183, in _do_run
    data = self.run_test()
  File ""/usr/local/lib/python3.9/dist-packages/ducktape/tests/runner_client.py"", line 243, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python3.9/dist-packages/ducktape/mark/_mark.py"", line 433, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_rest_test.py"", line 106, in test_rest_api
    self.verify_config(self.FILE_SOURCE_CONNECTOR, self.FILE_SOURCE_CONFIGS, configs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/connect/connect_rest_test.py"", line 219, in verify_config
    assert config_def == set(config_names){code}
On closer inspection, this is because of the new source connector EOS related configs added in [https://github.com/apache/kafka/pull/11775.] Adding the following new configs - 
{code:java}
offsets.storage.topic, transaction.boundary, exactly.once.support, transaction.boundary.interval.ms{code}
in the expected config defs [here|https://github.com/apache/kafka/blob/6f4778301b1fcac1e2750cc697043d674eaa230d/tests/kafkatest/tests/connect/connect_rest_test.py#L35] fixes the tests on the 3.3 branch. However, the tests still fail on trunk due to the changes from [https://github.com/apache/kafka/pull/12450.]

 

The plan to fix this is to raise two PRs against trunk patching connect_rest_test.py - the first one fixing the EOS configs related issue which can be backported to 3.3 and the second one fixing the issue related to propagation of full connector configs to tasks which shouldn't be backported to 3.3 (because the commit from https://github.com/apache/kafka/pull/12450 is only on trunk and not on 3.3)

 

 ",,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13809,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-31 08:26:39.0,,,,,,,,,,"0|z187eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-features.sh: add support for --metadata,KAFKA-14187,13479152,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,29/Aug/22 17:35,06/Sep/22 16:20,13/Jul/23 09:17,30/Aug/22 23:56,3.3,3.3.0,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,Fix the kafka-features.sh command so that we can upgrade to the new version as expected.,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14179,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-29 17:35:18.0,,,,,,,,,,"0|z185m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kraft bootstrap metadata file should use snapshot header/footer,KAFKA-14183,13478704,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jagsancio,hachikuji,hachikuji,25/Aug/22 18:30,28/Aug/22 03:08,13/Jul/23 09:17,28/Aug/22 03:08,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"The bootstrap checkpoint file that we use in kraft is intended to follow the usual snapshot format, but currently it does not include the header/footer control records. The main purpose of these at the moment is to set a version for the checkpoint file itself.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-25 18:30:19.0,,,,,,,,,,"0|z182uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoOpRecord incorrectly causes high controller queue time metric,KAFKA-14178,13478276,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,mumrah,mumrah,23/Aug/22 21:19,17/Oct/22 23:26,13/Jul/23 09:17,24/Aug/22 17:17,,,,,,,,,,,,,,,,3.3.0,,,,,,,controller,kraft,metrics,,,,0,,,,,,"When a deferred event is added to the queue in ControllerQuorum, we include the total time it sat in the queue as part of the ""EventQueueTimeMs"" metric in QuorumControllerMetrics.

With the introduction of NoOpRecords, the p99 value for this metric is equal to the frequency that we schedule the no-op records. E.g., if no-op records are scheduled every 5 seconds, we will see p99 EventQueueTimeMs of 5 seconds.

This makes it difficult (impossible) to see if there is some delay in the event processing on the controller.
",,cmccabe,jsancio,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 17 23:26:50 UTC 2022,,,,,,,,,,"0|z18080:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 21:25;cmccabe;it seems like in general, deferred events should not contribute to this metric. NoOpEvent is one of them but there are some others.;;;","17/Oct/22 23:26;jsancio;They should contribute to the metrics. The controller needs to make sure to subtract the deferred time from the computed delay.

You can imagine a large number of delay events that couldn't be process for a while after they triggered.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly support older kraft versions without FeatureLevelRecord,KAFKA-14177,13478244,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,23/Aug/22 15:31,26/Aug/22 01:25,13/Jul/23 09:17,26/Aug/22 01:25,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 15:31:45.0,,,,,,,,,,"0|z1800w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug: State stores lose state when tasks are reassigned under EOS wit…,KAFKA-14172,13477561,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,Horslev,Horslev,19/Aug/22 09:01,22/Jun/23 13:02,13/Jul/23 09:17,25/Apr/23 13:03,3.1.1,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,streams,,,,,,1,,,,,,"h1. State stores lose state when tasks are reassigned under EOS with standby replicas and default acceptable lag.

I have observed that state stores used in a transform step under a Exactly Once semantics ends up losing state after a rebalancing event that includes reassignment of tasks to previous standby task within the acceptable standby lag.

 

The problem is reproduceable and an integration test have been created to showcase the [issue|https://github.com/apache/kafka/pull/12540]. 

A detailed description of the observed issue is provided [here|https://github.com/apache/kafka/pull/12540/files?short_path=3ca480e#diff-3ca480ef093a1faa18912e1ebc679be492b341147b96d7a85bda59911228ef45]

Similar issues have been observed and reported to StackOverflow for example [here|https://stackoverflow.com/questions/69038181/kafka-streams-aggregation-data-loss-between-instance-restarts-and-rebalances].

 ",,ableegoldman,gray.john,guozhang,Horslev,mcascallares,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 05:17:41 UTC 2023,,,,,,,,,,"0|z17vug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 13:11;gray.john;My stateful/EOS Kafka apps also seem to be struggling on 3.1.0+, with a similar theme: it appears the restore consumers are not consuming all of their messages for a full restore before processing begins. This sad situation seems to happen consistently after Strimzi rolls out an upgrade to our cluster. Once the brokers are all rolled, it seems to trigger a rebalance in our stateful apps, and then we lose data. We do not have the extra disk space for standby replicas, so the acceptable.recovery.lag and related bits to the standby replicas are not at play for us. But the restore consumers fumbling data w/ EOS seems to be a big problem for us as well. ;;;","01/Sep/22 14:39;gray.john;I know next to nothing about the internal workings of Kafka, sadly, but I am noticing that KAFKA-12486 was introduced in 3.1.0, which is the version I started noticing problems. I see you helped out with that Jira, [~ableegoldman] , is there any possible way in your mind that it might cause weirdness with state restoration? ;;;","02/Sep/22 07:32;Horslev;[~gray.john] thanks for adding your experience. I agree the scenarios seems similar although the setup you describe seems different. 
If I understand your issue correctly then it seems related to rolling upgrades of your Kafka cluster ? 
This issue is triggered solely by adding new stream applications. ;;;","02/Sep/22 12:48;gray.john;[~Horslev] we utilize static membership for our stateful apps, so for us the cluster upgrades seem to be about the only time they really rebalance. So I do think we lose data because of the rebalancing EOS consumers. The biggest difference I think is that we don't use standby replicas, yet still are getting data loss during restores. I think having a standby replica with 0 max.recovery.lag would be the only solution to getting around this bug since I believe it is the restore consumers dropping the ball here, but, alas, we don't have the extra disk space for standby replicas.;;;","09/Dec/22 14:50;gray.john;I do hope this bug gets some eyes eventually, it has been pretty devastating for us. Is still a problem even on Streams 3.3.1 w/ broker 3.1.0. I amend my previous statement that this is related to our brokers rolling, it actually seems it can happen any time our restore consumers in EOS apps w/ big state try to restore said big state. Sadly, setting the acceptable.lag config does not help us because we do not have the extra space to run standby threads/replicas. 

We actually had to resort to dumping our keys to a sqlserver and then querying for ""should this key exist?"" when we are pulling keys from our state store. If the state store returns nothing and sqlserver says it has seen that key before, we kill the app, which then causes us to pull the state again, which then fixes the issue. Something is _very_ wrong with these restore consumers (or I am doing something horribly wrong in this app, although we never had this problem before Kafka 3.0.0 or 3.1.0).;;;","09/Mar/23 05:17;guozhang;[~gray.john][~Horslev] I took a deep look into this issue and I think I found the culprit. Here's a short summary:

1. When standbys are enabled, Kafka Streams could recycle a standby task (and its state stores) into an active, and vice versa.
2. When caching is enabled, we would bypass the caching layer when updating a standby task (i.e. via putInternal).

And these two together combined would cause an issue. Take a concrete example following https://github.com/apache/kafka/pull/12540's demo: let's say we have a task A with a cached state store S. 

* For a given host, originally the task was hosted as an active.
* A rebalance happens, and that task was recycled into a standby. At that time the cache is flushed, so that the underlying store and the cache layer are consistent, let's assume they are S1 (version 1).
* The standby task was updated for a period of time, where updates are directly written into the underlying store. Now the underlying store is S2 while the caching layer is still S1.
* A second rebalance happens, and that task was recycled again into an active. Then when that task is normally processing, a read into the store would hit the cache layer first, and very likely read out an older versioned S1 instead of S2. As a result, we have a duplicate: more specifically in the above PR's example, the {{count}} store would return an old counter and hence cause the resulted ID inferred from counter being used twice.

That also explains why the test would not fail if caching is disabled, or standby replicas are disabled (tested locally); I think this test could still fail even when acceptable lag is set to 0, but it is less likely to have a standby -> active and then -> standby again so maybe people may not easily observe it.

I have a hack fix (note, this is not for merging as it is just a hack) that is inherited from [~Horslev]'s integration test, which would clear the cache upon flushing it (which is called when the task manager is flushed). With this fix the test no longer fails.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft Controller: Possible NPE when we remove topics with any offline partitions in the cluster,KAFKA-14170,13477304,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,akhileshchg,akhileshchg,akhileshchg,17/Aug/22 20:46,30/Aug/22 17:53,13/Jul/23 09:17,30/Aug/22 17:53,3.3,,,,,,,,,,,,,,,3.3,,,,,,,kraft,,,,,,0,,,,,,"When we remove a topic, it goes through the following function in KRaft Controller replay method for RemoveTopicRecord:
{code:java}
void removeTopicEntryForBroker(Uuid topicId, int brokerId) {
    Map<Uuid, int[]> topicMap = isrMembers.get(brokerId);
    if (topicMap != null) {
        if (brokerId == NO_LEADER) {
            offlinePartitionCount.set(offlinePartitionCount.get() - topicMap.get(topicId).length);
        }
        topicMap.remove(topicId);
    }
} {code}

If the broker has any offline partitions but doesn't have offline partitions for the topic we're deleting, the above code will run into NPE because we directly access the `topicMap.get(topicId).length`",,akhileshchg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-17 20:46:55.0,,,,,,,,,,"0|z17u9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected UNKNOWN_SERVER_ERROR raised from kraft controller,KAFKA-14167,13476906,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,hachikuji,hachikuji,15/Aug/22 23:38,18/Aug/22 01:16,13/Jul/23 09:17,18/Aug/22 01:16,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"In `ControllerApis`, we have callbacks such as the following after completion:
{code:java}
    controller.allocateProducerIds(context, allocatedProducerIdsRequest.data)
      .handle[Unit] { (results, exception) =>
        if (exception != null) {
          requestHelper.handleError(request, exception)
        } else {
          requestHelper.sendResponseMaybeThrottle(request, requestThrottleMs => {
            results.setThrottleTimeMs(requestThrottleMs)
            new AllocateProducerIdsResponse(results)
          })
        }
      } {code}
What I see locally is that the underlying exception that gets passed to `handle` always gets wrapped in a `CompletionException`. When passed to `getErrorResponse`, this error will get converted to `UNKNOWN_SERVER_ERROR`. For example, in this case, a `NOT_CONTROLLER` error returned from the controller would be returned as `UNKNOWN_SERVER_ERROR`. It looks like there are a few APIs that are potentially affected by this bug, such as `DeleteTopics` and `UpdateFeatures`.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-15 23:38:28.0,,,,,,,,,,"0|z17rtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Built-in partitioner may create suboptimal batches with large linger.ms,KAFKA-14156,13476095,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,alivshits,alivshits,alivshits,10/Aug/22 06:44,15/Sep/22 00:45,13/Jul/23 09:17,15/Sep/22 00:45,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,producer ,,,,,,0,,,,,,"The new built-in ""sticky"" partitioner switches partitions based on the amount of bytes produced to a partition.  It doesn't use batch creation as a switch trigger.  The previous ""sticky"" DefaultPartitioner switched partition when a new batch was created and with small linger.ms (default is 0) could result in sending larger batches to slower brokers potentially overloading them.  See https://cwiki.apache.org/confluence/display/KAFKA/KIP-794%3A+Strictly+Uniform+Sticky+Partitioner for more detail.

However, the with large linger.ms, the new built-in partitioner may create suboptimal batches.  Let's consider an example, suppose linger.ms=500, batch.size=16KB (default) and we produce 24KB / sec, i.e. every 500ms we produce 12KB worth of data.  The new built-in partitioner would switch partition on every 16KB, so we could get into the following batching pattern:
 * produce 12KB to one partition in 500ms, hit linger, send 12KB batch
 * produce 4KB more to the same partition, now we've produced 16KB of data, switch partition
 * produce 12KB to the second partition in 500ms, hit linger, send 12KB batch
 * in the mean time the 4KB produced to the first partition would hit linger as well, sending 4KB batch
 * produce 4KB more to the second partition, now we've produced 16KB of data to the second partition, switch to 3rd partition

so in this scenario the new built-in partitioner produces a mix of 12KB and 4KB batches, while the previous DefaultPartitioner would produce only 12KB batches -- it switches on new batch creation, so there is no ""mid-linger"" leftover batches.

To avoid creation of batch fragmentation on partition switch, we can wait until the batch is ready before switching the partition, i.e. the condition to switch to a new partition would be ""produced batch.size bytes"" AND ""batch is not lingering"".  This may potentially introduce some non-uniformity into data distribution, but unlike the previous DefaultPartitioner, the non-uniformity would not be based on broker performance and won't re-introduce the bad pattern of sending more data to slower brokers.",,alivshits,ashwinpankaj,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 00:45:34 UTC 2022,,,,,,,,,,"0|z17mu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 00:45;junrao;Merged the PR to 3.3 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Persistent URP after controller soft failure,KAFKA-14154,13476046,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,09/Aug/22 23:42,30/Aug/22 21:50,13/Jul/23 09:17,15/Aug/22 18:40,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"We ran into a scenario where a partition leader was unable to expand the ISR after a soft controller failover. Here is what happened:

Initial state: leader=1, isr=[1,2], leader epoch=10. Broker 1 is acting as the current controller.

1. Broker 1 loses its session in Zookeeper.  

2. Broker 2 becomes the new controller.

3. During initialization, controller 2 removes 1 from the ISR. So state is updated: leader=2, isr=[2], leader epoch=11.

4. Broker 2 receives `LeaderAndIsr` from the new controller with leader epoch=11.

5. Broker 2 immediately tries to add replica 1 back to the ISR since it is still fetching and is caught up. However, the `BrokerToControllerChannelManager` is still pointed at controller 1, so that is where the `AlterPartition` is sent.

6. Controller 1 does not yet realize that it is not the controller, so it processes the `AlterPartition` request. It sees the leader epoch of 11, which is higher than what it has in its own context. Following changes to the `AlterPartition` validation in [https://github.com/apache/kafka/pull/12032/files,] the controller returns FENCED_LEADER_EPOCH.

7. After receiving the FENCED_LEADER_EPOCH from the old controller, the leader is stuck because it assumes that the error implies that another LeaderAndIsr request should be sent.

Prior to [https://github.com/apache/kafka/pull/12032/files|https://github.com/apache/kafka/pull/12032/files,], the way we handled this case was a little different. We only verified that the leader epoch in the request was at least as large as the current epoch in the controller context. Anything higher was accepted. The controller would have attempted to write the updated state to Zookeeper. This update would have failed because of the controller epoch check, however, we would have returned NOT_CONTROLLER in this case, which is handled in `AlterPartitionManager`.

It is tempting to revert the logic, but the risk is in the idempotency check: [https://github.com/apache/kafka/pull/12032/files#diff-3e042c962e80577a4cc9bbcccf0950651c6b312097a86164af50003c00c50d37L2369.] If the AlterPartition request happened to match the state inside the old controller, the controller would consider the update successful and return no error. But if its state was already stale at that point, then that might cause the leader to incorrectly assume that the state had been updated.

One way to fix this problem without weakening the validation is to rely on the controller epoch in `AlterPartitionManager`. When we discover a new controller, we also discover its epoch, so we can pass that through. The `LeaderAndIsr` request already includes the controller epoch of the controller that sent it and we already propagate this through to `AlterPartition.submit`. Hence all we need to do is verify that the epoch of the current controller target is at least as large as the one discovered through the `LeaderAndIsr`.",,alivshits,andyg2,hachikuji,mumrah,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 21:50:56 UTC 2022,,,,,,,,,,"0|z17mjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 21:50;alivshits;> 1. Broker 1 loses its session in Zookeeper. 

I think if we treat this error as fatal (fence itself or maybe just flush and restart), it should handle a whole class of split brain issues.  ZK timeouts are generally set such that the client would timeout before the ephemeral zknode is removed, so the client (broker in this case) has sometime to fence itself before the controller decides that it's dead.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken DynamicBrokerReconfigurationTest in 3.2 branch,KAFKA-14149,13475776,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,mimaison,mimaison,08/Aug/22 15:46,13/Sep/22 07:33,13/Jul/23 09:17,25/Aug/22 19:45,3.2.2,,,,,,,,,,,,,,,3.2.3,,,,,,,,,,,,,0,,,,,,"The backport of [https://github.com/apache/kafka/pull/12455] does not work in 3.2. The following tests are failing:

DynamicBrokerReconfigurationTest.testConfigDescribeUsingAdminClient(String).quorum=kraft
DynamicBrokerReconfigurationTest.testConsecutiveConfigChange(String).quorum=kraft
DynamicBrokerReconfigurationTest.testKeyStoreAlter(String).quorum=kraft
DynamicBrokerReconfigurationTest.testLogCleanerConfig(String).quorum=kraft
DynamicBrokerReconfigurationTest.testTrustStoreAlter(String).quorum=kraft
DynamicBrokerReconfigurationTest.testUpdatesUsingConfigProvider(String).quorum=kraft

Caused by :
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.InvalidRequestException: Invalid value org.apache.kafka.common.config.ConfigException: Dynamic reconfiguration of listeners is not yet supported when using a Raft-based metadata quorum for configuration Invalid dynamic configuration",,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-08 15:46:43.0,,,,,,,,,,"0|z17kvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some map objects in KafkaConfigBackingStore grow in size monotonically,KAFKA-14147,13475716,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,yash.mayya,yash.mayya,yash.mayya,08/Aug/22 10:08,23/Aug/22 14:24,13/Jul/23 09:17,22/Aug/22 14:43,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,,"Similar to https://issues.apache.org/jira/browse/KAFKA-8869

 

{{deferredTaskUpdates, connectorTaskCountRecords and connectorTaskConfigGenerations in KafkaConfigBackingStore are never updated when a connector is deleted, thus growing monotonically.}}",,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-08 10:08:18.0,,,,,,,,,,"0|z17kig:",9223372036854775807,,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AlterPartition is not idempotent when requests time out,KAFKA-14144,13475254,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,david.mao,david.mao,david.mao,05/Aug/22 00:35,09/Aug/22 16:06,13/Jul/23 09:17,09/Aug/22 16:06,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"[https://github.com/apache/kafka/pull/12032] changed the validation order of AlterPartition requests to fence requests with a stale partition epoch before we compare the leader and ISR contents.

This results in a loss of idempotency if a leader does not receive an AlterPartition response because retries will receive an INVALID_UPDATE_VERSION error.",,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-05 00:35:14.0,,,,,,,,,,"0|z17hp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaced disk can lead to loss of committed data even with non-empty ISR,KAFKA-14139,13474960,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,calvinliu,hachikuji,hachikuji,03/Aug/22 23:04,14/Apr/23 02:39,13/Jul/23 09:17,14/Apr/23 02:39,,,,,,,,,,,,,,,,3.5.0,,,,,,,,,,,,,0,,,,,,"We have been thinking about disk failure cases recently. Suppose that a disk has failed and the user needs to restart the disk from an empty state. The concern is whether this can lead to the unnecessary loss of committed data.

For normal topic partitions, removal from the ISR during controlled shutdown buys us some protection. After the replica is restarted, it must prove its state to the leader before it can be added back to the ISR. And it cannot become a leader until it does so.

An obvious exception to this is when the replica is the last member in the ISR. In this case, the disk failure itself has compromised the committed data, so some amount of loss must be expected.

We have been considering other scenarios in which the loss of one disk can lead to data loss even when there are replicas remaining which have all of the committed entries. One such scenario is this:

Suppose we have a partition with two replicas: A and B. Initially A is the leader and it is the only member of the ISR.
 # Broker B catches up to A, so A attempts to send an AlterPartition request to the controller to add B into the ISR.
 # Before the AlterPartition request is received, replica B has a hard failure.
 # The current controller successfully fences broker B. It takes no action on this partition since B is already out of the ISR.
 # Before the controller receives the AlterPartition request to add B, it also fails.
 # While the new controller is initializing, suppose that replica B finishes startup, but the disk has been replaced (all of the previous state has been lost).
 # The new controller sees the registration from broker B first.
 # Finally, the AlterPartition from A arrives which adds B back into the ISR even though it has an empty log.

(Credit for coming up with this scenario goes to [~junrao] .)

I tested this in KRaft and confirmed that this sequence is possible (even if perhaps unlikely). There are a few ways we could have potentially detected the issue. First, perhaps the leader should have bumped the leader epoch on all partitions when B was fenced. Then the inflight AlterPartition would be doomed no matter when it arrived.

Alternatively, we could have relied on the broker epoch to distinguish the dead broker's state from that of the restarted broker. This could be done by including the broker epoch in both the `Fetch` request and in `AlterPartition`.

Finally, perhaps even normal kafka replication should be using a unique identifier for each disk so that we can reliably detect when it has changed. For example, something like what was proposed for the metadata quorum here: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Voter+Changes].",,ableegoldman,adupriez,calvinliu,divijvaidya,hachikuji,Hangleton,ijuma,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 02:39:20 UTC 2023,,,,,,,,,,"0|z17fvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 02:48;ableegoldman;[~hachikuji]  bumping this to 3.5.0 as we are past code freeze for 3.4;;;","13/Jan/23 09:38;Hangleton;Hi, [~hachikuji] , thank you for reporting this scenario and the very clear description of the issue. Is this something which is still prioritized and are you welcoming additional contributors on it?;;;","26/Jan/23 19:35;adupriez;Started to look at this JIRA. Please let me know if there is already an assignee or another stream of work which supersedes this one?

Regarding the use case, one question: is the controller failover between 4 and 6 necessary to trigger the scenario? Or is it enough for the {{AlterPartition}} request to be delayed long enough for broker B to have the time to disconnect then reconnect with an empty log directory before being processed by the controller?


;;;","30/Jan/23 12:55;adupriez;Could reproduce without ZK controller change.;;;","31/Jan/23 21:58;calvinliu;Hi [~adupriez] , Thanks for checking this issue. I have been working on a KIP for this one for a while. It will have protocol changes to include the broker epoch in the AlterPartition and Fetch requests. Will share more details when the KIP is published. 

Sorry I did not assign the ticket to me earlier. Can you assign the ticket to me?;;;","01/Feb/23 09:52;adupriez;Hi Calvin, thanks for following up.

Yes, sure, I assigned the JIRA back to you.

Will there be opportunities to contribute on the KIP? I guess it depends on the scope and code surface.

I was looking to automate the reproduction scenario in Kafka {{core}} tests. Please correct me if I am wrong, but it seems one of the obstacle was to simulate the sequence of receipt of requests so as to introduce a voluntary large delay before the controller could receive the then stale {{AlterPartition}} request. Would you know if such a framework is available today in the codebase along with core components to perform such simulations?

[https://github.com/apache/kafka/pull/12873] introduces a simulation framework in the client module, I wonder if extension of scope to server modules is possible.;;;","02/Feb/23 17:30;calvinliu;Hi Alex, I am not aware of such simulation frameworks existed. Will take a look at the PR. Thanks!;;;","03/Feb/23 22:07;calvinliu;Hi Alex, just posted the KIP. Let me know if you have any comments.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-903%3A+Replicas+with+stale+broker+epoch+should+not+be+allowed+to+join+the+ISR;;;","08/Feb/23 14:03;adupriez;Thanks for the KIP and the call-out. I went through it and asked a few questions on the dev mailing list.;;;","14/Apr/23 02:39;calvinliu;It is resolved in https://issues.apache.org/jira/browse/KAFKA-14617;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AlterConfigs in KRaft does not generate records for unchanged values,KAFKA-14136,13474928,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,03/Aug/22 18:29,13/Sep/22 07:33,13/Jul/23 09:17,04/Aug/22 20:31,,,,,,,,,,,,,,,,3.2.3,3.3.0,3.4.0,,,,,kraft,,,,,,0,,,,,,"In ZK, when handling LegacyAlterConfigs or IncrementalAlterConfigs, we call certain code paths regardless of what values are included in the request. We utilize this behavior to force a broker to reload a keystore or truststore from disk (we sent an AlterConfig with the keystore path unchanged).

In KRaft, however, we have an optimization to only generate ConfigRecords if the incoming AtlerConfig request will result in actual config changes. This means the broker never receives any records for ""no-op"" config changes and we cannot trigger certain code paths. 
",,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-03 18:29:32.0,,,,,,,,,,"0|z17fp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams terminates on topic check,KAFKA-14128,13474532,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Cerchie,pkleindl,pkleindl,01/Aug/22 13:12,23/Feb/23 07:48,13/Jul/23 09:17,23/Feb/23 07:48,3.0.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,streams,,,,,,0,,,,,,"Our streams application shut down unexpectedly after some network issues that should have been easily recoverable.

Logs:

 
{code:java}
2022-07-29 13:39:37.854  INFO 25843 --- [348aefeff-admin] org.apache.kafka.clients.NetworkClient   : [AdminClient clientId=L.DII.A-b1355e4a-b909-4da1-a832-dd3348aefeff-admin] Disconnecting from node 3 due to request timeout.
2022-07-29 13:39:37.854  INFO 25843 --- [348aefeff-admin] org.apache.kafka.clients.NetworkClient   : [AdminClient clientId=L.DII.A-b1355e4a-b909-4da1-a832-dd3348aefeff-admin] Cancelled in-flight METADATA request with correlation id 985 due to node 3 being disconnected (elapsed time since creation: 60023ms, elapsed time since send: 60023ms, request timeout: 30000ms)
2022-07-29 13:39:37.867 ERROR 25843 --- [-StreamThread-1] o.a.k.s.p.i.InternalTopicManager         : stream-thread [main] Unexpected error during topic description for L.DII.A-COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog.
Error message was: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1659101977830, tries=1, nextAllowedTryMs=1659101977955) timed out at 1659101977855 after 1 attempt(s)
2022-07-29 13:39:37.869  INFO 25843 --- [-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [L.DII.A-b1355e4a-b909-4da1-a832-dd3348aefeff-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
{code}
I think the relevant code is in [https://github.com/apache/kafka/blob/31ff6d7f8af57e8c39061f31774a61bd1728904e/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java#L524|https://github.com/apache/kafka/blob/31ff6d7f8af57e8c39061f31774a61bd1728904e/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java#L523-L550]
{code:java}
topicFuture.getValue().get();{code}
without a timeout value cannot throw a TimeoutException, so the TimeoutException of the AdminClient will be an ExecutionException and hit the last else branch where the StreamsException is thrown.

Possible fix:

Use the KafkaFuture method with timeout:
{code:java}
public abstract T get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException,
TimeoutException;{code}
instead of 
{code:java}
public abstract T get() throws InterruptedException, ExecutionException;{code}
 ",Any,Cerchie,mjsax,pkleindl,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9576,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 21 14:33:23 UTC 2022,,,,,,,,,,"0|z17d94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 14:33;Cerchie;working on this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test DynamicBrokerReconfigurationTest.testKeyStoreAlter,KAFKA-14122,13473966,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,divijvaidya,divijvaidya,divijvaidya,28/Jul/22 10:09,03/Aug/22 06:38,13/Jul/23 09:17,02/Aug/22 17:48,,,,,,,,,,,,,,,,3.4.0,,,,,,,consumer,core,system tests,,,,0,flaky,flaky-test,,,,"CI Build: [https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-12439/2/testReport/?cloudbees-analytics-link=scm-reporting%2Ftests%2Ffailed] 

Failure log:


{code:java}
org.opentest4j.AssertionFailedError: Duplicates not expected ==> expected: <false> but was: <true>
	at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at app//org.junit.jupiter.api.AssertFalse.assertFalse(AssertFalse.java:40)
	at app//org.junit.jupiter.api.Assertions.assertFalse(Assertions.java:235)
	at app//kafka.server.DynamicBrokerReconfigurationTest.stopAndVerifyProduceConsume(DynamicBrokerReconfigurationTest.scala:1579)
	at app//kafka.server.DynamicBrokerReconfigurationTest.testKeyStoreAlter(DynamicBrokerReconfigurationTest.scala:399)
	at java.base@17.0.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base@17.0.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base@17.0.1/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base@17.0.1/java.lang.reflect.Method.invoke(Method.java:568)
	at app//org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at app//org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at app//org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at app//org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at app//org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at app//org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at app//org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105) {code}",,cadonna,divijvaidya,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14117,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 02 12:57:05 UTC 2022,,,,,,,,,,"0|z179rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 14:50;cadonna;Encountered again:

{code:java}
org.opentest4j.AssertionFailedError: Duplicates not expected ==> expected: <false> but was: <true>
	at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at app//org.junit.jupiter.api.AssertFalse.assertFalse(AssertFalse.java:40)
	at app//org.junit.jupiter.api.Assertions.assertFalse(Assertions.java:235)
	at app//kafka.server.DynamicBrokerReconfigurationTest.stopAndVerifyProduceConsume(DynamicBrokerReconfigurationTest.scala:1579)
	at app//kafka.server.DynamicBrokerReconfigurationTest.testKeyStoreAlter(DynamicBrokerReconfigurationTest.scala:399)
	at java.base@17.0.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base@17.0.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base@17.0.1/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base@17.0.1/java.lang.reflect.Method.invoke(Method.java:568)
	at app//org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
{code}

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-12453/3/testReport/kafka.server/DynamicBrokerReconfigurationTest/Build___JDK_17_and_Scala_2_13___testKeyStoreAlter__/;;;","02/Aug/22 12:55;divijvaidya;Another build failure: [https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-12459/3/testReport/?cloudbees-analytics-link=scm-reporting%2Ftests%2Ffailed] 


{code:java}
org.opentest4j.AssertionFailedError: Duplicates not expected ==> expected: <false> but was: <true>
	at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at app//org.junit.jupiter.api.AssertFalse.assertFalse(AssertFalse.java:40)
	at app//org.junit.jupiter.api.Assertions.assertFalse(Assertions.java:235)
	at app//kafka.server.DynamicBrokerReconfigurationTest.stopAndVerifyProduceConsume(DynamicBrokerReconfigurationTest.scala:1579)
	at app//kafka.server.DynamicBrokerReconfigurationTest.testKeyStoreAlter(DynamicBrokerReconfigurationTest.scala:399)
	at java.base@17.0.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base@17.0.1/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base@17.0.1/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base@17.0.1/java.lang.reflect.Method.invoke(Method.java:568)
	at app//org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at app//org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at app//org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149) {code};;;","02/Aug/22 12:57;divijvaidya;[~showuon] please review...this flaky test is impacting multiple PRs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Password configs are logged in plaintext in KRaft,KAFKA-14115,13473822,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,mumrah,mumrah,mumrah,27/Jul/22 16:16,13/Sep/22 07:32,13/Jul/23 09:17,04/Aug/22 20:31,,,,,,,,,,,,,,,,3.2.3,3.3.0,3.4.0,,,,,kraft,,,,,,0,,,,,,"While investigating KAFKA-14111, I also noticed that ConfigurationControlManager is logging sensitive configs in plaintext at INFO level.


{code}
[2022-07-27 12:14:09,927] INFO [Controller 1] ConfigResource(type=BROKER, name='1'): set configuration listener.name.external.ssl.key.password to bar (org.apache.kafka.controller.ConfigurationControlManager)
{code}

Once this new config reaches the broker, it is logged again, but this time it is redacted

{code}
[2022-07-27 12:14:09,957] INFO [BrokerMetadataPublisher id=1] Updating broker 1 with new configuration : listener.name.external.ssl.key.password -> [hidden] (kafka.server.metadata.BrokerMetadataPublisher)
{code}",,cmccabe,dengziming,mumrah,Prem237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 04 20:33:09 UTC 2022,,,,,,,,,,"0|z178vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 16:38;Prem237;Hi [~mumrah], We can use our existing method from the KafkaConfig utility to hide the password.

I have just started contributing, can I assign this issue to myself? ;;;","27/Jul/22 17:45;cmccabe;KafkaConfig is not accessible from the metadata package. You can use {{ConfigUtils#configMapToRedactedString}}, however.;;;","27/Jul/22 17:59;Prem237;Sure, Thanks [~cmccabe] .;;;","27/Jul/22 18:05;mumrah;[~Prem237] ConfigurationControlManager has a reference to KafkaConfigSchema which can help determine if a config is sensitive or not;;;","28/Jul/22 08:28;Prem237;Thanks [~mumrah]. Will go through KafkaConfigSchema. ;;;","04/Aug/22 20:33;mumrah;Fixed as part of KAFKA-14136;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dynamic config update fails for ""password"" configs in KRaft",KAFKA-14111,13473633,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,26/Jul/22 20:33,13/Sep/22 07:33,13/Jul/23 09:17,03/Aug/22 18:01,,,,,,,,,,,,,,,,3.2.3,3.3.0,3.4.0,,,,,kraft,,,,,,0,,,,,,"Two related bugs found when working on updating DynamicBrokerReconfigurationTest for KRaft. 

Firstly, if we issue an AlterConfig (or IncrementalAlterConfig) for a broker config that is defined as a ""password"", it will succeed on the controller, but throw an error when the broker handles it. 

For example, on a vanilla cluster running ""config/kraft/server.properties""

{code}
/bin/kafka-configs.sh --bootstrap-server localhost:9092  --alter --broker 1 --add-config listener.name.external.ssl.key.password=foo 
{code}

results in

{code}
[2022-07-26 16:24:05,049] ERROR Dynamic password config listener.name.external.ssl.key.password could not be decoded, ignoring. (kafka.server.DynamicBrokerConfig)
org.apache.kafka.common.config.ConfigException: Password encoder secret not configured
	at kafka.server.DynamicBrokerConfig.$anonfun$passwordEncoder$1(DynamicBrokerConfig.scala:352)
	at scala.Option.getOrElse(Option.scala:201)
	at kafka.server.DynamicBrokerConfig.passwordEncoder(DynamicBrokerConfig.scala:352)
	at kafka.server.DynamicBrokerConfig.decodePassword$1(DynamicBrokerConfig.scala:393)
	at kafka.server.DynamicBrokerConfig.$anonfun$fromPersistentProps$5(DynamicBrokerConfig.scala:404)
	at kafka.server.DynamicBrokerConfig.$anonfun$fromPersistentProps$5$adapted(DynamicBrokerConfig.scala:402)
	at kafka.utils.Implicits$MapExtensionMethods$.$anonfun$forKeyValue$1(Implicits.scala:62)
	at scala.collection.MapOps.foreachEntry(Map.scala:244)
	at scala.collection.MapOps.foreachEntry$(Map.scala:240)
	at scala.collection.AbstractMap.foreachEntry(Map.scala:405)
	at kafka.server.DynamicBrokerConfig.fromPersistentProps(DynamicBrokerConfig.scala:402)
	at kafka.server.DynamicBrokerConfig.$anonfun$updateBrokerConfig$1(DynamicBrokerConfig.scala:300)
	at kafka.server.DynamicBrokerConfig.updateBrokerConfig(DynamicBrokerConfig.scala:299)
	at kafka.server.BrokerConfigHandler.processConfigChanges(ConfigHandler.scala:221)
	at kafka.server.metadata.BrokerMetadataPublisher.$anonfun$publish$15(BrokerMetadataPublisher.scala:212)
	at java.base/java.util.HashMap$KeySet.forEach(HashMap.java:1008)
	at kafka.server.metadata.BrokerMetadataPublisher.$anonfun$publish$14(BrokerMetadataPublisher.scala:190)
	at kafka.server.metadata.BrokerMetadataPublisher.$anonfun$publish$14$adapted(BrokerMetadataPublisher.scala:189)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.metadata.BrokerMetadataPublisher.publish(BrokerMetadataPublisher.scala:189)
	at kafka.server.metadata.BrokerMetadataListener.kafka$server$metadata$BrokerMetadataListener$$publish(BrokerMetadataListener.scala:293)
	at kafka.server.metadata.BrokerMetadataListener$HandleCommitsEvent.$anonfun$run$2(BrokerMetadataListener.scala:126)
	at kafka.server.metadata.BrokerMetadataListener$HandleCommitsEvent.$anonfun$run$2$adapted(BrokerMetadataListener.scala:126)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.metadata.BrokerMetadataListener$HandleCommitsEvent.run(BrokerMetadataListener.scala:126)
	at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
	at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
	at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
	at java.base/java.lang.Thread.run(Thread.java:833)
{code}.


If a {{password.encoder.secret}} is supplied, this still fails but with:


{code}
[2022-07-26 16:27:23,247] ERROR Dynamic password config listener.name.external.ssl.key.password could not be decoded, ignoring. (kafka.server.DynamicBrokerConfig)
java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 3
	at java.base/java.lang.String.checkBoundsBeginEnd(String.java:4604)
	at java.base/java.lang.String.substring(String.java:2707)
	at kafka.utils.CoreUtils$.$anonfun$parseCsvMap$1(CoreUtils.scala:173)
	at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
	at kafka.utils.CoreUtils$.parseCsvMap(CoreUtils.scala:171)
	at kafka.utils.PasswordEncoder.decode(PasswordEncoder.scala:88)
	at kafka.server.DynamicBrokerConfig.decodePassword$1(DynamicBrokerConfig.scala:393)
	at kafka.server.DynamicBrokerConfig.$anonfun$fromPersistentProps$5(DynamicBrokerConfig.scala:404)
	at kafka.server.DynamicBrokerConfig.$anonfun$fromPersistentProps$5$adapted(DynamicBrokerConfig.scala:402)
	at kafka.utils.Implicits$MapExtensionMethods$.$anonfun$forKeyValue$1(Implicits.scala:62)
	at scala.collection.MapOps.foreachEntry(Map.scala:244)
	at scala.collection.MapOps.foreachEntry$(Map.scala:240)
	at scala.collection.AbstractMap.foreachEntry(Map.scala:405)
	at kafka.server.DynamicBrokerConfig.fromPersistentProps(DynamicBrokerConfig.scala:402)
	at kafka.server.DynamicBrokerConfig.$anonfun$updateBrokerConfig$1(DynamicBrokerConfig.scala:300)
	at kafka.server.DynamicBrokerConfig.updateBrokerConfig(DynamicBrokerConfig.scala:299)
	at kafka.server.BrokerConfigHandler.processConfigChanges(ConfigHandler.scala:221)
	at kafka.server.metadata.BrokerMetadataPublisher.$anonfun$publish$15(BrokerMetadataPublisher.scala:212)
	at java.base/java.util.HashMap$KeySet.forEach(HashMap.java:1008)
	at kafka.server.metadata.BrokerMetadataPublisher.$anonfun$publish$14(BrokerMetadataPublisher.scala:190)
	at kafka.server.metadata.BrokerMetadataPublisher.$anonfun$publish$14$adapted(BrokerMetadataPublisher.scala:189)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.metadata.BrokerMetadataPublisher.publish(BrokerMetadataPublisher.scala:189)
	at kafka.server.metadata.BrokerMetadataListener.kafka$server$metadata$BrokerMetadataListener$$publish(BrokerMetadataListener.scala:293)
	at kafka.server.metadata.BrokerMetadataListener$StartPublishingEvent.run(BrokerMetadataListener.scala:258)
	at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
	at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
	at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
	at java.base/java.lang.Thread.run(Thread.java:833)
{code}",,dengziming,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-26 20:33:59.0,,,,,,,,,,"0|z177ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Jetty for CVE fixes,KAFKA-14107,13473550,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ajborley,ajborley,ajborley,26/Jul/22 12:28,26/Sep/22 07:05,13/Jul/23 09:17,05/Aug/22 22:48,2.8.1,3.0.1,3.1.1,3.2.0,,,,,,,,,,,,2.8.2,3.0.2,3.1.2,3.2.3,3.3.0,,,core,,,,,,1,security,,,,,"There are a couple of CVEs for Jetty:

- [CVE-2022-2048](https://nvd.nist.gov/vuln/detail/CVE-2022-2048)

- [CVE-2022-2047](https://nvd.nist.gov/vuln/detail/CVE-2022-2047)

Fixed by upgrading to 9.4.48.v20220622+",,ajborley,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14100,,,,,,,KAFKA-14100,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 12:37:20 UTC 2022,,,,,,,,,,"0|z1777c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 12:37;ajborley;I think this is a dup of https://issues.apache.org/jira/browse/KAFKA-14100;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perform CRC validation on KRaft Batch Records and Snapshots,KAFKA-14104,13473397,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,Niket Goel,Niket Goel,Niket Goel,25/Jul/22 20:40,08/Aug/22 22:42,13/Jul/23 09:17,08/Aug/22 22:42,3.2.0,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"Today we stamp the BatchRecord header with a CRC [1] and verify that CRC before the log is written to disk [2]. The CRC checks should also be verified when the records are read back from disk. The same procedure should be followed for KRaft snapshots as well.

[1] [https://github.com/apache/kafka/blob/6b76c01cf895db0651e2cdcc07c2c392f00a8ceb/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L501=] 

[2] [https://github.com/apache/kafka/blob/679e9e0cee67e7d3d2ece204a421ea7da31d73e9/core/src/main/scala/kafka/log/UnifiedLog.scala#L1143]",,Niket Goel,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13806,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 03 00:46:14 UTC 2022,,,,,,,,,,"0|z1769c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 00:46;Niket Goel;PR here – https://github.com/apache/kafka/pull/12457;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No REST API request logs in Kafka connect,KAFKA-14099,13473048,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,zigarn,zigarn,zigarn,23/Jul/22 07:42,12/Oct/22 14:29,13/Jul/23 09:17,12/Oct/22 14:29,3.2.0,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,pull-request-available,,,,,"Prior to 2.2.1, when an REST API request was performed, there was a request log in the log file:
{code:java}
[2022-07-23 07:18:16,128] INFO 172.18.0.1 - - [23/Jul/2022:07:18:16 +0000] ""GET /connectors HTTP/1.1"" 200 2 ""-"" ""curl/7.81.0"" 66 (org.apache.kafka.connect.runtime.rest.RestServer:62)
{code}
Since 2.2.1, no more request logs.

 

With a bisect, I found the problem comes from [PR 6651|https://github.com/apache/kafka/pull/6651] to fix KAFKA-8304

From what I understand of the problem, the ContextHandlerCollection is added in the Server ([https://github.com/dongjinleekr/kafka/blob/63a6130af30536d67fca5802005695a84c875b5e/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java#L195]) before handlers are really added in the ContextHandlerCollection ([https://github.com/dongjinleekr/kafka/blob/63a6130af30536d67fca5802005695a84c875b5e/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java#L296]).
I don't know the impact on other handlers, but clearly it doesn't work for the RequestLogHandler.

 

A solution I found for the logging issue is to set the RequestLog directly in the server without using an handlers:
{code:java}
diff --git i/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java w/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java
index ab18419efc..4d09cc0e6c 100644
--- i/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java
+++ w/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java
@@ -187,6 +187,11 @@ public class RestServer {
     public void initializeServer() {
         log.info(""Initializing REST server"");
 
+        Slf4jRequestLogWriter slf4jRequestLogWriter = new Slf4jRequestLogWriter();
+        slf4jRequestLogWriter.setLoggerName(RestServer.class.getCanonicalName());
+        CustomRequestLog requestLog = new CustomRequestLog(slf4jRequestLogWriter, CustomRequestLog.EXTENDED_NCSA_FORMAT + "" %{ms}T"");
+        jettyServer.setRequestLog(requestLog);
+
         /* Needed for graceful shutdown as per `setStopTimeout` documentation */
         StatisticsHandler statsHandler = new StatisticsHandler();
         statsHandler.setHandler(handlers);
@@ -275,14 +280,7 @@ public class RestServer {
             configureHttpResponsHeaderFilter(context);
         }
 
-        RequestLogHandler requestLogHandler = new RequestLogHandler();
-        Slf4jRequestLogWriter slf4jRequestLogWriter = new Slf4jRequestLogWriter();
-        slf4jRequestLogWriter.setLoggerName(RestServer.class.getCanonicalName());
-        CustomRequestLog requestLog = new CustomRequestLog(slf4jRequestLogWriter, CustomRequestLog.EXTENDED_NCSA_FORMAT + "" %{ms}T"");
-        requestLogHandler.setRequestLog(requestLog);
-
         contextHandlers.add(new DefaultHandler());
-        contextHandlers.add(requestLogHandler);
 
         handlers.setHandlers(contextHandlers.toArray(new Handler[0]));
         try {
{code}
Same issue raised on StackOverflow: [https://stackoverflow.com/questions/67699702/no-rest-api-logs-in-kafka-connect]",,ChrisEgerton,zigarn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 22:05:05 UTC 2022,,,,,,,,,,"0|z17440:",9223372036854775807,,ChrisEgerton,,,,,,,,,,,,,,,,,,"25/Jul/22 22:05;ChrisEgerton;[~zigarn] Since you filed a PR to address this (thanks again for the fix!), I've assigned the ticket to you. Feel free to unassign at any point if you want to signal to others that nobody is actively working on this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky ExactlyOnceSourceIntegrationTest.testFencedLeaderRecovery,KAFKA-14093,13472625,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,mimaison,mimaison,21/Jul/22 08:46,25/Jul/22 19:45,13/Jul/23 09:17,25/Jul/22 13:38,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest > testFencedLeaderRecovery FAILED
    java.lang.AssertionError: expected org.apache.kafka.connect.runtime.rest.errors.ConnectRestException to be thrown, but nothing was thrown

",,ChrisEgerton,mimaison,soarez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 08:45;mimaison;org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testFencedLeaderRecovery.test.stdout;https://issues.apache.org/jira/secure/attachment/13047070/org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testFencedLeaderRecovery.test.stdout",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 22 20:23:58 UTC 2022,,,,,,,,,,"0|z171i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 08:46;mimaison;cc [~ChrisEgerton];;;","21/Jul/22 09:01;mimaison;I've only been able to reproduce it once locally but I found a couple of other instances from our CI: 
- https://github.com/apache/kafka/runs/7208515607
- https://github.com/apache/kafka/runs/7428998555;;;","22/Jul/22 20:23;ChrisEgerton;Ah, I think the root cause of this is rather simple; the Javadoc for the test calls out that it uses a one-node cluster, and the [check we do|https://github.com/apache/kafka/blob/679e9e0cee67e7d3d2ece204a421ea7da31d73e9/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExactlyOnceSourceIntegrationTest.java#L493-L494] to make sure that the leader is up and running (and has had a chance to instantiate its transactional producer for the config topic) relies on that fact. However, I forgot to actually reduce the cluster size to one in the test, so right now it's running with three workers.

As a result, it's possible that the readiness check we're doing hits a follower worker and passes before the leader has had a chance to finish startup.

I've published a quick fix that reduces the cluster to a single worker, and also adds client IDs for producers created for testing so that they can be distinguished from any producers created by the Connect cluster.

I've also filed KAFKA-14098 as a potential follow-up item to add meaningful client IDs to the internal clients set up by Connect workers.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky ExactlyOnceSourceIntegrationTest.testSeparateOffsetsTopic,KAFKA-14089,13472447,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,mimaison,mimaison,20/Jul/22 10:22,28/Jul/22 15:35,13/Jul/23 09:17,28/Jul/22 15:33,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"It looks like the sequence got broken around ""65535, 65537, 65536, 65539, 65538, 65541, 65540, 65543""",,ChrisEgerton,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/22 10:19;mimaison;failure.txt;https://issues.apache.org/jira/secure/attachment/13047017/failure.txt","21/Jul/22 08:28;mimaison;org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testSeparateOffsetsTopic.test.stdout;https://issues.apache.org/jira/secure/attachment/13047069/org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testSeparateOffsetsTopic.test.stdout",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 21 22:58:29 UTC 2022,,,,,,,,,,"0|z170eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 10:22;mimaison;cc [~ChrisEgerton];;;","20/Jul/22 23:28;ChrisEgerton;Thanks [~mimaison]. We don't assert on order of records, just that the expected seqnos were present in any order, so the wonkiness around 65535 isn't actually an issue (and it's even present in the stringified representation of both the expected _and_ the actual seqno sets).

After doing some Bash scrubbing on the file attached to the ticket, it looks like seqnos start to be missing (i.e., they're in the expected set but not the actual) between 114463 and 114754. Not every seqno in that range is missing, but there's 105 missing in total. After that, starting at 114755, there's 105 extra (i.e., in the actual set but not the expected) seqnos.

Given that the issues crop up at the very end of the seqno set, it seems like this could be caused by non-graceful shutdown of the worker after exactly-once support is disabled, or even possibly the recently-discovered KAFKA-14079. -It's a little worrisome, though, since the results here indicate possible data loss.- Actually, on second thought, this is probably not data loss, since we're reading the records that have been produced to Kafka, but not necessarily the records whose offsets have been committed.

If this was on Jenkins, do you have a link to the CI run that caused it? Or if it was encountered elsewhere, do you have any logs available? I'll try to kick off some local runs but I'm in the middle of stress-testing my laptop with the latest KIP-618 system tests and may not be able to reproduce locally.

I suspect a fix for this would involve reading the last-committed offset for each task, then only checking seqnos for that task up to the seqno in that offset. But I'd like to have a better idea of what exactly is causing the failure before pulling the trigger on that, especially if it's unclean task/worker shutdown and we can find a way to fix that instead of adjusting our tests to handle sloppy shutdowns.;;;","21/Jul/22 08:29;mimaison;I've hit this issue locally and I can reproduce it fairly consistently. See attached logs

[^org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testSeparateOffsetsTopic.test.stdout] ;;;","21/Jul/22 22:58;ChrisEgerton;Thanks Mickael. I put together a draft fix [here|https://github.com/apache/kafka/pull/12429], although I still haven't been able to replicate the failure locally. If you have time, would you mind giving it a try and see if it has positive effects in your environment? I can also kick off several Jenkins builds by re-triggering CI runs, although that will be more time-consuming as it will run the build for the whole project instead of just Connect.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Source task will not commit offsets and develops memory leak if ""error.tolerance"" is set to ""all""",KAFKA-14079,13471880,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,cshannon,cshannon,cshannon,16/Jul/22 16:16,26/Jul/22 20:18,13/Jul/23 09:17,18/Jul/22 22:08,3.2.0,,,,,,,,,,,,,,,3.2.1,3.3.0,3.4.0,,,,,KafkaConnect,,,,,,0,,,,,,"KAFKA-13348 added the ability to ignore producer exceptions by setting {{error.tolerance}} to {{{}all{}}}.  When this is set to all a null record metadata is passed to commitRecord() and the task continues.

The issue is that records are tracked by {{SubmittedRecords}} and the first time an error happens the code does not ack the record with the error and just skips it so it will not have the offsets committed or be removed from SubmittedRecords before calling commitRecord(). 

This leads to a bug where future offsets won't be committed anymore and also a memory leak because the algorithm that removes acked records from the internal map to commit offsets [looks |https://github.com/apache/kafka/blob/3.2.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SubmittedRecords.java#L177] at the head of the Deque where the records are tracked in and if it sees the record is unacked it will not process anymore removals. This leads to all new records that go through the task to continue to be added and not have offsets committed and never removed from tracking until an OOM error occurs.

The fix is to make sure to ack the failed records so they can have their offsets commited and be removed from tracking. This is fine to do as the records are intended to be skipped and not reprocessed. Metrics also need to be updated as well.

 ",,brianjohnson,ChrisEgerton,cshannon,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 20:18:34 UTC 2022,,,,,,,,,,"0|z16www:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"16/Jul/22 16:27;cshannon;I submitted a fix for the 3.2.x branch. The fix is relevant to 3.3.0 as well but a lot of refactoring was done for KAFKA-10000 so if the community agrees this is a good fix I can also create another PR for trunk to fix it there as well. ;;;","17/Jul/22 17:43;ChrisEgerton;[~cshannon] is it correct to say that, in addition to leaking resources, another consequence of this bug is that source tasks become unable to commit some or all source offsets? It might be worth updating the title to reflect that since, in addition to the increased memory utilization we'd expect from an ever-growing deque of records, users may also discover this issue by observing that the offsets for a source connector have become stuck on one or more source partitions. Thoughts?;;;","17/Jul/22 22:07;cshannon;[~ChrisEgerton] - I agree, I updated the title and description and pushed a new PR update.;;;","19/Jul/22 16:03;rhauch;Following up with some additional detail:

This issue can affect users that are upgrading to AK 3.2.0, even if they don't modify any Connect worker config or connector configurations. For example, if a user has a pre-AK 3.2.0 Connect installation running with one or more source connector configurations that use {{{}error.tolerance=all{}}}, then when that Connect installation is upgraded to AK 3.2.0 _and_ subsequently the producer fails to send and ack messages generated by the source connector (e.g., message too large, etc.), then Connect will continue to write records to topics by will no longer commit source offsets for that connector. As mentioned above, Connect will accumulate those additional records in-memory, causing the worker to eventually fail with an OOM.

Unfortunately, restarting is not likely to be helpful, either: the source offsets are not changed/committed once this condition happens, so upon restart the connector will resume from the previously-committed source offsets and will likely regenerate the same problematic messages as before, triggering the problem again and causing the same OOM.

The only way to recover is to fix the underlying problem reported by the producer (e.g., message too large), and restart the Connect workers. Luckily the problems reported by the producer are captured in the worker logs.Note that changing the connector configuration to use {{error.tolerance=none}} will cause the connector to stop/fail as soon as the producer fails to write a record to the topic (e.g., message too large), and will not generate duplicate messages beyond the first problematic one (like with {{{}error.tolerance=all{}}}). But again, the underlying problem must be corrected before the connector can be restarted successfully.

This issue does not affect:
 * sink connectors;
 * source connector configurations that use {{{}error.tolerance=none{}}}, which is the default behavior; or
 * source connectors that never use or rely upon source offsets (a smallish fraction of all source connector types)

Most source connectors do rely upon source offsets, though, so this is a fairly serious issue.

Thanks, [~cshannon] and [~ChrisEgerton] for the quick work and review of these PRs. Both PRs linked above (one for the `trunk` branch and one for the `3.2` branch) have been merged. The `3.2` PR was merged before the first 3.2.1 RC, and so the AK 3.2.1 release should include this fix.;;;","26/Jul/22 20:18;rhauch;Merged to the `3.3` branch with permission from the 3.3 release manager.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica fetches to follower should return NOT_LEADER error,KAFKA-14078,13471687,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,15/Jul/22 05:28,25/Jul/22 20:25,13/Jul/23 09:17,25/Jul/22 20:25,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"After the fix for KAFKA-13837, if a follower receives a request from another replica, it will return UNKNOWN_LEADER_EPOCH even if the leader epoch matches. We need to do epoch leader/epoch validation first before we check whether we have a valid replica.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-15 05:28:03.0,,,,,,,,,,"0|z16vq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issues with KafkaStreams.CloseOptions,KAFKA-14076,13471640,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jhughes,jhughes,jhughes,14/Jul/22 20:55,21/Jul/22 14:42,13/Jul/23 09:17,21/Jul/22 14:41,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"The new `close(CloseOptions)` function has a few bugs.  ([https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L1518-L1561)]

Notably, it needs to remove CGs per StreamThread.",,jhughes,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13217,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 21 06:15:19 UTC 2022,,,,,,,,,,"0|z16vfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 21:01;jhughes;CloseOptions was introduced in https://github.com/apache/kafka/commit/9dc332f5ca34b80af369646f767c40c6b189f831.;;;","21/Jul/22 06:15;mjsax;Marking this as blocker for 3.3, because this fixes KIP-812 which has a broken implementation right now. We need to either get this merger or revert the KIP.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crashed MirrorCheckpointConnector appears as running in REST API,KAFKA-14072,13471243,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,mimaison,mimaison,12/Jul/22 15:07,06/Jan/23 10:38,13/Jul/23 09:17,06/Jan/23 10:38,3.1.0,,,,,,,,,,,,,,,3.5.0,,,,,,,KafkaConnect,mirrormaker,,,,,0,,,,,,"In one cluster I had a partially crashed MirrorCheckpointConnector instance. It had stopped mirroring offsets and emitting metrics completely but the connector and its single task were still reporting as running in the REST API.

Looking at the logs, I found this stacktrace:

{code:java}
java.lang.NullPointerException
	at org.apache.kafka.connect.mirror.MirrorCheckpointTask.checkpoint(MirrorCheckpointTask.java:187)
	at org.apache.kafka.connect.mirror.MirrorCheckpointTask.lambda$checkpointsForGroup$2(MirrorCheckpointTask.java:171)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
	at java.base/java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1764)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
	at org.apache.kafka.connect.mirror.MirrorCheckpointTask.checkpointsForGroup(MirrorCheckpointTask.java:173)
	at org.apache.kafka.connect.mirror.MirrorCheckpointTask.sourceRecordsForGroup(MirrorCheckpointTask.java:157)
	at org.apache.kafka.connect.mirror.MirrorCheckpointTask.poll(MirrorCheckpointTask.java:139)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:291)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:248)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:186)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:241)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
WARN [prod-source->sc-prod-target.MirrorCheckpointConnector|task-0] Failure polling consumer state for checkpoints. (org.apache.kafka.connect.mirror.MirrorCheckpointTask) [task-thread-prod-source->sc-prod-target.MirrorCheckpointConnector-0]
{code}

Not sure if it's related but prior this exception, there's quite a lot of:

{code:java}
ERROR [prod-source->sc-prod-target.MirrorCheckpointConnector|task-0] WorkerSourceTask{id=prod-source->sc-prod-target.MirrorCheckpointConnector-0} failed to send record to prod-source.checkpoints.internal:  (org.apache.kafka.connect.runtime.WorkerSourceTask) [kafka-producer-network-thread | connector-producer-prod-source->sc-prod-target.MirrorCheckpointConnector-0]
org.apache.kafka.common.KafkaException: Producer is closed forcefully.
	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:760)
	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:747)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:283)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}

and some users had started consumers in the target cluster hence causing these log lines:

{code:java}
ERROR [prod-source->sc-prod-target.MirrorCheckpointConnector|task-0] [AdminClient clientId=adminclient-137] OffsetCommit request for group id <GROUP_ID> and partition <TP> failed due to unexpected error UNKNOWN_MEMBER_ID. (org.apache.kafka.clients.admin.internals.AlterConsumerGroupOffsetsHandler) [kafka-admin-client-thread | adminclient-137]
{code}

Unfortunately I don't have the full history, so it's unclear if this happened while stopping but the connector stayed in this state for several hours until it was explicitly deleted via the REST API.


",,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14545,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 10:38:19 UTC 2023,,,,,,,,,,"0|z16szs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 10:38;mimaison;This looks like it's the same issue as KAFKA-14545;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2022-34917: Kafka message parsing can cause ooms with small antagonistic payloads,KAFKA-14063,13470821,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,dpcollins-google,dpcollins-google,10/Jul/22 00:01,03/Feb/23 05:46,13/Jul/23 09:17,19/Sep/22 16:16,3.2.0,,,,,,,,,,,,,,,2.8.2,3.0.2,3.1.2,3.2.3,3.3.0,,,generator,,,,,,0,,,,,,"When parsing code receives a payload for a variable length field where the length is specified in the code as some arbitrarily large number (assume INT32_MAX for example) this will immediately try to allocate an ArrayList to hold this many elements, before checking whether this is a reasonable array size given the available data. 

The fix for this is to instead throw a runtime exception if the length of a variably sized container exceeds the amount of remaining data. Then, the worst a user can do is force the server to allocate 8x the size of the actual delivered data (if they claim there are N elements for a container of Objects (i.e. not a byte string) and each Object bottoms out in an 8 byte pointer in the ArrayList's backing array).

This was identified by fuzzing the kafka request parsing code.",,dpcollins-google,millie,mswathi,omkreddy,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 13:56:30 UTC 2022,,,,,,,,,,"0|z16qe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 15:10;omkreddy;CVE Reference: https://nvd.nist.gov/vuln/detail/CVE-2022-34917;;;","22/Sep/22 07:56;millie;Is it possible to consider that the vulnerability is fixed by this commit?
{code:java}
Revision: b7dd40ff2bfb4e0cd726c1168e039d828daf113d
Author: Manikumar Reddy <manikumar.reddy@gmail.com>
Date: 2022/5/16 21:55:02
Message:
MINOR: Add configurable max receive size for SASL authentication requests
This adds a new configuration `sasl.server.max.receive.size` that sets the maximum receive size for requests before and during authentication.
Reviewers: Tom Bentley <tbentley@redhat.com>, Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Manikumar Reddy <manikumar.reddy@gmail.com>
Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
----
Modified: checkstyle/suppressions.xml
Modified: clients/src/main/java/org/apache/kafka/common/config/internals/BrokerSecurityConfigs.java
Modified: clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java
Modified: clients/src/test/java/org/apache/kafka/common/security/TestSecurityConfig.java
Modified: clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java
Modified: clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticatorTest.java
Modified: core/src/main/scala/kafka/server/KafkaConfig.scala
Modified: core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala {code};;;","22/Sep/22 08:25;omkreddy;commit for 2.8 branch: [https://github.com/apache/kafka/commit/14951a83e3fdead212156e5532359500d72f68bc]

commit for 3.0 branch: [https://github.com/apache/kafka/commit/aaceb6b79bfcb1d32874ccdbc8f3138d1c1c00fb]

commit for 3.1 branch: [https://github.com/apache/kafka/commit/c1295662768e64b4467e27c3d5158f95f2307657]

commit for 3.2 branch: [https://github.com/apache/kafka/commit/2bfa24b2bd416e7b8c4a0c566b984c43904fdecb];;;","23/Sep/22 02:12;millie;[~omkreddy]

thanks a lot :);;;","27/Sep/22 05:28;mswathi;hi [~omkreddy],

Will there be an official PR created for this commit, or can we pick up the commits you've mentioned in your previous comment?;;;","27/Sep/22 13:56;omkreddy;There wont be official PR.  Yes, you can pickup the commit from respective branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OAuth client token refresh fails with SASL extensions,KAFKA-14062,13470819,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,09/Jul/22 18:52,12/Jul/22 09:06,13/Jul/23 09:17,12/Jul/22 09:06,3.1.0,3.1.1,3.2.0,3.3,3.3.0,,,,,,,,,,,3.1.2,3.2.1,3.3,,,,,admin,clients,consumer,producer ,security,,0,OAuth,,,,,"While testing OAuth for Connect an issue surfaced where authentication that was successful initially fails during token refresh. This appears to be due to missing SASL extensions on refresh, though those extensions were present on initial authentication.

During token refresh, the Kafka client adds and removes any SASL extensions. If a refresh is attempted during the window when the extensions are not present in the subject, the refresh fails with the following error:
{code:java}
[2022-04-11 20:33:43,250] INFO [AdminClient clientId=adminclient-8] Failed authentication with <host>/<IP> (Authentication failed: 1 extensions are invalid! They are: xxx: Authentication failed) (org.apache.kafka.common.network.Selector){code}",,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-09 18:52:40.0,,,,,,,,,,"0|z16qds:",9223372036854775807,,enether,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction markers may be lost during cleaning if data keys conflict with marker keys,KAFKA-14055,13470592,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,hachikuji,hachikuji,hachikuji,07/Jul/22 19:41,10/Jul/22 17:37,13/Jul/23 09:17,10/Jul/22 17:36,2.4.1,2.5.1,2.6.3,2.7.2,2.8.1,3.0.1,3.1.1,3.2.0,,,,,,,,3.0.2,3.1.2,3.2.1,3.3.0,,,,,,,,,,0,,,,,,"We have been seeing recently hanging transactions occur on streams changelog topics quite frequently. After investigation, we found that the keys used in the changelog topic conflict with the keys used in the transaction markers (the schema used in control records is 4 bytes, which happens to be the same for the changelog topics that we investigated). When we build the offset map prior to cleaning, we do properly exclude the transaction marker keys, but the bug is the fact that we do not exclude them during the cleaning phase. This can result in the marker being removed from the cleaned log before the corresponding data is removed when there is a user record with a conflicting key at a higher offset. A side effect of this is a so-called ""hanging"" transaction, but the bigger problem is that we lose the atomicity of the transaction. ",,fvaleri,hachikuji,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-07 19:41:49.0,,,,,,,,,,"0|z16ozk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected client shutdown as TimeoutException is thrown as IllegalStateException,KAFKA-14054,13470553,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,donald.n,donald.n,07/Jul/22 14:52,14/Apr/23 19:00,13/Jul/23 09:17,14/Apr/23 19:00,3.1.0,3.1.1,3.2.0,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,streams,,,,,,0,,,,,," Re: https://forum.confluent.io/t/bug-timeoutexception-is-thrown-as-illegalstateexception-causing-client-shutdown/5460/2

1) TimeoutException is thrown as IllegalStateException in {_}org.apache.kafka.streams.processor.internals.StreamTask#commitNeeded{_}. Which causes the client to shutdown in {_}org.apache.kafka.streams.KafkaStreams#getActionForThrowable{_}.

2) Should Timeout be a recoverable error which is expected to be handled by User?

3) This issue is exposed by change KAFKA-12887 which was introduced in kafka-streams ver 3.1.0

*code referenced*
{code:java|title=org.apache.kafka.streams.processor.internals.StreamTask#commitNeeded}
public boolean commitNeeded() {
        if (commitNeeded) {
            return true;
        } else {
            for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {
                final TopicPartition partition = entry.getKey();
                try {
                    final long offset = mainConsumer.position(partition);
                    if (offset > entry.getValue() + 1) {
                        commitNeeded = true;
                        entry.setValue(offset - 1);
                    }
                } catch (final TimeoutException error) {
                    // the `consumer.position()` call should never block, because we know that we did process data
                    // for the requested partition and thus the consumer should have a valid local position
                    // that it can return immediately

                    // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`
                    throw new IllegalStateException(error);
                } catch (final KafkaException fatal) {
                    throw new StreamsException(fatal);
                }
            }

            return commitNeeded;
        }
    }
{code}


{code:java|title=org.apache.kafka.streams.KafkaStreams#getActionForThrowable}
private StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse getActionForThrowable(final Throwable throwable,
                                                                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler) {
        final StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse action;
        if (wrappedExceptionIsIn(throwable, EXCEPTIONS_NOT_TO_BE_HANDLED_BY_USERS)) {
            action = SHUTDOWN_CLIENT;
        } else {
            action = streamsUncaughtExceptionHandler.handle(throwable);
        }
        return action;
    }

    private void handleStreamsUncaughtException(final Throwable throwable,
                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,
                                                final boolean skipThreadReplacement) {
        final StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse action = getActionForThrowable(throwable, streamsUncaughtExceptionHandler);
        if (oldHandler) {
            log.warn(""Stream's new uncaught exception handler is set as well as the deprecated old handler."" +
                    ""The old handler will be ignored as long as a new handler is set."");
        }
        switch (action) {
            case REPLACE_THREAD:
                if (!skipThreadReplacement) {
                    log.error(""Replacing thread in the streams uncaught exception handler"", throwable);
                    replaceStreamThread(throwable);
                } else {
                    log.debug(""Skipping thread replacement for recoverable error"");
                }
                break;
            case SHUTDOWN_CLIENT:
                log.error(""Encountered the following exception during processing "" +
                        ""and Kafka Streams opted to "" + action + ""."" +
                        "" The streams client is going to shut down now. "", throwable);
                closeToError();
                break;
{code}

 *Stacktrace*

{code:java|title=error log kafka-streams v. 3.1.0}
2022-06-22 13:58:35,796 ERROR thread=[com_stmartin_hammer_v3_platform-hammer-facade-fdc90fab-ed3a-4e62-b458-e73f80e6975d-StreamThread-1] logger=o.a.k.s.KafkaStreams - stream-client [com_stmartin_hammer_v3_platform-hammer-facade-fdc90fab-ed3a-4e62-b458-e73f80e6975d] Encountered the following exception during processing and Kafka Streams opted to SHUTDOWN_CLIENT. The streams client is going to shut down now.
org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition com_stmartin_hammer_v3_command_pte_hammercommand--demo--compacted-4 could be determined
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:642)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:576)
Caused by: java.lang.IllegalStateException: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition com_stmartin_hammer_v3_command_pte_hammercommand--demo--compacted-4 could be determined
        at org.apache.kafka.streams.processor.internals.StreamTask.commitNeeded(StreamTask.java:1185)
        at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:1111)
        at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1084)
        at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:1071)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:817)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:604)
        ... 1 common frames omitted
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition com_stmartin_hammer_v3_command_pte_hammercommand--demo--compacted-4 could be determined
2022-06-22 13:58:35,796  INFO thread=[com_stmartin_hammer_v3_platform-hammer-facade-fdc90fab-ed3a-4e62-b458-e73f80e6975d-StreamThread-1] logger=o.a.k.s.KafkaStreams - stream-client [com_stmartin_hammer_v3_platform-hammer-facade-fdc90fab-ed3a-4e62-b458-e73f80e6975d] State transition from RUNNING to PENDING_ERROR
{code}
",,ableegoldman,donald.n,lbrutschy,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 13:20:17 UTC 2022,,,,,,,,,,"0|z16oqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 13:20;lbrutschy;Bug confirmed with 3.1.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft remote controllers do not create metrics reporters,KAFKA-14051,13470418,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rndgstn,rndgstn,rndgstn,06/Jul/22 20:31,15/Aug/22 13:58,13/Jul/23 09:17,15/Aug/22 13:58,3.3,,,,,,,,,,,,,,,,,,,,,,kraft,,,,,,0,,,,,,"KRaft remote controllers (KRaft nodes with the configuration value process.roles=controller) do not create the configured metrics reporters defined by the configuration key metric.reporters.  The reason is because KRaft remote controllers are not wired up for dynamic config changes, and the creation of the configured metric reporters actually happens during the wiring up of the broker for dynamic reconfiguration, in the invocation of DynamicBrokerConfig.addReconfigurables(KafkaBroker).",,dengziming,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-06 20:31:01.0,,,,,,,,,,"0|z16nww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Netty and Jackson for CVE fixes,KAFKA-14044,13469979,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tomncooper,tomncooper,tomncooper,04/Jul/22 16:26,18/Oct/22 22:51,13/Jul/23 09:17,05/Jul/22 07:28,3.2.0,,,,,,,,,,,,,,,3.3.0,,,,,,,core,,,,,,0,security,,,,,"There are a couple of CVEs for netty and Jackson:

Netty: [CVE-2022-24823|https://www.cve.org/CVERecord?id=CVE-2022-24823] - Fixed by upgrading to 4.1.77+

Jackson: [CVE-2020-36518|https://www.cve.org/CVERecord?id=CVE-2020-36518] - Fixed by upgrading to 2.13.0+",,tomncooper,,,,,,,,,,,,,,,,,,KAFKA-14320,,,,,KAFKA-13969,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-04 16:26:17.0,,,,,,,,,,"0|z16lao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix KRaft AlterConfigPolicy usage,KAFKA-14039,13469711,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,01/Jul/22 18:03,01/Sep/22 18:00,13/Jul/23 09:17,15/Aug/22 23:26,,,,,,,,,,,,,,,,3.3.0,3.4.0,,,,,,,,,,,,0,,,,,,"In ConfigurationControlManager, we are currently passing all the configuration values known to the controller down into the AlterConfigPolicy. This does not match the behavior in ZK mode where we only pass configs which were included in the alter configs request.

This can lead to different unexpected behavior in custom AlterConfigPolicy implementations",,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-01 18:03:12.0,,,,,,,,,,"0|z16jn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kraft controller local time not computed correctly.,KAFKA-14036,13469536,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,hachikuji,hachikuji,30/Jun/22 23:05,01/Jul/22 04:07,13/Jul/23 09:17,01/Jul/22 04:07,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"In `ControllerApis`, we are missing the logic to set the local processing end time after `handle` returns. As a consequence of this, the remote time ends up reported as the local time in the request level metrics.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-30 23:05:08.0,,,,,,,,,,"0|z16ik8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QuorumController handleRenounce throws NPE,KAFKA-14035,13469530,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Niket Goel,Niket Goel,Niket Goel,30/Jun/22 22:06,11/May/23 16:56,13/Jul/23 09:17,01/Jul/22 04:09,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.1,3.3.0,,,,,,,,,,,0,,,,,,"Sometimes when the controller is rolled you can encounter the following exception, after which the controller in-memory state seems to become inconsistent with the Metadata Log.

 

[Controller 1] handleRenounce[23]: failed with unknown server exception NullPointerException at epoch -1 in XXXX us. Reverting to last committed offset XXXX.",,akshaykumar001,iblis,Niket Goel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14946,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,Thu Apr 27 08:11:23 UTC 2023,,,,,,,,,,"0|z16iiw:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"27/Apr/23 08:11;akshaykumar001;https://issues.apache.org/jira/browse/KAFKA-14946

 

[2023-04-13 01:49:17,411] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37110, but in the new epoch 37111, the leader is (none). Reverting to last committed offset 28291464. (org.apache.kafka.controller.QuorumController)
[2023-04-13 01:49:17,531] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=37112, voters=[1, 2, 3], electionTimeoutMs=982) (org.apache.kafka.raft.QuorumState)

[2023-04-13 02:00:33,902] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37116, but in the new epoch 37117, the leader is (none). Reverting to last committed offset 28292807. (org.apache.kafka.controller.QuorumController)
[2023-04-13 02:00:33,936] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=37118, voters=[1, 2, 3], electionTimeoutMs=1497) (org.apache.kafka.raft.QuorumState)

[2023-04-13 02:00:35,014] ERROR [Controller 1] processBrokerHeartbeat: unable to start processing because of NotControllerException. (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:12:21,883] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37129, but in the new epoch 37131, the leader is (none). Reverting to last committed offset 28294206. (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] WARN [Controller 1] Renouncing the leadership due to a metadata log event. We were the leader at epoch 37141, but in the new epoch 37142, the leader is (none). Reverting to last committed offset 28294325. (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] INFO [Controller 1] writeNoOpRecord: failed with NotControllerException in 16561838 us (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] INFO [Controller 1] maybeFenceReplicas: failed with NotControllerException in 8520846 us (org.apache.kafka.controller.QuorumController)

[2023-04-13 02:13:41,328] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2023-04-13 02:13:41,329] INFO [BrokerLifecycleManager id=1] Unable to send a heartbeat because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2023-04-13 02:13:41,351] ERROR Encountered fatal fault: exception while renouncing leadership (org.apache.kafka.server.fault.ProcessExitingFaultHandler)
java.lang.NullPointerException
        at org.apache.kafka.timeline.SnapshottableHashTable$HashTier.mergeFrom(SnapshottableHashTable.java:125)
        at org.apache.kafka.timeline.Snapshot.mergeFrom(Snapshot.java:68)
        at org.apache.kafka.timeline.SnapshotRegistry.deleteSnapshot(SnapshotRegistry.java:236)
        at org.apache.kafka.timeline.SnapshotRegistry$SnapshotIterator.remove(SnapshotRegistry.java:67)
        at org.apache.kafka.timeline.SnapshotRegistry.revertToSnapshot(SnapshotRegistry.java:214)
        at org.apache.kafka.controller.QuorumController.renounce(QuorumController.java:1232)
        at org.apache.kafka.controller.QuorumController.access$3300(QuorumController.java:150)
        at org.apache.kafka.controller.QuorumController$QuorumMetaLogListener.lambda$handleLeaderChange$3(QuorumController.java:1076)
        at org.apache.kafka.controller.QuorumController$QuorumMetaLogListener.lambda$appendRaftEvent$4(QuorumController.java:1101)
        at org.apache.kafka.controller.QuorumController$ControlEvent.run(QuorumController.java:496)
        at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
        at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
        at java.lang.Thread.run(Thread.java:750)
[2023-04-13 02:13:41,385] INFO [BrokerServer id=1] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dequeue time for forwarded requests is ignored to set,KAFKA-14032,13469075,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,yufeiyan1220,yufeiyan1220,29/Jun/22 10:23,06/Jul/22 20:21,13/Jul/23 09:17,06/Jul/22 20:21,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"It seems like `requestDequeueTimeNanos` is ignored to set.
As a property of a `Request object`, `requestDequeueTimeNanos` is set only when handlers manage to poll and handle this request from `requestQueue`, however, handlers only poll the request from envelop request once, but calls handle method twice, which lead to an ignorance of `requestDequeueTimeNanos` for parsed forwarded requests.

The parsed envelop requests have `requestDequeueTimeNanos` = -1, and it affect the correctness of statistics and metrics of `LocalTimeMs`.",,yufeiyan1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-29 10:23:56.0,,,,,,,,,,"0|z16glk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer stuck during cooperative rebalance for Commit offset in onJoinPrepare,KAFKA-14024,13468435,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,aiquestion,aiquestion,aiquestion,26/Jun/22 09:07,06/Sep/22 02:29,13/Jul/23 09:17,20/Jul/22 02:13,3.2.0,,,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,clients,,,,,,0,new-consumer-threading-should-fix,,,,,"Hi 

In https://issues.apache.org/jira/browse/KAFKA-13310. we tried to fix a issue that consumer#poll(duration) will be returned after the provided duration. It's because if rebalance needed, we'll try to commit current offset first before rebalance synchronously. And if the offset committing takes too long, the consumer#poll will spend more time than provided duration. To fix that, we change commit sync with commit async before rebalance (i.e. onPrepareJoin).

 

However, in this ticket, we found the async commit will keep sending a new commit request during each Consumer#poll, because the offset commit never completes in time. The impact is that the existing consumer will be kicked out of the group after rebalance timeout without joining the group. That is, suppose we have consumer A in group G, and now consumer B joined the group, after the rebalance, only consumer B in the group.

 

The workaround for this issue is to change the assignor back to eager assignors, ex: StickyAssignor, RoundRobinAssignor.

 

To fix the issue, we come out 2 solutions:
 # we can explicitly wait for the async commit complete in onPrepareJoin, but that would let the KAFKA-13310 issue happen again.
 # 2.we can try to keep the async commit offset future currently inflight. So that we can make sure each Consumer#poll, we are waiting for the future completes

 

Besides, there's also another bug found during fixing this bug. Before KAFKA-13310, we commitOffset sync with rebalanceTimeout, which will retry when retriable error until timeout. After KAFKA-13310, we thought we have retry, but we'll retry after partitions revoking. That is, even though the retried offset commit successfully, it still causes some partitions offsets un-committed, and after rebalance, other consumers will consume overlapping records.

 

 

===

[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L752]

 

we didn't wait for client to receive commit offset response here, so onJoinPrepareAsyncCommitCompleted will be false in cooperative rebalance, and client will loop in invoking onJoinPrepare.

I think the EAGER mode don't have this problem because it will revoke the partitions even if onJoinPrepareAsyncCommitCompleted=false and will not try to commit next round.

reproduce:
 * single node Kafka version 3.2.0 && client version 3.2.0
 * topic1 have 5 partititons
 * start a consumer1 (cooperative rebalance)
 * start another consumer2 (same consumer group)
 * consumer1 will hang for a long time before re-join
 * from server log consumer1 rebalance timeout before joineGroup and re-join with another memberId

consume1's log keeps printing:

16:59:16 [main] DEBUG o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-xx-1, groupId=xxx] Executing onJoinPrepare with generation 54 and memberId consumer-xxx-1-fd3d04a8-009a-4ed1-949e-71b636716938 (ConsumerCoordinator.java:739)
16:59:16 [main] DEBUG o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-xxx-1, groupId=xxx] Sending asynchronous auto-commit of offsets \{topic1-4=OffsetAndMetadata{offset=5, leaderEpoch=0, metadata=''}} (ConsumerCoordinator.java:1143)

 

and coordinator's log:

[2022-06-26 17:00:13,855] INFO [GroupCoordinator 0]: Preparing to rebalance group xxx in state PreparingRebalance with old generation 56 (__consumer_offsets-30) (reason: Adding new member consumer-xxx-1-fa7fe5ec-bd2f-42f6-b5d7-c5caeafe71ac with group instance id None; client reason: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)) (kafka.coordinator.group.GroupCoordinator)
[2022-06-26 17:00:43,855] INFO [GroupCoordinator 0]: Group xxx removed dynamic members who haven't joined: Set(consumer-xxx-1-d62a0923-6ca6-48dd-a84e-f97136d4603a) (kafka.coordinator.group.GroupCoordinator)
[2022-06-26 17:00:43,856] INFO [GroupCoordinator 0]: Stabilized group xxx generation 57 (__consumer_offsets-30) with 3 members (kafka.coordinator.group.GroupCoordinator)
[2022-06-26 17:00:44,048] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group xxx in CompletingRebalance state. Created a new member id consumer-xxx-1-f0298aa0-711c-498e-bdfd-1dd205d7b640 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2022-06-26 17:00:44,053] INFO [GroupCoordinator 0]: Assignment received from leader consumer-xxx-1-e842a14c-eff7-4b55-9463-72b9c2534afd for group xxx for generation 57. The group has 3 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2022-06-26 17:00:44,243] INFO [GroupCoordinator 0]: Preparing to rebalance group xxx in state PreparingRebalance with old generation 57 (__consumer_offsets-30) (reason: Adding new member consumer-xxx-1-f0298aa0-711c-498e-bdfd-1dd205d7b640 with group instance id None; client reason: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)) (kafka.coordinator.group.GroupCoordinator)",,aiquestion,guozhang,kirktrue,mumrah,pnee,showuon,tombentley,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 02:29:57 UTC 2022,,,,,,,,,,"0|z16d1s:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,"27/Jun/22 06:05;showuon;[~aiquestion] , thanks for reporting the issue. One question to clarify: is this issue always happening while cooperative rebalancing? I'm wondering why does the test not catch this issue.;;;","27/Jun/22 06:50;aiquestion;[~showuon] yes it always happens. ;;;","27/Jun/22 06:59;showuon;I see. Let's add some tests to make sure it won't happen again. Thanks.;;;","28/Jun/22 21:32;guozhang;cc [~kirktrue] [~philipnee] as well.;;;","29/Jun/22 10:26;showuon;[~aiquestion] , I updated the Jira title and the description. Welcome to modify it to make it much clear.;;;","15/Jul/22 15:01;mumrah;[~showuon] / [~guozhang] is this a blocker for a bug fix release on 3.2.x? I'm currently working on the 3.2.1 plan and this issue is the only open blocker. How close are we to having a fix? 

Could we wait on this issue until a future release (3.2.2)?;;;","16/Jul/22 08:10;showuon;[~mumrah] , thanks for asking. I think this is a blocker for v3.2.1 since when consumer in v3.2.0 with cooperative assignor, consumer rebalance be hanging. PR has already 3 rounds of review. It should be completed soon. 

cc [~aiquestion] ;;;","19/Jul/22 20:53;guozhang;Hello [~mumrah], I took a look at the ticket and also the PR (https://github.com/apache/kafka/pull/12349/files) as well, and I agree with [~showuon] that this is a pretty bad regression that we should consider fixing asap and hence worthy as a blocker for 3.2.1.

As for the PR, personally I'd simplify it a bit than the current fix, to `onJoinPrepare` more re-entrant and idempotent: more specifically when the caller thread of `poll` enters `onJoinPrepare`, it will check if there's already a commit in-flight already and is completed, and if not send out the request and return from `onJoinPrepare` immediately, and hence return from the `poll` call as well; and the next `poll` call would re-enter `onJoinPrepare` and check if the commit request has completed; only if the maintained commit future has been completed then would it continue within the function to revoke partitions, trigger callbacks etc. In this way we would not need a separate timer inside the `onJoinPrepare` for the commit itself. But since [~showuon] is almost done reviewing it I think I would leave it to him, rather not block on merging it.

In the new rebalance protocol (KIP-848) we would have a much simpler model on the client side so hopefully we would not fall in this awkward design pattern any more.;;;","19/Jul/22 20:57;guozhang;Thanks to [~aiquestion] for filing this and also submitting the PR, I've added you as a contributor and assigned the ticket to you too.;;;","20/Jul/22 02:13;showuon;> In this way we would not need a separate timer inside the `onJoinPrepare` for the commit itself. 

[~guozhang] , thanks for the suggestion. Yes, that looks simpler! I like it. But since release time approaching and there will be new rebalance protocol (KIP-848) coming soon, I'm going to merge it as is. But again, thanks for the comment. I learned something from it. Thanks.

 

[~aiquestion] , thanks again for finding the issue and the PR!;;;","06/Sep/22 02:29;pnee;Hey [~aiquestion] and [~showuon]  - Seem like the changes here is causing duplicated consumption (KAFKA-14196), which is a regression, if I'm understanding the logics correctly.  In short, I think the poll loop now will continue to fetch data and continue to wait for the async commit to complete.  So the issue here is, the fetcher will progress beyond the current commit, and once the async commit is completed, the partition will get revoked and the previous progress is lost.  I think we need to get the timing right to reproduce this issue, i.e. I think it usually happens when the async commit doesn't complete during the initial poll loop.

 

I could be wrong, but that's my finding so far.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in Producer,KAFKA-14020,13468347,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,alivshits,vvcephei,vvcephei,24/Jun/22 19:59,22/Jul/22 07:27,13/Jul/23 09:17,20/Jul/22 15:31,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,producer ,,,,,,0,,,,,,"[https://github.com/apache/kafka/commit/f7db6031b84a136ad0e257df722b20faa7c37b8a] introduced a 10% performance regression in the KafkaProducer under a default config.

 

The context for this result is a benchmark that we run for Kafka Streams. The benchmark provisions 5 independent AWS clusters, including one broker node on an i3.large and one client node on an i3.large. During a benchmark run, we first run the Producer for 10 minutes to generate test data, and then we run Kafka Streams under a number of configurations to measure its performance.

Our observation was a 10% regression in throughput under the simplest configuration, in which Streams simply consumes from a topic and does nothing else. That benchmark actually runs faster than the producer that generates the test data, so its thoughput is bounded by the data generator's throughput. After investigation, we realized that the regression was in the data generator, not the consumer or Streams.

We have numerous benchmark runs leading up to the commit in question, and they all show a throughput in the neighborhood of 115,000 records per second. We also have 40 runs including and after that commit, and they all show a throughput in the neighborhood of 105,000 records per second. A test on [trunk with the commit reverted |https://github.com/apache/kafka/pull/12342] shows a return to around 115,000 records per second.

Config:
{code:java}
final Properties properties = new Properties();
properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker);
properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
{code}
Here's the producer code in the data generator. Our tests were running with three produceThreads.
{code:java}
 for (int t = 0; t < produceThreads; t++) {
    futures.add(executorService.submit(() -> {
        int threadTotal = 0;
        long lastPrint = start;
        final long printInterval = Duration.ofSeconds(10).toMillis();
        long now;
        try (final org.apache.kafka.clients.producer.Producer<String, String> producer = new KafkaProducer<>(producerConfig(broker))) {
            while (limit > (now = System.currentTimeMillis()) - start) {
                for (int i = 0; i < 1000; i++) {
                    final String key = keys.next();
                    final String data = dataGen.generate();

                    producer.send(new ProducerRecord<>(topic, key, valueBuilder.apply(key, data)));

                    threadTotal++;
                }

                if ((now - lastPrint) > printInterval) {
                    System.out.println(Thread.currentThread().getName() + "" produced "" + numberFormat.format(threadTotal) + "" to "" + topic + "" in "" + Duration.ofMillis(now - start));
                    lastPrint = now;
                }
            }
        }
        total.addAndGet(threadTotal);
        System.out.println(Thread.currentThread().getName() + "" finished ("" + numberFormat.format(threadTotal) + "") in "" + Duration.ofMillis(now - start));
    }));
}{code}
As you can see, this is a very basic usage.",,junrao,kirktrue,rleslie,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10888,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 20 15:31:36 UTC 2022,,,,,,,,,,"0|z16ci8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/22 16:11;vvcephei;{color:#1d1c1d}FYI, just setting the partitioner back to the {color}{{DefaultPartitioner}}{color:#1d1c1d} does not appear to help. The throughput of that test was 105k±2k records per second.{color}

{color:#1d1c1d}Code under test: {color}[https://github.com/apache/kafka/commit/6c67adb8beedafca0316d1c9ec4a3c219aaec219];;;","12/Jul/22 21:09;vvcephei;Hey [~alivshits] , thanks for your work on [https://github.com/apache/kafka/pull/12365] .

I've just re-run the same benchmark above and confirmed that your PR fixes the perf regression. Thank you!

As a reminder, this was the baseline for ""good"" performance:
Commit: [{{e3202b9}}|https://github.com/apache/kafka/commit/e3202b99999ef4c63aab2e5ab049978704282792] (the parent of the problematic commit)
TPut: *118k±1k*

And when I ran the same benchmark on [{{3a6500b}}|https://github.com/apache/kafka/commit/3a6500bb12b8c5716f7d99b6cec1c521f6f029c2] , I got:
TPut: *117k±1k*;;;","20/Jul/22 15:31;junrao;merged the PR to 3.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revoke more partitions than expected in Cooperative rebalance,KAFKA-14016,13462972,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,aiquestion,aiquestion,23/Jun/22 12:21,23/May/23 03:19,13/Jul/23 09:17,03/May/23 16:58,3.3.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,clients,,,,,,0,new-rebalance-should-fix,,,,,"In https://issues.apache.org/jira/browse/KAFKA-13419 we found that some consumer didn't reset generation and state after sync group fail with REABALANCE_IN_PROGRESS error.

So we fixed it by reset generationId (no memberId) when  sync group fail with REABALANCE_IN_PROGRESS error.

But this change missed the reset part, so another change made in https://issues.apache.org/jira/browse/KAFKA-13891 make this works.

After apply this change, we found that: sometimes consumer will revoker almost 2/3 of the partitions with cooperative enabled. Because if a consumer did a very quick re-join, other consumers will get REABALANCE_IN_PROGRESS in syncGroup and revoked their partition before re-jion. example:
 # consumer A1-A10 (ten consumers) joined and synced group successfully with generation 1 
 # New consumer B1 joined and start a rebalance
 # all consumer joined successfully and then A1 need to revoke partition to transfer to B1
 # A1 do a very quick syncGroup and re-join, because it revoked partition
 # A2-A10 didn't send syncGroup before A1 re-join, so after the send syncGruop, will get REBALANCE_IN_PROGRESS
 # A2-A10 will revoke there partitions and re-join

So in this rebalance almost every partition revoked, which highly decrease the benefit of Cooperative rebalance 

I think instead of ""{*}resetStateAndRejoin{*} when *RebalanceInProgressException* errors happend in {*}sync group{*}"" we need another way to fix it.

Here is my proposal:
 # revert the change in https://issues.apache.org/jira/browse/KAFKA-13891
 # In Server Coordinator handleSyncGroup when generationId checked and group state is PreparingRebalance. We can send the assignment along with the error code REBALANCE_IN_PROGRESS. ( i think it's safe since we verified the generation first )
 # When get the REBALANCE_IN_PROGRESS error in client, try to apply the assignment first and then set the rejoinNeeded = true to make it re-join immediately ",,ableegoldman,aiquestion,dajac,guozhang,kirktrue,pnee,showuon,tmancill,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 19:00;tmancill;CooperativeStickyAssignorBugReproduction.java;https://issues.apache.org/jira/secure/attachment/13056744/CooperativeStickyAssignorBugReproduction.java",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 00:06:13 UTC 2023,,,,,,,,,,"0|z15fbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 21:19;guozhang;Thanks [~aiquestion] for filing this ticket! Also cc [~kirktrue] [~philipnee] to bring to their radar.;;;","28/Jun/22 21:28;guozhang;In the long run, as we refactored our rebalance protocol (KIP incoming :) this issue should be gone as we would not have REBALANCE_IN_PROGRESS anymore, since the brokers take full responsibility on the installation of the new assignment.

At the moment, though, I feel changing the broker code may not be most effective since that requires upgrading the brokers, which may be preferred by many users.

I'm wondering if we can just make the change on the client only, i.e. when we are in cooperative mode, instead of reset state and revoke everything, we only reset the generation state and re-join the group with the existing assignment. Would that also be applicable?;;;","29/Jun/22 16:00;aiquestion;[~guozhang] do you mean, set assignment's generation to current generation if client get a REBALANCE_IN_PROGRESS in syncGroupResponse?

Yes, I think that can work.  Client will ignore 1 round of assignment
 * if the assignment is adding partition: the partition will be paused 1 more round of rebalane
 * if the assignment is revoking parititon: need another round rebalance to revoke the partition

In normal cases only the Consumer which need to revoke partition will trigger a re-join, the total partitions need to be revoked is descreasing and will finally get to a Stable.;;;","27/Oct/22 08:23;ableegoldman;Hey, I realize there's a lot of history leading up to this issue and the associated ""fix"", so forgive me for missing anything while I'm getting up to speed – but taking a step back, before we jump into the discussion about alternative fixes for KAFKA-13891 can we flesh out the actual underlying problem & take stock of what symptoms people have actually seen vs theorized about?

Sorry for pushing back on this, it just seems like we've been playing whack-a-mole with these rebalancing issues lately, and the moles have started to whack us back. I just want to encourage us all to approach this carefully so we're not having the exact same conversation and reaching for yet more alternative fixes by the next release.

[~aiquestion] I guess this question is mostly directed at you, as the original reporter of KAFKA-13891: were you able to reproduce or experienced this in a real application, or was this mainly filed to follow up on the suggestion for it in KAFKA-13419? Sorry to repeat a bit of the conversation over on KAFKA-13419, but for full context here I was curious about the actual symptoms of KAFKA-13891 vs the scenario outlined in that ticket. Specifically, I think we need to expand and/or elaborate on the effect of having an old (but valid) generation when the consumer is still the most recent owner to claim its given partitions. The sticky assignment algorithm should account for this and IIRC will basically consider whoever has the highest valid generation for a partition as its previous owner, which in this case would always be the consumer that received this REBALANCE_IN_PROGRESS error.

 

To summarize, basically I'm not convinced that failing to reset the generation has any impact on the assignor in this case, ie that there is an actual bug/problem described in KAFKA-13891 – in fact, forcing the consumer to reset everything when a rebalance is restarted seems actively detrimental, as evidenced by this exact ticket. I would vote for just reverting the changes we made for that and call it a day until/unless we see concrete evidence/symptoms that are impacting a real application.

[~showuon] / [~aiquestion] / [~guozhang] Thoughts?;;;","27/Oct/22 09:58;showuon;[~ableegoldman] , thanks for the comment. Yes, we have the logic in sticky assignor to protect multiple consumer claiming the same partition in the same highest generation. The original thought is that the same logic didn't exist in custom assignor. But after [KIP-792|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336614] get implemented and merged (which should happen in next release v3.4.0), we should not worry about it anymore.

Therefore, for this issue, I'd also vote for just reverting the changes.

Thanks.

 

 ;;;","27/Oct/22 15:59;aiquestion;[~ableegoldman] 

Yes, we experienced KAFKA-13891

Here is the full history of our case:

We have several consumer group which has more than 2000 consumers. And we are using Kafka Broker 2.3 && Kakfa Client 2.5. We enabled the build-in StaticMembership && Coopearative Rebalance to avoid STW time in rebalance.
 # we found that in some cases the partition will be duplicated assigned, so we patched KAFKA-12984  , KAFKA-12983,  KAFKA-13406
 # after we deployed online, we found that some consumer group will rebalance for a long time ( 2 hours) before it finally get Stable,  so we then patched KAFKA-13891.
 # after deployed, we experienced more partition lag when rebalance happens. Then i created this issue and try to get some advise.
 # We finally workaround it by 'ignore the generation value when leader calculate assignment' ( just set every memeber's generation to unknonw )  && revert KAFKA-13891.  And after we go online for more than 2 months, it looks good now.

 

For ""The sticky assignment algorithm should account for this and IIRC will basically consider whoever has the highest valid generation for a partition as its previous owner"", i think in the code does't implement in this way [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java#L127]

Please kindly correct me if i'm wrong. In this code we clear all previous owned parittions if we got a higer geneartion, so only the ownedPartitions with highest generation will be valid

 

I think ""The sticky assignment algorithm should account for this and IIRC will basically consider whoever has the highest valid generation for a partition as its previous owner"" is also a fix for this.

 ;;;","27/Oct/22 16:12;aiquestion;I also vote for reverting KAFKA-13891 since the case happens randomly and doesn't have a big impact.

But i don't think  [KIP-792|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336614] can solve this problem. It moved generationId from userData to ConsumerProtocolSubscription and didn't change the logic of build-in CooperativeStickyAssignor.;;;","27/Oct/22 23:53;ableegoldman;[~aiquestion] would you be interested in submitting a PR for reverting  KAFKA-13891? It still needs a PR but Luke or I can merge it as soon as the PR build finishes running. In the meantime I'll look into the assignor's handling of this kind of thing and submit a patch if we're missing the logic for this (which from your report, it sounds like we are);;;","28/Oct/22 05:44;aiquestion;[~ableegoldman] Okay~ Submitted one : https://github.com/apache/kafka/pull/12794;;;","28/Oct/22 09:23;dajac;> The sticky assignment algorithm should account for this and IIRC will basically consider whoever has the highest valid generation for a partition as its previous owner

I think that it does not work like this at the moment but it should. ;;;","22/Mar/23 21:01;pnee;Hey all, in particular, [~showuon] [~aiquestion] 

I've been trying to understand and watch this issue for a while - I'm curious, what exactly do we want to achieve here? Is it that we want to minimize the partition movement as much as possible? In the case when all partitions are correctly assigned. If one consumer joins with an earlier generation, is the desired behavior to have nothing revoked? (Assuming the partition assignment is already in a good state).

 

I wrote a small test to demonstrate what I think we want:
{code:java}
public void testExpiredGenerationNoPartitionMovement() {
Map<String, Integer> partitionsPerTopic = new HashMap<>();
partitionsPerTopic.put(topic, 3);

subscriptions.put(consumer1, buildSubscriptionV1(topics(topic), partitions(tp(topic, 0)), 2));
subscriptions.put(consumer2, buildSubscriptionV1(topics(topic), partitions(tp(topic, 1), tp(topic, 2)), 1));

Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);
assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));
assertEquals(partitions(tp(topic, 1), tp(topic, 2)), assignment.get(consumer2));
...
}{code};;;","23/Mar/23 01:37;showuon;It's been a while that I cannot remember it clearly. But it looks like this issue no longer existed after we revert the previous change. So I think we can close it.;;;","23/Mar/23 01:44;pnee;Hmm, maybe it's a different issue.  For 3.3.2 we are seeing partitions being assigned before revocation.  I think that violates the contract.;;;","23/Mar/23 19:03;tmancill;We are seeing this issue with 3.3.2.  Quoting from one of our engineers (who doesn't yet have a Jira account here):
{quote}Our goal is to ensure that {{onPartitionsRevoked()}} happens before {{onPartitionsAssigned()}} for every partition.  From our testing, in cases where partitions from younger generations are part of the rebalance, it is possible to violate this contract and a partition is reassigned in one rebalance cycle.
{quote}
I am attaching our repro case [^CooperativeStickyAssignorBugReproduction.java];;;","24/Mar/23 20:49;pnee;Hey [~tmancill] - Thanks for chipping in, I think what you are seeing here is a different issue.  In your example, the consumers joining with a different generation are still holding onto some partitions, and therefore causes these partitions getting assigned and revoked within a single rebalance cycle. I think normally, the consumer with the younger generation would revoke these partitions prior to joining - see the onPartitionsLost.  It appears to be a different issue than this one so... could you open a separate ticket with some client log? I think it might be hard to debug based on your example. Thanks!;;;","24/Mar/23 21:20;pnee;Actually [~tmancill] - I think this is similar to your problem? https://issues.apache.org/jira/browse/KAFKA-14639;;;","24/Mar/23 22:48;pnee;Hey [~showuon] - It doesn't seem like the issue was resolved, but instead, the issue appears sporadically, so it didn't get a follow-up.

I see [~aiquestion] and [~ableegoldman] previously mentioned that we could patch the assignor logic to also account for the partition owner of the latest generation. Is it something we intend to do?;;;","25/Mar/23 00:30;tmancill;Yes - thank you for the redirect [~pnee] !;;;","05/May/23 00:06;ableegoldman;Nice job [~pnee] – thanks for taking the time to patch this finally. To clarify, since there are a lot of tickets and PRs floating around here, the assignor fix is in [pull/13550|https://github.com/apache/kafka/pull/13550];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigProvider with ttl fails to restart tasks,KAFKA-14015,13454616,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yash.mayya,rozza,rozza,21/Jun/22 17:07,06/Sep/22 13:06,13/Jul/23 09:17,06/Sep/22 13:06,,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"According to the [KIP-297|https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations#KIP297:ExternalizingSecretsforConnectConfigurations-SecretRotation]:
{quote} * When the Herder receives the onChange() call, it will check a new connector configuration property config.reload.action which can be one of the following:
 ** The value restart, which means to schedule a restart of the Connector and all its Tasks. This will be the default.
 ** The value none, which means to do nothing.{quote}
However, the [restartConnector|https://github.com/apache/kafka/blob/3.2.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java#L287-L294] method only restarts the connector and does not restart any tasks.  Suggest calling {{restartConnectorAndTasks}} instead.

The result is changed configurations provided by the ConfigProvider are not picked up and existing tasks continue to use outdated configuration.",,rozza,sagarrao,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 13:59:30 UTC 2022,,,,,,,,,,"0|z13zr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/22 09:39;sagarrao;[~rozza] , Thanks for filing this issue. There seems to be a mismatch in what the KIP talks about and what's happening in the code indeed. I think the `restartConnectorAndTasks` was added well after the KIP was written. Just curious, have you noticed that tasks continue working with outdated configuration? In that case, does explicit restart of the task mitigate the issue?

Also, one thing that I would like to point out is that we can't just replace `restartConnector` call with `restartConnectorAndTasks` to get this working. That's because the current `restartConnector` invocation doesn't restart immediately but instead waits for ttl (which could be lease duration for vault keys for example). In that case, we might need a new implementation of this. What do you think about this [~ChrisE] , [~rayokota] , [~yash.mayya] ? 

 ;;;","26/Aug/22 13:52;yash.mayya;Hm this looks like it maybe a bug in the StandaloneHerder. In the DistributedHerder, [restartConnector calls startConnector|https://github.com/apache/kafka/blob/38103ffaa962ef5092baffb884c84f8de3568501/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1023] after stopping the connector which then gets the latest connector config (i.e. after substituting all variable configs with their values from the config transformer). It then asks the connector to supply it with a new set of task configs, and then if it detects that the task configs have changed, it will write the new task configs to the config topic or forward it to the leader if it isn't the leader (which then eventually results in the old set of tasks being stopped and the new tasks being started).;;;","26/Aug/22 13:59;yash.mayya;In essence, this should work as expected when Connect is being run in distributed mode. In standalone mode, it looks like there is a similar flow [here|https://github.com/apache/kafka/blob/38103ffaa962ef5092baffb884c84f8de3568501/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java#L392-L407] (i.e. get latest connector configs -> ask connector for task configs -> check if there's a change in the task configs -> if so, stop old set of tasks and start new set of tasks). However, that seems to only be called if i) configs are updated directly via PUT /config endpoint; ii) connector explicitly requests task reconfiguration via its context; iii) connector is resumed via the REST API. So we might need to make some changes to ensure that task reconfiguration is requested when [restartConnector|https://github.com/apache/kafka/blob/38103ffaa962ef5092baffb884c84f8de3568501/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java#L287] is called in the StandaloneHerder (might be as simple as adding a call to [updateConnectorTasks|https://github.com/apache/kafka/blob/38103ffaa962ef5092baffb884c84f8de3568501/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java#L392-L407] in startConnector);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"passing a ""method"" into the `Utils.closeQuietly` method cause NPE",KAFKA-14012,13450940,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sagarrao,showuon,showuon,20/Jun/22 09:16,23/Aug/22 14:26,13/Jul/23 09:17,20/Aug/22 09:53,3.2.0,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"Utils.closeQuietly method accepts `AutoCloseable` object, and close it. But there are some places we passed ""method"" into Utils.closeQuietly, which causes the object doesn't get closed as expected. 

I found it appeared in:

- WorkerConnector
- AbstractWorkerSourceTask
- KafkaConfigBackingStore

 ",,ChrisEgerton,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 13:49:35 UTC 2022,,,,,,,,,,"0|z13duo:",9223372036854775807,,ChrisEgerton,,,,,,,,,,,,,,,,,,"20/Jun/22 13:32;ChrisEgerton;[~showuon] when does this no-op occur? The bugs that I've found related to {{Utils::closeQuietly}} have been NPEs caused by method references of null objects; I believe as long as the object is non-null and the correct method is referenced, things should go smoothly.;;;","20/Jun/22 13:49;showuon;You're right, it will be NPE after passing in the ""method"" as parameter. Thanks for correction. I've updated the JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alterISR request won't retry when receiving retriable error,KAFKA-14010,13450887,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,20/Jun/22 03:02,05/Aug/22 15:38,13/Jul/23 09:17,01/Jul/22 03:14,3.2.0,,,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,core,,,,,,0,,,,,,"When submitting the AlterIsr request, we register a future listener to handle the response [here|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/cluster/Partition.scala#L1585-L1610]. When receiving retriable error, we expected the AlterIsr request will get retried. And then, we'll re-submit the request again. 

However, before the future listener got called, we didn't clear the `unsentIsrUpdates`, which causes we failed to ""enqueue"" the request because we thought there's an in-flight request. We use ""try/finally"" to make sure the unsentIsrUpdates got cleared ([here|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/AlterPartitionManager.scala#L362-L370]), but it happened ""after"" we retry the request

Although the AlterIsr request will get sent next time when the follower sent next fetch request to the leader, we still need to fix this issue to make sure the AlterIsr request is sent successfully as we expected.",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-20 03:02:30.0,,,,,,,,,,"0|z13diw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebalance timeout should be updated when static member rejoins,KAFKA-14009,13450789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,zoushengfu,zoushengfu,zoushengfu,19/Jun/22 06:47,24/Nov/22 14:23,13/Jul/23 09:17,24/Nov/22 14:22,2.3.1,2.6.1,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,consumer,core,,,,,0,,,,,,"When consumer use static membership rebalance protocol, consumer want to reduce rebalance timeout but it do not take effect",,zoushengfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-19 06:47:14.0,,,,,,,,,,"0|z13cx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect header converters are never closed,KAFKA-14007,13450687,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sagarrao,ChrisEgerton,ChrisEgerton,17/Jun/22 20:25,23/Aug/22 14:26,13/Jul/23 09:17,20/Aug/22 09:53,,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"The [HeaderConverter interface|https://github.com/apache/kafka/blob/1e21201ea24389bdaccb8a462f3a53e356b58a58/connect/api/src/main/java/org/apache/kafka/connect/storage/HeaderConverter.java#L27] extends {{Closeable}}, but {{HeaderConverter::close}} is never actually invoked anywhere. We can and should start invoking it, probably wrapped in [Utils::closeQuietly|https://github.com/apache/kafka/blob/1e21201ea24389bdaccb8a462f3a53e356b58a58/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L999-L1010] so that any invalid logic in that method for custom header converters that has to date gone undetected will not cause new task failures.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-17 20:25:17.0,,,,,,,,,,"0|z13cao:",9223372036854775807,,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinGroupRequestData 'reason' can be too large,KAFKA-13998,13450305,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jhughes,jhughes,jhughes,15/Jun/22 21:29,30/Jun/22 15:55,13/Jul/23 09:17,20/Jun/22 14:49,3.2.0,,,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,,,,,,,0,,,,,,"We saw an exception like this: 

```org.apache.kafka.streams.errors.StreamsException: java.lang.RuntimeException: 'reason' field is too long to be serialized 3 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:627) 4 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:551) 5Caused by: java.lang.RuntimeException: 'reason' field is too long to be serialized 6 at org.apache.kafka.common.message.JoinGroupRequestData.addSize(JoinGroupRequestData.java:465) 7 at org.apache.kafka.common.protocol.SendBuilder.buildSend(SendBuilder.java:218) 8 at org.apache.kafka.common.protocol.SendBuilder.buildRequestSend(SendBuilder.java:187) 9 at org.apache.kafka.common.requests.AbstractRequest.toSend(AbstractRequest.java:101) 10 at org.apache.kafka.clients.NetworkClient.doSend(NetworkClient.java:524) 11 at org.apache.kafka.clients.NetworkClient.doSend(NetworkClient.java:500) 12 at org.apache.kafka.clients.NetworkClient.send(NetworkClient.java:460) 13 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:499) 14 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:255) 15 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236) 16 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215) 17 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:437) 18 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:371) 19 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:542) 20 at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1271) 21 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235) 22 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1215) 23 at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:969) 24 at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:917) 25 at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:736) 26 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:589) 27 ... 1 more```

This appears to be caused by the code passing an entire stack trace in the `rejoinReason`.  See https://github.com/apache/kafka/blob/3.2.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L481",,jhughes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-15 21:29:38.0,,,,,,,,,,"0|z139y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.cleaner.io.max.bytes.per.second cannot be changed dynamically ,KAFKA-13996,13450240,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tyamashi-oss,tyamashi-oss,tyamashi-oss,15/Jun/22 13:49,08/Jul/22 13:24,13/Jul/23 09:17,08/Jul/22 13:04,3.2.0,,,,,,,,,,,,,,,3.3.0,,,,,,,config,core,log cleaner,,,,0,,,,,,"- log.cleaner.io.max.bytes.per.second cannot be changed dynamically using bin/kafka-configs.sh
- Reproduction procedure:
-# Create a topic with cleanup.policy=compact
{code:java}
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --replication-factor 1 --partitions 1 --topic my-topic --config cleanup.policy=compact --config cleanup.policy=compact --config segment.bytes=104857600 --config compression.type=producer
{code}
-# Change log.cleaner.io.max.bytes.per.second=10485760 using bin/kafka-configs.sh
{code:java}
bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-default --alter --add-config log.cleaner.io.max.bytes.per.second=10485760
{code}
-# Send enough messages(> segment.bytes=104857600) to activate Log Cleaner
-# logs/log-cleaner.log, configuration by log.cleaner.io.max.bytes.per.second=10485760 is not reflected and Log Cleaner does not slow down (>= log.cleaner.io.max.bytes.per.second=10485760).
{code:java}
[2022-06-15 14:52:14,988] INFO [kafka-log-cleaner-thread-0]:
        Log cleaner thread 0 cleaned log my-topic-0 (dirty section = [39786, 81666])
        3,999.0 MB of log processed in 2.7 seconds (1,494.4 MB/sec).
        Indexed 3,998.9 MB in 0.9 seconds (4,218.2 Mb/sec, 35.4% of total time)
        Buffer utilization: 0.0%
        Cleaned 3,999.0 MB in 1.7 seconds (2,314.2 Mb/sec, 64.6% of total time)
        Start size: 3,999.0 MB (41,881 messages)
        End size: 0.1 MB (1 messages)
        100.0% size reduction (100.0% fewer messages)
 (kafka.log.LogCleaner)
{code}
- Problem cause:
-- log.cleaner.io.max.bytes.per.second is used in Throttler in LogCleaner, however, it is only passed to Throttler at initialization time.
--- https://github.com/apache/kafka/blob/4380eae7ceb840dd93fee8ec90cd89a72bad7a3f/core/src/main/scala/kafka/log/LogCleaner.scala#L107-L112
-- Need to change Throttler configuration value at reconfigure() of LogCleaner.
 --- https://github.com/apache/kafka/blob/4380eae7ceb840dd93fee8ec90cd89a72bad7a3f/core/src/main/scala/kafka/log/LogCleaner.scala#L192-L196
- A workaround is that restarting every broker adding log.cleaner.io.max.bytes.per.second to config/server.properties",,tyamashi-oss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-15 13:49:51.0,,,,,,,,,,"0|z139js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update features will fail in KRaft mode,KAFKA-13990,13449993,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dengziming,dengziming,dengziming,14/Jun/22 11:06,01/Sep/22 01:40,13/Jul/23 09:17,01/Sep/22 01:40,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"We return empty supported features in Controller ApiVersionResponse, so the {{quorumSupportedFeature}} will always return empty, we should return Map(metadata.version -> latest)
 ",,dengziming,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-14 11:06:12.0,,,,,,,,,,"0|z1380w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get connectors call fails when plugin removed,KAFKA-13989,13449983,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gharris1727,Sadura,Sadura,14/Jun/22 10:08,08/Nov/22 17:19,13/Jul/23 09:17,08/Nov/22 17:19,3.1.0,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"When a connector plugin gets removed from the Kafka Connect, the {{/connectors?expand=info}} or {{/connectors?expand=status}} call returns

{ ""error_code"": 500, ""message"": ""Failed to find any class that implements Connector and which name matches ..."" }

without any other connector info. My expectation would be to return all valid connectors and an error status for the unsupported one.

The {{/connectors}} call on the other hand, returns the defined connector list.

 ",,Sadura,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-14 10:08:55.0,,,,,,,,,,"0|z137yo:",9223372036854775807,,cegerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorSourceTask commitRecord throws NPE if SMT is filtering out source record,KAFKA-13985,13449802,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,RensGroothuijsen,jacopor,jacopor,13/Jun/22 13:10,15/Sep/22 13:56,13/Jul/23 09:17,15/Sep/22 13:56,3.1.0,3.2.0,3.3.0,,,,,,,,,,,,,3.4.0,,,,,,,mirrormaker,,,,,,0,,,,,,"Applying a SMT that filters out messages it can brings to enter in this path:

From WorkerSourceTask.java
{code:java}
final SourceRecord record = transformationChain.apply(preTransformRecord);
final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(record);
if (producerRecord == null || retryWithToleranceOperator.failed()) {
    counter.skipRecord();
    commitTaskRecord(preTransformRecord, null);
    continue;
} {code}
 

Then to:
{code:java}
private void commitTaskRecord(SourceRecord record, RecordMetadata metadata) {
        try {
            task.commitRecord(record, metadata);
        } catch (Throwable t) {
            log.error(""{} Exception thrown while calling task.commitRecord()"", this, t);
        }
}{code}

Finally
From MirrorSourceTask.java
{code:java}
    @Override
    public void commitRecord(SourceRecord record, RecordMetadata metadata) {
        try {
            if (stopping) {
                return;
            }
            if (!metadata.hasOffset()) {
                log.error(""RecordMetadata has no offset -- can't sync offsets for {}."", record.topic());
                return;
            }

...{code}
 
Causing a NPE because metadata is null. 
This the exception.

{code:java}
[2022-06-13 12:31:33,094] WARN Failure committing record. (org.apache.kafka.connect.mirror.MirrorSourceTask:190)
java.lang.NullPointerException
    at org.apache.kafka.connect.mirror.MirrorSourceTask.commitRecord(MirrorSourceTask.java:177)
    at org.apache.kafka.connect.runtime.WorkerSourceTask.commitTaskRecord(WorkerSourceTask.java:463)
    at org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords(WorkerSourceTask.java:358)
    at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:257)
    at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
    at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source) {code}

In my understanding this is well handled and it does not have negative impacts because it's handled by MirrorSourceTask.commitRecord, without leaving the exception be forwarded outside of it. 

But probably is preferred to handle it checking if metadata != null.
So skipping commit but safely and silently

[EDIT]
Actually, going a bit in deep, there is a small side-effect.

If the latest message elaborated was filtered out (so not committed by MirrorSourceTask), if MM2 instance is rebooted, this message will be re-read by consumer, because offset was not committed (and probably filtered out if configurations wasn't change).

But probably this behavior is fine considering MM2's nature

 ",,jacopor,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13632,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-13 13:10:02.0,,,,,,,,,,"0|z136ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fail the creation with ""/"" in resource name in zk ACL",KAFKA-13983,13449782,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,amansnh,amansnh,amansnh,13/Jun/22 11:08,08/Jul/22 11:17,13/Jul/23 09:17,08/Jul/22 10:21,,,,,,,,,,,,,,,,3.3.0,,,,,,,core,,,,,,0,,,,,,"Currently, resource names in ACLS can contain any special characters, but resource names with some special characters are not a valid zookeeper path entry.

For example resource name {color:#de350b}{{test/true}} {color}is not a valid zookeeper path entry.

Zookeeper will create a child node, name as {color:#de350b}{{true}}{color} inside the {color:#de350b}{{test}}{color} node.

This will create two problems:-
 # If there is *one*  ACL with a resource name {color:#de350b}{{test}}{color} it can't be deleted because if there is only one, Kafka tries to delete the node as well by thinking it will be empty which is not true it has the child node {{{color:#de350b}true{color}.}}
 # When broker restarts {color:#de350b}{{ACL cache}}{color}(which is used for ACL operations like describe, authorization etc) update from zookeeper and Kafka only looks for  ACLs that are direct child nodes of resource type in the ACL tree. 

 ",,amansnh,mcascallares,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 05:05:42 UTC 2022,,,,,,,,,,"0|z136q0:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"13/Jun/22 14:28;mcascallares;Would it be possible to add a mention in ACL docs that resources with '/' are not valid? At least until the sanitization is done.

Thanks!;;;","26/Jun/22 07:00;amansnh;[~mcascallares] Which ACL docs are you talking about, could you please add the link here?

 

Thanks;;;","27/Jun/22 05:05;mcascallares;What about adding some comments about the need of sanitize here [https://kafka.apache.org/documentation/#security_authz?]



Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reassignment cancellation causes stray replicas,KAFKA-13972,13449080,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,08/Jun/22 22:05,06/Feb/23 18:51,13/Jul/23 09:17,06/Feb/23 18:51,,,,,,,,,,,,,,,,3.4.1,,,,,,,,,,,,,0,,,,,,"A stray replica is one that is left behind on a broker after the partition has been reassigned to other brokers or the partition has been deleted. We found one case where this can happen is after a cancelled reassignment. When a reassignment is cancelled, the controller sends `StopReplica` requests to any of the adding replicas, but it does not necessarily bump the leader epoch. Following [KIP-570|[https://cwiki.apache.org/confluence/display/KAFKA/KIP-570%3A+Add+leader+epoch+in+StopReplicaRequest],] brokers will ignore `StopReplica` requests if the leader epoch matches the current partition leader epoch. So we need to bump the epoch whenever we need to ensure that `StopReplica` will be received.",,divijvaidya,hachikuji,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 20:42:37 UTC 2023,,,,,,,,,,"0|z132fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 20:42;hachikuji;The patch still needs to be picked into 3.4 after we have finished with 3.4.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Atomicity violations caused by improper usage of ConcurrentHashMap,KAFKA-13971,13449048,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,Kvicii,divijvaidya,divijvaidya,08/Jun/22 16:36,23/Aug/22 14:25,13/Jul/23 09:17,23/Aug/22 14:25,,,,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,0,newbee,newbie,,,,"*Code:* [https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java#L81-L84] 

*Problem*
We are using a {{{}ConcurrentHashMap{}}}, but usage of {{{}get(){}}}, {{null}} check, and {{put()}} may not be thread-safe at lines: 81, 82, and 84. Two threads can perform this same check at the same time and one thread can overwrite the value written by the other thread.

*Fix*
Consider replacing {{put()}} with {{putIfAbsent()}} to help prevent accidental overwriting. {{putIfAbsent()}} puts the value only if the {{ConcurrentHashMap}} does not contain the key and therefore avoids overwriting the value written there by the other thread's {{{}putIfAbsent(){}}}.",,divijvaidya,Kvicii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 16:21:36 UTC 2022,,,,,,,,,,"0|z1328g:",9223372036854775807,,ChrisEgerton,,,,,,,,,,,,,,,,,,"09/Jun/22 16:21;Kvicii;looks good. Iagree.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker should not generator snapshot until been unfenced,KAFKA-13968,13448920,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dengziming,dengziming,dengziming,08/Jun/22 05:10,13/Jul/22 02:08,13/Jul/23 09:17,12/Jul/22 15:20,,,,,,,,,,,,,,,,3.3.0,,,,,,,kraft,,,,,,0,,,,,," 

There is a bug when computing `FeaturesDelta` which cause us to generate snapshot on every commit.

 

[2022-06-08 13:07:43,010] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 0... (kafka.server.metadata.BrokerMetadataSnapshotter:66)

[2022-06-08 13:07:43,222] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 2... (kafka.server.metadata.BrokerMetadataSnapshotter:66)

[2022-06-08 13:07:43,727] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 3... (kafka.server.metadata.BrokerMetadataSnapshotter:66)

[2022-06-08 13:07:44,228] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 4... (kafka.server.metadata.BrokerMetadataSnapshotter:66)

 

Before a broker being unfenced, it won't starting publishing metadata, so it's meaningless to  generate a snapshot.",,dengziming,divijvaidya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 28 10:28:50 UTC 2022,,,,,,,,,,"0|z131g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 10:28;divijvaidya;Changing the priority to blocker for 3.3.0 release since this is a critical bug fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test `QuorumControllerTest.testUnregisterBroker`,KAFKA-13966,13448893,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,hachikuji,hachikuji,08/Jun/22 01:53,23/Feb/23 13:23,13/Jul/23 09:17,23/Feb/23 13:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"We have seen the following assertion failure in `QuorumControllerTest.testUnregisterBroker`:
{code:java}
org.opentest4j.AssertionFailedError: expected: <2> but was: <0>
	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)
	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:166)
	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:161)
	at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:628)
	at org.apache.kafka.controller.QuorumControllerTest.testUnregisterBroker(QuorumControllerTest.java:494) {code}
I reproduced it by running the test in a loop. It looks like what happens is that the BrokerRegistration request is able to get interleaved between the leader change event and the write of the bootstrap metadata. Something like this:
 # handleLeaderChange() start
 # appendWriteEvent(registerBroker)
 # appendWriteEvent(bootstrapMetadata)
 # handleLeaderChange() finish
 # registerBroker() -> writes broker registration to log
 # bootstrapMetadata() -> writes bootstrap metadata to log",,christo_lolov,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 13:23:26 UTC 2023,,,,,,,,,,"0|z131a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 13:23;christo_lolov;Since the related pull request has been merged I will be closing this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topology Description ignores context.forward,KAFKA-13963,13448789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tkaszuba,tkaszuba,tkaszuba,07/Jun/22 12:50,28/Jun/22 01:52,13/Jul/23 09:17,28/Jun/22 01:52,2.7.2,,,,,,,,,,,,,,,,,,,,,,streams,,,,,,0,,,,,,"I have a simple topology:
{code:java}
      val topology = new Topology
      topology
        .addSource(""source"", Serdes.stringSerde.deserializer, Serdes.stringSerde.deserializer, inputTopic)
        .addProcessor(
          ""process"",
          new ProcessorSupplier[String, String] {
            override def get(): Processor[String, String] =
              new RecordCollectorProcessor()
          },
          ""source""
        ) {code}
And a simple processor that uses context.forward to forward messages:
{code:java}
  private class ContextForwardProcessor extends AbstractProcessor[String, String]() {    override def process(key: String, value: String): Unit =
      context().forward(""key"", ""value"", To.child(""output""))    override def close(): Unit = ()
  }  {code}
when I call topology.describe() I receive this:
{noformat}
Topologies:
   Sub-topology: 0
    Source: source (topics: [input])
      --> process
    Processor: process (stores: [])
      --> none
      <-- source {noformat}
Ignoring the fact that this will not run since it will throw a runtime exception why is the To.child ignored?

Taking it one point further if I add multiple sinks to the topology like so:
{code:java}
val topology = new Topology
      topology
        .addSource(""source"", Serdes.stringSerde.deserializer, Serdes.stringSerde.deserializer, inputTopic)
        .addProcessor(
          ""process"",
          new ProcessorSupplier[String, String] {
            override def get(): Processor[String, String] =
              new ContextForwardProcessor()
          },
          ""source""
        )
        .addSink(""sink"", ""output1"", Serdes.stringSerde.serializer(), Serdes.stringSerde.serializer(), ""process"")
        .addSink(""sink2"", ""output2"", Serdes.stringSerde.serializer(), Serdes.stringSerde.serializer(), ""process"")  {code}
but have the processor only output to ""output1"" it is in no way reflected in the described topology graph.

I assume this is by design since it's a lot more work to interpret what the context.forward is doing but when I tried to look for this information in the java doc I couldn't find it.

 ",,mjsax,tkaszuba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 14 08:24:37 UTC 2022,,,,,,,,,,"0|z130n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 20:08;mjsax;TopologyDescription only describes the structure of you graph of operators. In your first example, you only added two nodes to the graph (""source"" and ""process"") and there is no node ""output"", and thus it's not contained in the `TopologyDescription`.

It's not really possible to take the business logic (ie, what `forward()` is doing) into account – at least I have not idea how this could be done with reasonable effort.

It's for sure not a bug. We should either close this ticket and change it into a feature request.;;;","07/Jun/22 21:04;tkaszuba;Ok, this is what I thought. Is it worth updating the java doc to mention this? The developers I work with were surprised that context.forward is not covered. We rely heavily on the generated topology graphs for impact analysis.

Btw, I think you can get around the context forward exception and the need for registering sinks if you use the internal RecordCollector, which I feel should be better hidden from the streams api users since it's a class cast exception waiting to happen. I can open up a separate bug for that if it makes sense.
{code:java}
collector = context.asInstanceOf[RecordCollector.Supplier].recordCollector {code};;;","08/Jun/22 00:31;mjsax;{quote}Is it worth updating the java doc to mention this?
{quote}
Updating docs can never hurt :) – are you interested in doing a PR?
{quote}if you use the internal RecordCollector, which I feel should be better hidden from the streams api users
{quote}
Yes, you should NEVER use internal stuff... Not sure how we could ""better hide"" it though? Seems not to be possible as long as we are using Java 8...
{quote}I can open up a separate bug for that if it makes sense.
{quote}
Don't think it's a bug? It (unfortunately) how Java works.;;;","14/Jun/22 08:24;tkaszuba;Created PR for the docs: https://github.com/apache/kafka/pull/12293/files;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller should unfence Broker with busy metadata log,KAFKA-13959,13448376,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dengziming,jagsancio,jagsancio,03/Jun/22 18:45,12/Aug/22 17:03,13/Jul/23 09:17,12/Aug/22 17:03,3.3.0,,,,,,,,,,,,,,,3.3.0,,,,,,,kraft,,,,,,0,,,,,,"https://issues.apache.org/jira/browse/KAFKA-13955 showed that it is possible for the controller to not unfence a broker if the committed offset keeps increasing.

 

One solution to this problem is to require the broker to only catch up to the last committed offset when they last sent the heartbeat. For example:
 # Broker sends a heartbeat with current offset of {{{}Y{}}}. The last commit offset is {{{}X{}}}. The controller remember this last commit offset, call it {{X'}}
 # Broker sends another heartbeat with current offset of {{{}Z{}}}. Unfence the broker if {{Z >= X}} or {{{}Z >= X'{}}}.

Another solution is to unfence the broker when the applied offset of the broker has reached the offset of its own broker registration record.

This change should also set the default for MetadataMaxIdleIntervalMs back to 500.",,dengziming,jagsancio,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 10:32:07 UTC 2022,,,,,,,,,,"0|z12y3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 18:55;jagsancio;[~dengziming] [~showuon] Are you interested in working on this issue?;;;","04/Jun/22 13:48;dengziming;I will take a look at this if no one assigned.;;;","05/Jun/22 07:10;showuon;Thanks [~dengziming] !;;;","07/Jun/22 10:04;dengziming;When BrokerLifecycleManager is starting up, it will send heartbeat every 10 milliseconds rather than 2000 milliseconds: 

`scheduleNextCommunication(NANOSECONDS.convert(10, MILLISECONDS))`

which is already smaller than 500ms, so the reason for this bug is more complex, I need more time to investigate.;;;","07/Jun/22 11:24;showuon;[~dengziming] , if it's 10 ms heartbeat, how could it not be able to catch up with 500ms no-op records?;;;","07/Jun/22 11:25;showuon;Sorry, I didn't see your last sentence. Thanks for the investigation! Looking forward to knowing the root cause! :);;;","08/Jun/22 10:25;dengziming;I haven't find the root cause, I just print the brokerOffset and controllerOffset when heartbeat, I find that every time the brokerOffset bump, the controllerOffset will also bump. 

```

time: 1654679131904 broker 0 brokerOffset:27 controllerOffset:28
time: 1654679132115 broker 0 brokerOffset:27 controllerOffset:28
time: 1654679132381 broker 0 brokerOffset:28 controllerOffset:29
time: 1654679132592 broker 0 brokerOffset:28 controllerOffset:29
time: 1654679132878 broker 0 brokerOffset:29 controllerOffset:30
time: 1654679133089 broker 0 brokerOffset:29 controllerOffset:30
time: 1654679133299 broker 0 brokerOffset:30 controllerOffset:31
time: 1654679133509 broker 0 brokerOffset:30 controllerOffset:31

```

I try to increase the interval of heartbeats but got the same result, and if I set numberControllerNodes to 1,  this problem disappear. I think this may be related to the logic of how we compute leader hw and follower hw.  ;;;","08/Jun/22 15:14;jagsancio;[~dengziming], If you haven't, maybe looking at the KRaft side of the implementation may help. Specially at the LEOs and HWM reported by KRaft for both the controller and the broker(s), any pending FETCH request(s) and how often brokers sends FETCH requests.;;;","09/Jun/22 10:29;showuon;[~dengziming] [~jagsancio] , I did some investigation today, and here's my finding:
 # broker heartbeat to active controller won't fetch any data or increase the offset. broker just sends the current offset and some broker info to the controller. So, even if we have small interval of heartbeat, it still won't help.
 # So, when will the broker offset increased? It only happened in broker metadataListener. the metadataListener is listening to raftClient. And raftClient is polling metadata from active controller.
 # About when the highwatermark will be updated in active controller: Whenever there's record append to the active controller log, it won't update the highwatermark, until there are voters fetch records from active controller and also update the highwatermark. ex: current active controller is in highwatermark 9, and a record append to active controller log to offset 10, it'll wait, until voters send fetch request to active controller to update highwatermark, and then, commit the offset 10 record, update the new highwatermark to 10, to make sure the record is replicated to a majority of the voters.
 # So, that explains what we saw in the issue:
 ## active controller send no-op message to metadata topic, active controller append into log, but don't update highwatermark (still 9)
 ## broker raftClient fetch records from active controller,
 ## active controller return the records to offset 9, and then update the highwatermark to 10
 ## broker metaListener will operate the records
 ## broker send heartbeat to active controller with offest 9
 ## since offset 9 is < active controller highwatermark 10
 ## keep trying, and in the meantime, no-op message sent again, and back to step 1

 ;;;","09/Jun/22 10:32;showuon;So I think the proposed solution should fix the issue.

One solution to this problem is to require the broker to only catch up to the last committed offset when they last sent the heartbeat. For example:
 # Broker sends a heartbeat with current offset of {{{}Y{}}}. The last commit offset is {{{}X{}}}. The controller remember this last commit offset, call it {{X'}}
 # Broker sends another heartbeat with current offset of {{{}Z{}}}. Unfence the broker if {{Z >= X}} or {{{}Z >= X'{}}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose logdirs total and usable space via Kafka API,KAFKA-13958,13448324,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,mimaison,mimaison,03/Jun/22 12:14,14/Jun/22 12:21,13/Jul/23 09:17,14/Jun/22 12:21,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,JIRA for KIP-827: https://cwiki.apache.org/confluence/display/KAFKA/KIP-827%3A+Expose+logdirs+total+and+usable+space+via+Kafka+API,,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-03 12:14:29.0,,,,,,,,,,"0|z12xrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StoreQueryIntegrationTest.shouldQuerySpecificActivePartitionStores,KAFKA-13957,13448259,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mdedetrich,ableegoldman,ableegoldman,03/Jun/22 05:36,05/Feb/23 02:46,13/Jul/23 09:17,05/Jul/22 10:44,,,,,,,,,,,,,,,,3.3.0,,,,,,,streams,,,,,,0,flaky-test,,,,,"Failed on a local build so I have the full logs (attached)
{code:java}
java.lang.AssertionError: Unexpected exception thrown while getting the value from store.
Expected: is (a string containing ""Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING"" or a string containing ""The state store, source-table, may have migrated to another instance"" or a string containing ""Cannot get state store source-table because the stream thread is STARTING, not RUNNING"")
     but: was ""The specified partition 1 for store source-table does not exist.""
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.verifyRetrievableException(StoreQueryIntegrationTest.java:539)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.lambda$shouldQuerySpecificActivePartitionStores$5(StoreQueryIntegrationTest.java:241)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.until(StoreQueryIntegrationTest.java:557)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQuerySpecificActivePartitionStores(StoreQueryIntegrationTest.java:183)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:833) {code}",,ableegoldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/22 05:43;ableegoldman;StoreQueryIntegrationTest.shouldQuerySpecificActivePartitionStores.rtf;https://issues.apache.org/jira/secure/attachment/13044600/StoreQueryIntegrationTest.shouldQuerySpecificActivePartitionStores.rtf",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 13 22:23:46 UTC 2022,,,,,,,,,,"0|z12xdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 11:04;ableegoldman;Failed again;;;","13/Jun/22 12:44;mdedetrich;I am working on this issue, I managed to replicate the exact same exception.;;;","13/Jun/22 22:23;mdedetrich;I have fixed the test at https://github.com/apache/kafka/pull/12289;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite retry timeout is not working,KAFKA-13952,13447875,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,yash.mayya,jmalek,jmalek,01/Jun/22 08:27,08/Sep/22 14:18,13/Jul/23 09:17,08/Sep/22 14:14,,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,0,,,,,,"The [documentation|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java#L129] for {{errors.retry.timeout}} property says:
{noformat}
The maximum duration in milliseconds that a failed operation will be reattempted. The default is 0, which means no retries will be attempted. Use -1 for infinite retries.{noformat}

But it seems that value {{-1}} is not respected by the [RetryWithToleranceOperator|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java] that simply compares elapsed time until {{startTime + errorRetryTimeout}} is exceeded.

I was also not able to find any conversion of the raw config value before {{RetryWithToleranceOperator}} is initialized.
I run a simple test with a connector using mocked transformation plugin that throws the {{RetriableException}} and it seems to prove my claim.

I'm not sure if it's documentation or implementation error or maybe I've missed something.",,jmalek,showuon,yash.mayya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 03 14:28:21 UTC 2022,,,,,,,,,,"0|z12v00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 14:28;yash.mayya;[~jmalek] this definitely looks like a bug in the implementation of RetryWithToleranceOperator. Thanks for discovering this issue and filing a bug JIRA. I've raised a PR to fix this issue - https://github.com/apache/kafka/pull/12478;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Format strings appropriately for their argument types,KAFKA-13947,13447474,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,christo_lolov,divijvaidya,divijvaidya,30/May/22 08:52,10/Jun/22 11:56,13/Jul/23 09:17,10/Jun/22 11:56,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,n00b,newbee,newbie,,,"At various places in the code base, String.format() uses '%s' to format long or integer. This is a potential locale-sensitive handling issue.

The fix should be made at the following places:

1. [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/compress/KafkaLZ4BlockInputStream.java#L176]
2. [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java#L473]
3. [https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java#L1264]
4. [https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/ReplicatedCounter.java#L94]
5. https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java#L192
6. [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopics.java#L120]
7. [https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/LeaderState.java#L356]
8. https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/secured/Retry.java#L52

Note that the files listed above have more such lines where %s is incorrectly used.",,christo_lolov,divijvaidya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 30 12:34:22 UTC 2022,,,,,,,,,,"0|z12sj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 12:34;christo_lolov;Heya, I volunteer to pick this up!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
setMetadataDirectory() method in builder for ControllerNode has no parameters,KAFKA-13946,13447388,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,clara0,clara0,29/May/22 16:30,30/May/22 22:55,13/Jul/23 09:17,30/May/22 22:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"In core/src/test/java/kafka/testkit/ControllerNode.java, the method setMetadataDirectory for the builder has no parameters and is assigning the variable metadataDirectory to itself.
{code:java}
public Builder setMetadataDirectory() {
    this.metadataDirectory = metadataDirectory;
    return this;
}
{code}",,clara0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-29 16:30:21.0,,,,,,,,,,"0|z12s00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shutting down broker can be elected as partition leader in KRaft,KAFKA-13944,13447276,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,hachikuji,hachikuji,27/May/22 20:03,08/Jun/22 16:32,13/Jul/23 09:17,08/Jun/22 16:32,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,kip-500,,,,,"When a broker requests shutdown, it transitions to the CONTROLLED_SHUTDOWN state in the controller. It is possible for the broker to remain unfenced in this state until the controlled shutdown completes. When doing an election, the only thing we generally check is that the broker is unfenced, so this means we can elect a broker that is in controlled shutdown. 

Here are a few snippets from a recent system test in which this occurred:
{code:java}
// broker 2 starts controlled shutdown
[2022-05-26 21:17:26,451] INFO [Controller 3001] Unfenced broker 2 has requested and been granted a controlled shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
 
// there is only one replica, so we set leader to -1
[2022-05-26 21:17:26,452] DEBUG [Controller 3001] partition change for _foo-1 with topic ID _iUQ72T_R4mmZgI3WrsyXw: leader: 2 -> -1, leaderEpoch: 0 -> 1, partitionEpoch: 0 -> 1 (org.apache.kafka.controller.ReplicationControlManager)

// controlled shutdown cannot complete immediately
[2022-05-26 21:17:26,529] DEBUG [Controller 3001] The request from broker 2 to shut down can not yet be granted because the lowest active offset 177 is not greater than the broker's shutdown offset 244. (org.apache.kafka.controller.BrokerHeartbeatManager)
[2022-05-26 21:17:26,530] DEBUG [Controller 3001] Updated the controlled shutdown offset for broker 2 to 244. (org.apache.kafka.controller.BrokerHeartbeatManager)

// later on we elect leader 2 again
[2022-05-26 21:17:27,703] DEBUG [Controller 3001] partition change for _foo-1 with topic ID _iUQ72T_R4mmZgI3WrsyXw: leader: -1 -> 2, leaderEpoch: 1 -> 2, partitionEpoch: 1 -> 2 (org.apache.kafka.controller.ReplicationControlManager)

// now controlled shutdown is stuck because of the newly elected leader
[2022-05-26 21:17:28,531] DEBUG [Controller 3001] Broker 2 is in controlled shutdown state, but can not shut down because more leaders still need to be moved. (org.apache.kafka.controller.BrokerHeartbeatManager)
{code}",,dajac,hachikuji,heyingquan,jagsancio,,,,,,,,,,,,,,,,,,KAFKA-13916,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 16:32:05 UTC 2022,,,,,,,,,,"0|z12rb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 17:16;jagsancio;When fixing this lets improve the logging so that the replica control manager logs the reason that triggered the election.;;;","03/Jun/22 21:16;jagsancio;Looks like this issue is addressed by https://issues.apache.org/jira/browse/KAFKA-13916

 ;;;","08/Jun/22 16:32;dajac;This has been fixed by https://github.com/apache/kafka/pull/12240.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky test QuorumControllerTest.testMissingInMemorySnapshot(),KAFKA-13943,13447232,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,divijvaidya,divijvaidya,divijvaidya,27/May/22 13:47,06/Jul/22 03:08,13/Jul/23 09:17,06/Jul/22 03:08,,,,,,,,,,,,,,,,3.3,3.3.0,,,,,,unit tests,,,,,,0,flaky-test,,,,,"Test failed at [https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-12197/3/tests] 
{noformat}
[2022-05-27 09:34:42,382] INFO [Controller 0] Creating new QuorumController with clusterId wj9LhgPJTV-KYEItgqvtQA, authorizer Optional.empty. (org.apache.kafka.controller.QuorumController:1484)
[2022-05-27 09:34:42,393] DEBUG [LocalLogManager 0] Node 0: running log check. (org.apache.kafka.metalog.LocalLogManager:479)
[2022-05-27 09:34:42,394] DEBUG [LocalLogManager 0] initialized local log manager for node 0 (org.apache.kafka.metalog.LocalLogManager:622)
[2022-05-27 09:34:42,396] INFO [LocalLogManager 0] Node 0: registered MetaLogListener 1774961169 (org.apache.kafka.metalog.LocalLogManager:640)
[2022-05-27 09:34:42,397] DEBUG [LocalLogManager 0] Node 0: running log check. (org.apache.kafka.metalog.LocalLogManager:479)
[2022-05-27 09:34:42,397] DEBUG [LocalLogManager 0] Node 0: Executing handleLeaderChange LeaderAndEpoch(leaderId=OptionalInt[0], epoch=1) (org.apache.kafka.metalog.LocalLogManager:520)
[2022-05-27 09:34:42,398] DEBUG [Controller 0] Executing handleLeaderChange[1]. (org.apache.kafka.controller.QuorumController:438)
[2022-05-27 09:34:42,398] INFO [Controller 0] Becoming the active controller at epoch 1, committed offset -1, committed epoch -1, and metadata.version 5 (org.apache.kafka.controller.QuorumController:950)
[2022-05-27 09:34:42,398] DEBUG [Controller 0] Creating snapshot -1 (org.apache.kafka.timeline.SnapshotRegistry:197)
[2022-05-27 09:34:42,399] DEBUG [Controller 0] Processed handleLeaderChange[1] in 951 us (org.apache.kafka.controller.QuorumController:385)
[2022-05-27 09:34:42,399] INFO [Controller 0] Initializing metadata.version to 5 (org.apache.kafka.controller.QuorumController:926)
[2022-05-27 09:34:42,399] INFO [Controller 0] Setting metadata.version to 5 (org.apache.kafka.controller.FeatureControlManager:273)
[2022-05-27 09:34:42,400] DEBUG [Controller 0] Creating snapshot 9223372036854775807 (org.apache.kafka.timeline.SnapshotRegistry:197)
[2022-05-27 09:34:42,400] DEBUG [Controller 0] Read-write operation bootstrapMetadata(1863535402) will be completed when the log reaches offset 9223372036854775807. (org.apache.kafka.controller.QuorumController:725)
[2022-05-27 09:34:42,402] DEBUG append(batch=LocalRecordBatch(leaderEpoch=1, appendTimestamp=10, records=[ApiMessageAndVersion(RegisterBrokerRecord(brokerId=0, incarnationId=kxAT73dKQsitIedpiPtwBw, brokerEpoch=-9223372036854775808, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=9092, securityProtocol=0)], features=[], rack=null, fenced=true) at version 0)]), prevOffset=1) (org.apache.kafka.metalog.LocalLogManager$SharedLogData:247)
[2022-05-27 09:34:42,402] INFO [Controller 0] Registered new broker: RegisterBrokerRecord(brokerId=0, incarnationId=kxAT73dKQsitIedpiPtwBw, brokerEpoch=-9223372036854775808, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='localhost', port=9092, securityProtocol=0)], features=[], rack=null, fenced=true) (org.apache.kafka.controller.ClusterControlManager:368)
[2022-05-27 09:34:42,403] WARN [Controller 0] registerBroker: failed with unknown server exception RuntimeException at epoch 1 in 2449 us.  Reverting to last committed offset -1. (org.apache.kafka.controller.QuorumController:410)java.lang.RuntimeException: Can't create a new snapshot at epoch 1 because there is already a snapshot with epoch 9223372036854775807    at org.apache.kafka.timeline.SnapshotRegistry.getOrCreateSnapshot(SnapshotRegistry.java:190)    at org.apache.kafka.controller.QuorumController$ControllerWriteEvent.run(QuorumController.java:723)    at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)    at java.base/java.lang.Thread.run(Thread.java:833){noformat}
{noformat}
Full stack trace

java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownServerException: java.lang.RuntimeException: Can't create a new snapshot at epoch 1 because there is already a snapshot with epoch 9223372036854775807
    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
    at org.apache.kafka.controller.QuorumControllerTest.registerBrokers(QuorumControllerTest.java:1014)
    at org.apache.kafka.controller.QuorumControllerTest.testMissingInMemorySnapshot(QuorumControllerTest.java:907)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutInvocation.proceed(TimeoutInvocation.java:46)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
    at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)
    at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)
    at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)
    at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
    at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
    at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
    at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
    at jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)
    at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
    at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
    at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
    at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
    at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
    at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
    at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
    at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
    at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
Caused by: org.apache.kafka.common.errors.UnknownServerException: java.lang.RuntimeException: Can't create a new snapshot at epoch 1 because there is already a snapshot with epoch 9223372036854775807
Caused by: java.lang.RuntimeException: Can't create a new snapshot at epoch 1 because there is already a snapshot with epoch 9223372036854775807
    at org.apache.kafka.timeline.SnapshotRegistry.getOrCreateSnapshot(SnapshotRegistry.java:190)
    at org.apache.kafka.controller.QuorumController$ControllerWriteEvent.run(QuorumController.java:723)
    at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
    at java.base/java.lang.Thread.run(Thread.java:833){noformat}",,divijvaidya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 10 08:54:03 UTC 2022,,,,,,,,,,"0|z12r1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/22 14:23;divijvaidya;The test is failing because in some situation the KRaftClient.scheduleAppend() for a message of type `bootstrapMetadata` is being called from  a node which is either not the current leader/controller OR from a leader with wrong epoch because the test is creating a file with LONG_MAX offset at [https://github.com/apache/kafka/blob/trunk/metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java#L229-L237]

Perhaps [~mumrah] may know about the situation where this scenario can occur?;;;","10/Jun/22 08:54;divijvaidya;I have fixed the bug which was causing a snapshot with LONG_MAX at [https://github.com/apache/kafka/pull/12224] 

Also note that there are other tests such as QuorumControllerTest.testSnapshotOnlyAfterConfiguredMinBytes failing due to same bug


{noformat}
Error Messagejava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownServerException: java.lang.RuntimeException: Can't create a new snapshot at epoch 1 because there is already a snapshot with epoch 9223372036854775807Stacktracejava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownServerException: java.lang.RuntimeException: Can't create a new snapshot at epoch 1 because there is already a snapshot with epoch 9223372036854775807
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
	at org.apache.kafka.controller.QuorumControllerTest.testSnapshotOnlyAfterConfiguredMinBytes(QuorumControllerTest.java:691)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutInvocation.proceed(TimeoutInvocation.java:46)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke({noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogOffsetTest occasionally hangs during Jenkins build,KAFKA-13942,13447085,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mumrah,mumrah,26/May/22 20:18,08/Jun/22 00:51,13/Jul/23 09:17,08/Jun/22 00:51,,,,,,,,,,,,,,,,,,,,,,,unit tests,,,,,,0,,,,,,"[~hachikuji] parsed the log output of one of the recent stalled Jenkins builds and singled out LogOffsetTest as a likely culprit for not completing.

I looked closely at the following build which appeared to be stuck and found this test case had STARTED but not PASSED or FAILED.

15:19:58  LogOffsetTest > testFetchOffsetByTimestampForMaxTimestampWithUnorderedTimestamps(String) > kafka.server.LogOffsetTest.testFetchOffsetByTimestampForMaxTimestampWithUnorderedTimestamps(String)[2] STARTED
",,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-26 20:18:56.0,,,,,,,,,,"0|z12q6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DescribeQuorum returns INVALID_REQUEST if not handled by leader,KAFKA-13940,13446902,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,25/May/22 22:39,17/Aug/22 22:59,13/Jul/23 09:17,17/Aug/22 22:59,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"In `KafkaRaftClient.handleDescribeQuorum`, we currently return INVALID_REQUEST if the node is not the current raft leader. This is surprising and doesn't work with our general approach for retrying forwarded APIs. In `BrokerToControllerChannelManager`, we only retry after `NOT_CONTROLLER` errors. It would be more consistent with the other Raft APIs if we returned NOT_LEADER_OR_FOLLOWER, but that also means we need additional logic in `BrokerToControllerChannelManager` to handle that error and retry correctly. ",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-25 22:39:09.0,,,,,,,,,,"0|z12p28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory Leak When Logging Is Disabled In InMemoryTimeOrderedKeyValueBuffer,KAFKA-13939,13446881,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jnewhouse,jnewhouse,jnewhouse,25/May/22 19:38,16/Jun/22 21:28,13/Jul/23 09:17,16/Jun/22 21:28,,,,,,,,,,,,,,,,3.3.0,,,,,,,streams,,,,,,0,,,,,,"If `loggingEnabled` is false, the `dirtyKeys` Set is not cleared within `flush()`, see [https://github.com/apache/kafka/blob/3.2/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java#L262.] However, dirtyKeys is still written to in the loop within `evictWhile`. This causes dirtyKeys to continuously grow for the life of the buffer. ",,guozhang,jnewhouse,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 15 18:03:21 UTC 2022,,,,,,,,,,"0|z12oxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 23:02;jnewhouse;One way to patch this would be something like
{code:java}
jnewhouse - kafka % git diff
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java
index 2909e2763f..b0ee755b3f 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java
@@ -403,7 +403,9 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere
                 delegate.remove();
                 index.remove(next.getKey().key());
 
-                dirtyKeys.add(next.getKey().key());
+                if (loggingEnabled) {
+                    dirtyKeys.add(next.getKey().key());
+                }
 
                 memBufferSize -= computeRecordSize(next.getKey().key(), bufferValue);
 
@@ -478,7 +480,9 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere
             serializedKey,
             new BufferValue(serializedPriorValue, serialChange.oldValue, serialChange.newValue, recordContext)
         );
-        dirtyKeys.add(serializedKey);
+        if (loggingEnabled) {
+            dirtyKeys.add(serializedKey);
+        }
         updateBufferMetrics();
     } {code}
Since `loggingEnabled` is final, we can just not track the dirty keys. The set is only read from if `loggingEnabled` is true.;;;","25/May/22 23:07;jnewhouse;If you search Stack Overflow you'll find occasional instances of people running into this problem, such as [https://stackoverflow.com/questions/59239783/kafka-streams-suppressed-feature-causes-oom-heavy-gc] 

and

[https://stackoverflow.com/questions/70651437/kafka-stream-oom-out-of-memory|https://stackoverflow.com/questions/70651437/kafka-stream-oom-out-of-memory/70660956#70660956]

 ;;;","31/May/22 23:55;mjsax;Thanks for reporting this issue – sound rather severs – I bumped the priority to blocker.

As you already have a fix, would you like to open a PR on GitHub?;;;","01/Jun/22 19:02;guozhang;Thanks [~jnewhouse], I looked at the code you pointed it out and I agree it's a bug indeed, and should be fixed asap. Please let us know if you'd like to open a PR to fix it.;;;","07/Jun/22 18:26;jnewhouse;I'll open a PR.;;;","07/Jun/22 18:26;jnewhouse;What's the protocol for back-porting a fix like this?;;;","07/Jun/22 18:57;guozhang;Once the PR is merged, we can cherry-pick the commit to old branches. But whether the fix would be release depends on whether we would have a bug-fix release (e.g. say 3.2.1) planned in the future.;;;","15/Jun/22 18:03;mjsax;Thanks for the PR. I added you to list of contributors and assigned the ticket to you. You can know also self-assign tickets.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid consumer lag when monitoring from a kafka streams application,KAFKA-13936,13446735,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,prashanthjbabu,prashanthjbabu,prashanthjbabu,25/May/22 07:56,30/Jun/22 16:32,13/Jul/23 09:17,30/Jun/22 16:31,,,,,,,,,,,,,,,,3.3.0,,,,,,,streams,,,,,,0,,,,,,"I have a kafka streams application and I'm trying to monitor the consumer lag via stream metrics.

Here's some code snippet


{code:java}
metrics = streams.metrics();
            lag = 0;
            for (Metric m : metrics.values()) {
                tags = m.metricName().tags();
                if ( m.metricName().name().equals(MONITOR_CONSUMER_LAG) && tags.containsKey(MONTOR_TAG_TOPIC) && 
                    tags.get(MONTOR_TAG_TOPIC).equals(inputTopic) ) {
                    partitionLag = Float.valueOf(m.metricValue().toString()).floatValue();
                    if ( !partitionLag.isNaN() ) {
                        lag += partitionLag;
                    }
                }
            }

{code}


Here MONITOR_CONSUMER_LAG is {{{}records-lag-max{}}}.

However these numbers dont match with the consumer lag we see in the kafka UI . is records-lag-max the right metric to track for a kafka streams application when the objective is to get consumer lag?",,mjsax,prashanthjbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 30 03:20:01 UTC 2022,,,,,,,,,,"0|z12o14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/22 05:23;mjsax;> we see in the kafka UI 

There is no ""Kafka UI"" – at least not as part of Apache Kafka. If you are using some other ""external"" UI, it's unclear how they compute/display the lag.

Two thories:
 * The don't sum the lag over all partitions but take the max over all partitions?
 * They compute the lag based on committed offsets to end offset difference – because offsets are committed only every 30 seconds by default, this computation does not match what the consumer reports, because the consumer uses ""current offset"" (not committed offset) to compute the lag

I think we need to close this ticket? I don't see any bug in Kafka (ie, consumer or Kafka Streams) that we would need to fix?;;;","26/May/22 07:13;prashanthjbabu;[~mjsax] Apologies for not being specific . by kafka-ui I meant https://github.com/provectus/kafka-ui . We're seeing the same data as kafka-ui tool when we query the broker directly via kafka-cli.  We're seeing a huge difference in numbers though , for a partition it would be around 10,000 in the streams application but around 600,000 in the broker side.;;;","01/Jun/22 00:01;mjsax;As mentioned above, offsets are by default committed every 30 seconds and thus the difference between committed offset to end offset (as reported by Kafka CLI tools, and presumably by provectus) are expected to be larger than the metric directly reported by the consumer that report the difference of it's current (not yet committed) position to end offset.

If you reduce the commit interval (not necessarily recommended), the difference should be smaller as the CLI should report a smaller number.

Overall, it seems you report expected behavior, and I don't see a bug. – Of course, we _could_ add a new consumer metric that report the difference between committed offset and end offset, but what would we gain? In the end, the consumer reported metric is more accurate compared to what the CLI reports.

Maybe you can explain why it is a problem that the numbers are not the same?;;;","01/Jun/22 03:47;prashanthjbabu;[~mjsax] Considering the explanation mentioned regarding one metric reporting lag based on committed offset (kafka broker) and the other reporting based on current offset ( kafka streams application ) , this makes sense and is expected behavior . I wasn't able to find documentation explaining this difference , hence I as a user was expecting them to be the same! This can be closed!;;;","02/Jun/22 23:24;mjsax;Might be worth to document for this case :) – Would you be interested to do a PR?;;;","03/Jun/22 12:26;prashanthjbabu;[~mjsax] sure ! I'd love to ! I'm not sure where the doc repos are and what would be the right place to mention this point;;;","03/Jun/22 17:15;mjsax;The docs are in the same repository as the code: [https://github.com/apache/kafka/tree/trunk/docs]

A good place might be the consumer monitoring section [https://kafka.apache.org/documentation/#consumer_fetch_monitoring] ?;;;","24/Jun/22 06:09;prashanthjbabu;[~mjsax] I looked into the doc section and thought of adding a NOTE under `The maximum lag in terms of number of records for any partition in this window` . However if i look for this text in the repository , I see it in the file FetcherMetricsRegistry.java . Is this where the change is supposed to happen ( and it gets auto generated to html somewhere else ) ? . I don't see a html file or any markdown document which has this text.;;;","27/Jun/22 23:36;mjsax;We do generate the HTML from code (I did not know that we also do it for metrics – I knew for client configs only – but it make sense I guess).;;;","30/Jun/22 03:20;prashanthjbabu;Thanks [~mjsax] . I've raised a PR here https://github.com/apache/kafka/pull/12367;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stuck SSL/TLS unit tests in case of authentication failure,KAFKA-13933,13446531,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,fvaleri,fvaleri,fvaleri,24/May/22 08:29,21/Dec/22 17:03,13/Jul/23 09:17,05/Jun/22 07:47,3.2.0,,,,,,,,,,,,,,,3.3.0,,,,,,,unit tests,,,,,,0,,,,,,"When there is an authentication error after the initial TCP connection, the Selector never becomes READY, and its SSL/TLS tests wait forever for readiness.

This is actually what happened to me while running SSL/TLS Selector unit tests using an OpenJDK build that does not support the required cipher suites.",,fvaleri,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-24 08:29:45.0,,,,,,,,,,"0|z12ms0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication data loss in some cases,KAFKA-13932,13446522,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,qinfuhui,qinfuhui,24/May/22 07:58,09/Jun/23 16:41,13/Jul/23 09:17,09/Jun/23 16:41,2.8.0,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,mirrormaker,,,,,,0,,,,,,"*Kafka version: 2.8.0*
*Replication flow: A -> B*
*Topic: DL2 -> A.DL2*
 
I use mm2 to replicate cluster A data to cluster B. After stop and restart mm2(before restart, the data in source cluster is expired), I found consumer group offset is bigger than topic partiton offset in target cluster which will cause data loss in the mirror topic.

 
*[Steps]*
1. Start mm2 with the attachment: connect-mirror-maker.properties.
2.Create topic DL2, push 5 messages and consume with group g1.  Offset info of DL2 and A.DL2：please see attachment offset1.png
3.Stop mm2 and *push 5 messages* to the topic DL2.  Offset info of DL2 and A.DL2： please see attachment offset2.png
4. After 1 hour, the data of topic DL2 is expired.
5.Restart mm2 and sync consumer group offset, found the consumer group offset is bigger than topic partiton offset. Offset info of DL2 and A.DL2: please see attachment offset3.png

 
*[Comments]*

In the method translateDownstream of OffsetSyncStore class, I found use the latest upstream consumer group offset and the latest offset-sync topic record's downstream offset to caculate the downstream offset:
  long upstreamStep = upstreamOffset - offsetSync.get().upstreamOffset();
  return OptionalLong.of(offsetSync.get().downstreamOffset() + upstreamStep);
 
If there is data is consumed and deleted in source cluster，but isn't replicated to target cluster , the downstream offset will bigger than topic partiton offset after consumer group offset synced to target cluster (the target cluster topic  has not active consumer). 
 
In the method syncGroupOffset of MirrorCheckpointTask class, there is no check for consumer group offset and topic partiton offset.

 ",,gharris1727,qinfuhui,rleslie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12468,,,,"24/May/22 07:59;qinfuhui;connect-mirror-maker.properties;https://issues.apache.org/jira/secure/attachment/13044112/connect-mirror-maker.properties","24/May/22 08:02;qinfuhui;offset1.png;https://issues.apache.org/jira/secure/attachment/13044113/offset1.png","24/May/22 08:07;qinfuhui;offset2.png;https://issues.apache.org/jira/secure/attachment/13044114/offset2.png","24/May/22 08:07;qinfuhui;offset3.png;https://issues.apache.org/jira/secure/attachment/13044115/offset3.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 16:41:29 UTC 2023,,,,,,,,,,"0|z12mq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 16:41;gharris1727;This was resolved as part of KAFKA-12468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kafka Connect Sink Connector Success after RetriableException, no commit offset to remote.",KAFKA-13927,13446271,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jbull,gjchoi,gjchoi,23/May/22 01:49,20/Sep/22 17:46,13/Jul/23 09:17,20/Sep/22 17:46,2.8.0,,,,,,,,,,,,,,,3.4.0,,,,,,,KafkaConnect,,,,,,1,,,,,,"I made a custom SinkConnector.

and I set retries with RetriableException.

 

In normal case, successfully offset commited with no LAG.

but one or more RetriableException occured and then, retry make success

there are LAG about sucess record, despite the retry success.

 ",,ChrisEgerton,gjchoi,jbull,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 27 03:13:44 UTC 2022,,,,,,,,,,"0|z12l68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/22 03:13;jbull;I've also encountered this. This is especially problematic when the stream is bursty and the last batch of messages encounters a RetriableException. In such a case, the offsets will never be committed resulting in sustained lag.

 

Proposed fix in https://github.com/apache/kafka/pull/12566/commits/109ad70e20206cc82ebb155776b42e547cdd243d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test TopicCommandIntegrationTest testDescribeAtMinIsrPartitions(String).quorum=kraft,KAFKA-13921,13446036,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,divijvaidya,divijvaidya,divijvaidya,20/May/22 10:09,21/May/22 18:49,13/Jul/23 09:17,21/May/22 18:49,,,,,,,,,,,,,,,,3.3.0,,,,,,,unit tests,,,,,,0,flaky-test,,,,,"[https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-12184/1/tests]


{code:java}
org.opentest4j.AssertionFailedError: expected: <1> but was: <7>	at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)	at app//org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)	at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)	at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)	at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:527)	at app//kafka.admin.TopicCommandIntegrationTest.testDescribeAtMinIsrPartitions(TopicCommandIntegrationTest.scala:704)	at java.base@11.0.12/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at java.base@11.0.12/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at java.base@11.0.12/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.base@11.0.12/java.lang.reflect.Method.invoke(Method.java:566)	at app//org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)	at app//org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)	at app//org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)	at app//org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)	at app//org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)	at app//org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)	at app//org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)	at app//org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)	at app//org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)	at app//org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)	at app//org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)	at app//org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at app//org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)	at app//org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)	at app//org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)	at app//org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at app//org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at app//org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at app//org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)	at app//org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)	at app//org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)	at app//org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)	at java.base@11.0.12/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183){code}
 ",,divijvaidya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 21 18:45:15 UTC 2022,,,,,,,,,,"0|z12jq0:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"21/May/22 18:45;divijvaidya;This is fixed in [https://github.com/apache/kafka/pull/12189] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rate is calculated as NaN for minimum config values,KAFKA-13911,13445676,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,divijvaidya,divijvaidya,divijvaidya,18/May/22 16:30,22/Aug/22 18:23,13/Jul/23 09:17,22/Aug/22 18:23,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"Implementation of connection creation rate quotas in Kafka is dependent on two configurations:
 # [quota.window.num|https://kafka.apache.org/documentation.html#brokerconfigs_quota.window.num]
 # [quota.window.size.seconds|https://kafka.apache.org/documentation.html#brokerconfigs_quota.window.size.seconds]

The minimum possible values of these configuration is 1 as per the documentation. However, 1 as a minimum value for quota.window.num is invalid and leads to failure for calculation of rate as demonstrated below.

As a proof of the bug, the following unit test fails:
{code:java}
@Test
public void testUseWithMinimumPossibleConfiguration() {
    final Rate r = new Rate();
    MetricConfig config = new MetricConfig().samples(1).timeWindow(1, TimeUnit.SECONDS);
    Time elapsed = new MockTime();
    r.record(config, 1.0, elapsed.milliseconds());
    elapsed.sleep(100);
    r.record(config, 1.0, elapsed.milliseconds());
    elapsed.sleep(1000);
    final Double observedRate = r.measure(config, elapsed.milliseconds());
    assertFalse(Double.isNaN(observedRate));
} {code}
 ",,divijvaidya,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 18:23:34 UTC 2022,,,,,,,,,,"0|z12hi0:",9223372036854775807,,ijuma,,,,,,,,,,,,,,,,,,"22/Aug/22 18:23;jagsancio;Closing as it was merged to trunk and 3.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent error codes returned from AlterConfig APIs,KAFKA-13899,13444881,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,13/May/22 16:40,17/May/22 00:44,13/Jul/23 09:17,17/May/22 00:42,,,,,,,,,,,,,,,,3.2.1,,,,,,,,,,,,,0,,,,,,"In the AlterConfigs/IncrementalAlterConfigs zk handler, we return INVALID_REQUEST and INVALID_CONFIG inconsistently. The problem is in `LogConfig.validate`. We may either return `ConfigException` or `InvalidConfigException`. When the first of these is thrown, we catch it and convert to INVALID_REQUEST. It seems more consistent to convert to INVALID_CONFIG.

Note that the kraft implementation returns INVALID_CONFIG consistently.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-13 16:40:16.0,,,,,,,,,,"0|z12cmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dedupe RemoveAccessControlEntryRecord in deleteAcls of AclControlManager,KAFKA-13892,13444200,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,andyg2,andyg2,andyg2,10/May/22 15:27,11/May/22 09:21,13/Jul/23 09:17,10/May/22 22:37,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"In [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java#L143] we loop through the ACL filters and and add RemoveAccessControlEntryRecord records to the response list for each matching ACL. I think there's a bug where if two filters match the same ACL, we create two RemoveAccessControlEntryRecord records for that same ACL. This is an issue because upon replay we throw an exception (https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java#L195) if the ACL is not in the in-memory data structures which will happen to the second RemoveAccessControlEntryRecord.

Maybe we can just de-dupe both List<AclDeleteResult> and List<ApiMessageAndVersion>? I think something like (just showing code for ApiMessageAndVersion):
{code:java}
private List<ApiMessageAndVersion> deDupeApiMessageAndVersion(List<ApiMessageAndVersion> messages) {
return new HashSet<>(messages).stream().collect(Collectors.toList());
}{code}
should suffice as I don't think the ordering matters within the list of response objects.  ",,andyg2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 10 22:38:30 UTC 2022,,,,,,,,,,"0|z128g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 19:11;andyg2;I dont think we need to dedupe List<AclDeleteResult>. It contains a list of results, where each result contains the ACLs that matched the filter. It should be OK for the same ACL to be in multiple AclDeleteResult results because it really could match multiple filters.;;;","10/May/22 22:38;andyg2;Merged in https://github.com/apache/kafka/commit/040b11d70594e0499e96014e17a307366b640444;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sync group failed with rebalanceInProgress error cause rebalance many rounds in coopeartive,KAFKA-13891,13444164,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,aiquestion,aiquestion,10/May/22 12:13,05/May/23 00:12,13/Jul/23 09:17,03/May/23 16:58,3.0.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,,,,clients,,,,,,0,,,,,,"This issue was first found in [KAFKA-13419|https://issues.apache.org/jira/browse/KAFKA-13419]

But the previous PR forgot to reset generation when sync group failed with rebalanceInProgress error. So the previous bug still exists and it may cause consumer to rebalance many rounds before final stable.

Here's the example ({*}bold is added{*}):
 # consumer A joined and synced group successfully with generation 1 *( with ownedPartition P1/P2 )*
 # New rebalance started with generation 2, consumer A joined successfully, but somehow, consumer A doesn't send out sync group immediately
 # other consumer completed sync group successfully in generation 2, except consumer A.
 # After consumer A send out sync group, the new rebalance start, with generation 3. So consumer A got REBALANCE_IN_PROGRESS error with sync group response
 # When receiving REBALANCE_IN_PROGRESS, we re-join the group, with generation 3, with the assignment (ownedPartition) in generation 1.
 # So, now, we have out-of-date ownedPartition sent, with unexpected results happened
 # *After the generation-3 rebalance, consumer A got P3/P4 partition. the ownedPartition is ignored because of old generation.*
 # *consumer A revoke P1/P2 and re-join to start a new round of rebalance*
 # *if some other consumer C failed to syncGroup before consumer A's joinGroup. the same issue will happens again and result in many rounds of rebalance before stable*

 ",,ableegoldman,aiquestion,cadonna,jdrean,kirktrue,mimaison,pnee,showuon,tmancill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 00:12:02 UTC 2023,,,,,,,,,,"0|z12888:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 17:31;jdrean;This ticket was reverted, see PR here: [https://github.com/apache/kafka/pull/12794]

You can find the discussion about this revert here: https://issues.apache.org/jira/browse/KAFKA-14016;;;","08/Nov/22 10:30;cadonna;[~jdrean] [~ableegoldman] [~dajac] [~showuon] Should we reopen this ticket then?;;;","09/Nov/22 00:47;ableegoldman;Reopening – original fix was reverted, we should instead fix this assignor-side by making it smarter about partition ownership across generations. Basically, it should take as the previous owner whichever consumer has the highest generation and claims it among their owned partitions

 

[~showuon] I probably won't be able to get to this within the next few days so if you're interested in picking up this fix go ahead and I'll find time to review – otherwise I will try to get to it in time for the 3.4 release;;;","10/Nov/22 02:09;showuon;I'm also very busy these days. [~aiquestion] , if you have time, welcome to take over to improve it.

 ;;;","15/Dec/22 02:40;ableegoldman;Bumping this to 3.5.0 since we're past 3.4.0 code freeze, but I'll try to find either time to work on it myself, or someone else who can pick it up so that it doesn't get lost;;;","09/Mar/23 19:55;kirktrue;[~pnee] please let me know if you need help on this. Happy to help where I can.;;;","28/Apr/23 16:35;pnee;[~dajac] - I think we resolve this and the subsequent issues (KAFKA-14016);;;","03/May/23 08:02;mimaison;We are past code freeze for 3.5 so moving this to the next release.;;;","03/May/23 16:55;pnee;Hey [~mimaison]  - this is actually the same issue as https://issues.apache.org/jira/browse/KAFKA-14639

 

I think the story is a bit complicated, but all these issues should be fixed by KAFKA-14639. So I think we can resolve them...

KAFKA-13891 (this)

KAFKA-14016

KAFKA-14639;;;","05/May/23 00:12;ableegoldman;Thanks [~pnee] – just to round out the above, the actual fix was in [pull/13550|https://github.com/apache/kafka/pull/13550];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AclsDelta to handle ACCESS_CONTROL_ENTRY_RECORD quickly followed by REMOVE_ACCESS_CONTROL_ENTRY_RECORD for same ACL,KAFKA-13889,13444041,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,andyg2,andyg2,andyg2,09/May/22 18:44,26/May/22 17:12,13/Jul/23 09:17,22/May/22 01:24,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"In [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/image/AclsDelta.java#L64] we store the pending deletion in the changes map. This could override a creation that might have just happened. This is an issue because in BrokerMetadataPublisher this results in us making a removeAcl call which finally results in [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java#L203] being executed and this code throws an exception if the ACL isnt in the Map yet. If the ACCESS_CONTROL_ENTRY_RECORD event never got processed by BrokerMetadataPublisher then the ACL wont be in the Map yet.

My feeling is we might want to make removeAcl idempotent in that it returns success if the ACL doesn't exist: no matter how many times removeAcl is called it returns success if the ACL is deleted. Maybe we’d just log a warning or something?

Note, I dont think the AclControlManager has this issue because it doesn't batch the events like AclsDelta does. However, we still do throw a RuntimeException here [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java#L197] - maybe we should still follow the same logic (if we make the fix suggested above) and just log a warning if the ACL doesnt exist in the Map?",,andyg2,cmccabe,dengziming,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 18 21:21:32 UTC 2022,,,,,,,,,,"0|z127i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 20:01;cmccabe;Thanks for picking this up, [~andyg2]!

I would really prefer that we not relax the invariants here. We don't want an actual bug to go undetected because we decided to ignore deletions of ACLs we couldn't locate.

I think the best way to handle this is to check if the ACL being deleted exists in the image when adding a new deletion. If the ACL exists in the Delta but not in the underlying image, we can just remove it entirely from the Delta, so calling code never sees it.;;;","09/May/22 20:41;andyg2;Thanks [~cmccabe]. Yeah that's fair point - if a bug caused us to pass along the wrong ID for example we might end up not deleting the ACL and not even noticing.

I think what you said makes sense. Since we have access to the AclImage we should be able to determine if we can just discard the ACL from the Map. ;;;","18/May/22 17:19;jagsancio;[~andyg2] in the description you mention that this is also an issue in `AclControlManagaer`. I glanced at PR 12160 and it doesn't look to fix this type in that PR. Are you planning to include it in that PR or should we file a Jira for the issue with `AclControlManager`?;;;","18/May/22 19:42;andyg2;[~jagsancio] With the solution [~cmccabe] suggested above, and the code I've implemented, AclControlManager doesnt need updating. Originally I suggested the code that calls removeAcl should catch the exception and continue on. If we had gone with this approach we'd need to add that logic in AclControlManager and BrokerMetadataPublisher because both places call removeAcl and I figured we should be consistent in our error handling. But as [~cmccabe] mentioned above this could hide bugs. So in the PR I've confined the changes to AclsDelta such that we wont return the delete event to BrokerMetadataPublisher and so we wont call removeAcl for an ACL that never got added. So with this approach neither AclControlManager nor BrokerMetadataPublisher need updating.

Having said all this, per [https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum] ""Note it is possible that a request could time out before the leader has successfully committed the records, and the client or the broker itself would retry, which would result in duplicated updates to the quorum. Since in Kafka's usage, all updates are overwrites which are idempotent (as the nature of configuration is a key-value mapping). Therefore, we do not need to implement serial number or request caching to achieve ""exactly-once""."" I'm wondering if we do need to handle duplicates which could result in removeAcl being called twice. [~hachikuji] [~cmccabe] Thoughts? 

FWIW I think the PR is still valid because we can still keep the logic in the PR regardless and it's strictly an improvement over what we have today. If we want to handle duplicate REMOVE_ACCESS_CONTROL_ENTRY_RECORD records I think it can be done in a separate, larger PR after more discussion (I can create a Jira if so).;;;","18/May/22 21:21;jagsancio;> I'm wondering if we do need to handle duplicates which could result in removeAcl being called twice

Why would there be duplicates? Does the controller check that the ACL exists before writing the remove record and successfully responding to the RPC? Note that the controller always handles the queued events using the in-memory state which may include uncommitted data.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DefaultStreamPartitioner may get ""stuck"" to one partition for unkeyed messages",KAFKA-13880,13443498,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,alivshits,alivshits,06/May/22 00:43,18/Jun/22 03:17,13/Jul/23 09:17,18/Jun/22 03:17,2.4.0,,,,,,,,,,,,,,,3.3.0,,,,,,,streams,,,,,,0,,,,,,"While working on KIP-794, I noticed that DefaultStreamPartitioner does not call .onNewBatch.  The ""sticky"" DefaultStreamPartitioner introduced as a result of https://issues.apache.org/jira/browse/KAFKA-8601 requires .onNewBatch call in order to switch to a new partitions for unkeyed messages, just calling .partition would return the same ""sticky"" partition chosen during the first call to .partition.  The partition doesn't change even if the partition leader is unavailable.

Ideally, for unkeyed messages the DefaultStreamPartitioner should take advantage of the new built-in partitioning logic introduced in [https://github.com/apache/kafka/pull/12049.]  Perhaps, it could return null partition for unkeyed message, so that KafkaProducer could run built-in partitioning logic.",,alivshits,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-06 00:43:38.0,,,,,,,,,,"0|z12468:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exponential backoff for reconnect does not work,KAFKA-13879,13443446,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chernyih@gmail.com,chernyih@gmail.com,chernyih@gmail.com,05/May/22 18:37,11/May/22 15:45,13/Jul/23 09:17,10/May/22 22:27,2.7.0,,,,,,,,,,,,,,,,,,,,,,network,,,,,,0,,,,,,"When a client connects to a SSL listener using PLAINTEXT security protocol, after the TCP connection is setup, the client considers the channel setup is complete (in reality the channel setup is not complete yet). The client issues API version request after that. When issuing API version request, reconnection exponential backoff is reset. Since the broker expects SSL handshake, client's API version request will cause the connection to disconnect. Reconnect will happen without exponential backoff since it has been reset.

[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/ClusterConnectionStates.java#L249.]  ",,chernyih@gmail.com,rleslie,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-05 18:37:48.0,,,,,,,,,,"0|z123uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky RackAwarenessIntegrationTest.shouldDistributeStandbyReplicasOverMultipleClientTags,KAFKA-13877,13443428,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,05/May/22 17:06,03/Aug/22 16:18,13/Jul/23 09:17,03/Aug/22 16:18,,,,,,,,,,,,,,,,,,,,,,,streams,unit tests,,,,,0,newbie,,,,,"The following test fails on local testbeds about once per 10-15 runs:

{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:87)
	at org.junit.Assert.assertTrue(Assert.java:42)
	at org.junit.Assert.assertTrue(Assert.java:53)
	at org.apache.kafka.streams.integration.RackAwarenessIntegrationTest.shouldDistributeStandbyReplicasOverMultipleClientTags(RackAwarenessIntegrationTest.java:192)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:53)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)


{code}",,guozhang,lkokhreidze,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 02 02:01:23 UTC 2022,,,,,,,,,,"0|z123qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/22 18:22;lkokhreidze;I will take this on. ;;;","05/May/22 18:42;guozhang;Thanks [~lkokhreidze]!;;;","25/Jul/22 17:20;guozhang;Hello [~lkokhreidze], are you still actively working on this flaky test?;;;","29/Jul/22 22:48;guozhang;[~lkokhreidze] ping again, please let me know if you are still working on it.;;;","02/Aug/22 02:01;guozhang;I've re-tested this case locally with 5+ times, each with 50 runs, and identified it is a flakiness by itself, not a real bug. The fix is summarized in https://github.com/apache/kafka/pull/12468.

On a side note, I think it's an overkill to really introduce the whole test class as an integration test, since all we need is to just test the assignor itself which could be a unit test. Running this suite with 9+ instances takes long time and is still vulnerable to all kinds of timing based flakiness (yes this PR alone cannot guarantee we've avoided all). So I will file a separate ticket for reducing this test into unit tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website updates to satisfy Apache privacy policies,KAFKA-13868,13443019,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,divijvaidya,mimaison,mimaison,03/May/22 19:30,28/Jul/22 07:05,13/Jul/23 09:17,28/Jul/22 07:05,,,,,,,,,,,,,,,,3.3.0,,,,,,,website,,,,,,0,,,,,,"The ASF has updated its privacy policy and all websites must be compliant.

The full guidelines can be found in [https://privacy.apache.org/faq/committers.html]

The Kafka website has a few issues, including:
- It's missing a link to the privacy policy: [https://privacy.apache.org/policies/privacy-policy-public.html]
- It's using Google Analytics
- It's using Google Fonts
- It's using scripts hosted on Cloudflare CDN
- Embedded videos don't have an image placeholder


As per the email sent to the PMC, all updates have to be done by July 22.
",,divijvaidya,githubbot,junrao,martijnvisser,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 14:10:11 UTC 2022,,,,,,,,,,"0|z1217s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 11:22;githubbot;divijvaidya opened a new pull request, #420:
URL: https://github.com/apache/kafka-site/pull/420

   As per the [Apache privacy policy](https://privacy.apache.org/faq/committers.html), Google Fonts are recommended to be hosted along with the website.
   
   This change  adds the fonts locally in the code
   
   # Testing
   Tested the website locally to ensure sanity
   
   **Before the change**
   ![Screenshot 2022-07-13 at 13 21 20](https://user-images.githubusercontent.com/71267/178722327-ef1c5d18-992f-40be-b1c1-492d8b643db0.png)
   
   **After the change (local)**
   ![Screenshot 2022-07-13 at 13 19 31](https://user-images.githubusercontent.com/71267/178722059-6f30459d-7bb8-496b-b907-e8e45a4aae8b.png)
    


;;;","13/Jul/22 11:23;divijvaidya;Fixed ""It's using Google Fonts"" -> [https://github.com/apache/kafka-site/pull/420] 

[~mimaison] please review.;;;","13/Jul/22 12:46;githubbot;divijvaidya commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1183179393

   @ijuma @mimaison please review.


;;;","13/Jul/22 14:43;githubbot;divijvaidya opened a new pull request, #421:
URL: https://github.com/apache/kafka-site/pull/421

   **Why**
   As per the [Apache branching policy](https://www.apache.org/foundation/marks/pmcs#navigation), every project website's main navigation system must feature certain text links back to key pages on the main www.apache.org website.
   
   **What**
   Added a new item 'Apache Software' to the top nav bar which includes the required links including Privacy Policy requirement outlined in https://issues.apache.org/jira/browse/KAFKA-13868 
   
   **Tested**
   Tested the change by running website locally. The new nav bar looks as follows:
   <img width=""1549"" alt=""Screenshot 2022-07-13 at 16 42 25"" src=""https://user-images.githubusercontent.com/71267/178761918-cf18f304-c13d-4a94-b567-1a6329bad4ee.png"">
   


;;;","13/Jul/22 14:44;divijvaidya;Addressed ""It's missing a link to the privacy policy"" -> [https://github.com/apache/kafka-site/pull/421]
cc: [~mimaison] for review;;;","13/Jul/22 14:44;githubbot;divijvaidya commented on PR #421:
URL: https://github.com/apache/kafka-site/pull/421#issuecomment-1183316551

   @mimaison please review.


;;;","13/Jul/22 16:15;githubbot;divijvaidya opened a new pull request, #422:
URL: https://github.com/apache/kafka-site/pull/422

   As per the [Apache privacy policy](https://privacy.apache.org/faq/committers.html), all JS files are recommended to be hosted along with the website so that we don't have a dependency on CDNs such as cloudflare.
   
   This change brings two JS libraries into the code base. 
   - prism: used for syntax highlighting
   - handlebars: templating library 
   
   # Testing
   Verified that syntax highlighting works as expected. No errors in the console.
   
   
   


;;;","13/Jul/22 16:16;divijvaidya;- It's using scripts hosted on Cloudflare CDN -> Fixed in https://github.com/apache/kafka-site/pull/422;;;","13/Jul/22 16:16;githubbot;divijvaidya commented on PR #422:
URL: https://github.com/apache/kafka-site/pull/422#issuecomment-1183421173

   @mimaison please review.


;;;","18/Jul/22 10:27;githubbot;mimaison commented on code in PR #420:
URL: https://github.com/apache/kafka-site/pull/420#discussion_r923193623


##########
css/fonts.css:
##########
@@ -0,0 +1,352 @@
+/* latin-ext */
+@font-face {
+    font-family: 'Cutive Mono';
+    font-style: normal;
+    font-weight: 400;
+    src: url(https://fonts.gstatic.com/s/cutivemono/v14/m8JWjfRfY7WVjVi2E-K9H6RMTm6o39ucNvc.woff2) format('woff2');

Review Comment:
   Doesn't this mean browsers will still fetch fonts from Google's servers?
   
   



;;;","18/Jul/22 10:31;githubbot;mimaison commented on PR #421:
URL: https://github.com/apache/kafka-site/pull/421#issuecomment-1187033048

   Thanks @divijvaidya for looking into this as the deadline is approaching quickly now!
   
   I'm not sure we need a full Apache menu. Have you considered just adding a Privacy Policy link in the footer. For example a bit like it's done on https://flink.apache.org/


;;;","18/Jul/22 10:35;githubbot;divijvaidya commented on PR #421:
URL: https://github.com/apache/kafka-site/pull/421#issuecomment-1187036010

   @mimaison thank you for your review.
   
   > I'm not sure we need a full Apache menu
   
   I think we do indeed need the full Apache menu. Please refer to [Apache branding policy](https://www.apache.org/foundation/marks/pmcs#navigation). Quoting a section from there:
   
   ```
   Whatever main navigation system your project website uses, it must feature certain text links back to key pages on the main www.apache.org website. These links can appear in whatever main navigation system your site uses on all top level pages for the project or subproject.
   
   ""License"" should link to: www.apache.org/licenses/
   
   ""Sponsorship"" or ""Donate"" should link to: www.apache.org/foundation/sponsorship.html
   
   ""Sponsors"", ""Thanks"" or ""Thanks to our Sponsors"" should link to: www.apache.org/foundation/thanks.html
   
   ""Security"" should link to either to a project-specific page detailing how users may securely report potential vulnerabilities, or to the main [www.apache.org/security/](https://www.apache.org/security/) page
   
   ""Privacy"" should link to: privacy.apache.org/policies/privacy-policy-public.html
   
   All projects must feature some prominent link back to the main ASF homepage at www.apache.org. This may either be a featured link in your main navigation system or a text link in your Main homepage text. A best practice is to include a short sentence or paragraph on the homepage noting that this project is an Apache project, and is part of a larger community of developers and users.
   ```
   Please let me know if I am missing something here.
   


;;;","18/Jul/22 10:41;githubbot;mimaison merged PR #422:
URL: https://github.com/apache/kafka-site/pull/422


;;;","18/Jul/22 14:20;githubbot;mimaison commented on PR #421:
URL: https://github.com/apache/kafka-site/pull/421#issuecomment-1187557669

   You are right, these links should be included. Thanks for following up!


;;;","18/Jul/22 14:25;githubbot;mimaison commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923435396


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software

Review Comment:
   Should we use `Apache Software Foundation` or `Apache` instead? 



;;;","18/Jul/22 14:33;githubbot;tombentley commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923438915


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/licenses/"">
+									License
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/events/current-event"">
+									Events
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/foundation/sponsorship.html"" target=""_blank"">
+									Sponsorship
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/foundation/thanks.html"">
+									Sponsors
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/security/"">

Review Comment:
   The Apache policy allows to link to a project specific page, and we already have this at https://kafka.apache.org/project-security. I think using that would be better that linking to the overarching Apache security page.



##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage

Review Comment:
   The policy says
   
   > This may either be a featured link in your main navigation system or a text link in your Main homepage text. A best practice is to include a short sentence or paragraph on the homepage noting that this project is an Apache project, and is part of a larger community of developers and users.
   
   So making the 'Apache' part of the homepage a link, and including this information further down (after the ""Trust and Ease of Use"" section) should also be acceptable, and indeed closer to the best practice. 
   
   Wdyt?



;;;","18/Jul/22 14:51;githubbot;divijvaidya commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923468457


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage

Review Comment:
   That is fair.
   
   Just to re-iterate so that we are on the same page, we want to remove the ""Apache Software"" section from the nav bar and instead add it to the footer of the homepage towards the bottom. That section will contain the necessary hyperlinks that are required by ASF. 
   
   I am a bit rusty with my front-end skills but let me try to see what I can do here :) 



;;;","18/Jul/22 15:17;githubbot;tombentley commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923497051


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage

Review Comment:
   Yeah, that's what I was suggesting, if it's acceptable to others. @mimaison, @ijuma please chime in if you disagree.



;;;","18/Jul/22 16:32;githubbot;divijvaidya commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923575015


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/licenses/"">
+									License
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/events/current-event"">
+									Events
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/foundation/sponsorship.html"" target=""_blank"">
+									Sponsorship
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/foundation/thanks.html"">
+									Sponsors
+								</a>
+							</li>
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/security/"">

Review Comment:
   done



;;;","18/Jul/22 16:33;githubbot;divijvaidya commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923575649


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software

Review Comment:
   This comment is not valid any more. I moved the links to footer which already have a link to Apache homepage.



;;;","18/Jul/22 16:38;githubbot;divijvaidya commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r923583008


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage

Review Comment:
   I have made the changes as suggested by @tombentley and added the ASF links to the footer. Note that there are some projects which choose to keep a separate section in nav bar for ASF links (e.g. [Spark](https://spark.apache.org/), [TinkerPop](https://tinkerpop.apache.org/)) while some other choose to keep it in the footer (e.g. [superset](https://superset.apache.org/), [Hudi](https://hudi.apache.org/)



;;;","18/Jul/22 16:54;githubbot;divijvaidya commented on code in PR #420:
URL: https://github.com/apache/kafka-site/pull/420#discussion_r923600664


##########
css/fonts.css:
##########
@@ -0,0 +1,352 @@
+/* latin-ext */
+@font-face {
+    font-family: 'Cutive Mono';
+    font-style: normal;
+    font-weight: 400;
+    src: url(https://fonts.gstatic.com/s/cutivemono/v14/m8JWjfRfY7WVjVi2E-K9H6RMTm6o39ucNvc.woff2) format('woff2');

Review Comment:
   you are right. This is my bad! Will fix.



;;;","18/Jul/22 18:20;githubbot;divijvaidya commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1188046357

   > As an alternative, have you tried getting rid of these custom fonts and using a standard one like helvetica?
   
   No. Roboto is a fairly common and popular font used across websites. I don't want to change the look/feel of the website as part of this change.
   
   I have added the font files now in this PR and also added the process of downloading them in the description. @mimaison, this is ready for your review.


;;;","19/Jul/22 07:21;githubbot;showuon commented on code in PR #421:
URL: https://github.com/apache/kafka-site/pull/421#discussion_r924146761


##########
includes/_top.htm:
##########
@@ -150,6 +150,48 @@
 							</li>
 						</ul>
 					</li>
+					<li class=""top-nav-item"" role=""menuitem"">
+						<a href=""#"" class=""top-nav-item-anchor"" aria-haspopup=""true"" aria-expanded=""false"" aria-controls=""nav-community-menu"">
+							Apache Software
+						</a>
+						<ul class=""top-nav-menu"" aria-hidden=""true"" role=""menu"" id=""nav-community-menu"" title=""Apache Homepage"">
+							<li class=""top-nav-menu-item"" role=""menuitem"">
+								<a class=""top-nav-anchor"" tabindex=""-1"" href=""https://www.apache.org/"" target=""_blank"">
+									Apache Homepage

Review Comment:
   Agree with @tombentley 's suggestion. 



;;;","19/Jul/22 07:39;githubbot;showuon commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1188708449

   @scott-confluent , could you help review this PR since you added the google font into this project? Thanks.


;;;","19/Jul/22 08:11;githubbot;divijvaidya commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1188739303

   > @divijvaidya , why do you use https://google-webfonts-helper.herokuapp.com/fonts/ to download the fonts, instead of the original link: [https://fonts.googleapis.com/css?family=Cutive+Mono|Roboto:100,300,400,700,900|Roboto+Condensed:300](https://fonts.googleapis.com/css?family=Cutive+Mono%7CRoboto:100,300,400,700,900%7CRoboto+Condensed:300) ? Will that make any difference?
   
   Hey @showuon 
   If you visit the link https://fonts.googleapis.com/css?family=Cutive+Mono%7CRoboto:100,300,400,700,900%7CRoboto+Condensed:300, the `url` still points to Google's CDN and hence, does not fulfil our objective of self-hosting the fonts. An alternative could be to download the fonts from the source https://fonts.google.com/ but they only allow to download font family in `ttf` format. In that case, we would need to use a third party tool to convert this `ttl` to `woff2` compression. Further this site, https://fonts.google.com/, does not allow (at least I couldn't find it) to download for different charset such as vietnamese etc.
   
   Note that helper tool I used https://google-webfonts-helper.herokuapp.com/fonts/  is using a MIT license and has [10K stars on Github](https://github.com/majodev/google-webfonts-helper), so I decided to trust it.


;;;","19/Jul/22 09:30;githubbot;showuon commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1188822349

   @divijvaidya , I see. But in https://fonts.googleapis.com/css?family=Cutive+Mono%7CRoboto:100,300,400,700,900%7CRoboto+Condensed:300 , we can download all the fonts in `woff2` format via the link inside the font css, ex: 
   
   ```
   /* latin-ext */
   @font-face {
     font-family: 'Cutive Mono';
     font-style: normal;
     font-weight: 400;
     src: url(https://fonts.gstatic.com/s/cutivemono/v14/m8JWjfRfY7WVjVi2E-K9H6RMTm6o39ucNvc.woff2) format('woff2');
     unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
   }
   ```
   We can download the font via https://fonts.gstatic.com/s/cutivemono/v14/m8JWjfRfY7WVjVi2E-K9H6RMTm6o39ucNvc.woff2 . 
   
   However, I don't insist that we should download the font from google. I just want to make sure we don't miss any fonts we are using in Kafka website now. So, my next question is, how do we know we only need these 9 fonts you added? I saw there are 44 fonts css in this link: https://fonts.googleapis.com/css?family=Cutive+Mono%7CRoboto:100,300,400,700,900%7CRoboto+Condensed:300 . Does that mean we actually only use 9 of them? How do you know that?
   
   Thank you again for helping working on this.


;;;","19/Jul/22 11:13;githubbot;divijvaidya commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1188923709

   > We can download the font via https://fonts.gstatic.com/s/cutivemono/v14/m8JWjfRfY7WVjVi2E-K9H6RMTm6o39ucNvc.woff2
   
   Oh yes. We can do that but I think that would be less maintainable. This is because when a new version of these fonts are available, we won't have a mechanism to update it. I can go ahead with this approach too if that makes it safer to accept this change. I don't have any strong opinions on this.
   
   > Does that mean we actually only use 9 of them? How do you know that?
   
   That is a very fair question to ask. The number of actual font files are lesser because the new font files contain all the charset in one file.
   
   I will wait till tomorrow for others to add their thoughts. After that I can change this PR with the approach that you mentioned. Sounds ok Luke?
   
   
   


;;;","19/Jul/22 12:23;githubbot;mimaison merged PR #421:
URL: https://github.com/apache/kafka-site/pull/421


;;;","19/Jul/22 12:43;githubbot;showuon commented on PR #420:
URL: https://github.com/apache/kafka-site/pull/420#issuecomment-1189006953

   
   
   > That is a very fair question to ask. The number of actual font files are lesser because the new font files contain all the charset in one file.
   
   I see. Thanks.
   
   > I will wait till tomorrow for others to add their thoughts. After that I can change this PR with the approach that you mentioned. Sounds ok Luke?
   
   Yes, let's see what other people's thoughts. But I don't think it needs to adopt my approach if the font output is the same. 
   
   Thank you. 


;;;","19/Jul/22 13:37;githubbot;divijvaidya opened a new pull request, #424:
URL: https://github.com/apache/kafka-site/pull/424

   As per the [ASF privacy policy](https://privacy.apache.org/faq/committers.html), Google Analytics should be replaced with Apache hosted version of Matomo to remain complaint with GDPR.
   
   Email thread where we received the site Id that is used with Matomo: https://lists.apache.org/thread/0rpo0ffcd70c2yxfnqfqk43oyg7c8x8d 
   
   ## Code changes
   - Remove Google Analytics script
   - Remote `google-site-verification` files which are used by Google to verify the ownership of site.
   - Add Matomo script to the `<head>` section (all JS scripts should ideally be placed there and not in `<body>`).
   
   ## Results
   After deploying the changes, we should be able to analyse the results at https://analytics.apache.org/index.php?module=MultiSites&action=index&idSite=1&period=day&date=yesterday


;;;","19/Jul/22 13:37;githubbot;divijvaidya commented on PR #424:
URL: https://github.com/apache/kafka-site/pull/424#issuecomment-1189065641

   @mimaison please review for the GDPR compliance.


;;;","19/Jul/22 13:45;githubbot;tombentley commented on code in PR #420:
URL: https://github.com/apache/kafka-site/pull/420#discussion_r924521925


##########
css/fonts.css:
##########
@@ -0,0 +1,82 @@
+/* cutive-mono-regular - latin-ext_latin */
+@font-face {
+    font-family: 'Cutive Mono';
+    font-style: normal;
+    font-weight: 400;
+    src: local(''),

Review Comment:
   Can you explain why the argument to `local` is the empty string? I'm no CSS expert, but from a quick google (https://stackoverflow.com/questions/3837249/font-face-src-local-how-to-use-the-local-font-if-the-user-already-has-it) it seems like it's supposed to be used for some kind of caching, but not with an empty string argument.



;;;","19/Jul/22 14:17;githubbot;divijvaidya commented on code in PR #420:
URL: https://github.com/apache/kafka-site/pull/420#discussion_r924561314


##########
css/fonts.css:
##########
@@ -0,0 +1,82 @@
+/* cutive-mono-regular - latin-ext_latin */
+@font-face {
+    font-family: 'Cutive Mono';
+    font-style: normal;
+    font-weight: 400;
+    src: local(''),

Review Comment:
   https://stackoverflow.com/a/22835957 explains the rationale (for smiley but empty string follows same logic). Though this is only necessary for IE6-8 (very old browsers) to handle an edge case (different font with same name locally). I think I would remove it to reduce confusion while reading code.
    
   



;;;","19/Jul/22 14:46;githubbot;tombentley commented on code in PR #420:
URL: https://github.com/apache/kafka-site/pull/420#discussion_r924595805


##########
css/fonts.css:
##########
@@ -0,0 +1,82 @@
+/* cutive-mono-regular - latin-ext_latin */
+@font-face {
+    font-family: 'Cutive Mono';
+    font-style: normal;
+    font-weight: 400;
+    src: local(''),

Review Comment:
   Ah, thanks! 



;;;","20/Jul/22 08:20;githubbot;mimaison merged PR #424:
URL: https://github.com/apache/kafka-site/pull/424


;;;","21/Jul/22 16:48;junrao;[~mimaison] : You mentioned ""As per the email sent to the PMC, all updates have to be done by July 22."", do you know when was the email sent to Kafka PMC related to this? I can't seem to find this. Thanks.;;;","21/Jul/22 16:59;mimaison;[~junrao] Yes the email was sent to the Kafka private list. Here is the link to the email: https://lists.apache.org/thread/w7w5vvc996qdtdhbmf7qf826g9v5fnyo;;;","22/Jul/22 17:09;githubbot;mimaison merged PR #420:
URL: https://github.com/apache/kafka-site/pull/420


;;;","22/Jul/22 17:11;mimaison;I think we've addressed all issues apart from the Youtube videos. Discussion about the videos on the dev list: https://lists.apache.org/thread/8drsbn0hgdhq4g1qgvm9g8pb5t4x42px;;;","25/Jul/22 15:45;junrao;Thanks, [~mimaison]. I see it now.;;;","25/Jul/22 17:33;githubbot;divijvaidya opened a new pull request, #427:
URL: https://github.com/apache/kafka-site/pull/427

   As per the [discussion in the community](https://lists.apache.org/thread/p24xvbf8nkvxpbj668vc0g3x3lojsnk4), we want to replace the embedded YouTube videos with hyperlinks to satisfy the [ASF privacy policy](https://privacy.apache.org/faq/committers.html).
   
   This code change replaces the embedded videos from two of the pages on the website with hyperlinks.
   
   **Before**
   ![Screenshot 2022-07-25 at 19 31 50](https://user-images.githubusercontent.com/71267/180839003-da0f967c-019b-449e-a7c3-3bbac37611dd.png)
   ![Screenshot 2022-07-25 at 19 32 13](https://user-images.githubusercontent.com/71267/180839020-b37a118d-b8ca-480e-8832-bd19b29cfbdd.png)
   
   **After**
   ![Screenshot 2022-07-25 at 19 32 01](https://user-images.githubusercontent.com/71267/180839040-591e67df-8053-4633-9c35-52d7fd32fd0c.png)
   ![Screenshot 2022-07-25 at 19 32 23](https://user-images.githubusercontent.com/71267/180839061-355b3e06-1e9c-40da-89b0-aefd95ee5be5.png)
   
   


;;;","25/Jul/22 17:33;githubbot;divijvaidya commented on PR #427:
URL: https://github.com/apache/kafka-site/pull/427#issuecomment-1194393568

   @bbejeck perhaps you would like to review this?


;;;","25/Jul/22 19:39;githubbot;bbejeck merged PR #427:
URL: https://github.com/apache/kafka-site/pull/427


;;;","25/Jul/22 19:39;githubbot;bbejeck commented on PR #427:
URL: https://github.com/apache/kafka-site/pull/427#issuecomment-1194536588

   Thanks for this fix @divijvaidya 


;;;","27/Jul/22 12:51;githubbot;divijvaidya opened a new pull request, #429:
URL: https://github.com/apache/kafka-site/pull/429

   This is a hot fix to merge the https://github.com/apache/kafka/pull/12438 into kafka-site to satisfy the ASF privacy policy requirements. 
   
   ## Testing
   Tested locally.
   ![Screenshot 2022-07-27 at 14 46 06](https://user-images.githubusercontent.com/71267/181250067-60fb0a07-b7e3-47e1-b61a-40b354ecddf7.png)
    


;;;","27/Jul/22 12:51;githubbot;divijvaidya commented on PR #429:
URL: https://github.com/apache/kafka-site/pull/429#issuecomment-1196687885

   @mimaison please review. Last one before we can close KAFKA-13868.


;;;","27/Jul/22 13:40;githubbot;showuon commented on PR #429:
URL: https://github.com/apache/kafka-site/pull/429#issuecomment-1196778133

   @mimaison , do you want to have another look? 


;;;","27/Jul/22 14:10;githubbot;mimaison merged PR #429:
URL: https://github.com/apache/kafka-site/pull/429


;;;"
 Fix ResponseSendTimeMs metric  in RequestChannel.scala  was removed repeatedly,KAFKA-13865,13442664,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,ZhaoBo03,ZhaoBo03,01/May/22 00:24,02/May/22 09:03,13/Jul/23 09:17,02/May/22 09:03,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,ResponseSendTimeMs metric was removed in line 576，but we removed it again in line 578.,,ZhaoBo03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-05-01 00:24:34.0,,,,,,,,,,"0|z11z14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent null config value when create topic in KRaft mode,KAFKA-13863,13442628,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,dengziming,dengziming,30/Apr/22 10:12,19/May/22 16:48,13/Jul/23 09:17,19/May/22 16:48,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,,,cmccabe,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 04 18:15:34 UTC 2022,,,,,,,,,,"0|z11yt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/22 18:15;cmccabe;We already validate that topic configurations can't be null in `ControllerConfigurationValidator`.

{code}
  override def validate(
    resource: ConfigResource,
    config: util.Map[String, String]
  ): Unit = {
    resource.`type`() match {
      case TOPIC =>
        validateTopicName(resource.name())
        val properties = new Properties()
        val nullTopicConfigs = new mutable.ArrayBuffer[String]()
        config.entrySet().forEach(e => {
          if (e.getValue() == null) {
            nullTopicConfigs += e.getKey()
          } else {
            properties.setProperty(e.getKey(), e.getValue())
          }
        })
        if (nullTopicConfigs.nonEmpty) {
          throw new InvalidRequestException(""Null value not supported for topic configs : "" +
            nullTopicConfigs.mkString("",""))
        }
{code}

I don't mind adding extra test coverage but there should be no need to change `ReplicationControlManager`, as far as I can see.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add And Subtract multiple config values is not supported in KRaft mode,KAFKA-13862,13442620,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,dengziming,dengziming,30/Apr/22 07:46,10/May/22 19:41,13/Jul/23 09:17,10/May/22 19:41,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,,,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-30 07:46:07.0,,,,,,,,,,"0|z11yrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validateOnly request field does not work for CreatePartition requests in Kraft mode.,KAFKA-13861,13442403,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,akhileshchg,akhileshchg,akhileshchg,28/Apr/22 22:30,11/May/22 18:23,13/Jul/23 09:17,04/May/22 17:54,,,,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,,,,,,,0,,,,,,`ControllerApis` ignores the validateOnly field and the `QuorumController` does not have any logic to handle the `validateOnly` requests for `CreatePartitions.,,akhileshchg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-28 22:30:27.0,,,,,,,,,,"0|z11xfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kraft should not shutdown metadata listener until controller shutdown is finished,KAFKA-13858,13441900,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,hachikuji,hachikuji,26/Apr/22 20:39,20/Feb/23 03:46,13/Jul/23 09:17,25/May/22 23:09,,,,,,,,,,,,,,,,3.3,,,,,,,,,,,,,0,kip-500,,,,,"When the kraft broker begins controlled shutdown, it immediately disables the metadata listener. This means that metadata changes as part of the controlled shutdown do not get sent to the respective components. For partitions that the broker is follower of, that is what we want. It prevents the follower from being able to rejoin the ISR while still shutting down. But for partitions that the broker is leading, it means the leader will remain active until controlled shutdown finishes and the socket server is stopped. That delay can be as much as 5 seconds and probably even worse.

In the zk world, we have an explicit request `StopReplica` which serves the purpose of shutting down both follower and leader, but we don't have something similar in kraft. For KRaft, we may not necessarily need an explicit signal like this. We know that the broker is shutting down, so we can treat partition changes as implicit `StopReplica` requests rather than going through the normal `LeaderAndIsr` flow.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 25 23:10:26 UTC 2022,,,,,,,,,,"0|z11ucg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 23:10;hachikuji;See also KAFKA-13916 which addresses a remaining problem related to controlled shutdown.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Acl documentation bug for wildcard '*',KAFKA-13852,13441298,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,hongwei.xiang,hongwei.xiang,23/Apr/22 13:55,24/Apr/22 09:07,13/Jul/23 09:17,24/Apr/22 09:07,3.1.0,3.1.2,3.2.0,,,,,,,,,,,,,3.3.0,,,,,,,docs,documentation,,,,,0,,,,,,"There is a Kafka Acl documentation bug for wildcard '*' in the [Examples|https://kafka.apache.org/documentation/#security_authz_examples].

The bug is when we run the below script in one folder which is not empty, we can not set ACL correctly. However, it works only the folder is empty.

We can find the scripts with wildcard '*' from the Kafka documentation.

 
{code:java}
// Adding Acls
> bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Peter --allow-host 198.51.200.1 --producer --topic * 

// List Acls
> bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic *{code}
 

Reproduce the issue:
 # Create a file foo.txt under an empty folder
 # Run the script to add an acl by using the wildcard resource '*'
 # We can find the resource name is 'foo.txt'. Not wildcard '*'

 
{code:java}
// code placeholder
(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ll
total 0
(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  touch foo.txt
(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ll
total 0
-rw-r--r--  1 hongwei.xiang  345931250     0B Apr 23 19:05 foo.txt
(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ~/bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Peter --allow-host 198.51.200.1 --producer --topic *
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=foo.txt, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW)
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=foo.txt, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW)
(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ~/bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Peter --allow-host 198.51.200.1 --topic * --producer
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=foo.txt, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW)
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=foo.txt, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW) 

(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ~/bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic *
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=foo.txt, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW){code}
 

To resolve the issue:

Add single quotes for the wildcard '*' in the script.
{code:java}
(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ~/bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Peter --allow-host 198.51.200.1 --producer --topic '*'
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW)
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW) 


(base)  hongwei.xiang@hongweixiang  ~/Downloads/test  ~/bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic '*'
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`:
    (principal=User:Peter, host=198.51.200.1, operation=WRITE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=CREATE, permissionType=ALLOW)
    (principal=User:Peter, host=198.51.200.1, operation=DESCRIBE, permissionType=ALLOW){code}
I've submitted a pull request: ""KAFKA-13852: Kafka Acl documentation bug for wildcard '*' #12090""","Mac OS, Linux",hongwei.xiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-23 13:55:57.0,,,,,,,,,,"0|z11qnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-metadata-shell is missing some record types,KAFKA-13850,13441167,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,mumrah,mumrah,22/Apr/22 13:57,25/Aug/22 13:03,13/Jul/23 09:17,25/Aug/22 07:07,,,,,,,,,,,,,,,,3.3,,,,,,,kraft,,,,,,0,,,,,,"Noticed while working on feature flags in KRaft, the in-memory tree of the metadata  (MetadataNodeManager) is missing support for a few of record types. 
 * DelegationTokenRecord
 * UserScramCredentialRecord (should we include this?)
 * FeatureLevelRecord
 * AccessControlEntryRecord

 ",,davidarthur,dengziming,mumrah,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13981,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 23 02:24:53 UTC 2022,,,,,,,,,,"0|z11pvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/22 02:13;dengziming;In fact I already found this problem, however, currently, DelegationTokenRecord and UserScramCredentialRecord are not used, FeatureLevelRecord will only be useful after KIP-778, so I'm waiting for KIP-778 to be done before I submitting this PR.;;;","23/Apr/22 02:24;davidarthur;Awesome! Good to know [~dengziming]. Can you assign this one to yourself? We can discuss the other records in the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clients remain connected after SASL re-authentication fails,KAFKA-13848,13441071,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,acsaki,acsaki,acsaki,22/Apr/22 07:30,04/Aug/22 07:53,13/Jul/23 09:17,10/Jun/22 14:26,3.1.0,,,,,,,,,,,,,,,3.3.0,,,,,,,clients,,,,,,0,Authentication,OAuth2,SASL,,,"Clients remain connected and able to produce or consume despite an expired OAUTHBEARER token.

The problem can be reproduced using the https://github.com/acsaki/kafka-sasl-reauth project by starting the embedded OAuth2 server and Kafka, then running the long running consumer in OAuthBearerTest and then killing the OAuth2 server thus making the client unable to re-authenticate.

Root cause seems to be SaslServerAuthenticator#calcCompletionTimesAndReturnSessionLifetimeMs failing to set ReauthInfo#sessionExpirationTimeNanos when tokens have already expired (when session life time goes negative), in turn causing KafkaChannel#serverAuthenticationSessionExpired returning false and finally SocketServer not closing the channel.

The issue is observed with OAUTHBEARER but seems to have a wider impact on SASL re-authentication.",https://github.com/acsaki/kafka-sasl-reauth,acsaki,gkomlossi,kirktrue,showuon,viktorsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 10 14:26:55 UTC 2022,,,,,,,,,,"0|z11pa0:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,"22/Apr/22 07:41;acsaki;Please assign this to me.

My fix is underway, there's an easy way to do it in SaslServerAuthenticator#calcCompletionTimesAndReturnSessionLifetimeMs but I'm struggling with writing the tests either in SaslServerAuthenticatorTest or in SaslAuthenticatorTest .

With SaslServerAuthenticatorTest my main issue is that I have to walk through the state machine thus making the test not focused enough and hard to follow. With SaslAuthenticatorTest, I cannot seem to parse responses to at least assert that SaslAuthenticateResponse#sessionLifetimeMs is set to a non-null value. Another way to make this more integration like test more explicit would be to make NioEchoServer emulate SocketServer's behavior by closing the channel and assert for that. (NetworkTestUtils#waitForChannelClose)

Please advise how it's best to test this.;;;","09/May/22 13:03;viktorsomogyi;[~acsaki] please write an email to the dev@kafka.apache.org email list so they can add you as a contributors. After this you'll be able to assign the jira to yourself. You can raise a PR regradless though.
(more on contribution: https://kafka.apache.org/contributing);;;","24/May/22 06:30;showuon;[~acsaki] , thanks for reporting the issue. The change makes sense to me. But I'd like to hear the code author's comments. cc [~rndgstn] [~rsivaram] .

 ;;;","10/Jun/22 14:26;acsaki;Thank you [~showuon] , [~tombentley] and Sam Barker for the review! I'm leaving ""fix version"" empty for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumer is unable to recover connection to group coordinator after commitOffsetsAsync exception,KAFKA-13840,13440757,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,showuon,kyle.stehbens,kyle.stehbens,20/Apr/22 18:48,16/Sep/22 02:13,13/Jul/23 09:17,16/Sep/22 02:12,2.6.1,2.7.2,2.8.1,3.0.0,3.1.0,,,,,,,,,,,3.2.1,,,,,,,clients,consumer,,,,,4,,,,,,"Hi, I've discovered an issue with the java Kafka client (consumer) whereby a timeout or any other retry-able exception triggered during an async offset commit, renders the client unable to recover its group co-coordinator and leaves the client in a broken state.

 

I first encountered this using v2.8.1 of the java client, and after going through the code base for all versions of the client, have found it affects all versions of the client from 2.6.1 onward.

I also confirmed that by rolling back to 2.5.1, the issue is not present.

 

The issue stems from changes to how the FindCoordinatorResponseHandler in 2.5.1 used to call clearFindCoordinatorFuture(); on both success and failure here:
[https://github.com/apache/kafka/blob/0efa8fb0f4c73d92b6e55a112fa45417a67a7dc2/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L783]

 

In all future version of the client this call is not made:
[https://github.com/apache/kafka/blob/839b886f9b732b151e1faeace7303c80641c08c4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L838]

 

What this results in, is when the KafkaConsumer makes a call to coordinator.commitOffsetsAsync(...), if an error occurs such that the coordinator is unavailable here:

[https://github.com/apache/kafka/blob/c5077c679c372589215a1b58ca84360c683aa6e8/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L1007]

 

then the client will try call:

[https://github.com/apache/kafka/blob/c5077c679c372589215a1b58ca84360c683aa6e8/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L1017]

However this will never be able to succeed as it perpetually returns a reference to a failed future: findCoordinatorFuture that is never cleared out.

 

This manifests in all future calls to commitOffsetsAsync() throwing a ""coordinator unavailable"" exception forever going forward after any retry-able exception causes the coordinator to close. 

Note we discovered this when we upgraded the kafka client in our Flink consumers from 2.4.1 to 2.8.1 and subsequently needed to downgrade the client. We noticed this occurring in our non-flink java consumers too running 3.x client versions.

 ",,Christian.Lorenz77,david.artiga,guozhang,kyle.stehbens,martijnvisser,mason6345,renqs,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 04 10:10:12 UTC 2022,,,,,,,,,,"0|z11ncg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/22 07:28;showuon;[~kyle.stehbens] , thanks for reporting this issue. Yes, this is a regression issue caused by KAFKA-10793 

I've checked, `coordinator.commitOffsetsAsync` is the remaining place we call `lookupCoordinator()` directly without clearing the `findCoordinatorFuture`. I'll work on a fix to it. cc [~wangguangyuan] [~ableegoldman] ;;;","21/Apr/22 07:36;showuon;But I'm wondering why the heartbeat thread doesn't help clear the `findCoordinatorFuture`?

[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1436-L1443]

 

[~kyle.stehbens] , do you have complete client logs for us investigation? Thanks.

You can remove the confidential info if you want. I just want to make sure if there are other reasons causes this issue. Thanks.;;;","21/Apr/22 08:03;showuon;[~kyle.stehbens] , sorry, another question, are you using `Consumer#assign()` or `Consumer#subscribe()` to subscribe topics? Do you think KAFKA-13563 fixes your issue?;;;","21/Apr/22 16:41;kyle.stehbens;Thanks [~showuon] !

In the particular case I was investigating we are using the KafkaSource (with check pointing enabled) in flink as our wrapper to the KafkaClient.

I'm fairly sure its doing consumer.assign() in order to start from check-pointed offsets rather than from the groups committed ones - though it is also working in group mode (as the missing commits and progress on each partition in the group is how we noticed this issue).

I did also see issue https://issues.apache.org/jira/browse/KAFKA-13563 but I'm not sure this fixes the issue (Mainly because looking at the change delta I can't see how the changes would affect the code paths in question)

I was also confused as to why the heartbeat wasn't working to clear out the failed future ref as well and I think it may be due to a subtle change in the heartbeart logic here:
[https://github.com/apache/kafka/blob/0efa8fb0f4c73d92b6e55a112fa45417a67a7dc2/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1273]

vs here:
[https://github.com/apache/kafka/blob/839b886f9b732b151e1faeace7303c80641c08c4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1367]

 

Possibly before hand the logic was only checking if state != MemberState.STABLE allowing the code to fall through to this check:

[https://github.com/apache/kafka/blob/0efa8fb0f4c73d92b6e55a112fa45417a67a7dc2/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1284]

which would have cleared the future on failure as the logic exists handler to do so.

The new logic will likely hit a continue; call further up never allowing the logic to reach here ( if (state.hasNotJoinedGroup() || hasFailed()) ) [https://github.com/apache/kafka/blob/839b886f9b732b151e1faeace7303c80641c08c4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1379]

 

I'll see if I can surface you some logs of this happening, but in the mean time the general pattern was:
A Timeout occurred from our broker calling commitAsync(); Marking the oordinator as unavailable(). (Timeout retry-able exception)

Next commitAsync(), and further commit asyncs all result in ""coordinator unavailable"" exceptions logged.;;;","21/Apr/22 18:07;kyle.stehbens;These are some of the (redacted) logs from when the issue starts occurring.

Note we have flink check pointing setup for every 30s, hence the calls to commitAsync() are attempted every 30s.

First the timeout is what triggers the issue (Caused because our brokers are likely too busy and we sometimes get a timeout once every few hours).

From then on forward, any further commit attempt results in CoordinatorNotAvailableException.

 
2022-04-18 05:10:14,006 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator [] - [Consumer clientId=auxiliary-6, groupId=writer.main.auxiliary] Group coordinator kaf9.mycompany.com:9092 (id: 2147482632 rack: null) is unavailable or invalid due to cause: error response REQUEST_TIMED_OUT.isDisconnected: false. Rediscovery will be attempted.
 
2022-04-18 05:10:40,713 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 13149 org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets. Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
 
2022-04-18 05:11:10,890 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 13150 org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets. Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
 
2022-04-18 05:11:41,051 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 13151 org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets. Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.;;;","25/Apr/22 07:17;showuon;[~kyle.stehbens] , thanks for the reply.

If it is using `consumer.assign()`, then the KAFKA-13563 should fix the issue. It fixes in another path, where we tried to do the `findCoordinatorFuture` clearing the same place. Anyway, the fix for KAFKA-13563 is in v3.1.1 and v3.2.0, which should be released soon (the RC build is out now). Could you give it a try after they release? Thank you.;;;","26/Apr/22 16:55;kyle.stehbens;Gotcha, i'll give these new versions a try on one of our less important jobs as soon as they are released and report back here with the results.;;;","17/Jun/22 00:23;kyle.stehbens;I confirmed that this problem is still present in 3.1.1 - observed in the Kafka client used in a mirror maker deploy today.

 ;;;","17/Jun/22 01:36;showuon;Thank you, Kyle!;;;","17/Jun/22 01:38;showuon;Oh, wait, the issue still happened in v3.1.1!? Could you share the client and broker logs?;;;","17/Jun/22 16:40;kyle.stehbens;Sure, looks like the same kind of of co-coordinator refresh loop where its got a stale findCoordinatorFuture that's not cleared after a re-connect.

MM logs show recurring batches of the following lines.

```
[2022-06-17 16:16:20,956] INFO [Consumer clientId=consumer-mm.dmz.to.internal-9, groupId=mm.dmz.to.internal] Node 2147483642 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-06-17 16:16:20,956] INFO [Consumer clientId=consumer-mm.dmz.to.internal-9, groupId=mm.dmz.to.internal] Group coordinator internal-kaf05.company.corp:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:916)
[2022-06-17 16:16:20,958] INFO [Consumer clientId=consumer-mm.dmz.to.internal-9, groupId=mm.dmz.to.internal] Discovered group coordinator internal-kaf05.company.corp:9092 (id: 2147483642 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-06-17 16:16:20,958] INFO [Consumer clientId=consumer-mm.dmz.to.internal-9, groupId=mm.dmz.to.internal] Group coordinator internal-kaf05.company.corp:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:916)
[2022-06-17 16:16:20,958] INFO [Consumer clientId=consumer-mm.dmz.to.internal-9, groupId=mm.dmz.to.internal] Requesting disconnect from last known coordinator internal-kaf05.company.corp:9092 (id: 2147483642 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:929)
[2022-06-17 16:16:21,060] INFO [Consumer clientId=consumer-mm.dmz.to.internal-9, groupId=mm.dmz.to.internal] Discovered group coordinator internal-kaf05.company.corp:9092 (id: 2147483642 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
```
 
The second log line here can only be triggered by this line:
[https://github.com/apache/kafka/blob/839b886f9b732b151e1faeace7303c80641c08c4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L892]
 
The coordinator is then re-discovered ok, but the 4th log line in the set then says its not disconnected and it needs to be re-discovered again.
This is because this check here:
[https://github.com/apache/kafka/blob/839b886f9b732b151e1faeace7303c80641c08c4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L244]
Returns a stale future that is not cleared properly from the lookupCoordinator(); call.

I should note there was nothing really of interest in the log of the broker at this time

Plenty of other clients are using the broker at this time so its nothing to do with broker side issues.

 

To me this looks like something sus is happening in the 
protected synchronized boolean ensureCoordinatorReady(final Timer timer) method.
As these log lines are almost certainly caused by the logic in this methd.;;;","19/Jun/22 08:40;showuon;> The coordinator is then re-discovered ok, but the 4th log line in the set then says its not disconnected and it needs to be re-discovered again.

Sorry, does that mean the coordinator discovery was successful in the end? And your question now is about why the 4th log line is shown after coordinator discovered? 
;;;","22/Jun/22 17:04;guozhang;[~showuon] I read through the discussions here and I feel it's probably because in [~kyle.stehbens]'s test there's no `poll` call triggered during the time `commitAsync` is called and the future is being waited. Since in your fix, the future is expected to be only cleared via the `poll` call which would trigger `ensureCoordinatorReady` since the HB thread is not created for manual `assign` mode (note that `ensureCoordinatorReady` was not triggered in `commitAsync`). However if user never calls `poll` again after `commitAsync` then that future would not be cleared.

The follow-up hotfix (https://github.com/apache/kafka/pull/12259/files) would be resilient to such a pattern since it triggers `ensureCoordinatorReady` inside the `commitAsync` call itself. WDYT?

Unfortunately that fix has not be included in any released versions yet. [~kyle.stehbens] would you be willing to try out testing it on top of trunk to see if it works then?;;;","23/Jun/22 01:53;showuon;Thanks [~guozhang] ! Make sense.;;;","08/Jul/22 19:41;mason6345;[~showuon] [~guozhang] any update on this issue? Did we confirm the hotfix solves this issue and has there been a release that includes the hotfix? ;;;","09/Jul/22 02:44;showuon;We are waiting for [~kyle.stehbens] 's confirmation.;;;","02/Aug/22 14:31;martijnvisser;Is there any progress to report on this [~kyle.stehbens] [~showuon] ? We're suspecting that this ticket is the cause of FLINK-28060;;;","02/Aug/22 16:21;kyle.stehbens;Hi, we rolled back our Kafka client to 2.5.1 in all our java apps which is the last known good version of the client before the Change set that broke this.

Unfortunately we cannot test this change our from a trunk build in our production environments, that's just not going to fly.

 

Personally, I think the change doesn't fully address the issue as it doesn't address what I think is the root cause which are the changes before and after this line:
[https://github.com/apache/kafka/blob/0efa8fb0f4c73d92b6e55a112fa45417a67a7dc2/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L1284]

The logic here make it so that the call to markCoordinatorUnknown(); at line 1288 is never called, where before the changes in 2.6.0 this check is done earlier around line 1275.

 In our case, the heart beat thread is absolutely running but can not recover the the coordinator because of this bug.

 

In my option the 2 opotions here are:

1 - revert the changes added in 2.6.0 that refactored all this code and introduced this bug - ostensibly that changes was made to fix a very rare race condition and exchanged a rare race condition with a very common to experience and far worse bug.

2 - Fix forward the changes and completely remove the findCoordinatorFuture variable and all reference to it. This future is only being used to gate calls to recovering the co-coordinator and this can be achieved through other means like locks or synchronized methods with appropriate condition checking.

 

[~martijnvisser] We downgraded our Kafka client to v 2.5.1 in our flink related projects to fix this issue for us.;;;","03/Aug/22 13:44;showuon;I'll take a look.;;;","03/Aug/22 18:23;guozhang;[~kyle.stehbens] Just to clarify, when the retriable error happens for the commitAsync, did the caller thread further triggers other functions on the consumer?

In the current code, as long as the caller triggers ""pull"", or another ""commitAsync"", the ""ensureCoordinatorReady"" function would be triggered which would clear the failed future and mark coordinator unknown, so I'm still a bit less clear how the client would unable to recover its group co-coordinator and leaves the client in a broken state.;;;","03/Aug/22 19:56;kyle.stehbens;[~guozhang]  In this instance, flink is continually calling commitAsync() on the consumer (in our case every 30 seconds) with new offsets but this doesn't recover the coordinator and all subsequent commits fail.;;;","03/Aug/22 21:05;guozhang;I think this issue is indeed fixed in the latest release (starting in 3.1) where upon `commitAsync` we would try to clear-and-discover the coordinator:

https://github.com/apache/kafka/pull/12259/files#diff-0029e982555d1fae10943b862924da962ca8e247a3070cded92c5f5a5960244fR954

Could you kindly check that code change and see if it would avoid the scenario you observed in the previous version?;;;","04/Aug/22 10:09;showuon;[~kyle.stehbens] , I've tested with Kafka 3.2.1 and it works well. Here are some logs:
{code:java}
# consumer tried to commit the offset, and failed with ""not_coordinator""
[info] 17:40:38.012 WARN  [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#0] o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=flink-kafka-testjob-0, groupId=flink-kafka-testjob] Offset commit failed on partition test-topic-1 at offset 21957: This is not the correct coordinator.
[info] 17:40:38.012 INFO  [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#0] o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=flink-kafka-testjob-0, groupId=flink-kafka-testjob] Group coordinator localhost:9093 (id: 2147483644 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted.
[info] 17:40:38.012 WARN  [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#0] o.a.f.c.k.s.reader.KafkaSourceReader - Failed to commit consumer offsets for checkpoint 16
[info] org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
[info] Caused by: org.apache.kafka.common.errors.NotCoordinatorException: This is not the correct coordinator.
.....

[info] 17:40:42.942 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 17 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1659606042941 for job 7b3ad6191a558f6d5b4276c1ebaeba39.

...
# we did send a FindCoordinator request and the offset commit succeeds in next try.
[info] 17:40:43.022 DEBUG [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#0] o.a.k.clients.consumer.KafkaConsumer - [Consumer clientId=flink-kafka-testjob-0, groupId=flink-kafka-testjob] Committing offsets: {test-topic-2=OffsetAndMetadata{offset=21631, leaderEpoch=null, metadata=''}, test-topic-1=OffsetAndMetadata{offset=21983, leaderEpoch=null, metadata=''}}
[info] 17:40:43.022 DEBUG [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#0] o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=flink-kafka-testjob-0, groupId=flink-kafka-testjob] Sending FindCoordinator request to broker localhost:9092 (id: 2 rack: null)
[info] 23f509e5-48a8-461e-b021-f4092edb00cc
[info] 17:40:43.023 DEBUG [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#0] o.apache.kafka.clients.NetworkClient - [Consumer clientId=flink-kafka-testjob-0, groupId=flink-kafka-testjob] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=3, clientId=flink-kafka-testjob-0, correlationId=1244) and timeout 30000 to node 2: FindCoordinatorRequestData(key='flink-kafka-testjob', keyType=0){code}
 

 

And as [~guozhang] said, in Flink, we did commitAsync in a period of time:

[https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java#L235]

 

And after our hotfix (which is included in v3.2.1/v3.3.0), we will clear previous find coordinator future, and send another request to find coordinator, if coordinator is unknown. It should fix this issue. Could you please try it again with v3.2.1 (just released)?

Thank you.

 

cc [~martijnvisser];;;","04/Aug/22 10:10;showuon;I should make it clear, I tested with the reproducer in https://issues.apache.org/jira/browse/FLINK-28060.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Return error for Fetch requests from unrecognized followers,KAFKA-13837,13440527,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,19/Apr/22 21:05,19/May/22 03:58,13/Jul/23 09:17,19/May/22 03:58,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"If the leader of a partition receives a request from a replica which is not in the current replica set, we currently return an empty fetch response with no error. I think the rationale for this is that the leader may not have received the latest `LeaderAndIsr` update which adds the replica, so we just want the follower to retry. The problem with this is that if the `LeaderAndIsr` request never arrives, then there might not be an external indication of a problem. This probably was the only reasonable option before we added the leader epoch to the `Fetch` request API. Now that we have it, it would be clearer to return an `UNKNOWN_LEADER_EPOCH` error to indicate that the (replicaId, leaderEpoch) tuple is not recognized. ",,hachikuji,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-19 21:05:36.0,,,,,,,,,,"0|z11ly0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch drain for nodes might have starving issue,KAFKA-13834,13440149,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,ruanliang,shizhenzhen,shizhenzhen,18/Apr/22 09:41,24/Apr/22 09:06,13/Jul/23 09:17,23/Apr/22 12:58,2.4.1,2.5.0,2.5.1,2.6.0,2.6.1,2.6.2,2.6.3,2.7.0,2.7.1,2.7.2,2.8.0,2.8.1,3.0.0,3.0.1,3.1.0,3.3.0,,,,,,,producer ,,,,,,0,producer,,,,,"h3. 问题代码 problem code

RecordAccumulator#drainBatchesForOneNode

!https://img-blog.csdnimg.cn/a4e309723c364586a46df8d94e49291f.png|width=786,height=266!  

问题出在这个, private int drainIndex;

The problem is this,private int drainIndex;
h3. 代码预期 code expectations

这端代码的逻辑, 是计算出发往每个Node的ProducerBatchs，是批量发送。

因为发送一次的请求量是有限的(max.request.size), 所以一次可能只能发几个ProducerBatch. 那么这次发送了之后, 需要记录一下这里是遍历到了哪个Batch, 下次再次遍历的时候能够接着上一次遍历发送。

简单来说呢就是下图这样

 

The logic of the code at this end is to calculate the ProducerBatchs sent to each Node, which is sent in batches.

Because the amount of requests sent at one time is limited (max.request.size), only a few ProducerBatch may be sent at a time. Then after sending this time, you need to record which Batch is traversed here, and the next time you traverse it again Can continue the last traversal send.

Simply put, it is as follows

 

!image-2022-04-18-17-36-47-393.png|width=798,height=526!

 

 

 
h3. 实际情况 The actual situation

但是呢, 因为上面的索引drainIndex 是一个全局变量, 是RecordAccumulator共享的。

那么通常会有很多个Node需要进行遍历, 上一个Node的索引会接着被第二个第三个Node接着使用,那么也就无法比较均衡合理的让每个TopicPartition都遍历到.

正常情况下其实这样也没有事情, 如果不出现极端情况的下，基本上都能遍历到。

怕就怕极端情况, 导致有很多TopicPartition不能够遍历到,也就会造成一部分消息一直发送不出去。

However, because the index drainIndex above is a global variable shared by RecordAccumulator.

Then there are usually many Nodes that need to be traversed, and the index of the previous Node will be used by the second and third Nodes, so it is impossible to traverse each TopicPartition in a balanced and reasonable manner.

Under normal circumstances, there is nothing wrong with this. If there is no extreme situation, it can basically be traversed.

I'm afraid of extreme situations, which will result in many TopicPartitions that cannot be traversed, and some messages will not be sent out all the time.
h3. 造成的影响 impact

导致部分消息一直发送不出去、或者很久才能够发送出去。

As a result, some messages cannot be sent out, or can take a long time to be sent out.
h3. 触发异常情况的一个Case /  A Case that triggers an exception

该Case场景如下：
 # 生产者向3个Node发送消息
 # 每个Node都是3个TopicPartition
 # 每个TopicPartition队列都一直源源不断的写入消息、
 # max.request.size 刚好只能存放一个ProdcuerBatch的大小。

就是这么几个条件,会造成每个Node只能收到一个TopicPartition队列里面的PrdoucerBatch消息。

开始的时候 drainIndex=0. 开始遍历第一个Node-0。 Node-0 准备开始遍历它下面的几个队列中的ProducerBatch，遍历一次 则drainIndex+1,发现遍历了一个队列之后,就装满了这一批次的请求。

那么开始遍历Node-1，这个时候则drainIndex=1，首先遍历到的是 第二个TopicPartition。然后发现一个Batch之后也满了。

那么开始遍历Node-1，这个时候则drainIndex=2，首先遍历到的是 第三个TopicPartition。然后发现一个Batch之后也满了。

这一次的Node遍历结束之后把消息发送之后

又接着上面的请求流程，那么这个时候的drainIndex=3了。

遍历Node-0,这个时候取模计算得到的是第几个TopicPartition呢？那不还是第1个吗。相当于后面的流程跟上面一模一样。

也就导致了每个Node的第2、3个TopicPartition队列中的ProducerBatch永远遍历不到。

也就发送不出去了。

 

The case scenario is as follows:

Producer sends message to 3 Nodes
Each Node is 3 TopicPartitions
Each TopicPartition queue has been continuously writing messages,
max.request.size can only store the size of one ProdcuerBatch.

It is these conditions that cause each Node to receive only one PrdoucerBatch message in the TopicPartition queue.

At the beginning drainIndex=0. Start traversing the first Node-0. Node-0 is ready to start traversing the ProducerBatch in several queues below it. After traversing once, drainIndex + 1. After traversing a queue, it is full of requests for this batch.

Then start traversing Node-1. At this time, drainIndex=1, and the second TopicPartition is traversed first. Then I found that a Batch was also full.

Then start traversing Node-1. At this time, drainIndex=2, and the third TopicPartition is traversed first. Then I found that a Batch was also full.

After this Node traversal is over, the message is sent

Then the above request process is followed, then drainIndex=3 at this time.

Traversing Node-0, which TopicPartition is obtained by taking the modulo calculation at this time? Isn't that the first one? Equivalent to the following process is exactly the same as above.

As a result, the ProducerBatch in the second and third TopicPartition queues of each Node can never be traversed.

It can't be sent.

!https://img-blog.csdnimg.cn/aa2cc2e7a9ff4536a1800d9117e02555.png#pic_center|width=660,height=394!

 
h3. 解决方案  solution

只需要每个Node，维护一个自己的索引就行了。

 

 

Only each Node needs to maintain its own index.

 ",,eyys,guozhang,rleslie,ruanliang,shizhenzhen,showuon,soundhearer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/22 09:36;shizhenzhen;image-2022-04-18-17-36-47-393.png;https://issues.apache.org/jira/secure/attachment/13042573/image-2022-04-18-17-36-47-393.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 20 18:28:27 UTC 2022,,,,,,,,,,"0|z11jzc:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,"18/Apr/22 11:03;ruanliang;[~guozhang]    I'm interested in this problem. Can you assign it to me;;;","18/Apr/22 17:12;guozhang;[~shizhenzhen] Thanks for filing the JIRA ticket. Could you translate the description in English so that everyone in the community can understand the issue? Thanks!;;;","18/Apr/22 17:14;guozhang;Also I'm wondering if you can resize the embedded pictures (they are very illustrative btw, thanks) a bit smaller for better rendering?;;;","19/Apr/22 06:57;showuon;[~shizhenzhen] , Thanks for reporting the issue. It is a bug for sure!

Thanks for translating it into English (although I read in Chinese version :) )

+1 for resizing the embeded picture to smaller one, or you can put the image as files and put them into Jira attachment like you did in the 2nd image ([^image-2022-04-18-17-36-47-393.png)]

 

Thanks.;;;","19/Apr/22 06:59;showuon;And BTW, the Jira title might also need to update. Ex: ""batch drain for nodes might have starving issue"";;;","19/Apr/22 11:43;ruanliang;[~showuon]  [~guozhang]   add the test case  [https://github.com/apache/kafka/pull/12066] ;;;","20/Apr/22 03:10;shizhenzhen;[~guozhang] [~showuon]   
Done!
Thanks！;;;","20/Apr/22 18:28;guozhang;[~ruanliang] I've added you to the contributor list and assigned the ticket to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure reasons sent by the consumer are small,KAFKA-13828,13439372,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,13/Apr/22 08:01,13/Apr/22 11:54,13/Jul/23 09:17,13/Apr/22 11:54,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,,"In KIP-800, we added the reason field to the JoinGroupRequest. This field is populated by the AbastractCordinator and its descendent. While reviewing Kafka Streams logs, I have noticed that a few reasons contains the string representation of rather large objects (e.g. assignment).

We should ensure that reasons are kept reasonably small.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-13 08:01:37.0,,,,,,,,,,"0|z11f74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that we can set log.flush.interval.ms with IncrementalAlterConfigs,KAFKA-13807,13438487,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,07/Apr/22 23:58,19/May/22 14:22,13/Jul/23 09:17,19/May/22 14:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,cmccabe,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-07 23:58:34.0,,,,,,,,,,"0|z119s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade vulnerable dependencies march 2022,KAFKA-13805,13438412,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,shivakumar,shivakumar,07/Apr/22 14:42,12/Jan/23 11:02,13/Jul/23 09:17,12/Jan/23 11:02,2.8.1,3.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,,0,secutiry,,,,,"https://nvd.nist.gov/vuln/detail/CVE-2020-36518

|Packages|Package Version|CVSS|Fix Status|
|com.fasterxml.jackson.core_jackson-databind| 2.10.5.1| 7.5|fixed in 2.13.2.1|
|com.fasterxml.jackson.core_jackson-databind|2.13.1|7.5|fixed in 2.13.2.1|

Our security scan detected the above vulnerabilities

upgrade to correct versions for fixing vulnerabilities",,cadonna,christo_lolov,jim_b_o,kirktrue,shivakumar,zhangzs,,,,,,,,,,,,,,,KAFKA-13658,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 11:02:06 UTC 2023,,,,,,,,,,"0|z119bc:",9223372036854775807,,abhidbms,,,,,,,,,,,,,,,,,,"07/Apr/22 17:43;cadonna;[~shivakumar] Are you referring to the following CVE?

https://nvd.nist.gov/vuln/detail/CVE-2020-36518

This CVE seems to affect 2.8.1, 3.0.1 but not 3.1.1 and 3.2.0 since the latter ones use 2.12.6.1 (see  KAFKA-13658).;;;","07/Apr/22 18:06;kirktrue;According to [https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind,] 2.13.0 still has the vulnerability. 2.13.2.1 looks to be the first version in the 2.13.x line that has the fix.;;;","07/Apr/22 18:25;cadonna;[~kirktrue] Under ""Known Affected Software Configurations"" the CVE says ""Up to (excluding) 2.12.6.1"". We are not using the 2.13.x line.;;;","07/Apr/22 18:36;kirktrue;[~cadonna] - Sorry for the confusion... I was mentioning the 2.13.x line because the description stated that the issue was ""fixed in 2.13.0"", which I don't believe is accurate.;;;","07/Apr/22 18:46;cadonna;[~kirktrue] Ah, got it! You are right! I updated the description. ;;;","12/Jan/23 11:02;christo_lolov;Both 2.8.2 and 3.0.2 have since been released and as far as I can tell they did not include this fix. As far as I can understand the conversation later Kafka versions are not affected by this problem because they use the recommended version. I am closing this ticket as resolved, but if I am wrong in my reasoning please reopen it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exception in scheduled task 'flush-log' (kafka.utils.KafkaScheduler),KAFKA-13802,13438100,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,sloeuillet,sloeuillet,06/Apr/22 07:47,11/Apr/22 03:06,13/Jul/23 09:17,11/Apr/22 03:06,3.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]: [2022-04-06 07:13:18,962] ERROR Uncaught exception in scheduled task 'flush-log' (kafka.utils.KafkaScheduler)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]: java.util.NoSuchElementException
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.ConcurrentSkipListMap$SubMap$SubMapIter.advance(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.ConcurrentSkipListMap$SubMap$SubMapValueIterator.next(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.next(JavaCollectionWrappers.scala:38)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.IterableOps.last(Iterable.scala:237)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.IterableOps.last$(Iterable.scala:235)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.AbstractIterable.last(Iterable.scala:919)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.IterableOps.lastOption(Iterable.scala:247)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.IterableOps.lastOption$(Iterable.scala:247)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at scala.collection.AbstractIterable.lastOption(Iterable.scala:919)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at kafka.log.LocalLog.flush(LocalLog.scala:174)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at kafka.log.UnifiedLog.$anonfun$flush$2(UnifiedLog.scala:1520)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at kafka.log.UnifiedLog.flush(UnifiedLog.scala:1707)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at kafka.log.UnifiedLog.$anonfun$roll$1(UnifiedLog.scala:1501)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:116)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
Apr 06 07:13:18 kafka07-messaging-ovh kafka-server-start.sh[1569498]:         at java.base/java.lang.Thread.run(Unknown Source)","Ubuntu 20.04.3
Adoptium 17.0.2_8
Kafka 2.13_3.1.0 with message format & interbroker protocol set to 2.8 (addition to a 2.8.1 cluster), rack awareness enabled on this node (broker.rack not yet set for every broker) => I add broker.rack each time I update a node from 2.8.1 to 3.1.0",showuon,sloeuillet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 11 03:06:01 UTC 2022,,,,,,,,,,"0|z117e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/22 03:06;showuon;Fixed in [#11605|https://github.com/apache/kafka/pull/11605] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka server does not respect MetricsReporter interface contract for dynamically configured reporters,KAFKA-13801,13437971,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,xvrl,xvrl,xvrl,05/Apr/22 19:28,07/Apr/22 18:42,13/Jul/23 09:17,07/Apr/22 08:13,,,,,,,,,,,,,,,,3.3.0,,,,,,,metrics,,,,,,0,,,,,,"MetricsReporter.contextChange contract states the method should always
be called first before MetricsReporter.init is called. This is done
correctly for reporters enabled by default (e.g. JmxReporter) but not
for metrics reporters configured dynamically",,xvrl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-05 19:28:36.0,,,,,,,,,,"0|z116m8:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer batch lost silently in TransactionManager,KAFKA-13794,13437487,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,ddrid,ddrid,03/Apr/22 07:25,10/Apr/23 19:17,13/Jul/23 09:17,05/Apr/22 17:07,,,,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,,,,,,,,,,0,,,,,,"Under the case of idempotence is enabled, when a batch reaches its request.timeout.ms but not yet reaches delivery.timeout.ms, it will be retried and wait for another request.timeout.ms. During the time of this interval, the delivery.timeout.ms may be reached and Sender will remove this in flight batch and bump the producer epoch because of the unresolved sequence, then the sequence of this partition will be reset to 0.

At this time, if a new batch is sent to the same partition and the former batch reaches request.timeout.ms again, we will see an exception being thrown out by NetworkClient:
{code:java}
[ERROR] [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Uncaught error in request completion:
 java.lang.IllegalStateException: We are re-enqueueing a batch which is not tracked as part of the in flight requests. batch.topicPartition: txn_test_1648891362900-2; batch.baseSequence: 0
   at org.apache.kafka.clients.producer.internals.RecordAccumulator.insertInSequenceOrder(RecordAccumulator.java:388) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.RecordAccumulator.reenqueue(RecordAccumulator.java:334) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.Sender.reenqueueBatch(Sender.java:668) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:622) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:548) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.Sender.lambda$sendProduceRequest$5(Sender.java:836) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:583) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:575) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:328) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243) ~[kafka-transaction-test-1.0-SNAPSHOT.jar:?]
   at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_102] {code}
The cause of this is the inflightBatchesBySequence in TransactionManager is not being remove correctly. One batch may be removed by another batch with the same sequence number.

The potential consequence of this I can think out is that the send progress will be blocked until the latter batch being expired by delivery.timeout.ms

 ",,ddrid,kirktrue,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-03 07:25:18.0,,,,,,,,,,"0|z113ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FetchResponse#`fetchData` and `forgottenTopics`: Assignment of lazy-initialized members should be the last step with double-checked locking,KAFKA-13791,13437117,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,lyunk,lyunk,01/Apr/22 02:29,05/Apr/22 07:28,13/Jul/23 09:17,05/Apr/22 07:28,3.0.1,,,,,,,,,,,,,,,3.3.0,,,,,,,clients,,,,,,0,,,,,,"Double-checked locking can be used for lazy initialization of volatile fields, but only if field assignment is the last step in the synchronized block. Otherwise, you run the risk of threads accessing a half-initialized object.

The problem is consistent with [KAFKA-13777|https://issues.apache.org/jira/projects/KAFKA/issues/KAFKA-13777]",,lyunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-01 02:29:55.0,,,,,,,,,,"0|z111f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaManager should be robust to all partition updates from kraft metadata log,KAFKA-13790,13437109,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,hachikuji,hachikuji,01/Apr/22 00:47,09/May/22 18:47,13/Jul/23 09:17,09/May/22 18:47,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"There are two ways that partition state can be updated in the zk world: one is through `LeaderAndIsr` requests and one is through `AlterPartition` responses. All changes made to partition state result in new LeaderAndIsr requests, but replicas will ignore them if the leader epoch is less than or equal to the current known leader epoch. Basically it works like this:
 * Changes made by the leader are done through AlterPartition requests. These changes bump the partition epoch (or zk version), but leave the leader epoch unchanged. LeaderAndIsr requests are sent by the controller, but replicas ignore them. Partition state is instead only updated when the AlterIsr response is received.
 * Changes made by the controller are made directly by the controller and always result in a leader epoch bump. These changes are sent to replicas through LeaderAndIsr requests and are applied by replicas.

The code in `kafka.server.ReplicaManager` and `kafka.cluster.Partition` are built on top of these assumptions. The logic in `makeLeader`, for example, assumes that the leader epoch has indeed been bumped. Specifically, follower state gets reset and a new entry is written to the leader epoch cache.

In KRaft, we also have two paths to update partition state. One is AlterPartition, just like in the zk world. The second is updates received from the metadata log. These follow the same path as LeaderAndIsr requests for the most part, but a big difference is that all changes are sent down to `kafka.cluster.Partition`, even those which do not have a bumped leader epoch. This breaks the assumptions mentioned above in `makeLeader`, which could result in leader epoch cache inconsistency. Another side effect of this on the follower side is that replica fetchers for updated partitions get unnecessarily restarted. There may be others as well.

We need to either replicate the same logic on the zookeeper side or make the logic robust to all updates including those without a leader epoch bump.",,cmccabe,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-04-01 00:47:23.0,,,,,,,,,,"0|z111dc:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove reason prefixing in JoinGroupRequest and LeaveGroupRequest,KAFKA-13783,13436601,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,30/Mar/22 08:11,07/Apr/22 08:47,13/Jul/23 09:17,31/Mar/22 12:34,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,,"KIP-800 introduced a mechanism to pass a reason in the join group request and in the leaver group request. A default reason is used unless one is provided by the user. In this case, the custom reason is prefixed by the default one.

When we tried to used this in Kafka Streams, we noted a significant degradation of the performances, see https://github.com/apache/kafka/pull/11873. It is not clear wether the prefixing is the root cause of the issue or not. To be on the safe side, I think that we should remove the prefixing. It does not bring much anyway as we are still able to distinguish a custom reason from the default one on the broker side.",,dajac,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 04 12:15:45 UTC 2022,,,,,,,,,,"0|z10ymg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 08:49;tombentley;[~dajac] do you want to update the KIP?;;;","04/Apr/22 11:40;dajac;[~tombentley] Done. I have updated the KIP and notified the vote thread as well.;;;","04/Apr/22 12:15;tombentley;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer may fail to add the correct partition to transaction,KAFKA-13782,13436552,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,30/Mar/22 01:39,06/Apr/22 05:20,13/Jul/23 09:17,05/Apr/22 17:11,,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,,,,,,0,,,,,,"In KAFKA-13412, we changed the logic to add partitions to transactions in the producer. The intention was to ensure that the partition is added in `TransactionManager` before the record is appended to the `RecordAccumulator`. However, this does not take into account the possibility that the originally selected partition may be changed if `abortForNewBatch` is set in `RecordAppendResult` in the call to `RecordAccumulator.append`. When this happens, the partitioner can choose a different partition, which means that the `TransactionManager` would be tracking the wrong partition.

I think the consequence of this is that the batches sent to this partition would get stuck in the `RecordAccumulator` until they timed out because we validate before sending that the partition has been added correctly to the transaction.

Note that KAFKA-13412 has not been included in any release, so there are no affected versions.

Thanks to [~alivshits] for identifying the bug.",,hachikuji,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 04 20:04:42 UTC 2022,,,,,,,,,,"0|z10ybk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 16:34;tombentley;This is currently the last remaining blocker for 3.1.1. [~hachikuji], not meaning to hassle you, but any idea when a fix might be available?;;;","04/Apr/22 20:04;hachikuji;[~tombentley] Apologies for the delay. PR is up: https://github.com/apache/kafka/pull/11995.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch from follower should never run the preferred read replica selection,KAFKA-13778,13436443,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ZhaoBo03,ZhaoBo03,ZhaoBo03,29/Mar/22 13:41,05/Apr/22 17:09,13/Jul/23 09:17,05/Apr/22 16:56,2.6.0,,,,,,,,,,,,,,,3.3.0,,,,,,,core,,,,,,0,,,,,,"The design purpose of the code is that only the leader broker can determine the preferred read-replica.

 
{code:java}
readFromLocalLog()
....
// If we are the leader, determine the preferred read-replica
val preferredReadReplica = clientMetadata.flatMap(
  metadata => findPreferredReadReplica(partition, metadata, replicaId, fetchInfo.fetchOffset, fetchTimeMs)) {code}
 

But in fact, since the broker does not judge whether it is the leader or not, the follower will also execute the preferred read-replica selection.
{code:java}
partition.leaderReplicaIdOpt.flatMap { leaderReplicaId =>
  // Don't look up preferred for follower fetches via normal replication and
  if (Request.isValidBrokerId(replicaId))
    None
  else { {code}",,ZhaoBo03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-29 13:41:06.0,,,,,,,,,,"0|z10xnk:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FetchResponse#responseData: Assignment of lazy-initialized members should be the last step with double-checked locking,KAFKA-13777,13436383,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,lyunk,lyunk,29/Mar/22 09:29,01/Apr/22 01:43,13/Jul/23 09:17,31/Mar/22 01:50,3.0.1,,,,,,,,,,,,,,,3.3.0,,,,,,,clients,,,,,,0,,,,,,"Assignment of lazy-initialized members should be the last step with double-checked locking.

now:

 
{code:java}
    public LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData(Map<Uuid, String> topicNames, short version) {
        if (responseData == null) {
            synchronized (this) {
                if (responseData == null) {
                    responseData = new LinkedHashMap<>();
                    data.responses().forEach(topicResponse -> {
                        String name;
                        if (version < 13) {
                            name = topicResponse.topic();
                        } else {
                            name = topicNames.get(topicResponse.topicId());
                        }
                        if (name != null) {
                            topicResponse.partitions().forEach(partition ->
                                responseData.put(new TopicPartition(name, partition.partitionIndex()), partition));
                        }
                    });
                }
            }
        }
        return responseData;
    } {code}
maybe should:

 

 
{code:java}
    public LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData(Map<Uuid, String> topicNames, short version) {
        if (responseData == null) {
            synchronized (this) {
                if (responseData == null) {
                    // *** change 1 ***
                    final LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseDataTmp = new LinkedHashMap<>();
                    data.responses().forEach(topicResponse -> {
                        String name;
                        if (version < 13) {
                            name = topicResponse.topic();
                        } else {
                            name = topicNames.get(topicResponse.topicId());
                        }
                        if (name != null) {
                            topicResponse.partitions().forEach(partition ->
                                    // *** change 2 ***
                                    responseDataTmp.put(new TopicPartition(name, partition.partitionIndex()), partition));
                        }
                    });
                    // *** change 3 ***
                    responseData = responseDataTmp;
                }
            }
        }
        return responseData;
    } {code}
 

 ",,lyunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 29 09:33:38 UTC 2022,,,,,,,,,,"0|z10xa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 09:33;lyunk;reference：

https://rules.sonarsource.com/java/RSPEC-3064;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2020-36518 - Upgrade jackson-databind to 2.12.6.1,KAFKA-13775,13436357,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,edwin092,edwin092,edwin092,29/Mar/22 08:44,01/Apr/22 01:41,13/Jul/23 09:17,30/Mar/22 18:43,3.0.0,3.0.1,3.1.0,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,,,,,,0,CVE,security,,,,"*CVE-2020-36518* vulnerability affects Jackson-Databind in Kafka (see [https://github.com/advisories/GHSA-57j2-w4cx-62h2]).

Upgrading to jackson-databind version *2.12.6.1* should address this issue.",,edwin092,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 29 10:50:14 UTC 2022,,,,,,,,,,"0|z10x4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 10:50;edwin092;https://github.com/apache/kafka/pull/11962;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data loss after recovery from crash due to full hard disk,KAFKA-13773,13436175,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,showuon,Timelad,Timelad,28/Mar/22 13:23,04/Jun/22 08:22,13/Jul/23 09:17,02/Jun/22 06:16,2.8.0,2.8.1,3.1.0,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,log,,,,,,0,,,,,,"While doing some testing of Kafka on Kubernetes, the data disk for kafka filled up, which led to all 3 nodes crashing. I increased the disk size for all three nodes and started up kafka again (one by one, waiting for the previous node to become available before starting the next one). After a little while two out of three nodes had no data anymore.

According to the logs, the log cleaner kicked in and decided that the latest timestamp on those partitions was '0' (i.e. 1970-01-01), and that is older than the 2 week limit specified on the topic.

 
{code:java}
2022-03-28 12:17:19,740 INFO [LocalLog partition=audit-trail-0, dir=/var/lib/kafka/data-0/kafka-log1] Deleting segment files LogSegment(baseOffset=0, size=249689733, lastModifiedTime=1648460888636, largestRecordTimestamp=Some(0)) (kafka.log.LocalLog$) [kafka-scheduler-0]
2022-03-28 12:17:19,753 INFO Deleted log /var/lib/kafka/data-0/kafka-log1/audit-trail-0/00000000000000000000.log.deleted. (kafka.log.LogSegment) [kafka-scheduler-0]
2022-03-28 12:17:19,754 INFO Deleted offset index /var/lib/kafka/data-0/kafka-log1/audit-trail-0/00000000000000000000.index.deleted. (kafka.log.LogSegment) [kafka-scheduler-0]
2022-03-28 12:17:19,754 INFO Deleted time index /var/lib/kafka/data-0/kafka-log1/audit-trail-0/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment) [kafka-scheduler-0]{code}
Using kafka-dump-log.sh I was able to determine that the greatest timestamp in that file (before deletion) was actually 1648460888636 ( 2022-03-28, 09:48:08 UTC, which is today). However since this segment was the 'latest/current' segment much of the file is empty. The code that determines the last entry (TimeIndex.lastEntryFromIndexFile)  doesn't seem to know this and just read the last position in the file, the file being mostly empty causes it to read 0 for that position.

The cleaner code seems to take this into account since UnifiedLog.deleteOldSegments is never supposed to delete the current segment, judging by the scaladoc, however in this case the check doesn't seem to do its job. Perhaps the detected highWatermark is wrong?

I've attached the logs and the zipped data directories (data files are over 3Gb in size when unzipped)

 

I've encountered this problem with both kafka 2.8.1 and 3.1.0.

I've also tried changing min.insync.replicas to 2: The issue still occurs.",,junrao,rleslie,showuon,Timelad,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/22 14:00;Timelad;DiskAndOffsets.png;https://issues.apache.org/jira/secure/attachment/13041788/DiskAndOffsets.png","28/Mar/22 13:16;Timelad;kafka-.zip;https://issues.apache.org/jira/secure/attachment/13041686/kafka-.zip","30/Mar/22 13:38;Timelad;kafka-2.7.0vs2.8.0.zip;https://issues.apache.org/jira/secure/attachment/13041781/kafka-2.7.0vs2.8.0.zip","30/Mar/22 13:57;Timelad;kafka-2.8.0-crash.zip;https://issues.apache.org/jira/secure/attachment/13041787/kafka-2.8.0-crash.zip","28/Mar/22 13:17;Timelad;kafka-logfiles.zip;https://issues.apache.org/jira/secure/attachment/13041685/kafka-logfiles.zip","31/Mar/22 09:43;Timelad;kafka-start-to-finish.zip;https://issues.apache.org/jira/secure/attachment/13041834/kafka-start-to-finish.zip",,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 09 02:50:58 UTC 2022,,,,,,,,,,"0|z10w0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 08:10;showuon;[~Timelad] , thanks for reporting this issue. You mentioned you encountered this issue in kafka 2.8.1 and 3.1.0. Did it happen before v2.8.1? I'd like to know if this is a regression issue, or a long existing issue. Thanks.;;;","29/Mar/22 09:37;Timelad;[~showuon] I haven't tested it on other versions, I might have some time tomorrow to do a sample of other versions. Are there any particular versions you're interested in?;;;","29/Mar/22 10:19;showuon;Thanks [~Timelad] , testing v2.7.0, and v2.6.0 should be good enough IMO. Thanks.;;;","29/Mar/22 13:09;showuon;[~junrao] , do you have any thoughts about this issue? Thanks.;;;","29/Mar/22 17:35;junrao;[~Timelad] : Thanks for filing the jira. I ran the following tool on one the log segments. The log file seems corrupted.  

 
{code:java}
bin/kafka-dump-log.sh  --files kafka-2-retention/audit-trail-0/00000000000000000000.log
baseOffset: 30432 lastOffset: 30432 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 1 isTransactional: false isControl: false position: 250051313 CreateTime: 1648460938666 size: 8216 magic: 2 compresscodec: none crc: 2462031276 isvalid: true
baseOffset: 30433 lastOffset: 30433 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 1 isTransactional: false isControl: false position: 250059529 CreateTime: 1648460938942 size: 8219 magic: 2 compresscodec: none crc: 563640287 isvalid: true
Found 5340 invalid bytes at the end of 00000000000000000000.log
{code}
 

If the file is used as it is to determine the max timestamp, it could lead to invalid timestamp. 

Normally, if the broker dies because of no disk space, on restarting, the broker will go through log recovery to check the validity of the data. However, from the log4j file, it doesn't seem there was log recovery. So, I am wondering if the broker crashed before that log segment was rolled and flushed, or after.

 

 ;;;","30/Mar/22 13:59;Timelad;[~showuon] I ran additional tests on 2.6.0 2.7.0 and 2.8.0. I was not able to reproduce it on 2.6.0 and 2.7.0, however 2.8.0 does have the issue.

[~junrao] You're right, log recovery seems to be skipped in 2.8.0, I've attached a new zip file with the logs from 2.7.0 and 2.8.0 at the first startup after the disk resize. The 2.7.0 logs have a log recovery step, however the 2.8.0 logs contain: 
{code:java}
2022-03-30 13:25:52,577 INFO Skipping recovery for all logs in /var/lib/kafka/data-0/kafka-log2 since clean shutdown file was found (kafka.log.LogManager) [main] {code}
It should be mentioned that since this is on Kubernetes, there are several attempts to restart the Kafka pod where Kafka tries, and fails, to start. Perhaps the clean shutdown file is created in one of these restarts. The new zip also contains the log of one of the failed restarts. Unfortunately its hard for me to get the log for the first failed restart.

I've also added a zipfile of the actual crash in case its useful.

 ;;;","30/Mar/22 16:35;junrao;[~Timelad] : Thanks for the additional logs. I took a quick look at kafka-2-2.8.0-before-resize.log. The broker did skip recovery, but it's not clear if the previous shutdown was clean or not. Do you have the log before that? During loading, the broker shut down abruptly due to the no space issue. After that, if the broker is restarted again, it should go through log recovery, did that happen?

 
{code:java}
2022-03-30 13:23:00,077 INFO Skipping recovery for all logs in /var/lib/kafka/data-0/kafka-log2 since clean shutdown file was found (kafka.log.LogManager) [main]
{code}
 

 ;;;","30/Mar/22 18:34;Timelad;{quote}'but it's not clear if the previous shutdown was clean or not'
{quote}
The previous shutdown should not have been clean, since there wasn't enough disk space, I've attached the log for the time of the crash in kafka-2.8.0-crash.zip. I'll try to see if I can get the log for the first restart after the first disk space crash tomorrow, it might give some more insight.
{quote}After that, if the broker is restarted again, it should go through log recovery, did that happen?
{quote}
Kafka skipped log recovery once it had enough disk space. See kafka-2.7.0vs2.8.0.zip, it contains the log4j logs of the first start after resizing the disk.;;;","30/Mar/22 18:49;junrao;[~Timelad] : I checked kafka-0-2.8.0-before-fail.log kafka-1-2.8.0-before-fail.log in kafka-2.8.0-crash.zip. They both seem to have timestamp after 13:23:00,077, which is the time when the recovery is skipped. ;;;","30/Mar/22 19:32;Timelad;[~junrao] Yes, thats true, it was a new different test run, (it had the same behavior though). I'll try to get a consistent run with logs in the right order tomorrow, unfortunately getting all the logs is a bit tricky sometimes in Kubernetes when pods are restarting.;;;","31/Mar/22 09:45;Timelad;[~junrao] I've added a new zip with the log files from start to finish for the test including initial data load, restarts and final restart after resizing the disks.;;;","31/Mar/22 19:18;junrao;[~Timelad] : Thanks for the log. This does seem to be a real issue. What happened is the following. The broker hit IOException in log loading during the initial startup.

 
{code:java}
2022-03-31 09:28:43,407 ERROR Error while writing to checkpoint file /var/lib/kafka/data-0/kafka-log0/__consumer_offsets-12/leader-epoch-checkpoint (kafka.server.LogDirFailureChannel) [log-recovery-/var/lib/kafka/data-0/kafka-log0]
java.io.IOException: No space left on device
    at java.base/java.io.FileOutputStream.writeBytes(Native Method)
    at java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)
    at java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)
    at java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)
    at java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)
    at java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)
    at java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)
    at java.base/java.io.BufferedWriter.flush(BufferedWriter.java:257)
    at org.apache.kafka.server.common.CheckpointFile.write(CheckpointFile.java:94)
    at kafka.server.checkpoints.CheckpointFileWithFailureHandler.write(CheckpointFileWithFailureHandler.scala:37)
    at kafka.server.checkpoints.LeaderEpochCheckpointFile.write(LeaderEpochCheckpointFile.scala:71)
    at kafka.server.epoch.LeaderEpochFileCache.flush(LeaderEpochFileCache.scala:291)
    at kafka.server.epoch.LeaderEpochFileCache.$anonfun$truncateFromEnd$1(LeaderEpochFileCache.scala:237)
    at kafka.server.epoch.LeaderEpochFileCache.truncateFromEnd(LeaderEpochFileCache.scala:234)
    at kafka.log.LogLoader$.$anonfun$load$12(LogLoader.scala:188)
    at kafka.log.LogLoader$.$anonfun$load$12$adapted(LogLoader.scala:188)
    at scala.Option.foreach(Option.scala:437)
    at kafka.log.LogLoader$.load(LogLoader.scala:188)
    at kafka.log.UnifiedLog$.apply(UnifiedLog.scala:1785)
    at kafka.log.LogManager.loadLog(LogManager.scala:282)
    at kafka.log.LogManager.$anonfun$loadLogs$13(LogManager.scala:368)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829){code}
 

Normally, this will cause the broker to have a hard shutdown, but only after the ReplicaManager is started, which happens after log loading. Then the IOException is also propagated to KafkaServer, which causes it to exit normally. As part of the normal exit, a clean shutdown file will be written. This causes the next broker restart to skip log recovery.

 
{code:java}
2022-03-31 09:28:43,411 ERROR There was an error in one of the threads during logs loading: org.apache.kafka.common.errors.KafkaStorageException: Error while writing to checkpoint file /var/lib/kafka/data-0/kafka-log0/__consumer_offsets-12/leader-epoch-checkpoint (kafka.log.LogManager) [main]
2022-03-31 09:28:43,414 ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer) [main]
2022-03-31 09:28:43,415 INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer) [main]
2022-03-31 09:28:43,418 INFO Shutting down. (kafka.log.LogManager) [main]2022-03-31 09:28:43,466 INFO Shutdown complete. (kafka.log.LogManager) [main]{code}
 

The issue with skipping recovery is that some of the preallocated timeindex file won't be shrunk to the right size and we will pick up some garbage as the timestamp.;;;","09/May/22 02:50;showuon;PR: https://github.com/apache/kafka/pull/12136;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitions are not correctly re-partitioned when the fetcher thread pool is resized,KAFKA-13772,13435947,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,yufeiyan1220,yufeiyan1220,26/Mar/22 10:00,31/Mar/22 12:55,13/Jul/23 09:17,31/Mar/22 12:55,1.1.0,3.0.1,,,,,,,,,,,,,,3.2.0,,,,,,,core,,,,,,0,,,,,,"The method 'resizeThreadPool' is suppose to redistributed all topic partitions to threads based on the new thread number, and they should be distributed  evenly. 
 
However, the resizing process which has a logic to add  fetcher to 'fetcherThreadMap' is within the iteration of 'fetcherThreadMap' all fetchers, which could lead to skipping some fetchers and these fetchers remain their topic partition assignment.
 
The affected scenario:  someone wants to add threads to improve fetch-thread performance(eg. replica fetch threads),  but the performance improvement is not as expect. ",,yufeiyan1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,scala,Mon Mar 28 03:44:56 UTC 2022,,,,,,,,,,"0|z10um8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 03:44;yufeiyan1220;The affected versions are from 1.1.0 to the latest;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression when Connect uses 0.10.x brokers due to recently added retry logic in KafkaBasedLog,KAFKA-13770,13435656,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rhauch,rhauch,rhauch,24/Mar/22 20:30,25/Mar/22 04:29,13/Jul/23 09:17,25/Mar/22 04:29,2.5.2,2.6.4,2.7.3,2.8.2,3.0.2,3.1.1,3.2.0,,,,,,,,,2.6.4,2.7.3,2.8.2,3.0.2,3.1.1,3.2.0,,KafkaConnect,,,,,,0,,,,,,"KAFKA-12879 recently modified Connect's `KafkaBasedLog` class to add retry logic when trying to get the latest offsets for the topic as the `KafkaBasedLog` starts up. This method calls a new method in `TopicAdmin` to read the latest offsets using retries.

When Connect is using an old broker (version 0.10.x or earlier), the old `KafkaBasedLog` logic would catch the `UnsupportedVersionException` thrown by the `TopicAdmin` method, and use the consumer to read offsets instead. The new retry logic unfortunately _wrapped_ the `UnsupportedVersionException` in a `ConnectException`, which means the `KafkaBasedLog` logic doesn't degrade and use the consumer, and instead fails.

The `TopicAdmin.retryEndOffsets(...)` method should propagate the `UnsupportedVersionException` rather than wrapping it. All other exceptions from the admin client are either retriable or already wrapped by a `ConnectException`. Therefore, it appears that `UnsupportedVersionException` is the only special case here.

KAFKA-12879 was backported to a lot of branches (tho only the revert was merged to 2.5), so this new fix should be as well. It does not appear any releases were made from any of those branches with the KAFKA-12879 change.",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12879,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 25 04:16:43 UTC 2022,,,,,,,,,,"0|z10sug:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"25/Mar/22 04:16;rhauch;Merged the changes to `trunk` after the PR builds had no Connect-related failures, and a [system test run of the Connect tests|https://jenkins.confluent.io/view/All/job/system-test-kafka-branch-builder/4823/] passed.

Also backported to the `3.2`, `3.1`, `3.0`, `2.8`, `2.7`, and `2.6` branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTable FK join can miss records if an upstream non-key-changing operation changes key serializer,KAFKA-13769,13435598,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Gerrrr,Gerrrr,Gerrrr,24/Mar/22 15:02,26/Jan/23 02:03,13/Jul/23 09:17,15/Apr/22 18:29,,,,,,,,,,,,,,,,3.3.3,3.4.0,,,,,,streams,,,,,,0,,,,,,"Consider a topology, where the source KTable is followed by a {{transformValues}} operation [that changes the key schema|https://github.com/apache/kafka/blob/db724f23f38cdb6c668a10681ea2a03bb11611ad/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L452] followed by a foreign key join. The FK join might miss records in such a topology because they might be sent to the wrong partitions.

As {{transformValues}} does not change the key itself, repartition won't happen after this operation. However, the KTable instance that calls {{doJoinOnForeignKey}} uses the new serde coming from {{transformValues}} rather than the original. As a result, all nodes in the FK join topology except for [SubscriptionResolverJoinProcessorSupplier|https://github.com/apache/kafka/blob/db724f23f38cdb6c668a10681ea2a03bb11611ad/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L1225-L1232] use the ""new"" serde. {{SubscriptionResolverJoinProcessorSupplier}} uses the old one because it uses [valueGetterSupplier|https://github.com/apache/kafka/blob/db724f23f38cdb6c668a10681ea2a03bb11611ad/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L1225] that in turn will retrieve the records from the topic.

A different serializer might serialize keys to different series of bytes, which will lead to sending them to the wrong partitions. To run into that issue, multiple things must happen:
* a topic should have more than one partition,
* KTable's serializer should be modified via a non-key-changing operation,
* the new serializer should serialize keys differently

In practice, it might happen if the key type is a {{Struct}} because it serializes to a JSON string {{columnName -> value}}. If the {{transformValues}} operation changes column names to avoid name clashes with the joining table, such join can lead to incorrect behavior.",,Gerrrr,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 26 02:03:52 UTC 2023,,,,,,,,,,"0|z10shs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/22 16:34;Gerrrr;https://github.com/apache/kafka/pull/11945;;;","26/Jan/23 02:03;mjsax;I just updated fixed version from 3.0.0 to 3.3.3 and 3.4.0. Cf https://issues.apache.org/jira/browse/KAFKA-14646 for details.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable producer idempotence by default in producers instantiated by Connect,KAFKA-13759,13435222,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kkonstantine,kkonstantine,kkonstantine,22/Mar/22 23:54,30/Mar/22 20:38,13/Jul/23 09:17,23/Mar/22 22:28,,,,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,,,,KafkaConnect,,,,,,0,,,,,,"https://issues.apache.org/jira/browse/KAFKA-7077 was merged recently referring to KIP-318. Before that in AK 3.0 idempotence was enabled by default across Kafka producers. 

However, some compatibility implications were missed in both cases. 

If idempotence is enabled by default Connect won't be able to communicate via its producers with Kafka brokers older than version 0.11. Perhaps more importantly, for brokers older than version 2.8 the {{IDEMPOTENT_WRITE}} ACL is required to be granted to the principal of the Connect worker. 

Given the above caveats, this ticket proposes to explicitly disable producer idempotence in Connect by default. This feature, as it happens today, can be enabled by setting worker and/or connector properties. However, enabling it by default should be considered in a major version upgrade and after KIP-318 is updated to mention the compatibility requirements and gets officially approved. ",,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7077,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 23 22:27:47 UTC 2022,,,,,,,,,,"0|z10q6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 22:27;kkonstantine;This issue has been now been merged on the 3.2 and 3.1 branches to avoid a breaking change when Connect contacts older brokers and idempotence is enabled in the producer by default. 
[~cadonna] [~tombentley] fyi. 
Hopefully this fix makes to the upcoming releases but please let me know if the targeted versions need to be adjusted. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker heartbeat event should have deadline,KAFKA-13755,13434615,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,cmccabe,mumrah,mumrah,18/Mar/22 19:22,19/Jun/22 21:36,13/Jul/23 09:17,19/Jun/22 21:36,,,,,,,,,,,,,,,,3.3,,,,,,,controller,,,,,,0,kip-500,,,,,"When we schedule the event for processing the broker heartbeat request in QuroumController, we do not give a deadline. This means that the event will only be processed after all other events which do have a deadline. In the case of the controller's queue getting filled up with deadline (i.e., ""deferred"") events, we may not process the heartbeat before the broker attempts to send another one.

 

 ",,cadonna,cmccabe,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 19 21:36:20 UTC 2022,,,,,,,,,,"0|z10mg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/22 23:12;cadonna;Removing from the 3.2.0 release since code freeze has passed.;;;","19/Jun/22 21:36;cmccabe;As of 3.3, we now use {{config.brokerHeartbeatIntervalMs}} as the deadline

It should be a good improvement for clusters under load;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client Compatability KafkaTest uses invalid idempotency configs,KAFKA-13750,13434229,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,jolshan,jolshan,16/Mar/22 22:52,18/Mar/22 14:59,13/Jul/23 09:17,17/Mar/22 17:02,,,,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,,,,,,,,,,0,,,,,,"With the switch to idempotency as a default, some of our tests broke including 

ClientCompatibilityFeaturesTest.run_compatibility_test for versions prior to 0.11 where EOS was enabled. We need to configure the producer correctly for these earlier versions.",,dengziming,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-16 22:52:32.0,,,,,,,,,,"0|z10k2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicConfigs and ErrorCode are not set in createTopics response in KRaft,KAFKA-13749,13434210,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,akhileshchg,akhileshchg,16/Mar/22 19:17,04/Aug/22 17:24,13/Jul/23 09:17,19/Jun/22 21:16,,,,,,,,,,,,,,,,,,,,,,,kraft,,,,,,0,,,,,,"Once the createTopics request is process in KRaft, the `CreatableTopicResult` is not set with the appropriate topic configs and error and this breaks KIP-525",,akhileshchg,cmccabe,dengziming,,,,,,,,,,,,,,,,,,,,,KAFKA-12502,KAFKA-13313,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 19 21:16:18 UTC 2022,,,,,,,,,,"0|z10jy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 03:47;dengziming;Thank you for reporting this, can you provided more details on the bug or show an example?;;;","19/Jun/22 21:16;cmccabe;This was fixed by this commit:
{code}
commit 62ea4c46a9be7388baeaef1c505d3e5798a9066f
Author: Colin Patrick McCabe <cmccabe@apache.org>
Date:   Fri Apr 1 10:50:25 2022 -0700

    KAFKA-13749: CreateTopics in KRaft must return configs (#11941)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not include file stream connectors in Connect's CLASSPATH and plugin.path by default,KAFKA-13748,13434203,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,kkonstantine,kkonstantine,16/Mar/22 18:51,28/Dec/22 16:27,13/Jul/23 09:17,30/Mar/22 20:47,,,,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,,,,KafkaConnect,,,,,,0,,,,,,"File stream connectors have been included with Kafka Connect distributions from the very beginning. These simple connectors were included to show case connector implementation but were never meant to be used in production and have been only available for the straightforward demonstration of Connect's capabilities through our quick start guides. 
 
 Given that these connectors are not production ready and yet they offer access to the local filesystem, with this ticket I propose to remove them from our deployments by default by excluding these connectors from the {{CLASSPATH}} or the default {{{}plugin.path{}}}. 
 
 The impact will be minimal. Quick start guides will require a single additional step of editing the {{plugin.path}} to include the single package that includes these connectors. Production deployments will remain unaffected because these are not production grade connectors. ",,kkonstantine,lcarettoni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 18 21:11:09 UTC 2022,,,,,,,,,,"0|z10jwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/22 20:47;kkonstantine;cc [~cadonna] [~tombentley] re: inclusion to the upcoming releases. ;;;","09/Apr/22 14:36;lcarettoni;Would you mind providing the CVE for this issue? Thanks;;;","18/Apr/22 21:11;lcarettoni;Since Apache is a CVE program partner, I would expect you to prepare a CVE for this issue. Should I request one to MITRE instead?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky kafka.admin.TopicCommandIntegrationTest.testDescribeUnderMinIsrPartitionsMixed,KAFKA-13746,13433963,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sagarrao,guozhang,guozhang,15/Mar/22 21:44,14/May/22 00:13,13/Jul/23 09:17,14/May/22 00:13,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"Example: https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11796/7/tests/

{code}
java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
	at kafka.admin.TopicCommandIntegrationTest.testDescribeUnderMinIsrPartitionsMixed(TopicCommandIntegrationTest.scala:686)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
{code}",,guozhang,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-15 21:44:41.0,,,,,,,,,,"0|z10ifk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky kafka.network.SocketServerTest.testNoOpActionResponseWithThrottledChannelWhereThrottlingAlreadyDone,KAFKA-13745,13433962,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sagarrao,guozhang,guozhang,15/Mar/22 21:42,14/May/22 11:17,13/Jul/23 09:17,14/May/22 02:44,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"Example: https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11796/7/tests/

{code}
org.opentest4j.AssertionFailedError: expected: <false> but was: <true>
	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at org.junit.jupiter.api.AssertFalse.assertFalse(AssertFalse.java:40)
	at org.junit.jupiter.api.AssertFalse.assertFalse(AssertFalse.java:35)
	at org.junit.jupiter.api.Assertions.assertFalse(Assertions.java:227)
	at kafka.network.SocketServerTest.testNoOpActionResponseWithThrottledChannelWhereThrottlingAlreadyDone(SocketServerTest.scala:751)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
{code}",,guozhang,Jack-Lee,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 14 11:17:12 UTC 2022,,,,,,,,,,"0|z10ifc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/22 07:33;Jack-Lee;[~sagarrao] I want to add the log warning to catch the exception , what do you think ? 

The stack as below : 



{code:java}
[2022-03-10 02:59:14,052] WARN [SocketServer listenerType=ZK_BROKER, nodeId=0] Unexpected error from /127.0.0.1 (channelId=127.0.0.1:43815-127.0.0.1:57029-0); closing connection (org.apache.kafka.common.network.Selector:619)
org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 101 larger than 100)
	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:105)
	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at kafka.network.Processor.poll(SocketServer.scala:1144)
	at kafka.network.Processor.run(SocketServer.scala:1047)
	at java.lang.Thread.run(Thread.java:748)
{code}

;;;","08/May/22 02:56;sagarrao;[~Jack-Lee] , Sorry I couldn't follow your comment. How do you think adding this exception would help ?;;;","12/May/22 16:45;sagarrao;[~guozhang] , i looked at few of the builds happening across the last 20 odd days. I couldn't find this test failing. Do you think this still needs fixing?;;;","14/May/22 02:43;guozhang;Hi [~sagarrao] I'm happy to resolve the ticket now, and if we see it again we can re-create.;;;","14/May/22 11:17;sagarrao;Thanks [~guozhang] . Sure we can re-create if needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kraft controller should prevent topics with conflicting metrics names from being created,KAFKA-13743,13433923,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,cmccabe,cmccabe,15/Mar/22 16:50,18/Jul/22 19:33,13/Jul/23 09:17,14/Apr/22 02:22,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,kip-500,,,,,"The kraft controller should prevent topics with conflicting metrics names from being created, like the zk code does.

Example:

{code}
[cmccabe@zeratul kafka1]$ ./bin/kafka-topics.sh --create --topic f.oo --bootstrap-server localhost:9092                                                         
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.                            
Created topic f.oo.                                                                                                                                                                           

[cmccabe@zeratul kafka1]$ ./bin/kafka-topics.sh --create --topic f_oo --bootstrap-server localhost:9092
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Error while executing topic command : Topic 'f_oo' collides with existing topics: f.oo
[2022-03-15 09:48:49,563] ERROR org.apache.kafka.common.errors.InvalidTopicException: Topic 'f_oo' collides with existing topics: f.oo
 (kafka.admin.TopicCommand$)
{code}",,cmccabe,dengziming,raphaelauv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 18 19:33:24 UTC 2022,,,,,,,,,,"0|z10i6o:",9223372036854775807,,cmccabe,,,,,,,,,,,,,,,,,,"09/Jun/22 10:36;raphaelauv;the issue is mark resolved but the PR was not merged;;;","18/Jul/22 19:33;cmccabe;The PR was merged, just not through the github UI;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster IDs should not have leading dash,KAFKA-13741,13433898,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,mumrah,mumrah,15/Mar/22 15:00,04/Apr/22 23:17,13/Jul/23 09:17,04/Apr/22 23:17,3.0.0,3.1.0,,,,,,,,,,,,,,3.2.0,,,,,,,kraft,,,,,,0,kip-500,,,,,"Since we use URL-safe base64 encoded Uuid's for cluster ID, it is possible for dash (""-"") characters to be present in the ID string. When a cluster ID has a leading dash, we can run into problems when running the Kafka bash scripts.

For example, if the ID ""-Xflm1nKSfOK8QGt_AXhxw"" is generated with ""random-uuid"" sub-command of kafka-storage.sh, we would then normally format the log directories like:

{code}
./bin/kafka-storage.sh format --config ./config/kraft/controller.properties \
    --cluster-id -Xflm1nKSfOK8QGt_AXhxw
{code}

This will not parse correctly as the argument parsing library will treat the cluster ID as an argument. It leads to the following error:

{code}
usage: kafka-storage format [-h] --config CONFIG --cluster-id CLUSTER_ID [--ignore-formatted]
kafka-storage: error: argument --cluster-id/-t: expected one argument
{code}

This can be worked around by putting the ID in quotes, or by using an ""="" between the ""--cluster-id"" and the Uuid.

We can solve this going forward by not generating random Uuid's that contain a leading dash.

",,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-15 15:00:20.0,,,,,,,,,,"0|z10i14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-stream-client-shutdown,KAFKA-13740,13433850,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,prashanthjbabu,prashanthjbabu,15/Mar/22 11:20,24/Feb/23 19:47,13/Jul/23 09:17,17/Mar/22 03:09,3.1.0,,,,,,,,,,,,,,,,,,,,,,streams,,,,,,0,,,,,,"I have an apache kafka streams application . I notice that it sometimes shutsdown when a rebalancing occurs with no real reason for the shutdown . It doesn't even throw an exception.

Here are some logs on the same 
{code:java}

[2022-03-08 17:13:37,024] INFO [Consumer clientId=svc-stream-collector-StreamThread-1-consumer, groupId=svc-stream-collector] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2022-03-08 17:13:37,024] ERROR stream-thread [svc-stream-collector-StreamThread-1] A Kafka Streams client in this Kafka Streams application is requesting to shutdown the application (org.apache.kafka.streams.processor.internals.StreamThread)
[2022-03-08 17:13:37,030] INFO stream-client [svc-stream-collector] State transition from REBALANCING to PENDING_ERROR (org.apache.kafka.streams.KafkaStreams)
old state:REBALANCING new state:PENDING_ERROR
[2022-03-08 17:13:37,031] INFO [Consumer clientId=svc-stream-collector-StreamThread-1-consumer, groupId=svc-stream-collector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[2022-03-08 17:13:37,032] INFO stream-thread [svc-stream-collector-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
[2022-03-08 17:13:37,032] INFO stream-thread [svc-stream-collector-StreamThread-1] State transition from PARTITIONS_REVOKED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
[2022-03-08 17:13:37,067] INFO stream-thread [svc-stream-collector-StreamThread-1] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request (org.apache.kafka.streams.processor.internals.StreamThread)
[2022-03-08 17:13:37,067] WARN stream-thread [svc-stream-collector-StreamThread-1] Detected that shutdown was requested. All clients in this app will now begin to shutdown (org.apache.kafka.streams.processor.internals.StreamThread)

{code}

I'm suspecting its because there are no `newly assigned partitions in the log below`
{code:java}

[2022-03-08 17:13:37,024] INFO [Consumer clientId=svc-stream-collector-StreamThread-1-consumer, groupId=svc-stream-collector] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

{code}

However I'm not exactly sure why this error occurs . Any help would be appreciated.",,cadonna,mjsax,prashanthjbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 17 08:11:09 UTC 2022,,,,,,,,,,"0|z10hqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/22 16:03;cadonna;Hi [~prashanthjbabu], Thank you for your report!
This might happen when one of your Streams clients within your Streams applications sends a shutdown request to all other Streams clients via the rebalancing protocol. So, the exception might be on a different Streams client. The shutdown request originates from the [StreamsUncaughtExceptionHandler|https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/errors/StreamsUncaughtExceptionHandler.html].

Did you set the Streams uncaught exception handler with {{KafkaStreams#setUncaughtExceptionHandler()}}?;;;","17/Mar/22 03:09;prashanthjbabu;[~cadonna]  Thank you so much for your response . Yes I had set *{{setUncaughtExceptionHandler}}* {{and I had returned SHUTDOWN_APPLICATION which is what caused the behavior that I was noticing . I changed it to SHUTDOWN_CLIENT and got the behavior I was expecting .}}

 

Thanks once again!;;;","17/Mar/22 08:11;cadonna;[~prashanthjbabu] Glad that you could sort it out!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sliding window without grace not working,KAFKA-13739,13433829,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tiboun,tiboun,tiboun,15/Mar/22 09:29,31/Mar/22 20:27,13/Jul/23 09:17,31/Mar/22 15:27,3.1.0,,,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,beginner,newbie,,,,"Hi everyone! I would like to understand why KafkaStreams DSL offer the ability to express a SlidingWindow with no grace period but seems that it doesn't work. [confluent's site|https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#sliding-time-windows] state that grace period is required and with the deprecated method, it's default to 24 hours.

Doing a basic sliding window with a count, if I set grace period to 1 ms, expected output is done. Based on the sliding window documentation, lower and upper bounds are inclusive.

If I set grace period to 0 ms, I can see that record is not skipped at KStreamSlidingWindowAggregate(l.126) but when we try to create the window and push the event in KStreamSlidingWindowAggregate#createWindows we call the method updateWindowAndForward(l.417). This method (l.468) check that {{{}windowEnd > closeTime{}}}.

closeTime is defined as {{observedStreamTime - window.gracePeriodMs}} (Sliding window configuration)

windowEnd is defined as {{{}inputRecordTimestamp{}}}.

 

For a first event with a record timestamp, we can assume that observedStreamTime is equal to inputRecordTimestamp.

 

Therefore, closeTime is {{inputRecordTimestamp - 0}} (gracePeriodMS) which results to {{{}inputRecordTimestamp{}}}.

If we go back to the check done in {{updateWindowAndForward}} method, then we have inputRecordTimestamp > inputRecordTimestamp which is always false. The record is then skipped for record's own window.

Stating that lower and upper bounds are inclusive, I would have expected the event to be pushed in the store and forwarded. Hence, the check would be {{{}windowEnd >= closeTime{}}}.

 

Is it a bug or is it intended ?

Thanks in advance for your explanations!

Best regards!",,bbejeck,lct45,mjsax,tiboun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,Thu Mar 31 15:27:34 UTC 2022,,,,,,,,,,"0|z10hm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/22 23:22;mjsax;Thanks for reporting this. Good catch. I think you analysis makes sense. Do you want to do a PR to fix it? Seems we have some testing gap for ""no grace period""... :( ;;;","16/Mar/22 15:10;lct45;I think your analysis makes sense and the change to make `windowEnd >= closeTime` makes sense to me _if_ we want to support a grace period of 0. The alternative here would be to strictly prohibit a grace period of 0 and require something larger.

 

I can't think of any reason to prevent a grace period of 0 so I think the above change seems good to me. It would be worth making that change and running the full suite of tests to make sure there aren't any assumptions that I can't think of right now;;;","16/Mar/22 19:14;mjsax;Thanks for chiming in [~lct45] \
{quote}_if_ we want to support a grace period of 
{quote}
Given that we support `SlidingWindow.ofTimeDifferenceNoGrace()` we should support 0ms grace period :) ;;;","20/Mar/22 13:15;tiboun;Hi, I'll try my best to open a PR in order to fix this.

Best regards;;;","21/Mar/22 17:36;mjsax;Thank you! – Let us know if you need any help.;;;","22/Mar/22 09:42;tiboun;Patch available here https://github.com/apache/kafka/pull/11928;;;","30/Mar/22 13:28;bbejeck;[~tiboun] I've assigned the ticket to you and added you as a contributor so you can self-assign tickets in the future.;;;","31/Mar/22 15:27;bbejeck;Resolved via https://github.com/apache/kafka/pull/11928;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky kafka.admin.LeaderElectionCommandTest.testPreferredReplicaElection,KAFKA-13737,13433768,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dengziming,guozhang,guozhang,15/Mar/22 02:33,07/Jun/23 20:19,13/Jul/23 09:17,19/Apr/22 06:16,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,,"Examples:

https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11895/1/tests

{code}
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeTopics
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at kafka.utils.TestUtils$.$anonfun$waitForLeaderToBecome$1(TestUtils.scala:1812)
	at scala.util.Try$.apply(Try.scala:210)
	at kafka.utils.TestUtils$.currentLeader$1(TestUtils.scala:1811)
	at kafka.utils.TestUtils$.waitForLeaderToBecome(TestUtils.scala:1819)
	at kafka.utils.TestUtils$.assertLeader(TestUtils.scala:1789)
	at kafka.admin.LeaderElectionCommandTest.testPreferredReplicaElection(LeaderElectionCommandTest.scala:172)
{code}",,guozhang,showuon,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-15071,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 19 06:16:16 UTC 2022,,,,,,,,,,"0|z10h8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/22 06:16;showuon;Resolved with PR: https://github.com/apache/kafka/pull/11681;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky kafka.network.SocketServerTest.closingChannelWithBufferedReceives,KAFKA-13736,13433766,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,guozhang,guozhang,15/Mar/22 02:26,15/Dec/22 02:57,13/Jul/23 09:17,15/Dec/22 02:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,flakey,flaky-test,,,,"Examples:

https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11895/1/tests

{code}
java.lang.AssertionError: receiveRequest timed out
	at kafka.network.SocketServerTest.receiveRequest(SocketServerTest.scala:140)
	at kafka.network.SocketServerTest.$anonfun$verifyRemoteCloseWithBufferedReceives$6(SocketServerTest.scala:1521)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at kafka.network.SocketServerTest.$anonfun$verifyRemoteCloseWithBufferedReceives$1(SocketServerTest.scala:1520)
	at kafka.network.SocketServerTest.verifyRemoteCloseWithBufferedReceives(SocketServerTest.scala:1483)
	at kafka.network.SocketServerTest.closingChannelWithBufferedReceives(SocketServerTest.scala:1431)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}",,cadonna,guozhang,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 05 13:02:25 UTC 2022,,,,,,,,,,"0|z10h8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 14:31;cadonna;Reopened the ticket since I got a failure on an unrelated PR:

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11956/1/testReport/?cloudbees-analytics-link=scm-reporting%2Ftests%2Ffailed;;;","05/Aug/22 13:02;jagsancio;This is a flaky test and I am not seeing this error in the 3.3 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky kafka.network.SocketServerTest.remoteCloseWithoutBufferedReceives,KAFKA-13735,13433719,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,guozhang,guozhang,14/Mar/22 20:59,29/Mar/22 08:25,13/Jul/23 09:17,28/Mar/22 08:49,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,,"Examples:

https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11705/13/tests

{code}
Stacktrace
java.lang.IllegalStateException: Channel closed too early
	at kafka.network.SocketServerTest.$anonfun$verifyRemoteCloseWithBufferedReceives$5(SocketServerTest.scala:1511)
	at scala.Option.getOrElse(Option.scala:201)
	at kafka.network.SocketServerTest.$anonfun$verifyRemoteCloseWithBufferedReceives$1(SocketServerTest.scala:1511)
	at kafka.network.SocketServerTest.verifyRemoteCloseWithBufferedReceives(SocketServerTest.scala:1482)
	at kafka.network.SocketServerTest.remoteCloseWithoutBufferedReceives(SocketServerTest.scala:1393)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
{code}",,guozhang,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-14 20:59:36.0,,,,,,,,,,"0|z10gy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"OAuth access token validation fails if it does not contain the ""sub"" claim",KAFKA-13730,13433371,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,kirktrue,dfonai,dfonai,11/Mar/22 15:46,27/Jul/22 16:21,13/Jul/23 09:17,27/Jul/22 11:16,3.1.0,,,,,,,,,,,,,,,3.4.0,,,,,,,clients,,,,,,0,,,,,,"Client authentication fails, when configured to use OAuth and the JWT access token does {*}not contain the sub claim{*}. This issue was discovered while testing Kafka integration with Ping Identity OAuth server. According to Ping's [documentation|https://apidocs.pingidentity.com/pingone/devguide/v1/api/#access-tokens-and-id-tokens]:
{quote}sub – A string that specifies the identifier for the authenticated user. This claim is not present for client_credentials tokens.
{quote}
In this case Kafka broker rejects the token regardless of the [sasl.oauthbearer.sub.claim.name|https://kafka.apache.org/documentation/#brokerconfigs_sasl.oauthbearer.sub.claim.name] property value.
 
----
 
Steps to reproduce:

1. Client configuration:
{noformat}
security.protocol=SASL_PLAINTEXT
sasl.mechanism=OAUTHBEARER
sasl.login.callback.handler.class=org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler
sasl.oauthbearer.token.endpoint.url=https://oauth.server.fqdn/token/endpoint
sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required\
 clientId=""kafka-client""\
 clientSecret=""kafka-client-secret"";
sasl.oauthbearer.sub.claim.name=client_id # claim name for the principal to be extracted from, needed for client side validation too
{noformat}
2. Broker configuration:
{noformat}
sasl.enabled.mechanisms=...,OAUTHBEARER
listener.name.sasl_plaintext.oauthbearer.sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;
listener.name.sasl_plaintext.oauthbearer.sasl.server.callback.handler.class=org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler
sasl.oauthbearer.jwks.endpoint.url=https://oauth.server.fqdn/jwks/endpoint
sasl.oauthbearer.expected.audience=oauth-audience # based on OAuth server setup
sasl.oauthbearer.sub.claim.name=client_id # claim name for the principal to be extracted from
{noformat}
3. Try to perform some client operation:
{noformat}
kafka-topics --bootstrap-server `hostname`:9092 --list --command-config oauth-client.properties
{noformat}
Result:

Client authentication fails due to invalid access token.
 - client log:

{noformat}
[2022-03-11 16:21:20,461] ERROR [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) failed authentication due to: {""status"":""invalid_token""} (org.apache.kafka.clients.NetworkClient)
[2022-03-11 16:21:20,463] WARN [AdminClient clientId=adminclient-1] Metadata update failed due to authentication error (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.SaslAuthenticationException: {""status"":""invalid_token""}
Error while executing topic command : {""status"":""invalid_token""}
[2022-03-11 16:21:20,468] ERROR org.apache.kafka.common.errors.SaslAuthenticationException: {""status"":""invalid_token""}
 (kafka.admin.TopicCommand$)
{noformat}
 - broker log:

{noformat}
[2022-03-11 16:21:20,150] WARN Could not validate the access token: JWT (claims->{""client_id"":""..."",""iss"":""..."",""iat"":1647012079,""exp"":1647015679,""aud"":[...],""env"":""..."",""org"":""...""}) rejected due to invalid claims or other invalid content. Additional details: [[14] No Subject (sub) claim is present.] (org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler)
org.apache.kafka.common.security.oauthbearer.secured.ValidateException: Could not validate the access token: JWT (claims->{""client_id"":""..."",""iss"":""..."",""iat"":1647012079,""exp"":1647015679,""aud"":[...],""env"":""..."",""org"":""...""}) rejected due to invalid claims or other invalid content. Additional details: [[14] No Subject (sub) claim is present.]
	at org.apache.kafka.common.security.oauthbearer.secured.ValidatorAccessTokenValidator.validate(ValidatorAccessTokenValidator.java:159)
	at org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler.handleValidatorCallback(OAuthBearerValidatorCallbackHandler.java:184)
	at org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler.handle(OAuthBearerValidatorCallbackHandler.java:169)
	at org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslServer.process(OAuthBearerSaslServer.java:156)
	at org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslServer.evaluateResponse(OAuthBearerSaslServer.java:101)
	at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.handleSaslToken(SaslServerAuthenticator.java:451)
	at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.authenticate(SaslServerAuthenticator.java:280)
	at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:181)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:543)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at kafka.network.Processor.poll(SocketServer.scala:989)
	at kafka.network.Processor.run(SocketServer.scala:892)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.jose4j.jwt.consumer.InvalidJwtException: JWT (claims->{""client_id"":""..."",""iss"":""..."",""iat"":1647012079,""exp"":1647015679,""aud"":[...],""env"":""..."",""org"":""...""}) rejected due to invalid claims or other invalid content. Additional details: [[14] No Subject (sub) claim is present.]
	at org.jose4j.jwt.consumer.JwtConsumer.validate(JwtConsumer.java:466)
	at org.jose4j.jwt.consumer.JwtConsumer.processContext(JwtConsumer.java:311)
	at org.jose4j.jwt.consumer.JwtConsumer.process(JwtConsumer.java:433)
	at org.apache.kafka.common.security.oauthbearer.secured.ValidatorAccessTokenValidator.validate(ValidatorAccessTokenValidator.java:157)
	... 12 more
[2022-03-11 16:21:20,154] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Failed authentication with /127.0.0.1 ({""status"":""invalid_token""}) (org.apache.kafka.common.network.Selector)
{noformat}",,dfonai,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 16 15:36:13 UTC 2022,,,,,,,,,,"0|z10et4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 16:33;dfonai;Kafka delegates access token parsing to jose4j library and configures JWT consumer to require the sub claim to be present:
[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/secured/ValidatorAccessTokenValidator.java#L134].

This restriction might not be necessary, especially when the client principal is expected to be in another JWT claim. If the required client principal claim is not present, [subject extraction logic|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/secured/ValidatorAccessTokenValidator.java#L175] will throw an exception and validation will fail.;;;","11/Mar/22 16:57;dfonai;Although it would be reasonable to expect that OAuth JWT access tokens contain the ""sub"" claim (as it is the case with OpenID ID tokens), I haven't found any specification requiring it:
* [OAuth 2.0 RFC|https://datatracker.ietf.org/doc/html/rfc6749] does not specify any token format.
* [JWT RFC|https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.2] states that use of ""sub"" claim is optional in JWTs.
* There is [RFC 9068: JSON Web Token (JWT) Profile for OAuth 2.0 Access Tokens|https://www.rfc-editor.org/rfc/rfc9068.html#name-data-structure] requiring the presence of sub claim, but it is in a proposed state yet.

With this in mind, it does not seem to be interfering in any way with OAuth standards to remove this requirement. However, it would have the benefit of extending Kafka OAuth support for additional OAuth providers.
 ;;;","11/Mar/22 19:39;kirktrue;[~dfonai] - thanks for filing this and providing a PR. I'll take a look as soon as I can, though not until next week, probably.;;;","11/Mar/22 19:40;kirktrue;I'm unable to assign this to [~dfonai]. I'll assign it to myself for now so that it doesn't fall of my radar.;;;","16/Mar/22 15:36;dfonai;[~kirktrue] thank you for the quick response. Please let me know if you need anything from my side.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushHttpMetricsReporter no longer pushes metrics when network failure is recovered.,KAFKA-13728,13433246,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,XiaoyiPeng,XiaoyiPeng,11/Mar/22 02:00,21/Mar/22 17:32,13/Jul/23 09:17,21/Mar/22 17:32,3.1.0,,,,,,,,,,,,,,,3.2.0,,,,,,,tools,,,,,,0,,,,,,"The class *PushHttpMetricsReporter* no longer pushes metrics when network failure is recovered.

I debugged the code and found the problem here :
[https://github.com/apache/kafka/blob/dc36dedd28ff384218b669de13993646483db966/tools/src/main/java/org/apache/kafka/tools/PushHttpMetricsReporter.java#L214-L221]

 

When we submit a task to the *ScheduledThreadPoolExecutor* that needs to be executed periodically, if the task throws an exception and is not swallowed, the task will no longer be scheduled to execute.",,XiaoyiPeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-11 02:00:14.0,,,,,,,,,,"0|z10e1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Edge case in cleaner can result in premature removal of ABORT marker,KAFKA-13727,13433240,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,11/Mar/22 01:03,15/Mar/22 20:17,13/Jul/23 09:17,15/Mar/22 20:17,,,,,,,,,,,,,,,,2.8.2,3.0.2,3.1.1,,,,,,,,,,,0,,,,,,"The log cleaner works by first building a map of the active keys beginning from the dirty offset, and then scanning forward from the beginning of the log to decide which records should be retained based on whether they are included in the map. The map of keys has a limited size. As soon as it fills up, we stop building it. The offset corresponding to the last record that was included in the map becomes the next dirty offset. Then when we are cleaning, we stop scanning forward at the dirty offset. Or to be more precise, we continue scanning until the end of the segment which includes the dirty offset, but all records above that offset are coped as is without checking the map of active keys. 

Compaction is complicated by the presence of transactions. The cleaner must keep track of which transactions have data remaining so that it can tell when it is safe to remove the respective markers. It works a bit like the consumer. Before scanning a segment, the cleaner consults the aborted transaction index to figure out which transactions have been aborted. All other transactions are considered committed.

The problem we have found is that the cleaner does not take into account the range of offsets between the dirty offset and the end offset of the segment containing it when querying ahead for aborted transactions. This means that when the cleaner is scanning forward from the dirty offset, it does not have the complete set of aborted transactions. The main consequence of this is that abort markers associated with transactions which start within this range of offsets become eligible for deletion even before the corresponding data has been removed from the log.

Here is an example. Suppose that the log contains the following entries:

offset=0, key=a

offset=1, key=b

offset=2, COMMIT

offset=3, key=c

offset=4, key=d

offset=5, COMMIT

offset=6, key=b

offset=7, ABORT

Suppose we have an offset map which can only contain 2 keys and the dirty offset starts at 0. The first time we scan forward, we will build a map with keys a and b, which will allow us to move the dirty offset up to 3. Due to the issue documented here, we will not detect the aborted transaction starting at offset 6. But it will not be eligible for deletion on this round of cleaning because it is bound by `delete.retention.ms`. Instead, our new logic will set the deletion horizon for this batch based to the current time plus the configured `delete.retention.ms`.

offset=0, key=a

offset=1, key=b

offset=2, COMMIT

offset=3, key=c

offset=4, key=d

offset=5, COMMIT

offset=6, key=b

offset=7, ABORT (deleteHorizon: N)

Suppose that the time reaches N+1 before the next cleaning. We will begin from the dirty offset of 3 and collect keys c and d before stopping at offset 6. Again, we will not detect the aborted transaction beginning at offset 6 since it is out of the range. This time when we scan, the marker at offset 7 will be deleted because the transaction will be seen as empty and now the deletion horizon has passed. So we end up with this state:

offset=0, key=a

offset=1, key=b

offset=2, COMMIT

offset=3, key=c

offset=4, key=d

offset=5, COMMIT

offset=6, key=b

Effectively it becomes a hanging transaction. The interesting thing is that we might not even detect it. As far as the leader is concerned, it had already completed that transaction, so it is not expecting any additional markers. The transaction index would have been rewritten without the aborted transaction when the log was cleaned, so any consumer fetching the data would see the transaction as committed. On the other hand, if we did a reassignment to a new replica, or if we had to rebuild the full log state during recovery, then we would suddenly detect it.

I am not sure how likely this scenario is in practice. I think it's fair to say it is an extremely rare case. The cleaner has to fail to clean a full segment at least two times and you still need enough time to pass for the marker's deletion horizon to be reached. Perhaps it is possible if the cardinality of keys is very high and the configured memory limit for the cleaner is low.",,hachikuji,tombentley,vincent81jiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 11 20:34:12 UTC 2022,,,,,,,,,,"0|z10e08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 20:34;hachikuji;I realized that this bug is more likely to occur than I first suspected. In fact, it only requires one incomplete cleaner pass over a segment. The problem is that the first pass fails to recreate the transaction index correctly since the aborted transactions are incomplete. Effectively the data from the aborted transaction becomes ""disconnected"" from the marker after the first pass because it is no longer present in the index. So any subsequent pass (whether complete or not) will preserve that incompleteness, which means the marker can be removed on any subsequent pass even if the data is still present. What is arguably worse than losing the marker, the data from this transaction effectively becomes committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-768 OAuth code mixes public and internal classes in same package,KAFKA-13725,13432961,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,09/Mar/22 22:31,29/Jun/23 21:07,13/Jul/23 09:17,23/Sep/22 07:46,3.1.0,3.1.1,3.2.0,3.3.0,,,,,,,,,,,,3.4.0,,,,,,,clients,security,,,,,0,OAuth,,,,,"The {{org.apache.kafka.common.security.oauthbearer.secured}} package from KIP-768 incorrectly mixed all of the classes (public and internal) in the package together.

This bug is to remove all but the public classes from that package and move the rest to a new {{{}org.apache.kafka.common.security.oauthbearer.internal.{}}}{{{}secured{}}} package. This should be back-ported to all versions in which the KIP-768 OAuth work occurs.",,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-09 22:31:06.0,,,,,,,,,,"0|z10ca8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Left-join still emit spurious results in stream-stream joins in some cases,KAFKA-13721,13432908,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,Nollain,Nollain,09/Mar/22 16:53,15/Mar/22 17:32,13/Jul/23 09:17,15/Mar/22 17:32,3.1.0,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,streams,,,,,,0,,,,,,"Stream-stream joins seems to still emit spurious results for some window configurations.

From my tests, it happened when setting before to 0 and having a grace period smaller than the window duration. More precisely it seems to happen when setting before and 
window duration > grace period + before
h2. how to reproduce
{code:java}
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.TestInputTopic;
import org.apache.kafka.streams.TestOutputTopic;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.TopologyTestDriver;
import org.apache.kafka.streams.kstream.JoinWindows;
import org.apache.kafka.streams.kstream.KStream;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.time.Duration;
import java.time.Instant;
import java.util.Properties;

public class SpuriousLeftJoinTest {

    static final Duration WINDOW_DURATION = Duration.ofMinutes(10);
    static final Duration GRACE = Duration.ofMinutes(6);
    static final Duration BEFORE = Duration.ZERO;
    static final String LEFT_TOPIC_NAME = ""LEFT_TOPIC"";
    static final String RIGHT_TOPIC_NAME = ""RIGHT_TOPIC"";
    static final String OUTPUT_TOPIC_NAME = ""OUTPUT_TOPIC"";


    private static TopologyTestDriver testDriver;
    private static TestInputTopic<String, Integer> inputTopicLeft;
    private static TestInputTopic<String, Integer> inputTopicRight;
    private static TestOutputTopic<String, Integer> outputTopic;

    public static Topology createTopology() {

        StreamsBuilder builder = new StreamsBuilder();

        KStream<String, Integer> leftStream = builder.stream(LEFT_TOPIC_NAME);
        KStream<String, Integer> rightStream = builder.stream(RIGHT_TOPIC_NAME);

        // return 1 if left join matched, otherwise 0
        KStream<String, Integer> joined = leftStream.leftJoin(
            rightStream,
            (value1, value2) -> {
                if(value2 == null){
                    return 0;
                }
                return 1;
            },
            JoinWindows.ofTimeDifferenceAndGrace(WINDOW_DURATION, GRACE)
                .before(BEFORE)
        );

        joined.to(OUTPUT_TOPIC_NAME);

        return builder.build();
    }


    @Before
    public void setup() {

        Topology topology = createTopology();

        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, ""test"");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""dummy:9092"");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class);
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.IntegerSerde.class);

        testDriver = new TopologyTestDriver(topology, props);

        inputTopicLeft = testDriver.createInputTopic(LEFT_TOPIC_NAME, Serdes.String().serializer(), Serdes.Integer().serializer());
        inputTopicRight = testDriver.createInputTopic(RIGHT_TOPIC_NAME, Serdes.String().serializer(), Serdes.Integer().serializer());

        outputTopic = testDriver.createOutputTopic(OUTPUT_TOPIC_NAME, Serdes.String().deserializer(), Serdes.Integer().deserializer());

    }

    @After
    public void tearDown() {
        testDriver.close();
    }

    @Test
    public void shouldEmitOnlyOneMessageForKey1(){
        Instant now = Instant.now();
        inputTopicLeft.pipeInput(""key1"", 12, now);
        inputTopicRight.pipeInput(""key1"", 13, now.plus(WINDOW_DURATION));

        // send later record to increase stream time & close the window
        inputTopicLeft.pipeInput(""other_key"", 1212122, now.plus(WINDOW_DURATION).plus(GRACE).plusSeconds(10));

        while (! outputTopic.isEmpty()){
            System.out.println(outputTopic.readKeyValue());
        }
    }


}
{code}
Stdout of previous code is
{noformat}
KeyValue(key1, 0)
KeyValue(key1, 1)
{noformat}
However it should be
{noformat}
KeyValue(key1, 1)
{noformat}",,mjsax,Nollain,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-09 16:53:12.0,,,,,,,,,,"0|z10byo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Few topic partitions remain under replicated after broker lose connectivity to zookeeper,KAFKA-13720,13432856,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,dhirendraks@gmail.com,dhirendraks@gmail.com,09/Mar/22 12:07,20/Jun/22 03:07,13/Jul/23 09:17,17/Jun/22 03:04,2.7.1,,,,,,,,,,,,,,,3.1.0,,,,,,,controller,,,,,,0,,,,,,"Few topic partitions remain under replicated after broker lose connectivity to zookeeper.
It only happens when brokers lose connectivity to zookeeper and it results in change in active controller. Issue does not occur always but randomly.
Issue never occurs when there is no change in active controller when brokers lose connectivity to zookeeper.
Following error message i found in the log file.


[2022-02-28 04:01:20,217] WARN [Partition __consumer_offsets-4 broker=1] Controller failed to update ISR to PendingExpandIsr(isr=Set(1), newInSyncReplicaId=2) due to unexpected UNKNOWN_SERVER_ERROR. Retrying. (kafka.cluster.Partition)
[2022-02-28 04:01:20,217] ERROR [broker-1-to-controller] Uncaught error in request completion: (org.apache.kafka.clients.NetworkClient)
java.lang.IllegalStateException: Failed to enqueue `AlterIsr` request with state LeaderAndIsr(leader=1, leaderEpoch=2728, isr=List(1, 2), zkVersion=4719) for partition __consumer_offsets-4
at kafka.cluster.Partition.sendAlterIsrRequest(Partition.scala:1403)
at kafka.cluster.Partition.$anonfun$handleAlterIsrResponse$1(Partition.scala:1438)
at kafka.cluster.Partition.handleAlterIsrResponse(Partition.scala:1417)
at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1(Partition.scala:1398)
at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1$adapted(Partition.scala:1398)
at kafka.server.AlterIsrManagerImpl.$anonfun$handleAlterIsrResponse$8(AlterIsrManager.scala:166)
at kafka.server.AlterIsrManagerImpl.$anonfun$handleAlterIsrResponse$8$adapted(AlterIsrManager.scala:163)
at scala.collection.immutable.List.foreach(List.scala:333)
at kafka.server.AlterIsrManagerImpl.handleAlterIsrResponse(AlterIsrManager.scala:163)
at kafka.server.AlterIsrManagerImpl.responseHandler$1(AlterIsrManager.scala:94)
at kafka.server.AlterIsrManagerImpl.$anonfun$sendRequest$2(AlterIsrManager.scala:104)
at kafka.server.BrokerToControllerRequestThread.handleResponse(BrokerToControllerChannelManagerImpl.scala:175)
at kafka.server.BrokerToControllerRequestThread.$anonfun$generateRequests$1(BrokerToControllerChannelManagerImpl.scala:158)
at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:586)
at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:578)
at kafka.common.InterBrokerSendThread.doWork(InterBrokerSendThread.scala:71)
at kafka.server.BrokerToControllerRequestThread.doWork(BrokerToControllerChannelManagerImpl.scala:183)
at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
 
under replication count goes to zero after the controller broker is restarted again. but this require manual intervention.
Expectation is that when broker reconnect with zookeeper cluster should come back to stable state with under replication count as zero by itself without any manual intervention.",,dhirendraks@gmail.com,showuon,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13483,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 03:07:38 UTC 2022,,,,,,,,,,"0|z10bn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/22 06:24;dhirendraks@gmail.com;Some more information
split brain issue is happening with the controller.
when brokers (including the active controller) lose connection with zookeeper, for few seconds 2 brokers are the active controller.
following is the log of broker 1 and broker 0. At the time when connection to zookeeper was lost broker 1 was the active controller

Broker 0 log:
[2022-03-14 04:02:29,813] WARN Session 0x3003837bae70005 for server zookeeper.svc.cluster.local/172.30.252.43:2181, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.io.IOException: Connection reset by peer  
        at java.base/sun.nio.ch.FileDispatcherImpl.read0(Native Method)  
        at java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)  
        at java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)  
        at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)  
        at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)  
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:358)  
        at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)  
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:365)  
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1223)  
[2022-03-14 04:02:29,816] INFO Unable to read additional data from server sessionid 0x3003a92cdbb0000, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:31,333] INFO Opening socket connection to server zookeeper.svc.cluster.local/172.30.252.43:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:31,335] INFO Socket connection established, initiating session, client: /10.130.96.38:34308, server: zookeeper.svc.cluster.local/172.30.252.43:2181 (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:31,349] INFO Session establishment complete on server zookeeper.svc.cluster.local/172.30.252.43:2181, sessionid = 0x3003a92cdbb0000, negotiated timeout = 4000 (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:31,710] INFO Opening socket connection to server zookeeper.svc.cluster.local/172.30.252.43:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:34,315] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 913 and epoch zk version is now 913 (kafka.controller.KafkaController)  

[2022-03-14 04:02:36,466] WARN [Partition __consumer_offsets-0 broker=0] Failed to update ISR to PendingExpandIsr(isr=Set(0), newInSyncReplicaId=2) due to unexpected UNKNOWN_SERVER_ERROR. Retrying. (kafka.cluster.Partition)  
[2022-03-14 04:02:36,467] ERROR [broker-0-to-controller] Uncaught error in request completion: (org.apache.kafka.clients.NetworkClient)  
java.lang.IllegalStateException: Failed to enqueue ISR change state LeaderAndIsr(leader=0, leaderEpoch=2995, isr=List(0, 2), zkVersion=5163) for partition __consumer_offsets-0  
        at kafka.cluster.Partition.sendAlterIsrRequest(Partition.scala:1379)  
        at kafka.cluster.Partition.$anonfun$handleAlterIsrResponse$1(Partition.scala:1413)  
        at kafka.cluster.Partition.handleAlterIsrResponse(Partition.scala:1392)  
        at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1(Partition.scala:1370)  
        at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1$adapted(Partition.scala:1370)  
        at kafka.server.DefaultAlterIsrManager.$anonfun$handleAlterIsrResponse$8(AlterIsrManager.scala:262)  
        at kafka.server.DefaultAlterIsrManager.$anonfun$handleAlterIsrResponse$8$adapted(AlterIsrManager.scala:259)  
        at scala.collection.immutable.List.foreach(List.scala:333)  
        at kafka.server.DefaultAlterIsrManager.handleAlterIsrResponse(AlterIsrManager.scala:259)  
        at kafka.server.DefaultAlterIsrManager$$anon$1.onComplete(AlterIsrManager.scala:179)  
        at kafka.server.BrokerToControllerRequestThread.handleResponse(BrokerToControllerChannelManager.scala:362)  
        at kafka.server.BrokerToControllerRequestThread.$anonfun$generateRequests$1(BrokerToControllerChannelManager.scala:333)  
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)  
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:584)  
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:576)  
        at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:74)  
        at kafka.server.BrokerToControllerRequestThread.doWork(BrokerToControllerChannelManager.scala:368)  
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)  
[2022-03-14 04:02:36,467] WARN [Broker id=0] Received update metadata request with correlation id 20 from an old controller 1 with epoch 912. Latest known controller epoch is 913 (state.change.logger)  

Broker 1 log:  
[2022-03-14 04:02:29,815] INFO Unable to read additional data from server sessionid 0x3003a92cdbb0002, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:29,815] INFO Unable to read additional data from server sessionid 0x10037c8c66b0001, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:31,715] INFO Opening socket connection to server zookeeper.svc.cluster.local/172.30.252.43:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:31,910] INFO Opening socket connection to server zookeeper.svc.cluster.local/172.30.252.43:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:34,722] INFO Socket error occurred: zookeeper.svc.cluster.local/172.30.252.43:2181: No route to host (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:34,724] INFO Socket error occurred: zookeeper.svc.cluster.local/172.30.252.43:2181: No route to host (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:35,177] DEBUG [Controller id=1] Updating ISRs for partitions: Set(__consumer_offsets-0). (kafka.controller.KafkaController)  
[2022-03-14 04:02:36,172] INFO Opening socket connection to server zookeeper.svc.cluster.local/172.30.252.43:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:36,174] INFO Socket connection established, initiating session, client: /10.130.72.182:43696, server: zookeeper.svc.cluster.local/172.30.252.43:2181 (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:36,181] WARN Unable to reconnect to ZooKeeper service, session 0x3003a92cdbb0002 has expired (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:36,181] INFO Unable to reconnect to ZooKeeper service, session 0x3003a92cdbb0002 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:36,181] INFO EventThread shut down for session: 0x3003a92cdbb0002 (org.apache.zookeeper.ClientCnxn)  
[2022-03-14 04:02:36,182] INFO [ZooKeeperClient Kafka server] Session expired. (kafka.zookeeper.ZooKeeperClient)  
[2022-03-14 04:02:36,454] ERROR [Controller id=1] Failed to update ISR for partition __consumer_offsets-0 (kafka.controller.KafkaController)  
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired  
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:134)  
        at kafka.zk.KafkaZkClient$.kafka$zk$KafkaZkClient$$unwrapResponseWithControllerEpochCheck(KafkaZkClient.scala:2008)  
        at kafka.zk.KafkaZkClient.$anonfun$retryRequestsUntilConnected$2(KafkaZkClient.scala:1770)  
        at scala.collection.StrictOptimizedIterableOps.map(StrictOptimizedIterableOps.scala:99)  
        at scala.collection.StrictOptimizedIterableOps.map$(StrictOptimizedIterableOps.scala:86)  
        at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:42)  
        at kafka.zk.KafkaZkClient.retryRequestsUntilConnected(KafkaZkClient.scala:1770)  
        at kafka.zk.KafkaZkClient.setTopicPartitionStatesRaw(KafkaZkClient.scala:204)  
        at kafka.zk.KafkaZkClient.updateLeaderAndIsr(KafkaZkClient.scala:262)  
        at kafka.controller.KafkaController.processAlterIsr(KafkaController.scala:2338)  
        at kafka.controller.KafkaController.process(KafkaController.scala:2468)  
        at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)  
        at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)  
        at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)  
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)  
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)  
        at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)  
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)  
[2022-03-14 04:02:36,455] INFO [Controller id=1 epoch=912] Sending UpdateMetadata request to brokers HashSet(0, 1, 2) for 1 partitions (state.change.logger)  
[2022-03-14 04:02:36,457] DEBUG [Controller id=1] Resigning (kafka.controller.KafkaController)  
[2022-03-14 04:02:36,457] DEBUG [Controller id=1] Unregister BrokerModifications handler for Set(0, 1, 2) (kafka.controller.KafkaController)  
[2022-03-14 04:02:36,555] DEBUG [Controller id=1] Broker 0 has been elected as the controller, so stopping the election process. (kafka.controller.KafkaController)  

Timelines from the log.
Broker 0 was elected new controller at 04:02:34
At 04:02:35 Broker 1 still think it is the active controller.
At 04:02:36 Broker 1 realized Broker 0 is the new active controller.

How to resolve this split brain issue ?;;;","17/Jun/22 03:03;showuon;[~dhirendraks@gmail.com], thanks for reporting the issue. I can confirm this issue is fixed (indirectly) in Kafka v3.1 and later. The root cause of this issue is that when controller is changing, the ISR update request should keep retrying, until the ""real"" controller got this request. So, in the log you see, there's only 1 error about ""Failed to update ISR to PendingExpandIsr"", and no more. It should keep retrying, but some bugs in the code cause it didn't. In v3.1 and later will not have this issue. Thanks.;;;","20/Jun/22 03:07;showuon;KAFKA-14010 is created to make sure it works as expected, although after v3.1.0, the AlterIsr request will send to Controller ""as long as the follower keeps fetching data from leader"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
connector restart cause duplicate tasks,KAFKA-13719,13432833,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,sunshujie,sunshujie,sunshujie,09/Mar/22 10:59,30/Mar/22 17:23,13/Jul/23 09:17,30/Mar/22 13:02,3.0.0,,,,,,,,,,,,,,,3.1.1,3.2.0,3.3.0,,,,,KafkaConnect,,,,,,1,,,,,,Restart connector with parameter includeTasks=true&onlyFailed=false cause duplicate tasks and duplicate message。,,ChrisEgerton,sunshujie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-09 10:59:15.0,,,,,,,,,,"0|z10bi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-topics describe topic with default config will show `segment.bytes` overridden config ,KAFKA-13718,13432583,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rjoerger,showuon,showuon,08/Mar/22 07:30,03/Jun/22 13:43,13/Jul/23 09:17,03/Jun/22 03:00,2.8.1,3.0.0,3.1.0,,,,,,,,,,,,,3.3.0,,,,,,,tools,,,,,,0,newbie,newbie++,,,,"Following the quickstart guide[1], when describing the topic just created with default config, I found there's a overridden config shown:

_> bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092_

_Topic: quickstart-events   TopicId: 06zRrzDCRceR9zWAf_BUWQ    PartitionCount: 1    ReplicationFactor: 1    *Configs: segment.bytes=1073741824*_
    _Topic: quickstart-events    Partition: 0    Leader: 0    Replicas: 0    Isr: 0_

 

This config result should be empty as in Kafka quick start page. Although the config value is what we expected (default 1GB value), this info display still confuse users.

 

Note: I checked the 2.8.1 build, this issue also happened.

 

[1]: [https://kafka.apache.org/quickstart]",,dengziming,hachikuji,mimaison,rjoerger,showuon,,,,,,,,,,,,,KAFKA-13875,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 03 13:17:57 UTC 2022,,,,,,,,,,"0|z109yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/22 09:54;mimaison;This has been the case for many releases, I get the same behavior with 2.2.0. At this stage, it probably makes more sense to update the quickstart.;;;","08/Mar/22 10:05;showuon;>  I get the same behavior with 2.2.0.

Oh, really!

> it probably makes more sense to update the quickstart.

Maybe we can have a better definition about what we expected to be shown in the `configs` value. My understanding is it showed the overridden configs. If so, it should be a bug. If not, we can update the quickstart accordingly.;;;","18/May/22 17:27;rjoerger;I'd love to help out on this particular Jira. I'm new to the project so I apologize for any silly questions. It seems that we as a community need to discuss what the output of the kafka-topics tooling should look like. When we're looking at the documentation, I don't see any documentation explicitly stating that the kafka-topics output will only show the non-default configurations. As I understand it this is something that is just ""known"" among the community but isn't necessarily explicit. This leads me to a few points of question. 

1) Do we want to open a new issue to update the documentation for kafka-topics to make that explicit (or am I mistaken on this?)

2) Looking at the documentation for segment.bytes, this (1073741824) is the default value. When I was investigating the code base, it seems to be the default value in there as well. I'm not sure how much benefit there would be to including this configuration vs. not including this configuration. 

3) I know this isn't an API change but is still user facing, would this require a KIP to change?

4) Where in the code base does this value reside? In my digging, it seems to reside in kafkaConfig.scala:112 but that's 1* 1024 * 1024 *1024 = 1073741824 so perhaps there is an issue of comparison? I'd love to tackle this but need some guidance if at all possible. 

Thanks;;;","19/May/22 00:18;dengziming;Hello [~rjoerger] 
 # this issue is enough to track the problem so another issue is unnecessary
 # We are not intend to include default `segment.bytes` in the output but it was printed(due to an bug), but this bug has been around for a long time so [~mimaison] suggest we keep this bug in the future(make it bug-compatible)
 # Since we are trying to keep its unchanged, so KIP is unnecessary
 # This is the source of the bug, currently we haven't find it, you can investigate it and add a comment to it to explain why we should keep this bug.;;;","19/May/22 02:13;showuon;Thanks for the answers, [~dengziming] !

[~rjoerger] , as Ziming said, we are unsure the root cause of this issue. Maybe you can investigate it first, and see if you find anything. My thought is that, since we didn't provide any new configs during create topics:

_> bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092_

 

So, the describe results should not contain any topic related configs, to avoid confusing users. As you mentioned, we never document anything about what the ""configs"" field should display, but from the code
{code:java}
val configsAsString = config.entries.asScala.filter(!_.isDefault).map { ce => s""${ce.name}=${ce.value}"" }.mkString("",""){code}
We can know it is trying to filter out anything ""not default"".

 

So, maybe we should first know why only the ""segment.bytes=1073741824"" showed here.

Thanks for your help.;;;","30/May/22 23:55;hachikuji;[~showuon] I think the reason `segment.bytes` shows up is that we have it defined in the default `server.properties`: [https://github.com/apache/kafka/blob/trunk/config/server.properties#L112.] Technically, the static configuration is overriding the default value (even though the overridden value is the same as the default). If you take it out of `server.properties`, then it is no longer displayed. I guess I'd call this correct, if perhaps slightly surprising, behavior. Perhaps we could ""fix"" it by commenting out the default in `server.properties`?;;;","31/May/22 02:37;showuon;[~hachikuji] , thanks for the comment. Yes, I can confirm that after removing the `log.segment.bytes` in server.properties, it won't display on the topic describe results.

[~rjoerger] , do you think this is a good fix? If so, welcome to submit a PR for it. Thank you.;;;","02/Jun/22 19:18;rjoerger;Hey everybody. Sorry for the radio silence on this. I actually came to the identical conclusion that [~hachikuji] came to and was spending time stepping through the request handling on the broker side to figure how it is being read in so that I could possible make a larger code.  Interestingly enough, [~showuon] , I found that if I remove the ""log"" prefix, I stop seeing that value in the topic config output so I'm not quite sure what the explanation is on that one. The one major difference I noticed though is that when running the TopicCommandIntegrationTest suite, I found that the value for the segment.bytes field is being returned as a default topic configuration. I'm not sure if commenting out the value will have any effect on the test suite, but I'll give it a shot and see if the tests report differently. 

 

Thanks everybody!;;;","03/Jun/22 13:17;rjoerger;There was a question on if segment.bytes should be included. As was decided in this Jira, we should remove it. This means that it should therefore be removed in the documentation as well. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumer.close throws authorization exception even when commit offsets is empty,KAFKA-13717,13432550,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vincent81jiang,vincent81jiang,vincent81jiang,08/Mar/22 03:58,10/Mar/22 02:59,13/Jul/23 09:17,10/Mar/22 02:59,,,,,,,,,,,,,,,,3.2.0,,,,,,,unit tests,,,,,,0,,,,,,"When offsets is empty and coordinator is unknown, KafkaConsumer.close doesn't throw exception before commit [https://github.com/apache/kafka/commit/4b468a9d81f7380f7197a2a6b859c1b4dca84bd9|https://github.com/apache/kafka/commit/4b468a9d81f7380f7197a2a6b859c1b4dca84bd9,].  After this commit, Kafka.close may throw authorization exception.

 

Root cause is because in the commit, the logic is changed to call lookupCoordinator even if offsets is empty. 

 

Even if a consumer doesn't have access to a group or a topic, it might be better to not throw authorization exception in this case because close() call doesn't touch actually access any resource.",,vincent81jiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-08 03:58:07.0,,,,,,,,,,"0|z109rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test IQv2StoreIntegrationTest,KAFKA-13714,13432510,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,07/Mar/22 21:02,22/Feb/23 16:56,13/Jul/23 09:17,24/Mar/22 03:37,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,,,,,,"I have observed multiple consistency violations in the IQv2StoreIntegrationTest. Since this is the first release of IQv2, and it's apparently a major flaw in the feature, we should not release with this bug outstanding. Depending on the time-table, we may want to block the release or pull the feature until the next release.

 

The first observation I have is from 23 Feb 2022. So far all observations point to the range query in particular, and all observations have been for RocksDB stores, including RocksDBStore, TimestampedRocksDBStore, and the windowed store built on RocksDB segments.

For reference, range queries were implemented on 16 Feb 2022: [https://github.com/apache/kafka/commit/b38f6ba5cc989702180f5d5f8e55ba20444ea884]

The window-specific range query test has also failed once that I have seen. That feature was implemented on 2 Jan 2022: [https://github.com/apache/kafka/commit/b8f1cf14c396ab04b8968a8fa04d8cf67dd3254c]

 

Here are some stack traces I have seen:
{code:java}
verifyStore[cache=true, log=true, supplier=ROCKS_KV, kind=PAPI]

java.lang.AssertionError: 
Expected: is <[1, 2, 3]>
     but: was <[1, 2]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1125)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:803)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:776)
 {code}
{code:java}
verifyStore[cache=true, log=true, supplier=TIME_ROCKS_KV, kind=PAPI]

java.lang.AssertionError: 
Expected: is <[1, 2, 3]>
     but: was <[1, 3]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1131)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:809)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:778)
	 {code}
{code:java}
verifyStore[cache=true, log=true, supplier=ROCKS_KV, kind=PAPI]

java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@35025a0a, executionInfo=[], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@38732364, executionInfo=[], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 2]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1129)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:807)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:780)
 {code}
{code:java}
verifyStore[cache=true, log=false, supplier=ROCKS_WINDOW, kind=DSL] 

    java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredWindowedKeyValueIterator@2a32fb6, executionInfo=[], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredWindowedKeyValueIterator@6107165, executionInfo=[], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
    Expected: is <[0, 1, 2, 3]> 
         but: was <[0, 2, 3]>
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
        at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleWindowRangeQuery(IQv2StoreIntegrationTest.java:1234)
        at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleWindowRangeQueries(IQv2StoreIntegrationTest.java:880)
        at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:793)
 {code}
 

Some observations:
 * After I added the whole query result to the failure message, we can see that the results are always past the desired position, even though they don't include all the data that should have been present in that position.
 * All the observed failures have happened with caching=true, but that it probably a red herring, since range queries skip the cache (cf fe72187cb15bf7dcc16e8630ed379e979c101151)
 * For a while, I thought that it might be a thread visibility problem with the iterators, since the missing record was always at the end of the range for some partition, but the window range failure is missing record 1, which is at the beginning of the range in partition 1.

I have been able to reproduce the failure locally, but only occasionally. I made some hacks to narrow down the space of possibilities: [https://github.com/vvcephei/kafka/commit/2a0776e52e378f1c59e98f352e3fa4f79c55842d]

I didn't have success running that one test until failure in IDEA. It has never failed for me in IDEA, even after thousands of attempts. In my testing branch, I added a loop to repeat one test configuration a thousand times in Gradle, but it still didn't fail reliably.

I also added a test to specifically check that RocksDB is giving the desired serialization both in one thread and across threads, and that test passes for me. My next thought is to expand that Tmp test to do the same with the RocksDBIterator class, or maybe just with a standalone RocksDBStore to see if we can reproduce it.",,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 21 00:57:30 UTC 2022,,,,,,,,,,"0|z109ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 19:04;vvcephei;Another local repro:

 
{code:java}
org.apache.kafka.streams.integration.IQv2StoreIntegrationTest > verifyStore[cache=true, log=true, supplier=TIME_ROCKS_KV, kind=PAPI] FAILED
    java.lang.AssertionError: Result:StateQueryResult{partitionResults={

0=SucceededQueryResult{

result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@3f702946, 
executionInfo=[
  Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 1153925ns, 
  Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 1165952ns, 
  Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 1181616ns, 
  Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@278667fd in 1260365ns
], 
position=Position{position={input-topic={0=1}}}}, 

1=SucceededQueryResult{

result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@42b6d0cc,

executionInfo=[
  Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 109311ns, 
  Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 116767ns, 
  Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 128961ns, 
  Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@684b31de in 185521ns
],
position=Position{position={input-topic={1=1}}}}}, 

globalResult=null}
    Expected: is <[1, 2, 3]>
         but: was <[1, 2]>
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
        at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1129)
        at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:807)
        at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:776)
 

{code}
 

logs:
{code:java}
[2022-03-17 07:31:56,286] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] Kafka Streams version: test-version (org.apache.kafka.streams.KafkaStreams:912)
[2022-03-17 07:31:56,286] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] Kafka Streams commit ID: test-commit-ID (org.apache.kafka.streams.KafkaStreams:913)
[2022-03-17 07:31:56,288] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:346)
[2022-03-17 07:31:56,295] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread:105)
[2022-03-17 07:31:56,297] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-17 07:31:56,304] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:397)
[2022-03-17 07:31:56,308] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration:126)
[2022-03-17 07:31:56,308] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-producer] Cluster ID: iZBZzURBQr6rMZEB6oxg7g (org.apache.kafka.clients.Metadata:287)
[2022-03-17 07:31:56,309] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-producer] ProducerId set to 10 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-17 07:31:56,311] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-7727452241269276867/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-17 07:31:56,311] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-17 07:31:56,311] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams:1316)
[2022-03-17 07:31:56,311] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:539)
[2022-03-17 07:31:56,311] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 07:31:56,312] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Subscribed to topic(s): input-topic (org.apache.kafka.clients.consumer.KafkaConsumer:968)
[2022-03-17 07:31:56,316] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to 3iGbO84ORnetMZd-sFB2NA (org.apache.kafka.clients.Metadata:402)
[2022-03-17 07:31:56,316] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to 3iGbO84ORnetMZd-sFB2NA (org.apache.kafka.clients.Metadata:402)
[2022-03-17 07:31:56,316] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Cluster ID: iZBZzURBQr6rMZEB6oxg7g (org.apache.kafka.clients.Metadata:287)
[2022-03-17 07:31:56,316] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Discovered group coordinator localhost:37071 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:879)
[2022-03-17 07:31:56,317] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-17 07:31:56,319] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Request joining group due to: need to re-join with the given member-id: app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1032)
[2022-03-17 07:31:56,319] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-17 07:31:56,321] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Successfully joined group with generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:609)
[2022-03-17 07:31:56,322] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics:75)
[2022-03-17 07:31:56,346] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] All members participating in this rebalance: 
83d014b6-61fa-4ba5-a7b3-f035b42c2138: [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:620)
[2022-03-17 07:31:56,347] WARN Unable to assign 1 of 1 standby tasks for task [0_0]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-17 07:31:56,347] WARN Unable to assign 1 of 1 standby tasks for task [0_1]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-17 07:31:56,347] INFO Decided on assignment: {83d014b6-61fa-4ba5-a7b3-f035b42c2138=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0, 0_1=0]) capacity: 1 assigned: 2]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor:96)
[2022-03-17 07:31:56,347] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] Assigned tasks [0_1, 0_0] including stateful [0_1, 0_0] to clients as: 
83d014b6-61fa-4ba5-a7b3-f035b42c2138=[activeTasks: ([0_0, 0_1]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:638)
[2022-03-17 07:31:56,347] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] Client 83d014b6-61fa-4ba5-a7b3-f035b42c2138 per-consumer assignment:
	prev owned active {}
	prev owned standby {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670=[]}
	assigned active {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670=[0_1, 0_0]}
	revoking active {}
	assigned standby {}
 (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:847)
[2022-03-17 07:31:56,347] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:866)
[2022-03-17 07:31:56,347] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Finished assignment for group at generation 1: {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670=Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:691)
[2022-03-17 07:31:56,350] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Successfully synced group in generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer-bbfac398-55bc-4e11-8586-94857e921670', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:784)
[2022-03-17 07:31:56,350] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Updating assignment with
	Assigned partitions:                       [input-topic-0, input-topic-1]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-0, input-topic-1]
	Revoked partitions (owned - assigned):     []
 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:422)
[2022-03-17 07:31:56,350] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:294)
[2022-03-17 07:31:56,350] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1345)
[2022-03-17 07:31:56,350] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Handle new assignment with:
	New active tasks: [0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:273)
[2022-03-17 07:31:56,351] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Adding newly assigned partitions: input-topic-0, input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:306)
[2022-03-17 07:31:56,351] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 07:31:56,352] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 07:31:56,352] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 07:31:56,354] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Resetting offset for partition input-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:37071 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 07:31:56,354] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Resetting offset for partition input-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:37071 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 07:31:56,446] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-17 07:31:56,447] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-17 07:31:56,447] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-17 07:31:56,473] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-17 07:31:56,473] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-17 07:31:56,473] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-17 07:31:56,476] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0, app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 (org.apache.kafka.clients.consumer.KafkaConsumer:1123)
[2022-03-17 07:31:56,476] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-17 07:31:56,476] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-17 07:31:56,479] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0 to 0 since the associated topicId changed from null to Fsdq60XXRrOCrwrWB38Lug (org.apache.kafka.clients.Metadata:402)
[2022-03-17 07:31:56,479] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 to 0 since the associated topicId changed from null to Fsdq60XXRrOCrwrWB38Lug (org.apache.kafka.clients.Metadata:402)
[2022-03-17 07:31:56,479] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Cluster ID: iZBZzURBQr6rMZEB6oxg7g (org.apache.kafka.clients.Metadata:287)
[2022-03-17 07:31:56,481] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:37071 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 07:31:56,482] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:37071 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 07:31:56,583] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-17 07:31:56,583] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-17 07:31:56,583] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread:838)
[2022-03-17 07:31:56,586] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 07:31:56,587] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-17 07:31:56,588] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 07:31:56,589] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-17 07:31:56,589] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Restoration took 238 ms for all tasks [0_0, 0_1] (org.apache.kafka.streams.processor.internals.StreamThread:862)
[2022-03-17 07:31:56,589] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 07:31:56,590] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-17 07:31:56,590] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Requesting the log end offset for input-topic-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-17 07:31:56,590] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973] Requesting the log end offset for input-topic-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-17 07:31:56,708] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-0 to 0 since the associated topicId changed from null to Fsdq60XXRrOCrwrWB38Lug (org.apache.kafka.clients.Metadata:402)
[2022-03-17 07:31:56,708] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 to 0 since the associated topicId changed from null to Fsdq60XXRrOCrwrWB38Lug (org.apache.kafka.clients.Metadata:402)
[2022-03-17 07:31:56,726] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-17 07:31:56,729] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1103)
[2022-03-17 07:31:56,729] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 07:31:56,730] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] Shutting down 1 stream threads (org.apache.kafka.streams.KafkaStreams:1363)
[2022-03-17 07:31:56,733] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1117)
[2022-03-17 07:31:56,738] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1229)
[2022-03-17 07:31:56,738] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2022-03-17 07:31:56,739] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-kv-store-changelog-1 (org.apache.kafka.clients.consumer.KafkaConsumer:1123)
[2022-03-17 07:31:56,745] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2022-03-17 07:31:56,745] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2022-03-17 07:31:56,746] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1229)
[2022-03-17 07:31:56,746] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2022-03-17 07:31:56,747] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-17 07:31:56,754] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2022-03-17 07:31:56,754] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] task [0_1] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2022-03-17 07:31:56,754] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1207)
[2022-03-17 07:31:56,761] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-17 07:31:56,773] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 07:31:56,774] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138-StreamThread-1] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1152)
[2022-03-17 07:31:56,774] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] Shutdown 1 stream threads complete (org.apache.kafka.streams.KafkaStreams:1381)
[2022-03-17 07:31:56,778] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-17 07:31:56,779] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973-83d014b6-61fa-4ba5-a7b3-f035b42c2138] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:1447)
[2022-03-17 07:31:56,779] INFO stream-thread [Test worker] Deleting task directory 0_0 for 0_0 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:553)
[2022-03-17 07:31:56,779] INFO stream-thread [Test worker] Deleting task directory 0_1 for 0_1 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:553)
[2022-03-17 07:31:56,780] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-7727452241269276867/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1159953973 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422) {code};;;","20/Mar/22 03:56;vvcephei;Continuing debugging:
h3. verifyStore[cache=true, log=true, supplier=TIME_ROCKS_KV, kind=PAPI]
{code:java}
java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@1f193686, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 2335793ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 3045186ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 3068465ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 3974765ns], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@31e72cbc, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 112183ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 775416ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 796244ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 849835ns], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 2]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1140)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:818)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:782)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
 {code}
and
{code:java}
 [2022-03-17 18:44:39,092] INFO Generating test cases according to random seed: -8162527669410615270 (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest:361)
[2022-03-17 18:44:40,554] INFO [Producer clientId=producer-1] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-17 18:44:40,575] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to XHEGE61rT1Cxas8wQz0v8g (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:40,576] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to XHEGE61rT1Cxas8wQz0v8g (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:40,579] INFO [Producer clientId=producer-1] Cluster ID: pv0OEZ6kSeqPd9lo5LuvSA (org.apache.kafka.clients.Metadata:287)
[2022-03-17 18:44:40,587] INFO [Producer clientId=producer-1] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-17 18:44:40,641] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1207)
[2022-03-17 18:44:40,687] WARN Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-1258703357936662057] (org.apache.kafka.streams.processor.internals.StateDirectory:138)
[2022-03-17 18:44:40,688] INFO No process id found on disk, got fresh process id f81ddd7f-d3c0-4935-bef7-a7c04679fba8 (org.apache.kafka.streams.processor.internals.StateDirectory:213)
[2022-03-17 18:44:40,706] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8] Kafka Streams version: test-version (org.apache.kafka.streams.KafkaStreams:912)
[2022-03-17 18:44:40,706] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8] Kafka Streams commit ID: test-commit-ID (org.apache.kafka.streams.KafkaStreams:913)
[2022-03-17 18:44:40,714] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:346)
[2022-03-17 18:44:40,737] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread:105)
[2022-03-17 18:44:40,739] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-17 18:44:40,744] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-producer] Cluster ID: pv0OEZ6kSeqPd9lo5LuvSA (org.apache.kafka.clients.Metadata:287)
[2022-03-17 18:44:40,744] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-producer] ProducerId set to 1 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-17 18:44:40,746] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:397)
[2022-03-17 18:44:40,754] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration:126)
[2022-03-17 18:44:40,773] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-1258703357936662057/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-17 18:44:40,773] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-17 18:44:40,774] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams:1316)
[2022-03-17 18:44:40,774] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:539)
[2022-03-17 18:44:40,774] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 18:44:40,775] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Subscribed to topic(s): input-topic (org.apache.kafka.clients.consumer.KafkaConsumer:968)
[2022-03-17 18:44:40,787] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to XHEGE61rT1Cxas8wQz0v8g (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:40,787] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to XHEGE61rT1Cxas8wQz0v8g (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:40,788] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Cluster ID: pv0OEZ6kSeqPd9lo5LuvSA (org.apache.kafka.clients.Metadata:287)
[2022-03-17 18:44:40,879] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread:838)
[2022-03-17 18:44:40,883] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Discovered group coordinator localhost:43847 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:879)
[2022-03-17 18:44:40,884] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-17 18:44:40,903] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Request joining group due to: need to re-join with the given member-id: app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1032)
[2022-03-17 18:44:40,903] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-17 18:44:40,913] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Successfully joined group with generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:609)
[2022-03-17 18:44:40,917] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics:75)
[2022-03-17 18:44:40,952] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] All members participating in this rebalance: 
f81ddd7f-d3c0-4935-bef7-a7c04679fba8: [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:620)
[2022-03-17 18:44:40,955] WARN Unable to assign 1 of 1 standby tasks for task [0_0]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-17 18:44:40,955] WARN Unable to assign 1 of 1 standby tasks for task [0_1]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-17 18:44:40,957] INFO Decided on assignment: {f81ddd7f-d3c0-4935-bef7-a7c04679fba8=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0, 0_1=0]) capacity: 1 assigned: 2]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor:96)
[2022-03-17 18:44:40,958] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] Assigned tasks [0_1, 0_0] including stateful [0_1, 0_0] to clients as: 
f81ddd7f-d3c0-4935-bef7-a7c04679fba8=[activeTasks: ([0_0, 0_1]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:638)
[2022-03-17 18:44:40,963] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] Client f81ddd7f-d3c0-4935-bef7-a7c04679fba8 per-consumer assignment:
	prev owned active {}
	prev owned standby {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec=[]}
	assigned active {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec=[0_1, 0_0]}
	revoking active {}
	assigned standby {}
 (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:847)
[2022-03-17 18:44:40,963] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:866)
[2022-03-17 18:44:40,964] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Finished assignment for group at generation 1: {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec=Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:691)
[2022-03-17 18:44:40,977] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Successfully synced group in generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer-7deeed76-5583-4763-adac-3178fe1629ec', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:784)
[2022-03-17 18:44:40,977] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Updating assignment with
	Assigned partitions:                       [input-topic-0, input-topic-1]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-0, input-topic-1]
	Revoked partitions (owned - assigned):     []
 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:422)
[2022-03-17 18:44:40,977] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:294)
[2022-03-17 18:44:40,978] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1345)
[2022-03-17 18:44:40,979] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Handle new assignment with:
	New active tasks: [0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:273)
[2022-03-17 18:44:40,990] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Adding newly assigned partitions: input-topic-0, input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:306)
[2022-03-17 18:44:40,990] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 18:44:41,067] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 18:44:41,067] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 18:44:41,095] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-17 18:44:41,099] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] task [0_0] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-17 18:44:41,099] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-17 18:44:41,107] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-17 18:44:41,108] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] task [0_1] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-17 18:44:41,108] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] task [0_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-17 18:44:41,118] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1, app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1123)
[2022-03-17 18:44:41,119] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-17 18:44:41,119] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-17 18:44:41,123] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 to 0 since the associated topicId changed from null to 7dPI54TvQKuXNT9_gH9PDA (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:41,123] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1 to 0 since the associated topicId changed from null to 7dPI54TvQKuXNT9_gH9PDA (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:41,123] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Cluster ID: pv0OEZ6kSeqPd9lo5LuvSA (org.apache.kafka.clients.Metadata:287)
[2022-03-17 18:44:41,129] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43847 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 18:44:41,130] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43847 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 18:44:41,176] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,177] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,230] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-17 18:44:41,231] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-17 18:44:41,232] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Resetting offset for partition input-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43847 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 18:44:41,233] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Resetting offset for partition input-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43847 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-17 18:44:41,233] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 18:44:41,234] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-17 18:44:41,235] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-17 18:44:41,235] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] task [0_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-17 18:44:41,236] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] Restoration took 246 ms for all tasks [0_0, 0_1] (org.apache.kafka.streams.processor.internals.StreamThread:862)
[2022-03-17 18:44:41,236] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-17 18:44:41,237] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-17 18:44:41,237] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Requesting the log end offset for input-topic-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-17 18:44:41,238] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767] Requesting the log end offset for input-topic-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-17 18:44:41,281] INFO handled query: org.apache.kafka.streams.query.KeyQuery@2d5f7182 : SucceededQueryResult{result=null, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 401498ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,282] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 1551726ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-17 18:44:41,282] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 94916ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-17 18:44:41,283] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 8651ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,283] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 5428ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,283] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 200152ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-17 18:44:41,283] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 172908ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-17 18:44:41,284] INFO handled query: org.apache.kafka.streams.query.KeyQuery@1be59f28 : SucceededQueryResult{result=null, executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,285] INFO handled query: org.apache.kafka.streams.query.RangeQuery@2bc9a775 : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 0, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 18910ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,285] INFO handled query: org.apache.kafka.streams.query.RangeQuery@27b000f7 : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 1, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 11974ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,376] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-0 to 0 since the associated topicId changed from null to 7dPI54TvQKuXNT9_gH9PDA (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:41,376] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-f81ddd7f-d3c0-4935-bef7-a7c04679fba8-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1671851767-kv-store-changelog-1 to 0 since the associated topicId changed from null to 7dPI54TvQKuXNT9_gH9PDA (org.apache.kafka.clients.Metadata:402)
[2022-03-17 18:44:41,388] INFO handled query: org.apache.kafka.streams.query.RangeQuery@29c2c826 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@253b380a, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 2335793ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,389] INFO handled query: org.apache.kafka.streams.query.RangeQuery@6818d900 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@3350ebdd, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 112183ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-17 18:44:41,390] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 3974765ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-17 18:44:41,391] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 849835ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-17 18:44:41,397] ERROR Failed assertion (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest:811)
java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@1f193686, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 2335793ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 3045186ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 3068465ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 3974765ns], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@31e72cbc, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 112183ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 775416ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 796244ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 849835ns], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 2]>{code};;;","20/Mar/22 06:09;vvcephei;Another:
{code:java}
verifyStore[cache=true, log=true, supplier=TIME_ROCKS_KV, kind=PAPI]

java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@72e789cb, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 87900ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 366097ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 373038ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@627d8516 in 400408ns], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@7c1812b3, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 27551ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 406916ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 413227ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@5c10285a in 427044ns], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 3]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1140)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:818)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:782)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) {code}
{code:java}
[2022-03-19 23:50:05,329] INFO Generating test cases according to random seed: 5012730987561423151 (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest:361)
[2022-03-19 23:50:06,811] INFO [Producer clientId=producer-1] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-19 23:50:06,834] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:06,834] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:06,836] INFO [Producer clientId=producer-1] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:06,844] INFO [Producer clientId=producer-1] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-19 23:50:06,895] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1207)
[2022-03-19 23:50:06,936] WARN Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-1417619087209305644] (org.apache.kafka.streams.processor.internals.StateDirectory:138)
[2022-03-19 23:50:06,937] INFO No process id found on disk, got fresh process id 9bee9275-4579-415c-adac-6aa6bbb0e5b1 (org.apache.kafka.streams.processor.internals.StateDirectory:213)
[2022-03-19 23:50:06,952] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] Kafka Streams version: test-version (org.apache.kafka.streams.KafkaStreams:912)
[2022-03-19 23:50:06,953] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] Kafka Streams commit ID: test-commit-ID (org.apache.kafka.streams.KafkaStreams:913)
[2022-03-19 23:50:06,960] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:346)
[2022-03-19 23:50:06,982] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread:105)
[2022-03-19 23:50:06,984] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-19 23:50:06,989] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-producer] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:06,990] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-producer] ProducerId set to 1 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-19 23:50:06,990] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:397)
[2022-03-19 23:50:06,999] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration:126)
[2022-03-19 23:50:07,016] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-1417619087209305644/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-19 23:50:07,017] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,017] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams:1316)
[2022-03-19 23:50:07,017] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:539)
[2022-03-19 23:50:07,018] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,018] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Subscribed to topic(s): input-topic (org.apache.kafka.clients.consumer.KafkaConsumer:968)
[2022-03-19 23:50:07,024] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:07,024] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:07,025] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:07,122] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread:838)
[2022-03-19 23:50:07,127] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Discovered group coordinator localhost:46203 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:879)
[2022-03-19 23:50:07,128] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-19 23:50:07,151] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Request joining group due to: need to re-join with the given member-id: app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1032)
[2022-03-19 23:50:07,152] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-19 23:50:07,160] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Successfully joined group with generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:609)
[2022-03-19 23:50:07,164] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics:75)
[2022-03-19 23:50:07,167] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] All members participating in this rebalance: 
9bee9275-4579-415c-adac-6aa6bbb0e5b1: [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:620)
[2022-03-19 23:50:07,172] INFO Decided on assignment: {9bee9275-4579-415c-adac-6aa6bbb0e5b1=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([]) capacity: 1 assigned: 2]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor:96)
[2022-03-19 23:50:07,172] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] Assigned tasks [0_1, 0_0] including stateful [] to clients as: 
9bee9275-4579-415c-adac-6aa6bbb0e5b1=[activeTasks: ([0_0, 0_1]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:638)
[2022-03-19 23:50:07,176] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] Client 9bee9275-4579-415c-adac-6aa6bbb0e5b1 per-consumer assignment:
	prev owned active {}
	prev owned standby {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166=[]}
	assigned active {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166=[0_1, 0_0]}
	revoking active {}
	assigned standby {}
 (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:847)
[2022-03-19 23:50:07,176] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:866)
[2022-03-19 23:50:07,177] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Finished assignment for group at generation 1: {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166=Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:691)
[2022-03-19 23:50:07,190] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Successfully synced group in generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer-bd3b269b-273d-4b10-94cd-03a661115166', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:784)
[2022-03-19 23:50:07,191] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Updating assignment with
	Assigned partitions:                       [input-topic-0, input-topic-1]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-0, input-topic-1]
	Revoked partitions (owned - assigned):     []
 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:422)
[2022-03-19 23:50:07,191] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:294)
[2022-03-19 23:50:07,191] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1345)
[2022-03-19 23:50:07,192] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Handle new assignment with:
	New active tasks: [0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:273)
[2022-03-19 23:50:07,205] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Adding newly assigned partitions: input-topic-0, input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:306)
[2022-03-19 23:50:07,205] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,213] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,213] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,223] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Resetting offset for partition input-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:07,223] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Resetting offset for partition input-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:07,232] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] State store kv-store is not logged and hence would not be restored (org.apache.kafka.streams.processor.internals.ProcessorStateManager:244)
[2022-03-19 23:50:07,232] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-19 23:50:07,234] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] State store kv-store is not logged and hence would not be restored (org.apache.kafka.streams.processor.internals.ProcessorStateManager:244)
[2022-03-19 23:50:07,234] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-19 23:50:07,236] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,237] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-19 23:50:07,238] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,239] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-19 23:50:07,239] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Restoration took 34 ms for all tasks [0_0, 0_1] (org.apache.kafka.streams.processor.internals.StreamThread:862)
[2022-03-19 23:50:07,239] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,240] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,240] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Requesting the log end offset for input-topic-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-19 23:50:07,242] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725] Requesting the log end offset for input-topic-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-19 23:50:07,320] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@621f89b8 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBWindowStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@621f89b8). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,320] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@621f89b8 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBWindowStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@621f89b8). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,322] INFO handled query: org.apache.kafka.streams.query.KeyQuery@37c2eacb : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 0, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 63428ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,323] INFO handled query: org.apache.kafka.streams.query.KeyQuery@37c2eacb : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 1, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 11858ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,423] INFO handled query: org.apache.kafka.streams.query.KeyQuery@37c2eacb : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 0, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 25735ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,423] INFO handled query: org.apache.kafka.streams.query.KeyQuery@37c2eacb : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 1, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 11002ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,462] INFO put data \x00\x00\x00\x01\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,462] INFO put position \x00\x00\x00\x01\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,462] INFO put data \x00\x00\x00\x03\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,462] INFO put position \x00\x00\x00\x03\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,465] INFO put data \x00\x00\x00\x00\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,466] INFO put position \x00\x00\x00\x00\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,466] INFO put data \x00\x00\x00\x02\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,466] INFO put position \x00\x00\x00\x02\x00\x00\x01\x7F\xA5\xA9$\xC0\x00\x00\x00\x00 @ null (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,524] INFO handled query: org.apache.kafka.streams.query.KeyQuery@37c2eacb : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBWindowStore) doesn't know how to execute the given query (org.apache.kafka.streams.query.KeyQuery@37c2eacb). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 49274ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,524] INFO handled query: org.apache.kafka.streams.query.KeyQuery@37c2eacb : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBWindowStore) doesn't know how to execute the given query (org.apache.kafka.streams.query.KeyQuery@37c2eacb). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 17003ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,525] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredWindowStore in 361193ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,525] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredWindowStore in 239158ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,526] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7e75bf2d : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBWindowStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7e75bf2d). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 10055ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,526] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7e75bf2d : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBWindowStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7e75bf2d). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBWindowStore in 5301ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,526] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredWindowStore in 200384ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,526] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredWindowStore in 165632ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,536] INFO handled query: WindowKeyQuery{key=\x00\x00\x00\x02, timeFrom=Optional[2022-03-20T04:50:00Z], timeTo=Optional[2022-03-20T04:50:00Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@451f87af, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,538] INFO handled query: WindowKeyQuery{key=\x00\x00\x00\x02, timeFrom=Optional[2022-03-20T04:50:00Z], timeTo=Optional[2022-03-20T04:50:00Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@2e3cdec2, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,541] INFO handled query: WindowKeyQuery{key=\x00\x00\x00\x02, timeFrom=Optional[2022-03-20T04:49:59.999Z], timeTo=Optional[2022-03-20T04:49:59.999Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@377008df, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,541] INFO handled query: WindowKeyQuery{key=\x00\x00\x00\x02, timeFrom=Optional[2022-03-20T04:49:59.999Z], timeTo=Optional[2022-03-20T04:49:59.999Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@540dbda9, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,542] INFO handled query: WindowKeyQuery{key=\x00\x00\x03\xE7, timeFrom=Optional[2022-03-20T04:50:00Z], timeTo=Optional[2022-03-20T04:50:00Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@6d4c273c, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,542] INFO handled query: WindowKeyQuery{key=\x00\x00\x03\xE7, timeFrom=Optional[2022-03-20T04:50:00Z], timeTo=Optional[2022-03-20T04:50:00Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@5a67e962, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,543] INFO handled query: WindowKeyQuery{key=\x00\x00\x03\xE7, timeFrom=Optional[2022-03-20T04:49:59.999Z], timeTo=Optional[2022-03-20T04:49:59.999Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@64bfd6fd, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,543] INFO handled query: WindowKeyQuery{key=\x00\x00\x03\xE7, timeFrom=Optional[2022-03-20T04:49:59.999Z], timeTo=Optional[2022-03-20T04:49:59.999Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedWindowStoreIterator@2ab2710, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,544] INFO handled query: WindowRangeQuery{key=Optional.empty, timeFrom=Optional[2022-03-20T04:50:00Z], timeTo=Optional[2022-03-20T04:50:00Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedKeyValueIterator@149f5761, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,545] INFO handled query: WindowRangeQuery{key=Optional.empty, timeFrom=Optional[2022-03-20T04:50:00Z], timeTo=Optional[2022-03-20T04:50:00Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedKeyValueIterator@1f193686, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,546] INFO handled query: WindowRangeQuery{key=Optional.empty, timeFrom=Optional[2022-03-20T04:49:59.999Z], timeTo=Optional[2022-03-20T04:49:59.999Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedKeyValueIterator@1d25c1c, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,546] INFO handled query: WindowRangeQuery{key=Optional.empty, timeFrom=Optional[2022-03-20T04:49:59.999Z], timeTo=Optional[2022-03-20T04:49:59.999Z]} : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.WindowStoreIteratorWrapper$WrappedKeyValueIterator@de88ac6, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,547] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,548] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1103)
[2022-03-19 23:50:07,548] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,548] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] Shutting down 1 stream threads (org.apache.kafka.streams.KafkaStreams:1363)
[2022-03-19 23:50:07,576] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1117)
[2022-03-19 23:50:07,581] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1229)
[2022-03-19 23:50:07,581] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2022-03-19 23:50:07,583] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-19 23:50:07,588] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2022-03-19 23:50:07,588] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2022-03-19 23:50:07,588] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1229)
[2022-03-19 23:50:07,588] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2022-03-19 23:50:07,589] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-19 23:50:07,592] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2022-03-19 23:50:07,592] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] task [0_1] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2022-03-19 23:50:07,593] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1207)
[2022-03-19 23:50:07,596] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-19 23:50:07,601] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,601] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1-StreamThread-1] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1152)
[2022-03-19 23:50:07,601] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] Shutdown 1 stream threads complete (org.apache.kafka.streams.KafkaStreams:1381)
[2022-03-19 23:50:07,604] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,604] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725-9bee9275-4579-415c-adac-6aa6bbb0e5b1] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:1447)
[2022-03-19 23:50:07,605] INFO stream-thread [Test worker] Deleting task directory 0_0 for 0_0 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:553)
[2022-03-19 23:50:07,605] INFO stream-thread [Test worker] Deleting task directory 0_1 for 0_1 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:553)
[2022-03-19 23:50:07,606] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-1417619087209305644/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-false-ROCKS_WINDOW-PAPI-262000725 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-19 23:50:07,615] WARN Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-1551948438810143662] (org.apache.kafka.streams.processor.internals.StateDirectory:138)
[2022-03-19 23:50:07,616] INFO No process id found on disk, got fresh process id d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4 (org.apache.kafka.streams.processor.internals.StateDirectory:213)
[2022-03-19 23:50:07,620] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] Kafka Streams version: test-version (org.apache.kafka.streams.KafkaStreams:912)
[2022-03-19 23:50:07,620] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] Kafka Streams commit ID: test-commit-ID (org.apache.kafka.streams.KafkaStreams:913)
[2022-03-19 23:50:07,621] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:346)
[2022-03-19 23:50:07,623] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread:105)
[2022-03-19 23:50:07,624] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-19 23:50:07,627] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:397)
[2022-03-19 23:50:07,629] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration:126)
[2022-03-19 23:50:07,629] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-producer] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:07,629] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-producer] ProducerId set to 2 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-19 23:50:07,631] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-1551948438810143662/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-19 23:50:07,631] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,631] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams:1316)
[2022-03-19 23:50:07,631] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:539)
[2022-03-19 23:50:07,631] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,631] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Subscribed to topic(s): input-topic (org.apache.kafka.clients.consumer.KafkaConsumer:968)
[2022-03-19 23:50:07,634] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:07,634] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:07,634] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:07,634] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Discovered group coordinator localhost:46203 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:879)
[2022-03-19 23:50:07,634] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-19 23:50:07,637] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Request joining group due to: need to re-join with the given member-id: app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1032)
[2022-03-19 23:50:07,637] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-19 23:50:07,638] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Successfully joined group with generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:609)
[2022-03-19 23:50:07,638] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics:75)
[2022-03-19 23:50:07,639] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] All members participating in this rebalance: 
d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4: [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:620)
[2022-03-19 23:50:07,639] INFO Decided on assignment: {d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([]) capacity: 1 assigned: 2]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor:96)
[2022-03-19 23:50:07,639] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] Assigned tasks [0_1, 0_0] including stateful [] to clients as: 
d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4=[activeTasks: ([0_0, 0_1]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:638)
[2022-03-19 23:50:07,639] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] Client d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4 per-consumer assignment:
	prev owned active {}
	prev owned standby {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228=[]}
	assigned active {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228=[0_1, 0_0]}
	revoking active {}
	assigned standby {}
 (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:847)
[2022-03-19 23:50:07,640] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:866)
[2022-03-19 23:50:07,640] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Finished assignment for group at generation 1: {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228=Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:691)
[2022-03-19 23:50:07,642] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Successfully synced group in generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer-7cea7a32-99f7-447a-b398-7a4edebb6228', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:784)
[2022-03-19 23:50:07,642] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Updating assignment with
	Assigned partitions:                       [input-topic-0, input-topic-1]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-0, input-topic-1]
	Revoked partitions (owned - assigned):     []
 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:422)
[2022-03-19 23:50:07,642] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:294)
[2022-03-19 23:50:07,642] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1345)
[2022-03-19 23:50:07,642] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Handle new assignment with:
	New active tasks: [0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:273)
[2022-03-19 23:50:07,643] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Adding newly assigned partitions: input-topic-0, input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:306)
[2022-03-19 23:50:07,643] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,644] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,644] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,647] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Resetting offset for partition input-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:07,647] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Resetting offset for partition input-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:07,765] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-19 23:50:07,767] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] State store kv-store is not logged and hence would not be restored (org.apache.kafka.streams.processor.internals.ProcessorStateManager:244)
[2022-03-19 23:50:07,768] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-19 23:50:07,791] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-19 23:50:07,792] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] State store kv-store is not logged and hence would not be restored (org.apache.kafka.streams.processor.internals.ProcessorStateManager:244)
[2022-03-19 23:50:07,792] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-19 23:50:07,794] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,795] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-19 23:50:07,797] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:07,798] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-19 23:50:07,798] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Restoration took 154 ms for all tasks [0_0, 0_1] (org.apache.kafka.streams.processor.internals.StreamThread:862)
[2022-03-19 23:50:07,798] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,798] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,799] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Requesting the log end offset for input-topic-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-19 23:50:07,799] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073] Requesting the log end offset for input-topic-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-19 23:50:07,799] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread:838)
[2022-03-19 23:50:07,808] INFO put data \x00\x00\x00\x00 @ Position{position={}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,808] INFO put position \x00\x00\x00\x00 @ Position{position={input-topic={0=0}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,809] INFO put data \x00\x00\x00\x01 @ Position{position={}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,809] INFO put position \x00\x00\x00\x01 @ Position{position={input-topic={1=0}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,809] INFO put data \x00\x00\x00\x02 @ Position{position={input-topic={0=0}}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,809] INFO put position \x00\x00\x00\x02 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,809] INFO put data \x00\x00\x00\x03 @ Position{position={input-topic={1=0}}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:07,809] INFO put position \x00\x00\x00\x03 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:07,832] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@616b241a : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@616b241a). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,832] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@616b241a : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@616b241a). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,833] INFO handled query: org.apache.kafka.streams.query.KeyQuery@b8e246c : SucceededQueryResult{result=null, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 132181ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,833] INFO handled query: org.apache.kafka.streams.query.KeyQuery@7cb2651f : SucceededQueryResult{result=[B@4441d567, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 28820ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,833] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@1f387978 in 484751ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,834] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@3e1624c7 in 277654ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,834] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@62b969c4 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@62b969c4). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 9983ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,834] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@62b969c4 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@62b969c4). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 7863ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,834] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 188536ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,834] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 185679ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,835] INFO handled query: org.apache.kafka.streams.query.KeyQuery@48535004 : SucceededQueryResult{result=[B@610df783, executionInfo=[], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,835] INFO handled query: org.apache.kafka.streams.query.KeyQuery@f3fcd59 : SucceededQueryResult{result=null, executionInfo=[], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,836] INFO handled query: org.apache.kafka.streams.query.RangeQuery@592238c5 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@1aa99005, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 65016ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,837] INFO handled query: org.apache.kafka.streams.query.RangeQuery@a20b94b : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@6ee8dcd3, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 36725ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,837] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@1f387978 in 680104ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,837] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@3e1624c7 in 253113ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,838] INFO handled query: org.apache.kafka.streams.query.RangeQuery@1cc9cfb2 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@16073fa8, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 34736ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,838] INFO handled query: org.apache.kafka.streams.query.RangeQuery@3bead518 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@cfbc8e8, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 33252ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,838] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@1f387978 in 293289ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,839] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@3e1624c7 in 245750ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,839] INFO handled query: org.apache.kafka.streams.query.RangeQuery@7918c7f8 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@14379273, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 31449ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,839] INFO handled query: org.apache.kafka.streams.query.RangeQuery@1c504e66 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@17740dae, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 25217ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,839] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@1f387978 in 241064ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,840] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@3e1624c7 in 217152ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,840] INFO handled query: org.apache.kafka.streams.query.RangeQuery@2257fadf : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDbIterator@3c6aa04a, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 46914ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,840] INFO handled query: org.apache.kafka.streams.query.RangeQuery@5c82cd4f : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDbIterator@7144655b, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 24911ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:07,841] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@1f387978 in 248926ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,841] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@3e1624c7 in 212110ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:07,841] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,842] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1103)
[2022-03-19 23:50:07,842] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,842] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] Shutting down 1 stream threads (org.apache.kafka.streams.KafkaStreams:1363)
[2022-03-19 23:50:07,910] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1117)
[2022-03-19 23:50:07,922] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1229)
[2022-03-19 23:50:07,922] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2022-03-19 23:50:07,923] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-19 23:50:07,929] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2022-03-19 23:50:07,929] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2022-03-19 23:50:07,930] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1229)
[2022-03-19 23:50:07,930] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2022-03-19 23:50:07,931] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-19 23:50:07,937] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2022-03-19 23:50:07,937] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] task [0_1] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2022-03-19 23:50:07,938] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1207)
[2022-03-19 23:50:07,942] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1077)
[2022-03-19 23:50:07,951] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,951] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4-StreamThread-1] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1152)
[2022-03-19 23:50:07,951] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] Shutdown 1 stream threads complete (org.apache.kafka.streams.KafkaStreams:1381)
[2022-03-19 23:50:07,954] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,955] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073-d89b1ba3-71d0-4a4c-b1c0-9fbaa8cf66a4] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:1447)
[2022-03-19 23:50:07,955] INFO stream-thread [Test worker] Deleting task directory 0_0 for 0_0 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:553)
[2022-03-19 23:50:07,956] INFO stream-thread [Test worker] Deleting task directory 0_1 for 0_1 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:553)
[2022-03-19 23:50:07,958] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-1551948438810143662/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-false-false-TIME_ROCKS_KV-PAPI--1355308073 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-19 23:50:07,964] WARN Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-7263456774646140643] (org.apache.kafka.streams.processor.internals.StateDirectory:138)
[2022-03-19 23:50:07,964] INFO No process id found on disk, got fresh process id 19351a20-e312-4bc9-b72d-719388794adc (org.apache.kafka.streams.processor.internals.StateDirectory:213)
[2022-03-19 23:50:07,968] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc] Kafka Streams version: test-version (org.apache.kafka.streams.KafkaStreams:912)
[2022-03-19 23:50:07,968] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc] Kafka Streams commit ID: test-commit-ID (org.apache.kafka.streams.KafkaStreams:913)
[2022-03-19 23:50:07,969] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:346)
[2022-03-19 23:50:07,972] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread:105)
[2022-03-19 23:50:07,973] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-19 23:50:07,976] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:397)
[2022-03-19 23:50:07,979] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-producer] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:07,979] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-producer] ProducerId set to 3 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-19 23:50:07,979] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration:126)
[2022-03-19 23:50:07,982] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-7263456774646140643/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-19 23:50:07,982] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:07,982] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams:1316)
[2022-03-19 23:50:07,983] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:539)
[2022-03-19 23:50:07,983] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:07,983] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Subscribed to topic(s): input-topic (org.apache.kafka.clients.consumer.KafkaConsumer:968)
[2022-03-19 23:50:07,986] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:07,986] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to ANuKr3aBTJCCZBfX79TG7g (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:07,986] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:07,986] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Discovered group coordinator localhost:46203 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:879)
[2022-03-19 23:50:07,987] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-19 23:50:07,989] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Request joining group due to: need to re-join with the given member-id: app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1032)
[2022-03-19 23:50:07,989] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-19 23:50:07,990] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Successfully joined group with generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:609)
[2022-03-19 23:50:07,991] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics:75)
[2022-03-19 23:50:08,016] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] All members participating in this rebalance: 
19351a20-e312-4bc9-b72d-719388794adc: [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:620)
[2022-03-19 23:50:08,016] WARN Unable to assign 1 of 1 standby tasks for task [0_0]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-19 23:50:08,017] WARN Unable to assign 1 of 1 standby tasks for task [0_1]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-19 23:50:08,017] INFO Decided on assignment: {19351a20-e312-4bc9-b72d-719388794adc=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0, 0_1=0]) capacity: 1 assigned: 2]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor:96)
[2022-03-19 23:50:08,017] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] Assigned tasks [0_1, 0_0] including stateful [0_1, 0_0] to clients as: 
19351a20-e312-4bc9-b72d-719388794adc=[activeTasks: ([0_0, 0_1]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:638)
[2022-03-19 23:50:08,018] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] Client 19351a20-e312-4bc9-b72d-719388794adc per-consumer assignment:
	prev owned active {}
	prev owned standby {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc=[]}
	assigned active {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc=[0_1, 0_0]}
	revoking active {}
	assigned standby {}
 (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:847)
[2022-03-19 23:50:08,018] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:866)
[2022-03-19 23:50:08,018] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Finished assignment for group at generation 1: {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc=Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:691)
[2022-03-19 23:50:08,020] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Successfully synced group in generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer-22292bd9-6e1c-40e1-8839-60958045aebc', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:784)
[2022-03-19 23:50:08,020] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Updating assignment with
	Assigned partitions:                       [input-topic-0, input-topic-1]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-0, input-topic-1]
	Revoked partitions (owned - assigned):     []
 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:422)
[2022-03-19 23:50:08,020] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:294)
[2022-03-19 23:50:08,020] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1345)
[2022-03-19 23:50:08,020] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Handle new assignment with:
	New active tasks: [0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:273)
[2022-03-19 23:50:08,022] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Adding newly assigned partitions: input-topic-0, input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:306)
[2022-03-19 23:50:08,022] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:08,023] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:08,023] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:08,025] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Resetting offset for partition input-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:08,025] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Resetting offset for partition input-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:08,092] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-19 23:50:08,093] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] task [0_0] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-19 23:50:08,093] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-19 23:50:08,101] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-19 23:50:08,102] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] task [0_1] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-19 23:50:08,102] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] task [0_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-19 23:50:08,107] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0, app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 (org.apache.kafka.clients.consumer.KafkaConsumer:1123)
[2022-03-19 23:50:08,107] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-19 23:50:08,107] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-19 23:50:08,110] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0 to 0 since the associated topicId changed from null to 5kaYD7hoSk2iMdnDtONsNw (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:08,110] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 to 0 since the associated topicId changed from null to 5kaYD7hoSk2iMdnDtONsNw (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:08,110] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Cluster ID: qwcb5jSAR96EM7WpRlrwaQ (org.apache.kafka.clients.Metadata:287)
[2022-03-19 23:50:08,112] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:08,112] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46203 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-19 23:50:08,183] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7d0614f : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7d0614f). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,184] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7d0614f : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@7d0614f). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,214] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-19 23:50:08,214] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-19 23:50:08,215] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread:838)
[2022-03-19 23:50:08,219] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:08,220] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-19 23:50:08,222] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-19 23:50:08,223] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] task [0_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-19 23:50:08,224] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] Restoration took 202 ms for all tasks [0_0, 0_1] (org.apache.kafka.streams.processor.internals.StreamThread:862)
[2022-03-19 23:50:08,224] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-19 23:50:08,224] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-19 23:50:08,224] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Requesting the log end offset for input-topic-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-19 23:50:08,225] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935] Requesting the log end offset for input-topic-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-19 23:50:08,286] INFO handled query: org.apache.kafka.streams.query.KeyQuery@6b667cb3 : SucceededQueryResult{result=null, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 96988ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,287] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@627d8516 in 1007193ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:08,288] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@5c10285a in 195818ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:08,288] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6f38a289 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6f38a289). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 24606ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,289] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6f38a289 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6f38a289). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 16029ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,289] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 472241ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:08,289] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 489708ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:08,290] INFO handled query: org.apache.kafka.streams.query.KeyQuery@61e3cf4d : SucceededQueryResult{result=null, executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,291] INFO handled query: org.apache.kafka.streams.query.RangeQuery@3cec79d3 : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 0, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 41763ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,292] INFO handled query: org.apache.kafka.streams.query.RangeQuery@64b70919 : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 1, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 34303ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,338] INFO put data \x00\x00\x00\x01 @ Position{position={}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:08,338] INFO put position \x00\x00\x00\x01 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:08,342] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-0 to 0 since the associated topicId changed from null to 5kaYD7hoSk2iMdnDtONsNw (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:08,342] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-19351a20-e312-4bc9-b72d-719388794adc-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-440178935-kv-store-changelog-1 to 0 since the associated topicId changed from null to 5kaYD7hoSk2iMdnDtONsNw (org.apache.kafka.clients.Metadata:402)
[2022-03-19 23:50:08,344] INFO put data \x00\x00\x00\x03 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:08,344] INFO put position \x00\x00\x00\x03 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:08,347] INFO put data \x00\x00\x00\x00 @ Position{position={}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:08,347] INFO put position \x00\x00\x00\x00 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:08,426] INFO handled query: org.apache.kafka.streams.query.RangeQuery@3157e4c0 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@4e31c3ec, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 87900ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,426] INFO put data \x00\x00\x00\x02 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-19 23:50:08,426] INFO put position \x00\x00\x00\x02 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-19 23:50:08,426] INFO handled query: org.apache.kafka.streams.query.RangeQuery@328902d5 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@6eaa21d8, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 27551ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-19 23:50:08,427] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@627d8516 in 400408ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:08,427] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@5c10285a in 427044ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-19 23:50:08,428] ERROR Failed assertion (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest:811)
java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@72e789cb, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 87900ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 366097ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 373038ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@627d8516 in 400408ns], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@7c1812b3, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 27551ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 406916ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 413227ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@5c10285a in 427044ns], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 3]> {code};;;","21/Mar/22 00:55;vvcephei;Added some more logs, and I think I'm onto something
{code:java}
verifyStore[cache=true, log=true, supplier=TIME_ROCKS_KV, kind=PAPI]

java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@1f193686, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 2302842ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 3051842ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 3074902ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 3956557ns], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@31e72cbc, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 415148ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 1033935ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 1053899ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 1106865ns], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 2]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQuery(IQv2StoreIntegrationTest.java:1140)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.shouldHandleRangeQueries(IQv2StoreIntegrationTest.java:818)
	at org.apache.kafka.streams.integration.IQv2StoreIntegrationTest.verifyStore(IQv2StoreIntegrationTest.java:782)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
 {code}
{code:java}
[2022-03-20 01:37:53,868] INFO Generating test cases according to random seed: 3481818707506022019 (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest:361)
[2022-03-20 01:37:55,517] INFO [Producer clientId=producer-1] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-20 01:37:55,539] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to GwzFnDucR7GoOE7-JcpPig (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:55,540] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to GwzFnDucR7GoOE7-JcpPig (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:55,542] INFO [Producer clientId=producer-1] Cluster ID: VTm-RGkKTXSNYDrXSmMFDQ (org.apache.kafka.clients.Metadata:287)
[2022-03-20 01:37:55,549] INFO [Producer clientId=producer-1] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-20 01:37:55,597] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1207)
[2022-03-20 01:37:55,653] WARN Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-5497958412092976029] (org.apache.kafka.streams.processor.internals.StateDirectory:138)
[2022-03-20 01:37:55,654] INFO No process id found on disk, got fresh process id bbd60d1e-b399-43f9-a631-2660932ea005 (org.apache.kafka.streams.processor.internals.StateDirectory:213)
[2022-03-20 01:37:55,671] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005] Kafka Streams version: test-version (org.apache.kafka.streams.KafkaStreams:912)
[2022-03-20 01:37:55,671] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005] Kafka Streams commit ID: test-commit-ID (org.apache.kafka.streams.KafkaStreams:913)
[2022-03-20 01:37:55,678] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread:346)
[2022-03-20 01:37:55,698] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread:105)
[2022-03-20 01:37:55,700] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer:532)
[2022-03-20 01:37:55,705] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-producer] Cluster ID: VTm-RGkKTXSNYDrXSmMFDQ (org.apache.kafka.clients.Metadata:287)
[2022-03-20 01:37:55,705] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-producer] ProducerId set to 1 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager:545)
[2022-03-20 01:37:55,708] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread:397)
[2022-03-20 01:37:55,731] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration:126)
[2022-03-20 01:37:55,757] WARN stream-thread [Test worker] Failed to delete state store directory of /tmp/kafka-5497958412092976029/app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877 for it is not empty (org.apache.kafka.streams.processor.internals.StateDirectory:422)
[2022-03-20 01:37:55,757] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-20 01:37:55,758] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams:1316)
[2022-03-20 01:37:55,758] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread:539)
[2022-03-20 01:37:55,758] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-20 01:37:55,759] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Subscribed to topic(s): input-topic (org.apache.kafka.clients.consumer.KafkaConsumer:968)
[2022-03-20 01:37:55,765] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Resetting the last seen epoch of partition input-topic-0 to 0 since the associated topicId changed from null to GwzFnDucR7GoOE7-JcpPig (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:55,765] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Resetting the last seen epoch of partition input-topic-1 to 0 since the associated topicId changed from null to GwzFnDucR7GoOE7-JcpPig (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:55,765] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Cluster ID: VTm-RGkKTXSNYDrXSmMFDQ (org.apache.kafka.clients.Metadata:287)
[2022-03-20 01:37:55,863] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread:838)
[2022-03-20 01:37:55,868] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Discovered group coordinator localhost:41335 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:879)
[2022-03-20 01:37:55,869] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-20 01:37:55,887] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Request joining group due to: need to re-join with the given member-id: app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1032)
[2022-03-20 01:37:55,887] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:548)
[2022-03-20 01:37:55,896] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Successfully joined group with generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:609)
[2022-03-20 01:37:55,899] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics:75)
[2022-03-20 01:37:55,934] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] All members participating in this rebalance: 
bbd60d1e-b399-43f9-a631-2660932ea005: [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:620)
[2022-03-20 01:37:55,937] WARN Unable to assign 1 of 1 standby tasks for task [0_0]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-20 01:37:55,937] WARN Unable to assign 1 of 1 standby tasks for task [0_1]. There is not enough available capacity. You should increase the number of application instances to maintain the requested number of standby replicas. (org.apache.kafka.streams.processor.internals.assignment.DefaultStandbyTaskAssignor:59)
[2022-03-20 01:37:55,939] INFO Decided on assignment: {bbd60d1e-b399-43f9-a631-2660932ea005=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0, 0_1=0]) capacity: 1 assigned: 2]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor:96)
[2022-03-20 01:37:55,940] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] Assigned tasks [0_1, 0_0] including stateful [0_1, 0_0] to clients as: 
bbd60d1e-b399-43f9-a631-2660932ea005=[activeTasks: ([0_0, 0_1]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:638)
[2022-03-20 01:37:55,944] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] Client bbd60d1e-b399-43f9-a631-2660932ea005 per-consumer assignment:
	prev owned active {}
	prev owned standby {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc=[]}
	assigned active {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc=[0_1, 0_0]}
	revoking active {}
	assigned standby {}
 (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:847)
[2022-03-20 01:37:55,945] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:866)
[2022-03-20 01:37:55,945] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Finished assignment for group at generation 1: {app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc=Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:691)
[2022-03-20 01:37:55,959] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Successfully synced group in generation Generation{generationId=1, memberId='app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer-f73bfa54-71ef-4dd7-b265-cba95a7008fc', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:784)
[2022-03-20 01:37:55,960] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Updating assignment with
	Assigned partitions:                       [input-topic-0, input-topic-1]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-0, input-topic-1]
	Revoked partitions (owned - assigned):     []
 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:422)
[2022-03-20 01:37:55,960] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1], userDataSize=135) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:294)
[2022-03-20 01:37:55,960] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1345)
[2022-03-20 01:37:55,961] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Handle new assignment with:
	New active tasks: [0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:273)
[2022-03-20 01:37:55,973] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Adding newly assigned partitions: input-topic-0, input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:306)
[2022-03-20 01:37:55,973] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-20 01:37:56,049] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-20 01:37:56,049] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-20 01:37:56,079] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-20 01:37:56,083] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] task [0_0] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-20 01:37:56,083] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-20 01:37:56,092] INFO Opening store kv-store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2022-03-20 01:37:56,093] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] task [0_1] State store kv-store did not find checkpoint offset, hence would default to the starting offset at changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:267)
[2022-03-20 01:37:56,093] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] task [0_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask:240)
[2022-03-20 01:37:56,104] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1, app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1123)
[2022-03-20 01:37:56,105] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-20 01:37:56,105] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:642)
[2022-03-20 01:37:56,108] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 to 0 since the associated topicId changed from null to LZC7bCMwRNWdTuGxR7-ELw (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:56,108] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1 to 0 since the associated topicId changed from null to LZC7bCMwRNWdTuGxR7-ELw (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:56,109] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Cluster ID: VTm-RGkKTXSNYDrXSmMFDQ (org.apache.kafka.clients.Metadata:287)
[2022-03-20 01:37:56,114] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41335 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-20 01:37:56,115] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41335 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-20 01:37:56,161] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,161] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@6a84bc3f). Contact the store maintainer if you need support for a new query type.', executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,215] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-20 01:37:56,216] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Finished restoring changelog app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 to store kv-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader:609)
[2022-03-20 01:37:56,217] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Resetting offset for partition input-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41335 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-20 01:37:56,217] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Resetting offset for partition input-topic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41335 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:399)
[2022-03-20 01:37:56,217] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Found no committed offset for partition input-topic-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-20 01:37:56,218] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-20 01:37:56,219] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Found no committed offset for partition input-topic-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1436)
[2022-03-20 01:37:56,220] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] task [0_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask:265)
[2022-03-20 01:37:56,220] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] Restoration took 247 ms for all tasks [0_0, 0_1] (org.apache.kafka.streams.processor.internals.StreamThread:862)
[2022-03-20 01:37:56,220] INFO stream-thread [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:233)
[2022-03-20 01:37:56,221] INFO stream-client [app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:345)
[2022-03-20 01:37:56,221] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Requesting the log end offset for input-topic-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-20 01:37:56,222] INFO [Consumer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-consumer, groupId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877] Requesting the log end offset for input-topic-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer:2265)
[2022-03-20 01:37:56,241] INFO updating position Position{position={}} with meta ProcessorRecordContext{topic='input-topic', partition=0, offset=0, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,241] INFO updating position Position{position={}} with meta ProcessorRecordContext{topic='input-topic', partition=1, offset=0, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,242] INFO updating position Position{position={input-topic={0=0}}} with meta ProcessorRecordContext{topic='input-topic', partition=0, offset=1, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,242] INFO updating position Position{position={input-topic={1=0}}} with meta ProcessorRecordContext{topic='input-topic', partition=1, offset=1, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,265] INFO handled query: org.apache.kafka.streams.query.KeyQuery@2d5f7182 : SucceededQueryResult{result=null, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 296480ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,265] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 1004958ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-20 01:37:56,266] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 65905ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-20 01:37:56,266] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 6242ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,266] INFO handled query: org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203 : FailedQueryResult{failureReason=UNKNOWN_QUERY_TYPE, failure='This store (class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore) doesn't know how to execute the given query (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest$UnknownQuery@f72203). Contact the store maintainer if you need support for a new query type.', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 6291ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,266] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 140590ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-20 01:37:56,266] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore in 127786ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-20 01:37:56,267] INFO handled query: org.apache.kafka.streams.query.KeyQuery@1be59f28 : SucceededQueryResult{result=null, executionInfo=[], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,267] INFO handled query: org.apache.kafka.streams.query.RangeQuery@2bc9a775 : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 0, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 12000ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,267] INFO handled query: org.apache.kafka.streams.query.RangeQuery@27b000f7 : FailedQueryResult{failureReason=NOT_UP_TO_BOUND, failure='For store partition 1, the current position Position{position={}} is not yet up to the bound PositionBound{position=Position{position={input-topic={0=1, 1=1}}}}', executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 8466ns], position=Position{position={}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,345] INFO put data \x00\x00\x00\x00 @ Position{position={}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-20 01:37:56,345] INFO updating position Position{position={}} with meta ProcessorRecordContext{topic='input-topic', partition=0, offset=1, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,346] INFO put position \x00\x00\x00\x00 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-20 01:37:56,356] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-0 to 0 since the associated topicId changed from null to LZC7bCMwRNWdTuGxR7-ELw (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:56,356] INFO [Producer clientId=app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-bbd60d1e-b399-43f9-a631-2660932ea005-StreamThread-1-producer] Resetting the last seen epoch of partition app-org.apache.kafka.streams.integration.IQv2StoreIntegrationTest-true-true-TIME_ROCKS_KV-PAPI-1226661877-kv-store-changelog-1 to 0 since the associated topicId changed from null to LZC7bCMwRNWdTuGxR7-ELw (org.apache.kafka.clients.Metadata:402)
[2022-03-20 01:37:56,361] INFO put data \x00\x00\x00\x02 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-20 01:37:56,361] INFO updating position Position{position={input-topic={0=1}}} with meta ProcessorRecordContext{topic='input-topic', partition=0, offset=1, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,362] INFO put position \x00\x00\x00\x02 @ Position{position={input-topic={0=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-20 01:37:56,370] INFO handled query: org.apache.kafka.streams.query.RangeQuery@29c2c826 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@253b380a, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 2302842ns], position=Position{position={input-topic={0=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,371] INFO put data \x00\x00\x00\x01 @ Position{position={}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-20 01:37:56,371] INFO updating position Position{position={}} with meta ProcessorRecordContext{topic='input-topic', partition=1, offset=1, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,372] INFO put position \x00\x00\x00\x01 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-20 01:37:56,372] INFO handled query: org.apache.kafka.streams.query.RangeQuery@6818d900 : SucceededQueryResult{result=org.apache.kafka.streams.state.internals.RocksDBRangeIterator@3350ebdd, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 415148ns], position=Position{position={input-topic={1=1}}}} (org.apache.kafka.streams.state.internals.StoreQueryUtils:140)
[2022-03-20 01:37:56,372] INFO put data \x00\x00\x00\x03 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:370)
[2022-03-20 01:37:56,373] INFO successful result for partition 0 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 3956557ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-20 01:37:56,374] INFO successful result for partition 1 Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 1106865ns (org.apache.kafka.streams.integration.utils.IntegrationTestUtils:182)
[2022-03-20 01:37:56,374] INFO updating position Position{position={input-topic={1=1}}} with meta ProcessorRecordContext{topic='input-topic', partition=1, offset=1, timestamp=1647758273806, headers=RecordHeaders(headers = [], isReadOnly = false)} (org.apache.kafka.streams.state.internals.StoreQueryUtils:150)
[2022-03-20 01:37:56,375] INFO put position \x00\x00\x00\x03 @ Position{position={input-topic={1=1}}} (org.apache.kafka.streams.state.internals.RocksDBStore:374)
[2022-03-20 01:37:56,380] ERROR Failed assertion (org.apache.kafka.streams.integration.IQv2StoreIntegrationTest:811)
java.lang.AssertionError: Result:StateQueryResult{partitionResults={0=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@1f193686, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 2302842ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 3051842ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 3074902ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@58294867 in 3956557ns], position=Position{position={input-topic={0=1}}}}, 1=SucceededQueryResult{result=org.apache.kafka.streams.state.internals.MeteredKeyValueStore$MeteredKeyValueTimestampedIterator@31e72cbc, executionInfo=[Handled in class org.apache.kafka.streams.state.internals.RocksDBTimestampedStore in 415148ns, Handled in class org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedKeyValueBytesStore via WrappedStateStore in 1033935ns, Handled in class org.apache.kafka.streams.state.internals.CachingKeyValueStore in 1053899ns, Handled in class org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore with serdes org.apache.kafka.streams.state.StateSerdes@67c277a0 in 1106865ns], position=Position{position={input-topic={1=1}}}}}, globalResult=null}
Expected: is <[1, 2, 3]>
     but: was <[1, 2]> {code}
 

Notice how the context's offset is wrong when we write 0 and 1 (offset is 1, but should be 0 for both of them).;;;","21/Mar/22 00:57;vvcephei;Oh, before I forget, here's how I'm getting repros:
{code:java}
I=0; while ./gradlew :streams:test -Prerun-tests --tests org.apache.kafka.streams.integration.IQv2StoreIntegrationTest --fail-fast; do (( I=$I+1 )); echo ""Completed run: $I""; sleep 1; done{code}
It usually takes about 500 iterations before it shows up. This depends on a gradle change that I'm planning to commit to support this use case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.kafka.test.MockSelector doesn't remove closed connections from its 'ready' field,KAFKA-13706,13431789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,vincent81jiang,vincent81jiang,vincent81jiang,03/Mar/22 18:34,04/Mar/22 13:45,13/Jul/23 09:17,04/Mar/22 13:45,,,,,,,,,,,,,,,,3.2.0,,,,,,,unit tests,,,,,,0,,,,,,"MockSelector.close(String id) method doesn't remove closed connection from ""ready"" field.",,vincent81jiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-03-03 18:34:42.0,,,,,,,,,,"0|z10534:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect RestClient overrides response status code on request failure,KAFKA-13702,13431234,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,elkkhan,elkkhan,elkkhan,01/Mar/22 12:31,02/Mar/23 15:14,13/Jul/23 09:17,20/Jul/22 10:08,,,,,,,,,,,,,,,,3.3.3,3.4.0,,,,,,KafkaConnect,,,,,,0,,,,,,"In case the submitted request status is >=400, the connect RestClient [throws|https://github.com/apache/kafka/blob/8047ba3800436d6162d0f8eb707e28857ab9eb68/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L133] a ConnectRestException with the proper response code, but it gets intercepted and [rethrown with 500 status code|https://github.com/apache/kafka/blob/8047ba3800436d6162d0f8eb707e28857ab9eb68/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L147], effectively overriding the actual failure status. ",,corlobin,elkkhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 11:10:54 UTC 2022,,,,,,,,,,"0|z101og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/22 21:52;corlobin;I've made the PR: https://github.com/apache/kafka/pull/11844;;;","21/Jun/22 11:10;elkkhan;[~corlobin] since your PR went stale, I've spun out [https://github.com/apache/kafka/pull/12320] with the suggested changes, I hope you don't mind;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessorContext does not expose Stream Time,KAFKA-13699,13431107,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,lqxshay,lqxshay,28/Feb/22 23:24,15/Mar/22 23:07,13/Jul/23 09:17,14/Mar/22 16:22,3.0.0,,,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,newbie,,,,,"As a KS developer, I would like to leverage [KIP-622|https://cwiki.apache.org/confluence/display/KAFKA/KIP-622%3A+Add+currentSystemTimeMs+and+currentStreamTimeMs+to+ProcessorContext] and access stream time in Processor Context.

_(Updated)_

However, the methods currentStreamTimeMs or currentSystemTimeMs is missing from for KStreams 3.0+.

Checked with [~mjsax] , the methods are absent from the Processor API , i.e.
 * org.apache.kafka.streams.processor.api.ProcessorContext",,guozhang,lqxshay,mimaison,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Sun Mar 13 04:09:17 UTC 2022,,,,,,,,,,"0|z100wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/22 18:15;guozhang;Hi [~lqxshay] thanks for filing the ticket. Just to clarify I think the function did exist in the deprecated old API:

```
org.apache.kafka.streams.processor.ProcessorContext
```

They are not in the new API:

```
org.apache.kafka.streams.processor.api.ProcessorContext
```

Is that right?;;;","01/Mar/22 18:52;mjsax;That's my understanding, too. Seems we missed to add both to the new `.api.ProcessorContext` API, as both KIPs kinda overlapped.

Not sure if we could fix this in 3.0 and 3.1 as KIP-622 originally landed in 3.0, or if we can only fix forward in 3.2?;;;","04/Mar/22 22:30;lqxshay;Hi team [~guozhang] [~mjsax] yes, as Matthias mentioned, the methods are missing in `.api.ProcessorContext` API.

(Matthias and I connected offline and discussed it wouldn't be possible to patch backwards in 2.7.0. );;;","08/Mar/22 22:53;mjsax;Seems the only question is, if we need to update the KIP? And if we can only fix forward in 3.2.0 (or if we can back-port to `3.0.1` and `3.1.1` ?

\cc [~mimaison] [~guozhang] WDYT? `3.0.1` is already on it's way...;;;","09/Mar/22 10:59;mimaison;Even if it was voted and partly landed in 3.0.0, I think we should only the missing APIs in 3.2.0.  ;;;","13/Mar/22 04:09;guozhang;I agree with [~mimaison], that we would only be able to fix forward in 3.2.0+.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft authorizer should check host address instead of name,KAFKA-13698,13430760,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,25/Feb/22 20:00,26/Feb/22 18:53,13/Jul/23 09:17,26/Feb/22 18:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,StandardAuthorizer currently uses `InetAddress.getHostName` to validate hosts specified by ACLs. It should use `InetAddress.getHostAddress` instead as we do in `AclAuthorizer`.,,hachikuji,sajid626,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-02-25 20:00:35.0,,,,,,,,,,"0|z0zyrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft authorizer should handle AclOperation.ALL,KAFKA-13697,13430744,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,25/Feb/22 18:06,26/Feb/22 18:53,13/Jul/23 09:17,26/Feb/22 18:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"AclOperation.ALL implies other permissions, but we are not currently checking for it in StandardAuthorizer.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-02-25 18:06:09.0,,,,,,,,,,"0|z0zyns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stream thread blocked-time-ns-total metric does not include producer metadata wait time,KAFKA-13692,13430566,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rohanpd,desai.p.rohan,desai.p.rohan,25/Feb/22 02:56,24/Mar/22 16:56,13/Jul/23 09:17,24/Mar/22 16:56,3.1.0,,,,,,,,,,,,,,,3.3.0,,,,,,,streams,,,,,,0,,,,,,"The stream thread blocked-time-ns-total metric does not include producer metadata wait time (time spent in `KafkaProducer.waitOnMetadata`). This can contribute significantly to actual total blocked time in some cases. For example, if a user deletes the streams sink topic, producers will wait until the max block timeout. This time does not get included in total blocked time when it should.",,desai.p.rohan,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-02-25 02:56:16.0,,,,,,,,,,"0|z0zxkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test EosIntegrationTest.shouldWriteLatestOffsetsToCheckpointOnShutdown[at_least_once],KAFKA-13690,13430509,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,ableegoldman,ableegoldman,24/Feb/22 20:06,23/Feb/23 13:24,13/Jul/23 09:17,23/Feb/23 13:24,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,,0,,,,,,"The _at_least_once_ version of the ""{*}EosIntegrationTest.shouldWriteLatestOffsetsToCheckpointOnShutdown""{*} test is occasionally failing with
h3. Error Message

java.lang.AssertionError: The committed records do not match what expected Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45)]> but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 10), KeyValue(0, 11), KeyValue(0, 13), KeyValue(0, 16), KeyValue(0, 20), KeyValue(0, 25), KeyValue(0, 31), KeyValue(0, 38)]>

 

Seems we are receiving more than the expected records.

...of course, this is an ALOS flavor of the {*}EOS{*}IntegrationTest, so perhaps we shouldn't be running this variant at all? Not sure if this explains the exact output we receive but it certainly seems suspicious

 

Added at_least_once in [https://github.com/apache/kafka/pull/11283]",,ableegoldman,christo_lolov,guozhang,marcolotz,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 13:24:47 UTC 2023,,,,,,,,,,"0|z0zx7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 20:55;guozhang;I took a quick look at https://github.com/apache/kafka/pull/11283 and I think we overlooked it while adding the test, that for ALOS we should be allowing duplicated records and hence the stores could be double counted.;;;","24/Feb/22 20:55;guozhang;I'll see if I can find a quick fix for this flakiness.;;;","10/Mar/22 01:23;guozhang;May cover `shouldCommitCorrectOffsetIfInputTopicIsTransactional`.;;;","23/Feb/23 13:24;christo_lolov;Since the linked pull request has been merged, I will close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractConfig log print information is incorrect,KAFKA-13689,13430149,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,RivenSun,RivenSun,RivenSun,23/Feb/22 08:23,24/Mar/22 16:32,13/Jul/23 09:17,05/Mar/22 08:08,3.0.0,,,,,,,,,,,,,,,3.3.0,,,,,,,config,,,,,,0,,,,,,"h1. 1.Example

KafkaClient version is 3.1.0, KafkaProducer init properties:

 
{code:java}
Properties props = new Properties();
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, false);
props.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 60003);{code}
 

 

Partial log of KafkaProducer initialization:
{code:java}
    ssl.truststore.location = C:\Personal File\documents\KafkaSSL\client.truststore.jks
    ssl.truststore.password = [hidden]
    ssl.truststore.type = JKS
    transaction.timeout.ms = 60003
    transactional.id = null
    value.serializer = class org.apache.kafka.common.serialization.StringSerializer[main] INFO org.apache.kafka.common.security.authenticator.AbstractLogin - Successfully logged in.
[main] WARN org.apache.kafka.clients.producer.ProducerConfig - The configuration 'transaction.timeout.ms' was supplied but isn't a known config.
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.1.0
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 37edeed0777bacb3
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1645602332999 {code}
From the above log, you can see that KafkaProducer has applied the user's configuration, {*}transaction.timeout.ms=60003{*}, the default value of this configuration is 60000.

But we can see another line of log:

[main] WARN org.apache.kafka.clients.producer.ProducerConfig - The configuration *'transaction.timeout.ms'* was supplied but isn't a *{color:#ff0000}known{color}* config.

 
h1. 2.RootCause：

1) ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG is set to {*}false{*}.

So the configurations related to the KafkaProducer transaction will not be requested.
See the source code: KafkaProducer#configureTransactionState(...) .

2) AbstractConfig#logUnused() -> AbstractConfig#unused()
{code:java}
public Set<String> unused() {
    Set<String> keys = new HashSet<>(originals.keySet());
    keys.removeAll(used);
    return keys;
} {code}
If a configuration has not been requested, the configuration will not be put into the used variable. SourceCode see as below:
AbstractConfig#get(String key)

 
{code:java}
protected Object get(String key) {
    if (!values.containsKey(key))
        throw new ConfigException(String.format(""Unknown configuration '%s'"", key));
    used.add(key);
    return values.get(key);
} {code}
h1.  
h1. Solution：

1. AbstractConfig#logUnused() method
Modify the log printing information of this method，and the unused configuration log print level can be changed to {*}INFO{*}, what do you think?
{code:java}
/**
 * Log infos for any unused configurations
 */
public void logUnused() {     for (String key : unused())
        log.info(""The configuration '{}' was supplied but isn't a used config."", key);
}{code}
 

 

2. AbstractConfig provides two new methods: logUnknown() and unknown()
{code:java}
/**
 * Log warnings for any unknown configurations
 */
public void logUnknown() {
    for (String key : unknown())
        log.warn(""The configuration '{}' was supplied but isn't a known config."", key);
} {code}
 
{code:java}
public Set<String> unknown() {
    Set<String> keys = new HashSet<>(originals.keySet());
    keys.removeAll(values.keySet());
    return keys;
} {code}
 

 

 ",,guozhang,RivenSun,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 24 00:58:41 UTC 2022,,,,,,,,,,"0|z0zv00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/22 08:26;RivenSun;Hi [~showuon]  , [~guozhang] 
Could you give any suggestions?
Thanks.;;;","23/Feb/22 08:56;showuon;[~RivenSun] , thanks for the report. Maybe we just modify the log message to something like this:

`log.warn(""The configuration '{}' was supplied but is not used. It could because it is not a known config, or some components is not enabled."", key);`

WDYT?;;;","23/Feb/22 09:07;RivenSun;Hi [~showuon] Thanks for your reply.

I recommend printing the logs separately, because `unused` and `unknown` are two concepts, and their severity is different. Printing them separately can make the information provided by the log clearer and more specific.

Put them together and print, users sometimes need to go to the official documentation to confirm whether this configuration is really an unknown or deprecated configuration.

What do you think?
Thanks.;;;","23/Feb/22 12:52;showuon;OK, sounds good to me. See if there are other opinions. Thanks.;;;","23/Feb/22 18:33;guozhang;Thanks [~RivenSun] for reporting this. I think logging separately for unused (since the corresponding feature is not enabled) and unknown with the former only in INFO is fine.;;;","24/Feb/22 00:41;RivenSun;[~showuon]  [~guozhang] 
Thanks for your advices.
I will create a PR to fix this issue.
Thanks.;;;","17/Mar/22 09:31;RivenSun;Hi [~showuon] [~guozhang] 
After rethinking the discussion in PR-11800, luke, I think it might be simpler as you said at the beginning to make the log output a bit more informative, and we don't need a KIP for that.

AbstractConfig#logUnused()
{code:java}
public void logUnused() {
    for (String key : unused())
        log.warn(""The configuration '{}' was supplied but is not used. It could because it is not a known config,"" +
                "" or some components are not enabled."", key);
} {code}
WDYT?
Thanks.;;;","20/Mar/22 00:16;guozhang;Hi, I think we can first revisit the meaningfulness of the ""logUnused"" function itself: in ""AbstractConfig"" we only record ""used"" configs from those parsed definitions, and the `unused` set is the diff of the ""originals"" (note, not the ""values"") and the ""used"". This means:

1) Any pluggable component's configs, since they are not defined in the corresponding Producer/Consumer/etcConfig, they would always be in the ""unused"" set. The pluggable components thsmeslves would not try to use the ""get"" call to retrieve them anyways, but would usually us `originals().get()`. I.e. they are really just ""unknown"".
2) Any configs that are not called via ""get"" or ""ignore"" would be in this ""unused"" set. I.e. they are ""unused"".

The intention of ""logUnused"" function itself, is to warn people just about the second case (as this JIRA is trying to resolve), since assuming that the code is bug-free, this should NOT happen --- thinking about this, when certain component is not enabled, we should call `config.ignore(...)` on the corresponding configs, like `TRANSACTION_TIMEOUT_CONFIG` in this case. With that, we should not have ""unused"" configs at all. On the other hand, have a log line for each ""unknown"" config may be a bit too verbose, e.g. in Streams we have a bunch of pluggable component resulting in quite a lot of log lines for each of their custom config. I think this is not the original intention of the ""logUnused"" function to actually warn about the ""unknown"" configs at all, and it's less valuable to do so as well.

There's an additional scenario though, for deprecated configs: when we deprecate a config, the source code may not request them anymore but if the user does not change their code, they may still set values for those deprecated configs which would then end up in the `unused` set. So we should also call `config.ignore()` on those deprecated configs if they are no longer used inside the source code (note that to achieve backward compatibility, some deprecated configs may still be retrieved inside the code, in that case we would not need to call `ignore()`).

So if you agree with me that ""logUnused"" should really be a WARN for the second case above since this should never happen, I'd suggest we:

1) use ""config.ignore"" in cases when certain component are disabled and hence their corresponding configs would not be requested, or when the config is deprecated and no longer requested.
2) change the log line to ""was supplied but isn't used"" (don't we feel it's weird to log ""it's not known"" in a function called ""logUnused""? :P).;;;","20/Mar/22 03:39;RivenSun;Hi [~guozhang] 
Thank you very much for your reply, that's great, I totally agree with you, `logUnused` should really do what you said.


For your case 1) : Any pluggable component's configs, since they are not defined in the corresponding Producer/Consumer/etcConfig, they would always be in the ""unused"" set.

I think Kafka generally uses the AbstractConfig.values() or AbstractConfig.valuesXXX() methods to return a RecordingMap object to the user-defined plugin module. Users can use RecordingMap.get() to retrieve these unKnown configs one by one.
So for these unKnown configs, they will never appear in the logUnused() output because they are not in the `originals` set. But unKnown configs may be stored in `used` set, right?;;;","20/Mar/22 04:17;guozhang;> So for these unKnown configs, they will never appear in the logUnused() output because they are not in the `originals` set. But unKnown configs may be stored in `used` set, right?

The `values` map is constructed as `this.values = definition.parse(this.originals);` so it should just be a subset of `originals` that are defined by the config itself. Those custom, hence unknown configs would not be in the `values` but would be in `originals`, and pluggable components would use `originals().get()` to retrieve those custom config values.

Since `unused` is calculated as the diff between `originals` and `used` (and `used` would only be a subset of `values` if we exclude those ignored ones), so they could appear in the logUnused.;;;","20/Mar/22 06:07;RivenSun;Hi [~guozhang] 
1. You are correct, `originals` is the set of configKeys passed in by the user, and will not contain configKeys other than those passed in by the user. As you said, those custom unknown configurations will all be in `originals`.
2. `this.values = definition.parse(this.originals);`
`values` is not a subset of `originals`, `values` will not contain any unknownConfigKeys, it is a collection of all `knownConfig` configurations.
This line of code is just to override the default value of these configuration items with the value of knownConfig passed in by the user.
3. How user-defined plug-in modules obtain their own custom configurations, refer to the source code `ChannelBuilders#channelBuilderConfigs`
This does indeed return the union of both sets config.values() and config.originals() to the user, who can call RecordingMap.get() to retrieve their own custom configuration.
4. `used` will also contain unknownConfigKeys after users retrieve their own custom  configuration, so `used` will not necessarily be a subset of `values`.

But as you said, since `unused` is calculated as the diff between `originals` and `used`, `logUnused` will still possibly output unknownConfigKeys.;;;","20/Mar/22 06:26;RivenSun;So after we ignore these configurations: knownConfig but the plugin is not enabled and deprecatedConfig, logUnused may still output all unknownConfig passed in by users but not retrieved by themselves.

So what we need to improve at the moment are:
1. Find all knownConfig but plugins not enabled and deprecatedConfig, `config.ignore` them.
2. Modify the logUnused method to the following form
{code:java}
/**
 * Log warnings for any unused configurations
 */
public void logUnused() {
    Set<String> unusedkeys = unused();
    if (!unusedkeys.isEmpty()) {
        log.warn(""These configurations '{}' were supplied but are not used."", unusedkeys);
    }
} {code}
[~guozhang] WDYT?
Thanks.;;;","22/Mar/22 04:10;guozhang;Hello [~RivenSun] I agree with you except some pondering about this statement:

4. `used` will also contain unknownConfigKeys after users retrieve their own custom  configuration, so `used` will not necessarily be a subset of `values`.

Since `used` will only be updated with either `get` or `ignore`, so assume we do not call `ignore` at the moment, `get` calls could only be used to retrieve those known configs, and unknown configs can only be retrieved from the original map itself. So `used` should not contain any unknown configs at all.

With that said, I think we are still in agreement on the two improvement points you mentioned in the last message since the `unused()` returned should still just have those not-used configs, just that I think those unknown ones would not be included.;;;","22/Mar/22 06:11;RivenSun;Hi [~guozhang] 
1. We all agree that users can get all their custom configurations in their own plugin modules by passing the `configs` variable through the `ChannelBuilders#channelBuilderConfigs` method.
In fact, the initialization methods of all KafKaClient (such as KafkaConsumer) that contain AbstractConfig parameters {*}are not public{*}, so users cannot directly access the corresponding type of AbstractConfig, and thus will not directly call the methods in AbstractConfig.
2. The `configs` variable mentioned above is actually of type AbstractConfig.RecordingMap. If users call configs.get(key) to retrieve their own configuration, the RecordingMap.get() method will eventually call the *AbstractConfig.ignore(key)* method, so `used` will contain user-defined unknown configuration.;;;","22/Mar/22 17:13;guozhang;Okay I think we are really on the same page here, just are using different terminologies :) Where I'm from is the Streams usage, e.g. let's say we create a StreamsConfig object from:

{code}
StreamsConfig config = new StreamsConfig(map);
{code}

If we call

{code}
config.get(PREDEFINED_CONFIG_NAME)
{code}

then the param must be a defined config name, otherwise it will throw;

If we call

{code}
config.originals().get(ANY_CONFIG_NAME)
{code}

then it tries to get from the underlying maps directly, and any config names including the custom unknown configs can be retrieved. So that's how custom configs are retrieved (like you said, some modules use `(config.originals() UNION config.values()) .get(ANY_CONFIG_NAME).` as well).

Now, I realize since `logUnused` is triggered at the construction of the consumer/producer clients, whereas those embedded module's custom configs are possibly not yet retrieved as they will only be constructed and initialized at a later time, in which case `used` set would not yet contain any of those unknown configs yet. As a result:

1) all defined, used configs should be included in `used` since they are retrieved via the first call above. This is the case we want to guarantee and WARN if not since it may indicates a bug.
2) all defined, but disabled configs are included in `used` since they are called via `config.ignore()`. This is what we want to fix in this JIRA.
3) all unknown configs may or may not be included in `used` and that's out of the AbstractConfig object's control.

So by doing the above two, we can fix this JIRA ticket which is case 2), but the `logUnused` could still contain case 1) or case 3) whereas we really only want to WARN on case 1) above. The latter issue is a bit out of the scope of this JIRA, hence I said just doing the above two is sufficient for this issue.

----------------------------

If we want to resolve the second as well in this JIRA, I'm thinking we can do sth. aligned with your proposal in the description as ""AbstractConfig provides two new methods: logUnknown() and unknown()"" but to avoid creating a KIP for adding public APIs, we can just do them inside the existing `logUnused`, as something like:

{code}
    public Set<String> unused() {
        Set<String> keys = new HashSet<>(values.keySet());   // here we take the diff between values and used.
        keys.removeAll(used);
        return keys;
    }

    // keep it private
    private Set<String> unknown() {
        Set<String> keys = new HashSet<>(originals.keySet());   // here we take the diff between originals and values.
        keys.removeAll(values);
        return keys;
    }

public void logUnused() {
    Set<String> unusedkeys = unused(); 
    for (String key : unused())
        log.warn(""The configuration '{}' was supplied but isn't a used."", key); // here we still log one line per config as a WARN

    Set<String> unusedkeys = unknown(); 
    if (!unknown.isEmpty()) {
        log.info(""These configurations '{}' were not known."", unusedkeys);  // here we log one line for all configs as INFO.
    }
} 
{code}
;;;","23/Mar/22 09:04;RivenSun;Hi [~guozhang]  I thought about it carefully, maybe we are thinking too complicated, or we have been looking at the logUnused() method from the perspective of Kafka.

{*}The logUnused() method is user-oriented{*}, in order to tell the user all the configurations he passed in, {*}whether these configurations are known or unknown{*}, which configurations are used and which are unused.
In short, the logUnused method should compare the difference between `originals` and `used`.

So my few points are:

1. Keep the existing logic of the unused() method, just modify the log output information of logUnused(). As you mentioned, printing ""isn't a known config"" in the logUnused() method is both incorrect and weird.
{code:java}
/**
 * Log warnings for any unused configurations
 */
public void logUnused() {
    Set<String> unusedkeys = unused();
    if (!unusedkeys.isEmpty()) {
        log.warn(""These configurations '{}' were supplied but are not used."", unusedkeys);
    }
} {code}
2. For unenable knownConfig, Kafka do not need to actively `ignore` it. 
As in the example in this JIRA, in the logUnused() method, the `enable.idempotence` configuration is not printed, because it really has been retrieved by Kafka.
But the `transaction.timeout.ms` configuration is printed. Because the user has passed in this configuration, but Kafka has not retrieved this configuration when calling the logUnused() method, it should be printed out.

3. For unKnownConfig , if the user has not retrieved these configurations in their own custom plugin module by the time the logUnused() method is called, then logUnused() will print these unretrieved custom configurations.
By the way, configurations that kafka has removed and deprecated configurations (with recommended configurations configured at the same time) can be viewed similar to unknownConfig. Maybe users will use these two types of configuration in their custom plugins.

WDYT？
Thanks.

 ;;;","23/Mar/22 23:20;guozhang;Hi [~RivenSun] I think I agree with you on the general note, my concern is that since we do not control when `logUnused` is called, at the time when not all provided values are retrieved, then we would log `config ... is not used.` which would then be a bit misleading since they are likely going to be used later indeed. But after a second thought, the semantics is fine we we just say `config  ... is not yet used.` at the time when `logUnused` is called, so it's really the responsibility of the caller regarding when they want to call this function to check which configs are not yet used.

So I think we can just like you said log it as (just adding `yet` at the end of the sentence).

{code}
public void logUnused() {
    Set<String> unusedkeys = unused();
    if (!unusedkeys.isEmpty()) {
        log.warn(""These configurations '{}' were supplied but are not used yet."", unusedkeys);
    }
} 
{code}

Since we always call `logUnused` at the end of the constructor of producer/consumer/admin, then it's very likely that those unknown configs are not retrieved yet and hence would be logged. For that, I'd say we update our web docs indicating this effect exactly to clear any possible confusions.;;;","24/Mar/22 00:58;RivenSun;[~guozhang] Thank you for your reply.
I will create a PR(11940) to optimize the log output of logUnused().
Please help to review PR when available.
Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure on ZOS due to IOException when attempting to fsync the parent directory,KAFKA-13674,13429055,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,hongyiz@cn.ibm.com,hongyiz@cn.ibm.com,17/Feb/22 10:15,28/Feb/22 04:44,13/Jul/23 09:17,28/Feb/22 04:44,3.0.0,,,,,,,,,,,,,,,3.2.0,,,,,,,clients,,,,,,0,pull-request-available,,,,,"It appears to be the similar issue with  [KAFKA-13391|https://issues.apache.org/jira/browse/KAFKA-13391] on ZOS due to fsync the parent directory. Kafka 3.0.0 failed to start on z/OS against the below error:

 

 
{panel}
ERROR Error while writing to checkpoint file /xxxx/recovery-point-offset-checkpoint (kafka.server.LogDirFailureChannel) java.io.IOException: EDC5121I Invalid argument.

 at sun.nio.ch.FileDispatcherImpl.force0(Native Method)

 at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:110)

 at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:400)

 at org.apache.kafka.common.utils.Utils.flushDir(Utils.java:959)

 at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:944)

 at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:918)

 at org.apache.kafka.server.common.CheckpointFile.write(CheckpointFile.java:98)

 at kafka.server.checkpoints.CheckpointFileWithFailureHandler.write(CheckpointFileWithFailureHandler.scala:37)

 at kafka.server.checkpoints.OffsetCheckpointFile.write(OffsetCheckpointFile.scala:68)

 at kafka.log.LogManager.$anonfun$checkpointRecoveryOffsetsInDir$1(LogManager.scala:679)

 at kafka.log.LogManager.$anonfun$checkpointRecoveryOffsetsInDir$1$adapted(LogManager.scala:675)

 at kafka.log.LogManager$$Lambda$553/0x00000000067b3f30.apply(Unknown Source)

 at scala.Option.foreach(Option.scala:437)

 at kafka.log.LogManager.checkpointRecoveryOffsetsInDir(LogManager.scala:675)

 at kafka.log.LogManager.$anonfun$shutdown$10(LogManager.scala:546)

 at kafka.log.LogManager.$anonfun$shutdown$10$adapted(LogManager.scala:539)

 at kafka.log.LogManager$$Lambda$548/0x00000000067b1230.apply(Unknown Source)

 at kafka.utils.Implicits$MapExtensionMethods$.$anonfun$forKeyValue$1(Implicits.scala:62)

 at kafka.log.LogManager$$Lambda$549/0x00000000067b1b30.apply(Unknown Source)

 at scala.collection.mutable.HashMap$Node.foreachEntry(HashMap.scala:633)

 at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:499)

 at kafka.log.LogManager.shutdown(LogManager.scala:539)

 at kafka.server.KafkaServer.$anonfun$shutdown$19(KafkaServer.scala:765)

 at kafka.server.KafkaServer$$Lambda$528/0x00000000c23c3630.apply$mcV$sp(Unknown Source)

 at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:68)

 at kafka.server.KafkaServer.shutdown(KafkaServer.scala:765)

 at kafka.server.KafkaServer.startup(KafkaServer.scala:466)

 at kafka.Kafka$.main(Kafka.scala:109)

 at kafka.Kafka.main(Kafka.scala)
{panel}
 

This problem is a blocker issue on Kafka 3.0.0 above on z/OS. We suspect that the attempt to fsync the directory should just be skipped on z/OS. We have tried to skip fsync code and then Kafka 3.0.0 can be started successfully. 

 ",,hongyiz@cn.ibm.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 28 04:44:05 UTC 2022,,,,,,,,,,"0|z0zob4:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,"23/Feb/22 06:34;hongyiz@cn.ibm.com;https://github.com/apache/kafka/pull/11793;;;","28/Feb/22 04:44;hongyiz@cn.ibm.com;Code has been merged into the trunk in the below PR:

https://github.com/apache/kafka/pull/11793;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in DynamicBrokerConfig,KAFKA-13672,13428907,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,lclarkenz,cadonna,cadonna,16/Feb/22 19:27,24/Mar/22 06:07,13/Jul/23 09:17,24/Mar/22 04:07,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,,"Stacktrace:

{code:java}
org.opentest4j.AssertionFailedError: expected: <2> but was: <1>
	at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at app//org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)
	at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:182)
	at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:177)
	at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:1141)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$waitForConfigOnServer$1(DynamicBrokerReconfigurationTest.scala:1500)
	at app//kafka.server.DynamicBrokerReconfigurationTest.waitForConfigOnServer(DynamicBrokerReconfigurationTest.scala:1500)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$waitForConfig$1(DynamicBrokerReconfigurationTest.scala:1495)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$waitForConfig$1$adapted(DynamicBrokerReconfigurationTest.scala:1495)
	at app//scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at app//scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at app//scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at app//kafka.server.DynamicBrokerReconfigurationTest.waitForConfig(DynamicBrokerReconfigurationTest.scala:1495)
	at app//kafka.server.DynamicBrokerReconfigurationTest.reconfigureServers(DynamicBrokerReconfigurationTest.scala:1440)
	at app//kafka.server.DynamicBrokerReconfigurationTest.resizeThreadPool$1(DynamicBrokerReconfigurationTest.scala:775)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$testThreadPoolResize$3(DynamicBrokerReconfigurationTest.scala:768)
	at app//scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:190)
	at app//kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize(DynamicBrokerReconfigurationTest.scala:784)
{code}

Job: https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11751/5/testReport/",,cadonna,lclarkenz,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 21 02:44:20 UTC 2022,,,,,,,,,,"0|z0zneo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 13:42;cadonna;https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11752/8/testReport/kafka.server/DynamicBrokerReconfigurationTest/Build___JDK_11_and_Scala_2_13___testThreadPoolResize__/

{code:java}
org.opentest4j.AssertionFailedError: expected: <2> but was: <1>
	at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at app//org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)
	at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:182)
	at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:177)
	at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:1141)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$waitForConfigOnServer$1(DynamicBrokerReconfigurationTest.scala:1500)
	at app//kafka.server.DynamicBrokerReconfigurationTest.waitForConfigOnServer(DynamicBrokerReconfigurationTest.scala:1500)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$waitForConfig$1(DynamicBrokerReconfigurationTest.scala:1495)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$waitForConfig$1$adapted(DynamicBrokerReconfigurationTest.scala:1495)
	at app//scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at app//scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at app//scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at app//kafka.server.DynamicBrokerReconfigurationTest.waitForConfig(DynamicBrokerReconfigurationTest.scala:1495)
	at app//kafka.server.DynamicBrokerReconfigurationTest.reconfigureServers(DynamicBrokerReconfigurationTest.scala:1440)
	at app//kafka.server.DynamicBrokerReconfigurationTest.resizeThreadPool$1(DynamicBrokerReconfigurationTest.scala:775)
	at app//kafka.server.DynamicBrokerReconfigurationTest.$anonfun$testThreadPoolResize$3(DynamicBrokerReconfigurationTest.scala:768)
	at app//scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:190)
	at app//kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize(DynamicBrokerReconfigurationTest.scala:784)
{code};;;","20/Mar/22 04:52;lclarkenz;I've renamed this issue (was {_}Flaky test {{kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize()}}{_}) because the test was actually exposing a race condition in {{DynamicBrokerConfig}} that occurred when the broker under test was running sufficiently slowly. Many thanks are due to [~showuon] for all of his guidance and insight, and for identifying the root cause.

{{DynamicBrokerConfig}} stores a mutable buffer of {{{}Reconfigurable{}}}s which are protected by a read lock and a write lock. However, these locks can't prevent the issue this test was exposing in the CI environment, which is -  if a {{Reconfigurable}} is added by one thread while another thread is updating dynamic configs (which involves iterating over the buffer of {{{}Reconfigurable{}}}s) then this causes a {{ConcurrentModificationException}} for the thread that is iterating, thus preventing some dynamic configs from being updated.

I'll be submitting a PR shortly to guard against this race condition.;;;","21/Mar/22 02:44;showuon;Great to hear that! But I'm wondering why this happened recently only, not before? Did we change anything cause that?
 
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log messages for source tasks with no offsets to commit are noisy and confusing,KAFKA-13669,13428642,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,15/Feb/22 20:43,16/Mar/23 12:38,13/Jul/23 09:17,17/Feb/22 01:51,,,,,,,,,,,,,,,,3.2.0,,,,,,,KafkaConnect,,,,,,1,,,,,,"The [messages|https://github.com/apache/kafka/blob/71cbff62b685ef2b6b3169c2e69693687ddf7b3f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L493-L500] that Connect emits during offset commit for source tasks that haven't accrued any new offsets since their last commit are logged at INFO level, which has confused some users who saw them and believed that something was wrong with their connector.

We introduced these fairly recently as part of the work in [https://github.com/apache/kafka/pull/11323] for KAFKA-12226.

We may want to lower the level of these messages to DEBUG in order to avoid giving users the impression that something's wrong with their connector, especially if there's a source task running that has gotten fully caught-up with the system it's polling and is simply waiting for more data to become available.",,ChrisEgerton,hifly81,jeqo,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14809,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 16 08:06:00 UTC 2022,,,,,,,,,,"0|z0zls8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/22 22:13;jeqo;I've experienced this messages on Kafka Datagen Source Connector using Confluent Platform (7.0.1) / Apache Kafka 3.0.1.

Very confusing, thought there was an issue, but data was still been produced.

 

Lowering the log level to DEBUG make sense to me.;;;","16/Feb/22 08:06;hifly81;Seen this with replicator and my customer thought it was an issue. Lowering the log level to DEBUG make sense to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed cluster authorization should not be fatal for producer,KAFKA-13668,13428630,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,hachikuji,hachikuji,15/Feb/22 20:12,04/May/23 17:00,13/Jul/23 09:17,04/May/23 17:00,,,,,,,,,,,,,,,,3.6.0,,,,,,,,,,,,,0,,,,,,"The idempotent producer fails fatally if the initial `InitProducerId` returns CLUSTER_AUTHORIZATION_FAILED. This makes the producer unusable until a new instance is constructed. For some applications, it is more convenient to keep the producer instance active and let the administrator fix the permission problem instead of going into a crash loop. Additionally, most applications will probably not be smart enough to reconstruct the producer instance, so if the application does not handle the error by failing, users will have to restart the application manually. 

I think it would be better to let the producer retry the `InitProducerId` request as long as the user keeps trying to use the producer. ",,dengziming,hachikuji,ijuma,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 11 23:52:44 UTC 2022,,,,,,,,,,"0|z0zlpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 02:15;showuon;Nice suggestion! We should improve it!;;;","11/Mar/22 23:52;ijuma;cc [~kirktrue];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft uses the wrong permission for adding topic partitions,KAFKA-13661,13427574,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,09/Feb/22 18:13,11/Feb/22 23:28,13/Jul/23 09:17,11/Feb/22 23:28,3.0.0,3.1.0,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,,,,,,0,,,,,,"[~cmccabe] caught this as part of KAFKA-13646. KRaft currently checks CREATE on the topic resource. It should be ALTER. This will be fixed in trunk as part of KAFKA-13646, but it would be good to fix for 3.0 and 3.1 as well.

Note this does not affect zookeeper-based clusters.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 09 22:29:35 UTC 2022,,,,,,,,,,"0|z0zf74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/22 22:29;hachikuji;Another minor difference in behavior is that KRaft allows CREATE on the Cluster resource. This is not allowed in zk clusters and was not documented by KIP-195: https://cwiki.apache.org/confluence/display/KAFKA/KIP-195%3A+AdminClient.createPartitions. Probably not a big deal to allow this, but I'm inclined nevertheless to drop it for consistency.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace log4j with reload4j,KAFKA-13660,13427564,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,FireBurn,FireBurn,FireBurn,09/Feb/22 17:02,13/Jan/23 16:43,13/Jul/23 09:17,30/Mar/22 19:42,2.4.0,3.0.0,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,logging,,,,,,0,,,,,,"Kafka is using a known vulnerable version of log4j, the reload4j project was created by the code's original authors to address those issues. It is designed as a drop in replacement without any api changes

 

https://reload4j.qos.ch/

 

I've raised a merge request, replacing log4j with reload4j, slf4j-log4j12 with slf4j-reload4j and bumping the slf4j version

 

This is my first time contributing to the Kafka project and I'm not too familiar with the process, I'll go back and amend my PR with this issue number",,cadonna,dongjin,FireBurn,fvaleri,showuon,tombentley,vikash08mishra,,,,,,,,,,,,,,,,KAFKA-14137,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/apache/kafka/pull/11743,,,,,,,,,,,9223372036854775807,,,,Wed Mar 30 10:37:05 UTC 2022,,,,,,,,,,"0|z0zf4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/22 01:36;dongjin;Hi [~FireBurn],

Thanks for your interest in this issue. I think reload4j is a promising project but, it seems not proven yet. Also, the log4j issue is already under progress with KAFKA-9366.

Plus, these kinds of issues need a process named Kafka Improvement Proposal. Please have a look at [this page|https://cwiki.apache.org/confluence/display/kafka/kafka+improvement+proposals].;;;","28/Mar/22 07:47;vikash08mishra;Hi [~cadonna] [~tombentley] , do we have any clarity on whether this will be taken up in either of 3.1.1 or 3.2 after log4j2 upgrade has been kicked out of 3.2. 
I see no update confirming the same in the discussion thread here: [https://lists.apache.org/thread/qo1y3249xldt4cpg6r8zkcq5m1q32bf1] 

Community has been waiting for months for log4j2 upgrade for Kafka, especially considering that current log4j version used in Kafka is out of support for long and hence always a threat apart from current CVE issues. So, at least having an alternative in form of reload4j in 3.1.1 and 3.2 would be a relief for all the Kafka users out there.

Thanks;;;","28/Mar/22 08:13;tombentley;[~vikash08mishra] I'm happy to include this in 3.1.1 if we can get the PR merged and [~cadonna] is willing to include it in 3.2 (I think it would be weird to do it in one and not the other). I did a bit of testing on Friday but want to do some more, and [~showuon] raised a question on the PR. ;;;","28/Mar/22 08:45;cadonna;[~vikash08mishra] I agree with [~tombentley] about merging it into 3.2.;;;","30/Mar/22 10:37;vikash08mishra;Thanks [~tombentley] and [~cadonna] for the updates.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade vulnerable dependencies jan 2022,KAFKA-13658,13427345,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,shivakumar,shivakumar,09/Feb/22 03:32,25/Jul/22 06:48,13/Jul/23 09:17,02/Mar/22 09:32,2.8.1,,,,,,,,,,,,,,,3.0.1,3.1.1,3.2.0,,,,,,,,,,,1,secutiry,,,,,"|Packages|Package Version|CVSS|Fix Status|
|com.fasterxml.jackson.core_jackson-databind| 2.10.5.1| 7.5| fixed in 2.14, 2.13.1, 2.12.6|
| | | | |

Our security scan detected the above vulnerabilities

upgrade to correct versions for fixing vulnerabilities",,cadonna,dominique,mimaison,pratimsc,shivakumar,showuon,,,,,,,,,,,,,KAFKA-13805,KAFKA-14100,KAFKA-13579,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 01 17:18:24 UTC 2022,,,,,,,,,,"0|z0zdso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/22 10:20;dominique;Hi,

For information here is the github issue on jackson repo: [https://github.com/FasterXML/jackson-databind/issues/3328.]

Snyk rates the vulnerability as 5.9: [https://security.snyk.io/vuln/SNYK-JAVA-COMFASTERXMLJACKSONCORE-2326698.]

For info, the vulnerability is applicable only when using JDK serialization/deserialization of JsonNode.

as a first step, could you tell us if kafka is actually using this kind of serialization/deserialization ?

Thanks;;;","28/Feb/22 22:11;pratimsc;This also affects version 3.0 and 3.1 kafka. 

Sysdig has reported it under [https://us2.app.sysdig.com/secure/#/scanning/vulnerabilities/VULNDB-275302] for Kafka 3.0 and Kafka 3.1, when scanning `quay.io/strimzi/operator:0.28.0` which pulls in `org.apache.kafka.kafka-clients-3.1.0.jar` and `org.apache.kafka.kafka-streams-3.1.0.jar` as dependencies. 
The kafka libraries themselves pulls in `com.fasterxml.jackson.core:jackson-databind:jar:2.11.3`.

This vulnerability seem to be fixed in version `com.fasterxml.jackson.core:jackson-databind:jar:2.13.1`. 
Ref: [https://github.com/FasterXML/jackson-databind/issues/3328]

 

[~showuon] - Let me know if anyway I can contribute.;;;","01/Mar/22 02:38;showuon;[~pratimsc] , go ahead to submit PR to fix this issue. Thanks.;;;","01/Mar/22 10:26;pratimsc;[~showuon]  - I have never contributed to Kafka source. Could you please point me to some doc/wiki that has guidelines. ;;;","01/Mar/22 11:06;cadonna;[~pratimsc] Have a look at https://kafka.apache.org/contributing

Would be great if you could do the PR soon since code freeze for 3.2.0 is end of this month. 

Thank you for your interest!
Looking forward to the PR!;;;","01/Mar/22 14:46;pratimsc;[~cadonna]  [~showuon] 

Looks like the fix was implemented as part of the pull request : [https://github.com/apache/kafka/pull/11656]

The jackson module has been updated to v2.12.6, which has fix for the reported vulnerability. 

Ref: [https://github.com/FasterXML/jackson-databind/issues/3328] 

I did not go ahead with the initial proposal to increase the version of Jacksion to 2.13.1 , which also has the fix.;;;","01/Mar/22 14:49;cadonna;[~pratimsc] Thank you for checking! ;;;","01/Mar/22 14:56;cadonna;[~mimaison] Is this relevant for the 3.0.1 release? I saw that jackson on the 3.0 branch is still on 2.12.3.;;;","01/Mar/22 15:32;mimaison;[~cadonna] Thanks for heads up. Yeah we want to update dependencies in 3.0 and 3.1 too;;;","01/Mar/22 15:50;cadonna;[~mimaison] Are you going to do that or should [~pratimsc] take a stab at it (if they are interested)?;;;","01/Mar/22 16:01;mimaison;As far as I can tell we're only missing the jackson update in 3.0 and 3.1, netty and jetty have already been updated.

So I'll cherry-pick https://github.com/apache/kafka/commit/0ab36e8b104ce4947f7e6fe04dc8fd80235deb22 into both 3.0 and 3.1. I'll update the ticket once done.;;;","01/Mar/22 17:18;mimaison;Back ported to 3.0 and 3.1. Can we resolve this ticket now?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandardAuthorizer should not finish loading until it is ready,KAFKA-13649,13427052,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,08/Feb/22 00:17,19/Jun/22 21:34,13/Jul/23 09:17,19/Jun/22 21:33,,,,,,,,,,,,,,,,3.3,,,,,,,,,,,,,0,kip-500,kip-801,,,,StandardAuthorizer should not finish loading until it reads up to the high water mark.,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13909,KAFKA-13937,KAFKA-13657,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 19 21:33:40 UTC 2022,,,,,,,,,,"0|z0zc14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/22 21:33;cmccabe;We fixed this in the early.start.listeners work.

For the broker StandardAuthorizer becomes ready when the metadata is declared to be ""caught up""

For the controller, StandardAuthorizer becomes ready when we first catch up with the high water mark;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Committed offsets could be deleted during a rebalance if a group did not commit for a while,KAFKA-13636,13426047,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,princem,Dabz,Dabz,01/Feb/22 19:36,02/May/22 18:43,13/Jul/23 09:17,10/Feb/22 16:38,2.4.0,2.5.1,2.6.2,2.7.2,2.8.1,3.0.0,,,,,,,,,,2.8.2,3.0.1,3.1.1,3.2.0,,,,core,offset manager,,,,,0,,,,,,"The group coordinator might delete invalid offsets during a group rebalance. During a rebalance, the coordinator is relying on the last commit timestamp ({_}offsetAndMetadata.commitTimestamp{_}) instead of the last state modification {_}timestamp (currentStateTimestamp{_}) to detect expired offsets.

 

This is relatively easy to reproduce by playing with group.initial.rebalance.delay.ms, offset.retention.minutes and offset.check.retention.interval, I uploaded an example on: [https://github.com/Dabz/kafka-example/tree/master/docker/offsets-retention] .

This script does:
 * Start a broker with: offset.retention.minute=2, o[ffset.check.retention.interval.ms=|http://offset.check.retention.interval.ms/]1000,  group.initial.rebalance.delay=20000
 * Produced 10 messages
 * Create a consumer group to consume 10 messages, and disable auto.commit to only commit a few times
 * Wait 3 minutes, then the Consumer get a {{kill -9}}
 * Restart the consumer after a few seconds
 * The consumer restart from {{auto.offset.reset}} , the offset got removed

 

The cause is due to the GroupMetadata.scala:
 * When the group get emptied, the {{subscribedTopics}} is set to {{Set.empty}} ([https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala#L520-L521])
 * When the new member joins, we add the new member right away in the group ; BUT the {{subscribedTopics}} is only updated once the migration is over (in the initNewGeneration) (which could take a while due to the {{{}group.initial.rebalance.delay{}}})
 * When the log cleaner got executed,  {{subscribedTopics.isDefined}} returns true as {{Set.empty != None}} (the underlying condition)
 * Thus we enter [https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala#L782-L785] with an empty {{subscribedTopics}} list and we are relying on the {{commitTimestamp}} regardless of the {{currentStateTimestamp}}

 

This seem to be a regression generated by KIP-496 https://cwiki.apache.org/confluence/display/KAFKA/KIP-496%3A+Administrative+API+to+delete+consumer+offsets#KIP496:AdministrativeAPItodeleteconsumeroffsets-ProposedChanges (KAFKA-8338, KAFKA-8370)",,Dabz,Gerrrr,kirktrue,rleslie,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8730,KAFKA-8338,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-02-01 19:36:05.0,,,,,,,,,,"0|z0z5ug:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MirrorMaker 2.0 NPE and Warning ""Failure to commit records"" for filtered records",KAFKA-13632,13425723,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,RensGroothuijsen,bertbaron,bertbaron,31/Jan/22 08:31,14/Feb/23 14:31,13/Jul/23 09:17,15/Sep/22 13:56,3.1.0,3.2.0,3.3.0,,,,,,,,,,,,,3.4.0,,,,,,,mirrormaker,,,,,,1,,,,,,"We have a setup where we filter records with MirrorMaker 2.0 (see below). This results in the following warning messages as a result of NPE's in MirrorSourceTask.commitRecord for each filtered record:
{code:java}
[2022-01-31 08:01:29,581] WARN [MirrorSourceConnector|task-0] Failure committing record. (org.apache.kafka.connect.mirror.MirrorSourceTask:190)
java.lang.NullPointerException
	at org.apache.kafka.connect.mirror.MirrorSourceTask.commitRecord(MirrorSourceTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.commitTaskRecord(WorkerSourceTask.java:463)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords(WorkerSourceTask.java:358)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:257)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}
The reason seems to be that for filtered records metadata is null. Note that in the overridden SourceTask.commitRecord the javadoc clearly states that metadata will be null if the record was filtered.

In our case we use a custom predicate, but the issue can be reproduced with the following configuration:
{code:java}
clusters = source,target

tasks.max = 1

source.bootstrap.servers = <cluster1>
target.bootstrap.servers = <cluster2>

offset.storage.replication.factor=1
status.storage.replication.factor=1
config.storage.replication.factor=1

source->target.enabled = true
source->target.topics = topic1
source->target.transforms=Filter
source->target.transforms.Filter.type=org.apache.kafka.connect.transforms.Filter
source->target.transforms.Filter.predicate=HeaderPredicate
source->target.predicates=HeaderPredicate
source->target.predicates.HeaderPredicate.type=org.apache.kafka.connect.transforms.predicates.HasHeaderKey
source->target.predicates.HeaderPredicate.name=someheader
 {code}
Each record with the header key 'someheader' will result in the NPE and warning message.

On a side note, we couldn't find clear documentation on how to configure (SMT) filtering with MirrorMaker 2 or whether this is supported at all, but apart from the NPE's and warning messages this seems to functionally work for us with our custom filter.

 ",,bertbaron,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13985,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-01-31 08:31:50.0,,,,,,,,,,"0|z0z3vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zookeeper.sync.time.ms is no longer used,KAFKA-13619,13424919,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tyamashi-oss,tyamashi-oss,tyamashi-oss,26/Jan/22 13:55,04/Feb/22 00:18,13/Jul/23 09:17,02/Feb/22 15:11,2.0.0,3.1.0,,,,,,,,,,,,,,,,,,,,,core,documentation,,,,,0,,,,,,"- zookeeper.sync.time.ms is no longer used. But it is present in the documentation (1)
 -- The implementation and documentation of zookeeper.sync.time.ms should be removed.
 -- As far as I can see, it was already out of use by v2.0.0.
 - I've submitted pull request : "" KAFKA-13619: zookeeper.sync.time.ms is no longer used [#11717|https://github.com/apache/kafka/pull/11717]""

(1) [https://kafka.apache.org/documentation/#brokerconfigs_zookeeper.sync.time.ms]",,tyamashi-oss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-01-26 13:55:12.0,,,,,,,,,,"0|z0yyyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leader replication quota is applied to consumer fetches,KAFKA-13614,13424430,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,david.mao,david.mao,david.mao,24/Jan/22 14:37,27/Jan/22 07:22,13/Jul/23 09:17,27/Jan/22 07:22,,,,,,,,,,,,,,,,3.2.0,,,,,,,core,,,,,,0,,,,,,in ReplicaManager.readFromLocalLog we check shouldLeaderThrottle regardless of whether the read is coming from a consumer or follower broker. This results in replication quota being applied to consumer fetches.,,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-01-24 14:37:29.0,,,,,,,,,,"0|z0yvyw:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Connect has a hard dependency on KeyGenerator.HmacSHA256,KAFKA-13613,13424408,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,that_guy,that_guy,24/Jan/22 13:35,05/Jul/22 10:34,13/Jul/23 09:17,05/Jul/22 10:34,3.0.0,,,,,,,,,,,,,,,3.3.0,,,,,,,KafkaConnect,,,,,,1,,,,,,"If a server is running Java 8 that has been configured for FIPS mode according to [openjdk-8-configuring_openjdk_8_on_rhel_with_fips-en-us.pdf|https://access.redhat.com/documentation/en-us/openjdk/8/pdf/configuring_openjdk_8_on_rhel_with_fips/openjdk-8-configuring_openjdk_8_on_rhel_with_fips-en-us.pdf] then the SunJCE provider is not available. As such the KeyGenerator HmacSHA256 is not available. The KeyGenerators I see available are

 * DES
 * ARCFOUR
 * AES
 * DESede

Out of these I think AES would be most appropriate, but that's not the point of this issue, just including for completeness.

When Kafka Connect is started in distributed mode on one of these servers I see the following stack trace

{noformat}
[2022-01-20 20:36:30,027] ERROR Stopping due to error (org.apache.kafka.connect.cli.ConnectDistributed)
java.lang.ExceptionInInitializerError
        at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:94)
        at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:79)
Caused by: org.apache.kafka.common.config.ConfigException: Invalid value HmacSHA256 for configuration inter.worker.key.generation.algorithm: HmacSHA256 KeyGenerator not available
        at org.apache.kafka.connect.runtime.distributed.DistributedConfig.validateKeyAlgorithm(DistributedConfig.java:504)
        at org.apache.kafka.connect.runtime.distributed.DistributedConfig.lambda$configDef$2(DistributedConfig.java:375)
        at org.apache.kafka.common.config.ConfigDef$LambdaValidator.ensureValid(ConfigDef.java:1043)
        at org.apache.kafka.common.config.ConfigDef$ConfigKey.<init>(ConfigDef.java:1164)
        at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:152)
        at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:172)
        at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:211)
        at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:373)
        at org.apache.kafka.connect.runtime.distributed.DistributedConfig.configDef(DistributedConfig.java:371)
        at org.apache.kafka.connect.runtime.distributed.DistributedConfig.<clinit>(DistributedConfig.java:196)
        ... 2 more
{noformat}

It appears the {{org.apache.kafka.connect.runtime.distributed.DistributedConfig}} is triggering a validation of the hard-coded default {{inter.worker.key.generation.algorithm}} property, which is {{HmacSHA256}}.

Ideally a fix would use the value from the configuration file before attempting to validate a default value.

Updates [2022/01/27]: I just tested on a FIPS-enabled version of OpenJDK 11 using the instructions at [configuring_openjdk_11_on_rhel_with_fips|https://access.redhat.com/documentation/en-us/openjdk/11/html-single/configuring_openjdk_11_on_rhel_with_fips/index], which resulted in the same issues. One workaround is to disable FIPS for Kafka Connect by passing in the JVM parameter {{-Dcom.redhat.fips=false}}, however, that means Kafka Connect and all the workers are out of compliance for anyone required to use FIPS-enabled systems.","RHEL 8.5
OpenJDK 1.8.0_312 or 11
Confluent Platform 7.0.1 (Kafka 3.0.0)",ChrisEgerton,mmillson,that_guy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Mar 23 02:43:13 UTC 2022,,,,,,,,,,"0|z0yvu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/22 14:30;mmillson;I think OpenJDK17 should work, as support for this type of key was added with SHA3 support for the SunPKCS11 provider in JDK16[1,2].

References:
[1][java-17-openjdk / rhel-8.5: Mac keys generated by KeyGenerator do not work with corresponding Mac in FIPS mode|https://bugzilla.redhat.com/show_bug.cgi?id=2007331]
[2][Add SHA3 support to SunPKCS11 provider|https://bugs.openjdk.java.net/browse/JDK-8256082];;;","14/Mar/22 21:29;ChrisEgerton;Thanks for reporting this, [~that_guy]. I'm working on a fix and would like to confirm something with you–is the primary use case here _running_ Kafka Connect on a FIPS-compliant JVM (or really, any JVM that doesn't come with the {{HmacSHA256}} key generator algorithm), or is it also necessary to be able to _build_ Kafka Connect from source on that kind of JVM, including running all unit and integration tests?;;;","23/Mar/22 02:43;that_guy;[~ChrisEgerton] The issue is only observed for running. We tend to utilize pre-compiled binaries whenever possible. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebalances while streams is in degraded state can cause stores to be reassigned and restore from scratch,KAFKA-13600,13423382,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,tim.patterson,tim.patterson,19/Jan/22 00:05,28/Mar/22 14:57,13/Jul/23 09:17,28/Mar/22 14:57,2.8.1,3.0.0,3.1.0,,,,,,,,,,,,,3.2.0,,,,,,,streams,,,,,,0,,,,,,"Consider this scenario:
 # A node is lost from the cluster.
 # A rebalance is kicked off with a new ""target assignment""'s(ie the rebalance is attempting to move a lot of tasks - see https://issues.apache.org/jira/browse/KAFKA-10121).
 # The kafka cluster is now a bit more sluggish from the increased load.
 # A Rolling Deploy happens triggering rebalances, during the rebalance processing continues but offsets can't be committed(Or nodes are restarted but fail to commit offsets)
 # The most caught up nodes now aren't within `acceptableRecoveryLag` and so the task is started in it's ""target assignment"" location, restoring all state from scratch and delaying further processing instead of using the ""almost caught up"" node.

We've hit this a few times and having lots of state (~25TB worth) and being heavy users of IQ this is not ideal for us.

While we can increase `acceptableRecoveryLag` to larger values to try get around this that causes other issues (ie a warmup becoming active when its still quite far behind)



The solution seems to be to balance ""balanced assignment"" with ""most caught up nodes"".

We've got a fork where we do just this and it's made a huge difference to the reliability of our cluster.

Our change is to simply use the most caught up node if the ""target node"" is more than `acceptableRecoveryLag` behind.
This gives up some of the load balancing type behaviour of the existing code but in practise doesn't seem to matter too much.

I guess maybe an algorithm that identified candidate nodes as those being within `acceptableRecoveryLag` of the most caught up node might allow the best of both worlds.

 

Our fork is

[https://github.com/apache/kafka/compare/trunk...tim-patterson:fix_balance_uncaughtup?expand=1]
(We also moved the capacity constraint code to happen after all the stateful assignment to prioritise standby tasks over warmup tasks)

Ideally we don't want to maintain a fork of kafka streams going forward so are hoping to get a bit of discussion / agreement on the best way to handle this.
More than happy to contribute code/test different algo's in production system or anything else to help with this issue",,cadonna,guozhang,mjsax,tim.patterson,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 15 10:46:59 UTC 2022,,,,,,,,,,"0|z0ypi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/22 19:17;guozhang;[~tim.patterson] Thanks for filing this ticket.

I'd like to clarify a few things to help my own understanding here: in step 5, ""The most caught up nodes now aren't within `acceptableRecoveryLag` and so the task is started in it's ""target assignment"" location"" could you explain a bit more about this? For a specific task, let's say it has an ongoing active host say A a.k.a. the most caught up node, and then a target host B which is not yet caught up. Let's say due to commit failure the active host A fails out of the `acceptableRecoveryLag`, A's lag should still be smaller than B so that the task would stay with A until B's within the `acceptableRecoveryLag`. Am I reading it wrong?;;;","20/Jan/22 19:29;tim.patterson;[~guozhang] Thats the desired result and the change I've made.
The current Implementation in Master only considers placing the task on nodes returned by this method

[https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java#L235]

So as soon as active hosts A falls out of the `acceptableRecoveryLag` in your example it will jump to target host B, even if the lag on target host B is far far behind(ie just starting to restore);;;","20/Jan/22 21:15;tim.patterson;Thinking about it, the same problem can be hit when we lose a node and some of the standby tasks for the lost actives have fallen a bit behind for whatever reason, In this case it wont promote the standbys to actives but instead start restoring those tasks from scratch on some other node;;;","21/Jan/22 13:50;cadonna;[~tim.patterson] I checked the assignor code and I agree with you that we only distinguish between caught-up and not caught-up. IIRC, we decided to go that way since considering task load and the rank of non caught-up clients turned out to be more complicated that we wanted to make the assignment algorithm, at that time.
If you have found a good trade-off between those dimensions, the best thing is to write up your proposal in a KIP. Although this change would not affect the public API, we usually discuss changes to the assignment algorithms in KIPs because they imply important behavioral changes. [KIP-441|https://cwiki.apache.org/confluence/x/0i4lBg] and [KIP-708|https://cwiki.apache.org/confluence/x/UQ5RCg] are examples for such KIPs.;;;","25/Jan/22 20:01;tim.patterson;Thanks [~cadonna] 
I'm not sure I have the perfect solution either, more just raising this to point out a bit of a hole in the existing implementation.
I do wonder if simply changing the definition of ""caught up"" in `tasksToCaughtUpClients` from ""within acceptableRecoveryLag of the head of the changelog topic"" to ""within acceptableRecoveryLag of the most caught up client"" might solve 80% of the issues with 20% of the effort/risk...

I'll have a bit of a read through the KIP process and have a bit more of a think.;;;","01/Feb/22 16:11;cadonna;[~tim.patterson] To come up with a perfect solution is not easy with such an optimization problem. If you think your solution is a good trade-off and you have good arguments then I think it is worth discussing on a KIP.

I am not sure that changing the definition of caught-up as you proposed is so straight forward. For example, in the case where all clients have no or little state all clients would be considered caught-up and the balance might suffer. What I want to say is that it is probably not just a change of a definition.

If you are interested, in the past I looked at the [linear balanced assignment problem|https://en.wikipedia.org/wiki/Assignment_problem#Balanced_assignment]. I considered the [Hungarian algorithm|https://en.wikipedia.org/wiki/Hungarian_algorithm] for a solution, more specifically I looked at [one of the most popular variants|https://link.springer.com/article/10.1007%2FBF02278710]. But I did not have time to go deep enough.;;;","03/Feb/22 21:38;vvcephei;Hi [~tim.patterson] , thanks for the report and the patch!

 

It sounds like you're reporting two things here:
 # a bug around the acceptable recovery lag.
 # an improvement on assignment balance

If we can discuss those things independently, then we can definitely merge the bugfix immediately. Depending on the impact of the improvement, it might also fall into the category of a simple ticket, or it might be more appropriate to have a KIP as [~cadonna] suggested.

Regarding the bug, I find it completely plausible that we have a bug, but I have to confess that I'm not 100% sure I understand the report. Is the situation that there's an active that's happens to be processing quite a bit ahead of the replicas, such that when the active goes offline, there's no ""caught-up"" node, and instead of failing the task over to the least-lagging node, we just assign it to a fresh node? If that's it, then it is certainly not the desired behavior.

The notion of acceptableRecoveryLag was introduced because follower replicas will always lag the active task, by definition. We want task ownership to be able to swap over from the active to a warm-up when it's caught up, but it will never be 100% caught up (because it is a follower until it takes over). acceptableRecoveryLag is a way to define a small amount of lag that ""acceptable"" so that when a warm-up is only lagging by that amount, we can consider it to be effectively caught up and move the active to the warm-up node.

As you can see, this has nothing at all to do with which nodes are eligible to take over when an active exits the cluster. In that case, it was always the intent that the most-caught-up node should take over active processing, regardless of its lag.

I've been squinting at our existing code, and also your patch ([https://github.com/apache/kafka/commit/a4b622685423fbfd68b1291dad85cc1f44b086f1)] . It looks to me like the flaw in the existing implementation is essentially just here:

[https://github.com/apache/kafka/commit/a4b622685423fbfd68b1291dad85cc1f44b086f1#diff-83a301514ee18b410df40a91595f6f1afd51f6152ff813b5789516cf5c3605baL92-L96]
{code:java}
// if the desired client is not caught up, and there is another client that _is_ caught up, then
// we schedule a movement, so we can move the active task to the caught-up client. We'll try to
// assign a warm-up to the desired client so that we can move it later on.{code}
which should indeed be just like what you described:
{code:java}
// if the desired client is not caught up, and there is another client that _is_ more caught up,
// then we schedule a movement [to] move the active task to the [most] caught-up client. 
// We'll try to assign a warm-up to the desired client so that we can move it later on.{code}
On the other hand, we should not lose this important predicate to determine whether a task is considered ""caught up:

[https://github.com/apache/kafka/commit/a4b622685423fbfd68b1291dad85cc1f44b086f1#diff-e50a755ba2a4d2f7306d1016d079018cba22f9f32993ef5dd64408d1a94d79acL245]
{code:java}
activeRunning(taskLag) || unbounded(acceptableRecoveryLag) || acceptable(acceptableRecoveryLag, taskLag) {code}
This captures a couple of subtleties in addition to the obvious ""a task is caught up if it's under the acceptable recovery lag"":
 # A running, active task doesn't have a real lag at all, but instead its ""lag"" is the sentinel value `-2`
 # You can disable the ""warm up"" phase completely by setting acceptableRecoveryLag to `Long.MAX_VALUE`, in which case, we ignore lags completely and consider all nodes to be caught up, even if they didn't report a lag at all.

 

One extra thing I like about your patch is this:

[https://github.com/apache/kafka/commit/a4b622685423fbfd68b1291dad85cc1f44b086f1#diff-83a301514ee18b410df40a91595f6f1afd51f6152ff813b5789516cf5c3605baR54-R56]
{code:java}
// Even if there is a more caught up client, as long as we're within allowable lag then
// its best just to stick with what we've got {code}
I agree that, if two nodes are within the acceptableRecoveryLag of each other, we should consider their lags to be effectively the same. That's something I wanted to do when we wrote this code, but couldn't figure out a good way to do it.

 

One thing I'd need more time on is the TaskMovementTest. At first glance, it looks like those changes are just about the slightly different method signature, but I'd want to be very sure that we're still testing the same invariants that we wanted to test.

Would you be willing to submit this bugfix as a PR so that we can formally review and merge it?;;;","03/Feb/22 23:19;tim.patterson;Thanks  [~cadonna]
yeah I had a bit of a think and I think you're right, it also gets a bit weird when dealing with replicas..
Like you almost have to decide what's ""caught up"", assign the actives, remove those node/partitions from the candidates and then recalc for the replicas to handle the case where theres a large gab between the most caught up and second most caught up.

[~vvcephei] 
> Is the situation that there's an active that's happens to be processing quite a bit ahead of the replicas, such that when the active goes offline, there's no ""caught-up"" node, and instead of failing the task over to the least-lagging node, we just assign it to a fresh node
Yeah that's it!.
Although what we also hit a couple of times is a variation on clusters with no replicas where the active is restarted but its failed to locally checkpoint in a couple of minutes, when it comes back up its seen as not being caught up and so the task is assigned to a fresh(ish) node
(of course this only occurs when the cluster is already wanting to move that task to a new home due to a node being added/removed recently)



One thing I haven't really taken into consideration/thought about is clusters with more than one replica, I'm not entirely convinced it works there although the unit tests do pass.
 

Did you mean submit as is,  or to create a minimal PR where I only try to address that flaw you've identified here
[https://github.com/apache/kafka/commit/a4b622685423fbfd68b1291dad85cc1f44b086f1#diff-83a301514ee18b410df40a91595f6f1afd51f6152ff813b5789516cf5c3605baL92-L96]

I can certainly have a go at that (it was a few months ago that I patched this so it might take me a bit to wrap my head around it again lol).

Thanks
Tim;;;","04/Feb/22 03:47;guozhang;Thanks guys for the great discussion here.

I think just changing (and of course still maintaining the subtlety of those edge cases) the behavior of ""if we cannot find any caught-up node, assign to a fresh node"" to ""if we cannot find any caught-up node, pick one that is closest to head"" as a general principal should be okay for just the scope of this ticket. We can leave further improvements of the assignment algorithm (I know Bruno/John already have some ideas) to a larger scoped KIP. WDYT?;;;","04/Feb/22 16:04;cadonna;I am fine with discussing the improvement on the PR and not in a KIP. I actually realized that the other improvements to the assignment algorithm included changes to the public API and therefore a KIP was needed. For me it is just important that we look really careful at the improvements because the assignment algorithm is a quite critical part of the system. 

Additionally, I did not want to discuss about a totally new assignment algorithm. I just linked the information for general interest.

Looking forward to the PR.;;;","15/Feb/22 10:46;tim.patterson;Hey sorry about the delay, I've finally been able to get a few hours in a row to be able to concentrate on this.
Trying to just fix ""if we cannot find any caught-up node, pick one that is closest to head"" with minimal other changes.
 
While it doesn't change much algorithmically, unfortunately its more than the 5-10 line change I was hoping for.
https://github.com/apache/kafka/pull/11760/files;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
idempotence producer is not enabled by default if not set explicitly,KAFKA-13598,13423209,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,18/Jan/22 08:53,07/Jun/22 01:23,13/Jul/23 09:17,07/Feb/22 17:58,3.0.0,3.1.0,,,,,,,,,,,,,,3.0.1,3.1.1,3.2.0,,,,,clients,config,,,,,1,,,,,,"In KAFKA-10619, we intended to enable idempotence by default, but this was not achieved due to a bug in the config validation logic. The change from acks=1 to acks=all worked correctly, however.

This is the following up for KIP-679: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-679%3A+Producer+will+enable+the+strongest+delivery+guarantee+by+default]

 

Note: In KAFKA-13673, we'll disable idempotent producer when acks/retries/max.in.flight config conflicts, to avoid breaking existing producers.",,dajac,d-t-w,ijuma,kirktrue,showuon,,,,,,,,,,,,,,,,,,KAFKA-13589,,,,,,,,KAFKA-10619,KAFKA-13673,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 07 01:23:11 UTC 2022,,,,,,,,,,"0|z0yofs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/22 15:13;ijuma;[~showuon] it would be good to determine if this is a 3.1.0 blocker. cc [~dajac] [~kirktrue];;;","18/Jan/22 17:54;dajac;That seems to be a bug and not a regression introduced in 3.1 so I lean towards fixing it in 3.0.1 and 3.1.1. What do you guys think? I will take a close look into it tomorrow.;;;","19/Jan/22 01:56;showuon;Since the memory leak issue: KAFKA-13597 is confirmed to be a bad dependency version for the interceptor, I think we don't have to fix this in v3.1.0. Thank you.;;;","19/Jan/22 09:09;dajac;Sounds good. Thanks [~showuon].;;;","04/Jun/22 02:01;d-t-w;This represents a breaking change where:
 # Broker version is < 2.8.0
 # Cluster has ACL configured, but no IDEMPOTENT_WRITE permission set
 # Producer has default configuration (or no config captured in KAFKA-13673)
 # Kafka Client library version is bumped to 3.2.0 in the producing application

The resulting production error has a ClusterAuthorizationException but it wasn't clear that a new ACL was required until we found KAFKA-13759.

I think 1-3 might be quite a common scenario for Kafka users in the wild, and 4 is very easy to do on the application side.

Perhaps it might be an idea to add a note to the 3.2.0 upgrade guide or release notes about this one.

More info: [https://kpow.io/articles/kafka-producer-breaking-change/];;;","04/Jun/22 02:51;showuon;[~d-t-w] , yes, adding a note to notable changes in 3.2.0 section is a good idea. Are you interested in submitting a PR for it? Thanks.;;;","04/Jun/22 03:28;d-t-w;Very happy to [~showuon] I will just copy points 1-4 into the notes if that's ok?

I'll raise a PR later this afternoon. Should I raise that PR on the apache-site github repository?;;;","04/Jun/22 06:45;showuon;[~d-t-w] , let's open a PR to Kafka repo first, we can discuss how to phrase it there. And then open a PR to kafka-site after merged.

For the points 1-4, I'm thinking we should make it into a sentence.

This is the current note for v3.2.0 related to this issue:

_Notable changes in 3.2.0_

    _- Idempotence for the producer is enabled by default if no conflicting configurations are set. In 3.0.0 and 3.1.0, a bug prevented this default from being applied, which meant that idempotence remained disabled unless the user had explicitly set enable.idempotence to true (See KAFKA-13598 for more details). This issue was fixed and the default is properly applied in 3.0.1, 3.1.1, and 3.2.0._

 

I'm thinking you can add a sub-bullet after it, ex:

_Notable changes in 3.2.0_

    _- Idempotence for the producer is enabled by default if no conflicting configurations are set. In 3.0.0 and 3.1.0, a bug prevented this default from being applied, which meant that idempotence remained disabled unless the user had explicitly set enable.idempotence to true (See KAFKA-13598 for more details). This issue was fixed and the default is properly applied in 3.0.1, 3.1.1, and 3.2.0._

           _-- When the broker version is lower than 2.8.0, and the client version is_ _3.0.1, 3.1.1, and later, the IDEMPOTENT_WRITE permission is required to produce data._

 

Something like that. Again, we can discuss it in the PR. Thank you.

 ;;;","07/Jun/22 01:23;d-t-w;Thanks Luke, I raised [https://github.com/apache/kafka/pull/12260];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky test ControllerIntegrationTest.testTopicIdUpgradeAfterReassigningPartitions,KAFKA-13592,13422150,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,Kvicii,dajac,dajac,12/Jan/22 08:13,07/Jun/22 00:57,13/Jul/23 09:17,07/Jun/22 00:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"{noformat}
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at kafka.controller.ControllerIntegrationTest.testTopicIdUpgradeAfterReassigningPartitions(ControllerIntegrationTest.scala:1239){noformat}",,dajac,divijvaidya,Kvicii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 27 13:59:27 UTC 2022,,,,,,,,,,"0|z0yhww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/22 14:27;Kvicii;[~dajac] hello, can i do this?;;;","17/Jan/22 09:11;dajac;[~Kvicii] Of course! Go for it. I think that the issue might be similar to this one: https://github.com/apache/kafka/pull/11666.;;;","18/Jan/22 00:07;Kvicii;[~dajac] I would like to give this a go so that I can get familiar with the Kafka code base. And Can you describe the reason for this issue and how to reproduce it.Thans a lot;;;","18/Jan/22 08:30;dajac;[~Kvicii] I don't know the reason of this issue... However, I am pretty sure that it is an issue similar to https://github.com/apache/kafka/pull/11666. I have noticed that the test fails from times to times in our CI. I guess that you could reproduce it by running the test in a while loop in bash. ;;;","18/Jan/22 09:37;Kvicii;[~dajac] thanks a lot.bro, pr is here, could you help me review?
links:
https://github.com/apache/kafka/pull/11687;;;","27/May/22 13:21;divijvaidya;This failed for me today at [https://github.com/apache/kafka/pull/12197/checks?check_run_id=6624823962] 
{code:java}
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at kafka.controller.ControllerIntegrationTest.testTopicIdUpgradeAfterReassigningPartitions(ControllerIntegrationTest.scala:1475)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98) {code};;;","27/May/22 13:59;Kvicii;[~divijvaidya] ok, let me see.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky test ControllerIntegrationTest.testTopicIdCreatedOnUpgrade,KAFKA-13591,13422002,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dajac,dajac,dajac,11/Jan/22 15:04,21/Jan/22 16:43,13/Jul/23 09:17,21/Jan/22 16:43,,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,,,,,,0,,,,,,"
{noformat}
org.opentest4j.AssertionFailedError: expected: not equal but was: <None>
	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
	at org.junit.jupiter.api.AssertNotEquals.failEqual(AssertNotEquals.java:276)
	at org.junit.jupiter.api.AssertNotEquals.assertNotEquals(AssertNotEquals.java:265)
	at org.junit.jupiter.api.AssertNotEquals.assertNotEquals(AssertNotEquals.java:260)
	at org.junit.jupiter.api.Assertions.assertNotEquals(Assertions.java:2798)
	at kafka.controller.ControllerIntegrationTest.testTopicIdCreatedOnUpgrade(ControllerIntegrationTest.scala:1140)
{noformat}
",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-01-11 15:04:38.0,,,,,,,,,,"0|z0yh08:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigExceptions thrown by FileConfigProvider during connector/task startup crash worker,KAFKA-13586,13421794,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dstelljes,gharris1727,gharris1727,10/Jan/22 19:17,01/Feb/23 21:32,13/Jul/23 09:17,18/Nov/22 14:50,2.0.0,,,,,,,,,,,,,,,3.3.2,3.4.0,,,,,,KafkaConnect,,,,,,0,,,,,,"If the filesystems of a multi-worker connect cluster are inconsistent, the FileConfigProvider may be able to find a configuration on worker A but not worker B.
This may lead to worker B experiencing a crash when given a connector/task assignment that was previously validated by worker A.

Steps to reproduce:
1. Configure a two-worker Connect cluster to use the FileConfigProvider
2. Place a secret file on worker A (leader) but not worker B (member).
3. Create a connector via REST which references the secret file on-disk.
4. Observe that the connector creation succeeds
5. Wait for a rebalance which assigns either the connector or task to worker B.

Expected behavior:
The connector/task is marked FAILED, and the exception is attributed to the FileConfigProvider not able to find the file.

Actual behavior:
Worker B prints this log message and shuts down:
{noformat}
[Worker clientId=connect-1, groupId=my-connect-cluster] Uncaught exception in herder work thread, exiting: 
2org.apache.kafka.common.config.ConfigException: Invalid value java.nio.file.NoSuchFileException: /path/to/secrets/file.properties for configuration Could not read properties from file /path/to/secrets/file.properties
	at org.apache.kafka.common.config.provider.FileConfigProvider.get(FileConfigProvider.java:92)
	at org.apache.kafka.common.config.ConfigTransformer.transform(ConfigTransformer.java:103)
	at org.apache.kafka.connect.runtime.WorkerConfigTransformer.transform(WorkerConfigTransformer.java:58)
	at org.apache.kafka.connect.runtime.distributed.ClusterConfigState.connectorConfig(ClusterConfigState.java:135)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startConnector(DistributedHerder.java:1464)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.processConnectorConfigUpdates(DistributedHerder.java:638)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:457)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:326)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:831){noformat}
Having an inconsistent filesystem is not a recommended configuration, but it is preferable in such situations to prevent such a connector configuration error from crashing the worker irrecoverably.

 ",,ChrisEgerton,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14670,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 22:03:05 UTC 2022,,,,,,,,,,"0|z0yfq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 22:03;ChrisEgerton;Dan Stelljes (@dstelljes on GitHub) is working on this right now; I've assigned to myself since they don't appear to have a Jira account and I'd like to signal to others that this is being worked on and move its status to ""In progress"".;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix `kafka.network.SocketServerTest.testUnmuteChannelWithBufferedReceives` flaky test,KAFKA-13584,13421717,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,dajac,dajac,10/Jan/22 13:35,18/Mar/22 08:01,13/Jul/23 09:17,18/Mar/22 08:01,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,,"{noformat}
java.lang.IllegalStateException: Channel closed too early
	at kafka.network.SocketServerTest.$anonfun$verifyRemoteCloseWithBufferedReceives$5(SocketServerTest.scala:1515)
	at scala.Option.getOrElse(Option.scala:189)
	at kafka.network.SocketServerTest.$anonfun$verifyRemoteCloseWithBufferedReceives$1(SocketServerTest.scala:1515)
	at kafka.network.SocketServerTest.verifyRemoteCloseWithBufferedReceives(SocketServerTest.scala:1486)
	at kafka.network.SocketServerTest.remoteCloseWithIncompleteBufferedReceive(SocketServerTest.scala:1407)
{noformat}
",,dajac,Kvicii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 18 08:01:42 UTC 2022,,,,,,,,,,"0|z0yf8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/22 14:58;Kvicii;[~dajac]  hello, I will try fix this test, maybe need you help.;;;","18/Mar/22 08:01;dajac;This commit [https://github.com/apache/kafka/commit/b27000ec6af6edfe8a6958dfcc3c0745667e25f4|https://github.com/apache/kafka/commit/b27000ec6af6edfe8a6958dfcc3c0745667e25f4.] might have fixed this issue as well. I think that we can close it for now. ;;;","18/Mar/22 08:01;dajac;Fixed by https://github.com/apache/kafka/commit/b27000ec6af6edfe8a6958dfcc3c0745667e25f4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`TestVerifiableProducer. test_multiple_kraft_security_protocols` consistently fails,KAFKA-13582,13421713,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rndgstn,dajac,dajac,10/Jan/22 13:22,10/Jan/22 19:59,13/Jul/23 09:17,10/Jan/22 19:59,3.1.0,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,,"`TestVerifiableProducer. test_multiple_kraft_security_protocols` consistently fails in 3.1 and trunk: [http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2021-12-27--001.system-test-kafka-3.1--1640613507--confluentinc--3.1]–3527156ac3/report.html.

It seems that the system test does not comply with the changes made in [https://github.com/apache/kafka/commit/36cc3dc2589ef279add3de59c6e7c4548e264eed.] We need to fix it.",,dajac,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 10 14:01:25 UTC 2022,,,,,,,,,,"0|z0yf80:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"10/Jan/22 13:50;rndgstn;This is purely a test configuration issue.  An example error message:

Exception in thread ""main"" org.apache.kafka.common.config.ConfigException: Controller listener with name CONTROLLER_SASL_SSL defined in controller.listener.names not found in listener.security.protocol.map  (an explicit security mapping for each controller listener is required if listener.security.protocol.map is non-empty, or if there are security protocols other than PLAINTEXT in use);;;","10/Jan/22 14:01;dajac;I have reduced the priority to Major as it is purely a configuration issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade vulnerable dependencies  ,KAFKA-13579,13421289,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,shivakumar,shivakumar,07/Jan/22 11:13,01/Mar/22 17:17,13/Jul/23 09:17,09/Feb/22 03:47,2.8.1,,,,,,,,,,,,,,,3.0.1,3.1.1,3.2.0,,,,,,,,,,,0,secutiry,,,,,"|Packages|Package Version|CVSS|Fix Status|
|io.netty_netty-codec  (CVE-2021-43797)|4.1.62.Final|6.5|fixed in 4.1.71|
|org.eclipse.jetty_jetty-server|9.4.43.v20210629|5.5|fixed in 9.4.44|
|org.eclipse.jetty_jetty-servlet|9.4.43.v20210629|5.5|fixed in 9.4.44|

Our security scan detected the above vulnerabilities

upgrade to correct versions for fixing vulnerabilities",,shivakumar,,,,,,,,,,,,,,,,,,KAFKA-13658,,,,,,,,KAFKA-13580,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-01-07 11:13:29.0,,,,,,,,,,"0|z0ycm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
