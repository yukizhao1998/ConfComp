Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Flink kubernetes operator dockerfile could not work with podman,FLINK-27834,13447431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,wangyang0918,wangyang0918,30/May/22 02:38,24/Nov/22 01:03,13/Jul/23 08:08,30/May/22 14:15,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"[1/2] STEP 16/19: COPY *.git ./.git

Error: error building at STEP ""COPY *.git ./.git"": checking on sources under ""/root/FLINK/release-1.0-rc2/flink-kubernetes-operator-1.0.0"": Rel: can't make  relative to /root/FLINK/release-1.0-rc2/flink-kubernetes-operator-1.0.0; copier: stat: [""/*.git""]: no such file or directory

 

podman version
Client:       Podman Engine
Version:      4.0.2
API Version:  4.0.2

 

 

I think the root cause is ""*.git"" is not respected by podman. Maybe we could simply copy the whole directory when building the image.

 
{code:java}
WORKDIR /app

COPY . .

RUN --mount=type=cache,target=/root/.m2 mvn -ntp clean install -pl !flink-kubernetes-docs -DskipTests=$SKIP_TESTS {code}",,gyfora,tison,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 14:15:27 UTC 2022,,,,,,,,,,"0|z12s9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 02:40;wangyang0918;[~gyfora] Given that the copied directory is only used for ephemeral maven build, do you have any concern on this?;;;","30/May/22 02:51;gyfora;I have bad feelings about this, it might lead to accidentally leaking credentials from local working directories.;;;","30/May/22 03:03;wangyang0918;Do you mean we might accidentally bundle the credentials into the image?;;;","30/May/22 03:05;gyfora;yes, the user can have anything in their local directory by accident;;;","30/May/22 03:06;gyfora;Maybe this is not a big concern, would love to hear what [~matyas] or [~jbusche] think;;;","30/May/22 03:08;wangyang0918;cc [~Tison] I remember you also have the same idea when fixing FLINK-27746.;;;","30/May/22 03:42;tison;Thanks for your notification [~wangyang0918]. I'll take a look today.;;;","30/May/22 07:02;morhidi;[~wangyang0918] Does our release / CI automatism rely on podman atm? If not I'm leaning towards documenting a workaround for local podman builds instead. Actually leaking with git history is also possible, but that's another topic I guess.;;;","30/May/22 07:44;tison;I second to [~matyas]'s opinions about git history and willing to know the release automatism.

Actually, it's saner to me that do not COPY .git to docker image at all. That means we don't build the image barely via ""docker build ."", but running a scripting to optionally generate git properties and the docker build process rely on that. It would be a variant of https://github.com/apache/flink-kubernetes-operator/pull/241 (running the shell script aside from {{mvn package}}).

Although, I'll try to prepare a patch based on .dockerignore for preview.

For ""credentials"" arguments, even in current approaches, those files won't go into the final image but only in intermediate build image, so I think it won't be a critical problem. cc [~gyfora];;;","30/May/22 08:44;wangyang0918;Since our release/CI does not rely on the podman, it makes sense to me that we document a workaround for local podman build now. And bundle the generated {{.flink-kubernetes-operator.version.properties}} in the release source, which will be done in FLINK-27759.;;;","30/May/22 09:04;wangyang0918;Compared with documenting the workaround for podman, I prefer [~tison]'s solution of {{"".dockerignore""}} + ""{{{}COPY . .{}}}"" for this PR.;;;","30/May/22 09:21;morhidi;I don't see yet how .dockerignore would prevent copying unintentional files, but seemingly it's not a concern, since they would end up in the build layer only.;;;","30/May/22 09:45;wangyang0918;Yes. We could not avoid the unintentional files unless we could configure the pattern in the {{{}.dockerignore{}}}. I also agree that it is not a concern.;;;","30/May/22 14:15;wangyang0918;Fixed via:

main: db13018975361e3271e26be64a3940a5710097e7

release-1.0: c8d2a71a61a943fb9b5cf31575a40c6988665740;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
addInEdge check state error,FLINK-27800,13446986,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Licho,Licho,26/May/22 10:14,16/Feb/23 08:06,13/Jul/23 08:08,16/Feb/23 08:06,1.15.3,1.16.1,1.17.0,,,,1.15.4,1.16.2,1.17.0,,API / Core,,,,,0,pull-request-available,,,"when add InEdge, the checkState fucntion check the edge whether is in outEdges list, this should check whether in inEdges list.

 
{code:java}
public void addInEdge(StreamEdge inEdge) {
    checkState(
            outEdges.stream().noneMatch(inEdge::equals),
            ""Adding not unique edge = %s to existing outEdges = %s"",
            inEdge,
            inEdges);
    if (inEdge.getTargetId() != getId()) {
        throw new IllegalArgumentException(""Destination id doesn't match the StreamNode id"");
    } else {
        inEdges.add(inEdge);
    }
} {code}",,Licho,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 16 08:03:39 UTC 2023,,,,,,,,,,"0|z12pkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 08:03;wanglijie;Fixed via
master: 756eeee8628ee7e81ca7b1b4ae80aa6cddd7284d
release-1.17: b64739a5ef976e003bf87250b41ae1142e541497
release-1.16: cca9ee1ac60810098bcf3dbf3013f4d6c91a10a8
release-1.15: c6b649bf937976038dbfcd00e59c51b8d886ad96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonTableUtils.getCollectionInputFormat cannot correctly handle None values,FLINK-27797,13446960,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zsigner,yunfengzhou,yunfengzhou,26/May/22 07:31,01/Jun/22 12:50,13/Jul/23 08:08,01/Jun/22 12:50,1.15.0,,,,,,1.15.1,,,,API / Python,,,,,0,pull-request-available,,,"In `PythonTableUtils.getCollectionInputFormat` there are implementations like follows.
This code can be found at [https://github.com/apache/flink/blob/8488368b86a99a064446ca74e775b67ffff0b94a/flink-python/src/main/java/org/apache/flink/table/utils/python/PythonTableUtils.java#L515]

```
c -> {
            if (c.getClass() != byte[].class || dataType instanceof PickledByteArrayTypeInfo) {
                return c;
            }
```

Here, the generated function did not check `c != null` before doing `c.getClass()`. which might cause that tables created through pyflink cannot parse it when values are `None`.",,hxbks2ks,yunfengzhou,Zsigner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 12:50:08 UTC 2022,,,,,,,,,,"0|z12pf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 11:21;Zsigner;can I get this ticket？;;;","01/Jun/22 12:50;hxbks2ks;Merged into master via 586715f23ef49939ab74e4736c58d71c643a64ba
Merged into release-1.15 via d5edd4c346985483179504e23d98182941b1657a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The primary key obtained from MySQL is incorrect by using MysqlCatalog,FLINK-27794,13446929,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dusukang,dusukang,dusukang,26/May/22 03:36,05/Jul/22 14:09,13/Jul/23 08:08,27/Jun/22 09:27,1.15.0,,,,,,1.15.2,1.16.0,,,Connectors / JDBC,,,,,0,pull-request-available,,,"I want to use MysqlCatalog to get the primary key of the database table `user`. The database table creation statement is as follows
{code:java}
CREATE TABLE flinksql_test.`user` (
  `uid` bigint(20) NOT NULL,
  `uname` varchar(36) DEFAULT NULL,
  `others` varchar(128) DEFAULT NULL,
  PRIMARY KEY (`uid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8; {code}
 

This is my test code:
{code:java}
import org.apache.flink.connector.jdbc.catalog.MySqlCatalog;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.catalog.CatalogBaseTable;
import org.apache.flink.table.catalog.ObjectPath;
import org.apache.flink.table.catalog.exceptions.TableNotExistException;import java.util.Optional;public class Demo02 {
    public static void main(String[] args) throws TableNotExistException {
        MySqlCatalog mySqlCatalog = new MySqlCatalog(""mysql-catalog"",
                ""flinksql_test"",
                ""root"",
                ""123456789"",
                String.format(""jdbc:mysql://127.0.0.1:3306""));
        CatalogBaseTable table = mySqlCatalog.getTable(new ObjectPath(""flinksql_test"", ""user""));
        Optional<Schema.UnresolvedPrimaryKey> primaryKey = table
                .getUnresolvedSchema()
                .getPrimaryKey();
        System.out.println(primaryKey);
    }
} {code}
 

The obtained primary key is (Host,User), but the primary key from Database is (uid)

!167908239-c6f3f0ad-af06-436f-87e2-85c60428b400.png!

 

I see, the value of the incoming catalog and schema is null, and the SQL splicing of the database to obtain the primary key does not add "" TABLE_SCHEMA LIKE ? AND""

!https://user-images.githubusercontent.com/68139929/167910188-6df6f3ec-cb33-49cc-91d5-61dcb1167c98.png!

!https://user-images.githubusercontent.com/68139929/167908960-cf873c66-a227-41fa-99ea-5cff8a181f29.png!

!https://user-images.githubusercontent.com/68139929/167909024-22b15192-0755-416e-8421-dab0fdfc0d15.png!

Later, it was found that there was also a user table in the self-contained MySQL database with the following structure:

 
{code:java}
CREATE TABLE mysql.`user` (
  `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '',
  `User` char(32) COLLATE utf8_bin NOT NULL DEFAULT '',
  `Select_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Insert_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Update_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Delete_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Drop_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Reload_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Shutdown_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Process_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `File_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Grant_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `References_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Index_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Alter_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Show_db_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Super_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_tmp_table_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Lock_tables_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Execute_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Repl_slave_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Repl_client_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_view_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Show_view_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_routine_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Alter_routine_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_user_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Event_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Trigger_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_tablespace_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `ssl_type` enum('','ANY','X509','SPECIFIED') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT '',
  `ssl_cipher` blob NOT NULL,
  `x509_issuer` blob NOT NULL,
  `x509_subject` blob NOT NULL,
  `max_questions` int(11) unsigned NOT NULL DEFAULT '0',
  `max_updates` int(11) unsigned NOT NULL DEFAULT '0',
  `max_connections` int(11) unsigned NOT NULL DEFAULT '0',
  `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0',
  `plugin` char(64) COLLATE utf8_bin NOT NULL DEFAULT 'caching_sha2_password',
  `authentication_string` text COLLATE utf8_bin,
  `password_expired` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `password_last_changed` timestamp NULL DEFAULT NULL,
  `password_lifetime` smallint(5) unsigned DEFAULT NULL,
  `account_locked` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_role_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Drop_role_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Password_reuse_history` smallint(5) unsigned DEFAULT NULL,
  `Password_reuse_time` smallint(5) unsigned DEFAULT NULL,
  `Password_require_current` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `User_attributes` json DEFAULT NULL,
  PRIMARY KEY (`Host`,`User`)
) /*!50100 TABLESPACE `mysql` */ ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0 COMMENT='Users and global privileges'; {code}
 

I think it while happen when there are multiple tables which have same table name, so we can pass the table schema to get primary key?",,dusukang,leonard,martijnvisser,RocMarshal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/22 03:44;dusukang;167908239-c6f3f0ad-af06-436f-87e2-85c60428b400.png;https://issues.apache.org/jira/secure/attachment/13044213/167908239-c6f3f0ad-af06-436f-87e2-85c60428b400.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 24 12:04:44 UTC 2022,,,,,,,,,,"0|z12p88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:51;martijnvisser;[~RocMarshal] Can you have a look? ;;;","31/May/22 12:46;RocMarshal;Thanks [~dusukang] for the reporting it and [~martijnvisser] ping. Please let me have a check.;;;","24/Jun/22 12:04;leonard;Fixed in:
 master(1.16) d0cee16e6db5f6279697924a2a18c685e576955b
 release-1.15: 1d0dac0e00739a4cf176cba6dc091662e48170c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka WakeupException during handling splits changes,FLINK-27762,13446690,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,zoucan,zoucan,25/May/22 06:00,17/Jun/22 02:45,13/Jul/23 08:08,17/Jun/22 02:45,1.14.3,,,,,,1.14.6,1.15.1,,,Connectors / Kafka,,,,,0,pull-request-available,,," 

We enable dynamic partition discovery in our flink job, but job failed when kafka partition is changed. 

Exception detail is shown as follows:
{code:java}
java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:350)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.kafka.common.errors.WakeupException
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:511)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:275)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)
	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1726)
	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1684)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.removeEmptySplits(KafkaPartitionSplitReader.java:315)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(KafkaPartitionSplitReader.java:200)
	at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
	... 6 more {code}
 

After preliminary investigation, according to source code of KafkaSource,

At first: 

method *org.apache.kafka.clients.consumer.KafkaConsumer.wakeup()* will be called if consumer is polling data.

Later: 

method *org.apache.kafka.clients.consumer.KafkaConsumer.position()* will be called during handle splits changes.

Since consumer has been waken up, it will throw WakeUpException.",,leonard,martijnvisser,mason6345,renqs,yunta,zoucan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 12:38:59 UTC 2022,,,,,,,,,,"0|z12nr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 07:07;martijnvisser;Can you please try this with the latest Flink 1.14 and/or Flink 1.15 version? ;;;","25/May/22 08:42;zoucan;[~martijnvisser] 

Thanks for your reply.

I have checked the source code of latest version, but there's no difference for this part.

It's difficult to test since this exception doesn't happen every time of partition change.

In my opinion, it happend only if method *org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.wakeUp()* is called while consumer is polling data. 

In this situation, above method will eventually call {*}org.apache.kafka.clients.consumer.KafkaConsumer.wakeup(){*}. 
{code:java}
// org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.wakeUp()

public void wakeUp() {
    wakeup = true;
    if (lastRecords == null) {
        // 1. consumer is polling data, execute org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.wakeUp() 
        splitReader.wakeUp()
    } else {
        elementsQueue.wakeUpPuttingThread(fetcherIndex);
    }
} {code}
 
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.wakeUp()

public void wakeUp() {
    // 2. execute org.apache.kafka.clients.consumer.KafkaConsumer.wakeup()
    consumer.wakeup();
} {code}
 

And in method {*}org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(){*}, 

*org.apache.kafka.clients.consumer.KafkaConsumer.position()* will be called.
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges()

public void handleSplitsChanges(SplitsChange<KafkaPartitionSplit> splitsChange) {
    // ignore irrelevant code...

    // 3.execute this.removeEmptySplits()
   removeEmptySplits();

    maybeLogSplitChangesHandlingResult(splitsChange);
} {code}
 
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.removeEmptySplits()

private void removeEmptySplits() {
    List<TopicPartition> emptyPartitions = new ArrayList<>();
   
    for (TopicPartition tp : consumer.assignment()) {
        // 4. execute org.apache.kafka.clients.consumer.KafkaConsumer.position()
        // since kafka consumer is waken up before, it will throw WakeUpException.
        if (consumer.position(tp) >= getStoppingOffset(tp)) {
            emptyPartitions.add(tp);
        }
    }

   // ignore irrelevant code...     
} {code}
 

Since our application is already online, we must evaluate the risk for upgrading version.

Anyway, I'll try to test with latest version and share the result.  And If you have any suggestion, please share with me.;;;","25/May/22 08:55;martijnvisser;[~renqs] WDYT?;;;","25/May/22 09:30;renqs;Thanks for reporting the issue [~zoucan]! I think it is indeed a bug that hard to reproduce. It could only happen if the wakeup operation is triggered after KafkaConsumer#poll returns and the flow is still in KafkaPartitionSplitReader#fetch. 

[~martijnvisser] could you assign the ticket to me? Thanks;;;","25/May/22 09:32;martijnvisser;Thank you [~renqs] I've assigned it to you!;;;","30/May/22 02:44;zoucan;[~renqs] 

Thanks for your effort for this issue.

I‘ve reviewed your pr and i have a question. The exception i met is in method {*}KafkaPartitionSplitReader#removeEmptySplits{*}. But i can't find any action for handling this exception in that method.
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader#removeEmptySplits

private void removeEmptySplits() {
    List<TopicPartition> emptyPartitions = new ArrayList<>();
   
    for (TopicPartition tp : consumer.assignment()) {
        // WakeUpException is thrown here.
        // since KafkaConsumer#wakeUp is called before,if execute KafkaConsumer#postion() in 'if statement' above, it will throw WakeUpException.   
        if (consumer.position(tp) >= getStoppingOffset(tp)) {
            emptyPartitions.add(tp);
        }
    }

   // ignore irrelevant code...     
} {code}
Maybe we should check whether *KafkaConsumer#wakeUp* is called before, or catch WakeUpException in *KafkaPartitionSplitReader#removeEmptySplits.*;;;","30/May/22 03:21;renqs;Thanks a lot for the review [~zoucan] ! What about moving the discussion about code and PRs to Github?;;;","02/Jun/22 21:43;mason6345;Hi [~renqs], I also recently encountered a similar issue internally

> It could only happen if the wakeup operation is triggered after KafkaConsumer#poll returns and the flow is still in KafkaPartitionSplitReader#fetch. 

I don't think so, because the exception is thrown from handling the split changes.

 ;;;","15/Jun/22 12:38;leonard;fixed in 
master(1.16):50c19d9f534f86e228f3e0937d92baf766a57165
release-1.15:a5c0c8776cb06507f12a00c93bd92a7ac6d18375
release-1.14: 4c9d99bbb5f3f250643b3a057866ec98cce04359;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE is thrown when executing PyFlink jobs in batch mode,FLINK-27760,13446679,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,25/May/22 03:51,26/May/22 01:10,13/Jul/23 08:08,26/May/22 01:10,1.13.0,,,,,,1.13.7,1.14.5,1.15.1,1.16.0,API / Python,,,,,0,pull-request-available,,,"This is the exception stack reported by one user:
{code}
022-05-25 11:39:32,792 [MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[I... -> PythonCal ... with job vertex id 71d9b8e1b249eaa7e67ef93fb483177f (63/100)#123] WARN org.apache.flink.runtime.taskmanager.Task [] - MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[I... -> PythonCal ... with job vertex id 71d9b8e1b249eaa7e67ef93fb483177f (63/100)#123 (6fa78755ff19ac9d0d57aba21840e834) switched from INITIALIZING to FAILED with failure cause: java.lang.NullPointerException
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.getKeyedStateBackend(AbstractStreamOperator.java:466)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processElementsOfCurrentKeyIfNeeded(AbstractPythonFunctionOperator.java:238)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processWatermark(AbstractPythonFunctionOperator.java:208)
at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.emitWatermark(OneInputStreamOperatorOutput.java:45)
at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:39)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:632)
at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:74)
at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.emitWatermark(OneInputStreamOperatorOutput.java:45)
at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:39)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:632)
at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:74)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark2(AbstractStreamOperator.java:649)
at org.apache.flink.table.runtime.operators.multipleinput.input.SecondInputOfTwoInput.processWatermark(SecondInputOfTwoInput.java:44)
at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory$StreamTaskNetworkOutput.emitWatermark(StreamMultipleInputProcessorFactory.java:318)
at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196)
at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:137)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:106)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:87)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:424)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:569)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:541)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:586)
at java.lang.Thread.run(Thread.java:877)
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 26 01:10:55 UTC 2022,,,,,,,,,,"0|z12noo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/22 01:10;dianfu;Fixed in:
- master via 1348dee741b5af89279a55cb26669e72940911b5
- release-1.15 via 6490cd52825eeeeaf4325e5bfb902d9dbd604f56
- release-1.14 via 47f0ad48679561b452b14e7d6fe36f7abd53e33b
- release-1.13 via 84c1bd45e060fd4cba2ceeb899045d9d9cd72e56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator helm chart release the Chart.yaml file doesn't have an apache license header,FLINK-27747,13446472,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,24/May/22 01:22,25/May/22 02:43,13/Jul/23 08:08,25/May/22 02:43,,,,,,,kubernetes-operator-1.0.0,,,,,,,,,0,pull-request-available,,,"When verifying the 1.0.0-rc1, [~gyfora] found that the Chart.yaml file doesn't have an apache license header.

It seems this is caused by {{helm package}} in the {{create_source_release.sh}}.

We also have this issue in the 0.1.0 release[1].

[1]. https://dist.apache.org/repos/dist/release/flink/flink-kubernetes-operator-0.1.0/",,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 02:43:54 UTC 2022,,,,,,,,,,"0|z12mew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 02:43;wangyang0918;Fixed via:

main: 710310b76d174526135e30ad236c32f74df45c78

release-1.0: 4a70fe364ec4a8a3c25efc49531cf09d50cf9f96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator docker image could not build with source release,FLINK-27746,13446469,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,nicholasjiang,wangyang0918,wangyang0918,24/May/22 01:17,25/May/22 02:42,13/Jul/23 08:08,25/May/22 02:42,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Could not build the Docker image from the source release, getting the
following error:


> [build 11/14] COPY .git ./.git:

------

failed to compute cache key: ""/.git"" not found: not found",,nicholasjiang,tison,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 02:42:47 UTC 2022,,,,,,,,,,"0|z12me8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 01:26;wangyang0918;cc [~nicholasjiang] would you like to have a look?

We should skip copying the .git directory when it does not exit.;;;","24/May/22 09:29;tison;[~wangyang0918] I think we can simply avoid COPY .git folder at all scenarios as we don't depend on those stuff.

BTW, basically, we don't assign others unless they ask for it.;;;","24/May/22 10:12;wangyang0918;I am afraid we could not simply remove the {{COPY}} of .git directory. We rely on it to get the commit id. Then it will be used in the logs.

{code:java}
[ INFO ]  Starting Flink Kubernetes Operator (Version: 1.0.0, Flink Version: 1.15.0, Rev:2417603, Date:2022-05-23T07:42:07+02:00)
{code}


This is introduced by [~nicholasjiang] and he pings me privately. That's why I assigned this ticket to him.;;;","24/May/22 10:24;tison;Thanks for sharing the background.

Then I think here we have two opinions:

1. Include .git folder in the released tarball.
2. Setup another approach to generate git properties before release and skip the related phase when building with release artifacts.

ZooKeeper uses similar maven plugin but unfortunately I find that if you build with released tarball the commit field is remain unresolved. It may not be a problem to show the info as ""${git.commit.time}"" barely but I'm not sure.

FYI, if we simply remove ""COPY .git"" the final properties are:

project.version=1.0.0
git.commit.id.abbrev=${git.commit.id.abbrev}
git.commit.time=${git.commit.time}

But to produce the docker image properly, we may still COPY .git when packaging in a git repo.;;;","24/May/22 10:42;nicholasjiang;[~tison], it's my mistake that I didn't comment on this ticket and thanks for [~wangyang0918] to explain. [~wangyang0918], shipping the .git is better solution at present and you need to update the current release process.;;;","24/May/22 10:56;wangyang0918;>> ZooKeeper uses similar maven plugin but unfortunately I find that if you build with released tarball the commit field is remain unresolved. It may not be a problem to show the info as ""${git.commit.time}"" barely but I'm not sure.

 

When building the image with source release, the UNKNOWN commit id in logs is reasonable to me.;;;","25/May/22 02:42;wangyang0918;Fixed via:

main: cf8f99b2c7cd61828ce94228d1b6b31c235d4cb8

release-1.0: d0482f91b6603bcdd28ac44130a8cd8bb553108e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix compatibility issues between Flink ML Stages,FLINK-27742,13446332,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,23/May/22 09:06,24/Jun/22 07:49,13/Jul/23 08:08,02/Jun/22 01:25,ml-2.0.0,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,,,"It is discovered that StringIndexer and LogisticRegression in Flink ML cannot be connected in a pipeline. The reason is that the output label column of StringIndexer is integer, while LogisticRegression can only accept input data whose labels are doubles.

In order to make Flink ML stages compatible with each other, the following changes need to be made.
- For stages who can only accept double-typed inputs, update their implementation to accept any numerical type.
- For stages that generates labels as integers, make them return labels as doubles.",,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-23 09:06:03.0,,,,,,,,,,"0|z12ljs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not showing checkpoint interval properly  in WebUI when checkpoint is disabled,FLINK-27734,13446247,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,22/May/22 15:44,20/Jun/22 07:26,13/Jul/23 08:08,10/Jun/22 09:43,1.15.0,,,,,,1.15.1,1.16.0,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,"Not showing checkpoint interval properly  in WebUI when checkpoint is disabled

!image-2022-05-22-23-42-46-365.png|width=1019,height=362!",,Feifan Wang,junhan,klion26,yunta,Zsigner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/22 15:42;Feifan Wang;image-2022-05-22-23-42-46-365.png;https://issues.apache.org/jira/secure/attachment/13044001/image-2022-05-22-23-42-46-365.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 07:26:31 UTC 2022,,,,,,,,,,"0|z12l0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 10:18;Feifan Wang;Hi [~wolli] , I have opened a pr to fix this, can you take a look ? ;;;","26/May/22 02:54;Feifan Wang;Hi [~junhany] ， can you help me confirm this problem ?;;;","27/May/22 09:07;Zsigner;Hi [~Feifan Wang] 

     I took a look at the pr you submitted, why did you change '0x7ffffffffffffffff' to 9223372036854776000, 

     I tested it and it should be 9223372036854775807, and then the page will display 'Periodic checkpoints disabled', 0x7ffffffffffffffff This seems to be no problem

 ;;;","27/May/22 15:15;Zsigner;Hi [~Feifan Wang] 
So I think this value should not be changed, it may be caused by other reasons;;;","27/May/22 15:23;Feifan Wang;Thanks for reply [~Zsigner] , I want just remove single quotes around 0x7fffffffffffffff firstly , but I find the lint-staged will change ""0x7fffffffffffffff"" to  ""0; x7fffffffffffffff"". Is there some advice suggestion for deal with it better ?;;;","27/May/22 15:32;Feifan Wang;[~Zsigner] , I think the problem is strict equals operator ( === ) require same type for return true.;;;","27/May/22 16:44;Zsigner;Hi [~Feifan Wang] 

   I also found that it may be due to an additional = sign. I am going to test it and submit a PR to deal with this issue. Since you think so, don't forget that 1.15 and 1.16 versions need to submit two requests. Thank you;;;","28/May/22 15:17;Feifan Wang;Hi [~Zsigner] , I tried changing ""==="" to ""=="", but there is a error : 

{code:java}
503:24  error  Expected `===` but received `==`  @angular-eslint/template/eqeqeq
506:24  error  Expected `!==` but received `!=`  @angular-eslint/template/eqeqeq
{code}

Should we modify configuration of eslint ?;;;","28/May/22 17:05;Feifan Wang;Hi [~Zsigner] , I updated the pr with modify eslint configuration , if think that's ok, I will submit another pr for flink-1.15.;;;","09/Jun/22 02:25;junhan;Hi [~Feifan Wang] . Thank you for fixing this. I left some comments in the PR. ;;;","10/Jun/22 09:43;yunta;merged in master: 18c13fe4b6788e3ec8af95551ebe3e598b32df61;;;","10/Jun/22 10:15;Feifan Wang;Hi [~yunta] , need we merge this change to release-1.15 ?;;;","15/Jun/22 05:32;yunta;[~Feifan Wang] I think it deserves to merge to release-1.15. However, the related code has changed, you can submit a PR targets for release-1.15 branch, then we can make it merged.;;;","17/Jun/22 03:11;Feifan Wang;Hi [~yunta] , I hive submitted a [PR|https://github.com/apache/flink/pull/19999] for release-1.15.;;;","20/Jun/22 07:26;yunta;merged in release-1.15: efee7405c3acf54bf2cc79b676cb881ac5f2c362;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rework on_timer output behind watermark bug fix,FLINK-27733,13446245,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,22/May/22 15:13,24/May/22 02:28,13/Jul/23 08:08,24/May/22 02:28,1.14.4,1.15.0,,,,,1.14.5,1.15.1,1.16.0,,API / Python,,,,,0,pull-request-available,,,"FLINK-27676 can be simplified by just checking isBundleFinished() before emitting watermark in AbstractPythonFunctionOperator, and this fix FLINK-27676 in python group window aggregate too.",,hxbks2ks,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 02:28:57 UTC 2022,,,,,,,,,,"0|z12l0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 02:28;hxbks2ks;merged into master via a0ef9eb46ad3896d6d87595dbe364f69d583794c
merged into release-1.15 via f413c40c8ab8145d3bdea8dbc6372961a598be37
merged into release-1.14 via 945c15341b93a9bfadc7b6ce239a96c2b7baf592;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector document code sink has an error,FLINK-27730,13446199,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,i-liu,i-liu,22/May/22 01:55,23/May/22 07:05,13/Jul/23 08:08,23/May/22 06:59,1.14.4,,,,,,1.14.4,,,,Documentation,,,,,0,,,,"Kafka Sink document sample code API call error.

[kafka sink|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/datastream/kafka/] 
{code:java}
KafkaSink<String> sink = KafkaSink.<String>builder()
        .setBootstrapServers(brokers)
        .setRecordSerializer(KafkaRecordSerializationSchema.builder()
            .setTopic(""topic-name"")
            .setValueSerializationSchema(new SimpleStringSchema())
            .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
            .build()
        )
        .build(); {code}
+setDeliveryGuarantee+ is the method of +KafkaSink+ not the method of {+}KafkaRecordSerializationSchema{+}, as follows:

 
{code:java}
KafkaSink<String> sink = KafkaSink.<String>builder()
        .setBootstrapServers(brokers)
        .setRecordSerializer(KafkaRecordSerializationSchema.builder()
            .setTopic(""topic-name"")
            .setValueSerializationSchema(new SimpleStringSchema())
            .build()
        )
      .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
      .build(); {code}
 

 ",Flink 1.14.4,martijnvisser,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/22 01:55;i-liu;kafka-sink.png;https://issues.apache.org/jira/secure/attachment/13043992/kafka-sink.png","22/May/22 12:20;i-liu;kafka_sink.png;https://issues.apache.org/jira/secure/attachment/13043997/kafka_sink.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 07:05:08 UTC 2022,,,,,,,,,,"0|z12kq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/22 11:25;Wencong Liu;Hello [~i-liu] , please show the relevant documentation link, and what is the specific error?;;;","22/May/22 12:20;i-liu;[~Wencong Liu] [kafka sink|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/datastream/kafka/]

Error is as follows:

!kafka_sink.png!;;;","23/May/22 06:57;martijnvisser;This has been previously reported and fixed already, it probably just needs a backport;;;","23/May/22 07:05;i-liu;[~martijnvisser] OK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dockerFile build results in five vulnerabilities,FLINK-27728,13446135,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jbusche,jbusche,jbusche,21/May/22 02:02,22/May/22 23:07,13/Jul/23 08:08,22/May/22 23:07,kubernetes-operator-0.1.0,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"A Twistlock security scan of the default flink-kubernetes-operator currently shows five fixable vulnerabilities.  One [~wangyang0918] and I are trying to fix in [FLINK-27654|https://issues.apache.org/jira/browse/FLINK-27654].

The other four are easily addressable if we update the underlying OS.  I'll propose a PR for this later this evening.

The four vulnerabilities are: 
1.  packageName: gzip

severity: Low

cvss: 0

riskFactors: Has fix,Recent vulnerability

CVE Link:  [https://security-tracker.debian.org/tracker/CVE-2022-1271] 

Description: DOCUMENTATION: No description is available for this CVE.              STATEMENT: This bug was introduced in gzip-1.3.10 and is relatively hard to exploit.  Red Hat Enterprise Linux 6 was affected but Out of Support Cycle because gzip was not listed in Red Hat Enterprise Linux 6 ELS Inclusion List. [https://access.redhat.com/articles/4997301]             MITIGATION: Red Hat has investigated whether possible mitigation exists for this issue, and has not been able to identify a practical example. Please update the affected package as soon as possible.

2.  packageName: openssl

severity: Critical

cvss: 9.8

riskFactors: Attack complexity: low,Attack vector: network,Critical severity,Has fix,Recent vulnerability

CVE Link: [https://security-tracker.debian.org/tracker/CVE-2022-1292] 

Description: 

The c_rehash script does not properly sanitise shell metacharacters to prevent command injection. This script is distributed by some operating systems in a manner where it is automatically executed. On such operating systems, an attacker could execute arbitrary commands with the privileges of the script. Use of the c_rehash script is considered obsolete and should be replaced by the OpenSSL rehash command line tool. Fixed in OpenSSL 3.0.3 (Affected 3.0.0,3.0.1,3.0.2). Fixed in OpenSSL 1.1.1o (Affected 1.1.1-1.1.1n). Fixed in OpenSSL 1.0.2ze (Affected 1.0.2-1.0.2zd).

3.  packageName: zlib

severity: High

cvss: 7.5

riskFactors: Attack complexity: low,Attack vector: network,Has fix,High severity

CVE Link: [https://security-tracker.debian.org/tracker/CVE-2018-25032] 

Description: zlib before 1.2.12 allows memory corruption when deflating (i.e., when compressing) if the input has many distant matches.

4.  packageName: openldap

severity: Critical

cvss: 9.8

riskFactors: Attack complexity: low,Attack vector: network,Critical severity,Has fix,Recent vulnerability

CVE Link: [https://security-tracker.debian.org/tracker/CVE-2022-29155] 

Description: In OpenLDAP 2.x before 2.5.12 and 2.6.x before 2.6.2, a SQL injection vulnerability exists in the experimental back-sql backend to slapd, via a SQL statement within an LDAP query. This can occur during an LDAP search operation when the search filter is processed, due to a lack of proper escaping.",,jbusche,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 22 23:07:00 UTC 2022,,,,,,,,,,"0|z12kc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 02:16;jbusche;Created PR [https://github.com/apache/flink-kubernetes-operator/pull/233] to address the four items.;;;","22/May/22 23:07;mbalassi;Fixed via 65ea03c744347448f2592877cc98d85c6ea36ef4 in release-1.0 and 503673259213fdf78279bccec6c3d1edddabec0e in main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job failed to start due to ""Time should be non negative""",FLINK-27712,13445963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sharonxr55,sharonxr55,20/May/22 05:52,14/Jun/22 15:03,13/Jul/23 08:08,10/Jun/22 10:25,1.14.4,,,,,,,,,,Runtime / Network,,,,,0,,,,"Happened intermittently. A restart made the issue go away.

Stack trace:
{code:java}
Time should be non negative
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
at org.apache.flink.runtime.throughput.ThroughputEMA.calculateThroughput(ThroughputEMA.java:44)
at org.apache.flink.runtime.throughput.ThroughputCalculator.calculateThroughput(ThroughputCalculator.java:80)
at org.apache.flink.streaming.runtime.tasks.StreamTask.debloat(StreamTask.java:792)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$scheduleBufferDebloater$4(StreamTask.java:784)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
{code}
 

JobManager error log is attached.

 

Maybe related to [Flink-25454|https://issues.apache.org/jira/browse/FLINK-25454]",,aliazov,martijnvisser,rmetzger,sharonxr55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/22 05:51;sharonxr55;flink_error.log.txt;https://issues.apache.org/jira/secure/attachment/13043947/flink_error.log.txt",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 15:03:53 UTC 2022,,,,,,,,,,"0|z12j9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 07:33;martijnvisser;I think this is resolved in Flink 1.15, see FLINK-25454;;;","10/Jun/22 10:25;rmetzger;I'm closing this ticket, but please comment on it if it occurs again in newer versions!;;;","14/Jun/22 15:03;aliazov;is it possible to merge this correction also for version 1.14? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the typo of  set_topics_pattern by changing it to set_topic_pattern for Pulsar Connector,FLINK-27711,13445962,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,20/May/22 05:46,21/May/22 09:04,13/Jul/23 08:08,21/May/22 09:03,1.15.0,,,,,,1.15.1,1.16.0,,,API / Python,Connectors / Pulsar,,,,0,pull-request-available,,,Update set_topics_pattern to set_topic_pattern,,ana4,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 21 09:03:15 UTC 2022,,,,,,,,,,"0|z12j9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 09:03;dianfu;Fixed in:
- master via 1ff8d3ec596dc9aebe4d402f6f90902e182ff70e
- release-1.15 via 454f351a2e75cc3e9f39a74a37f95d0d483bf19b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileChannelManagerImplTest.testDirectoriesCleanupOnKillWithoutCallerHook failed with The marker file was not found within 10000 msecs,FLINK-27703,13445831,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,hxbks2ks,hxbks2ks,19/May/22 12:51,14/Jul/22 02:23,13/Jul/23 08:08,14/Jul/22 02:23,1.16.0,,,,,,1.16.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-05-19T09:08:49.8088232Z May 19 09:08:49 [ERROR] Failures: 
2022-05-19T09:08:49.8090850Z May 19 09:08:49 [ERROR]   FileChannelManagerImplTest.testDirectoriesCleanupOnKillWithoutCallerHook:97->testDirectoriesCleanupOnKill:127 The marker file was not found within 10000 msecs
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35834&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9744",,gaoyunhaii,godfreyhe,hxbks2ks,lincoln.86xy,martijnvisser,Weijie Guo,Wencong Liu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 02:23:21 UTC 2022,,,,,,,,,,"0|z12igg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/22 12:07;Wencong Liu;Hello [~hxbks2ks] , I understand from code this test creates a file when start a new process, but it is not completed within 10s. Please confirm whether the test can be repeated locally?  cc [~gaoyunhaii], [~martijnvisser] ;;;","01/Jun/22 12:29;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36190&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702;;;","01/Jun/22 12:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36230&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53;;;","16/Jun/22 09:05;lincoln.86xy;another one: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36776&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","22/Jun/22 11:45;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37037&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8542;;;","28/Jun/22 07:56;gaoyunhaii;I'll have a look. ;;;","05/Jul/22 08:18;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8610

[~gaoyunhaii] Any updates from your end?;;;","08/Jul/22 02:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37849&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","08/Jul/22 03:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37836&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","11/Jul/22 02:24;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","11/Jul/22 02:28;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","11/Jul/22 02:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53;;;","11/Jul/22 02:33;hxbks2ks;HI [~gaoyunhaii], any updates on this issue?;;;","11/Jul/22 05:56;gaoyunhaii;Hi [~hxbks2ks] it seems to me that the file creation is delay either due to the process did not started in time or the file creation is delayed. I'll first extend the timeout and add some logs and keep an eye with the following runs. ;;;","13/Jul/22 10:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38134&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=6689;;;","14/Jul/22 02:23;hxbks2ks;Thanks [~gaoyunhaii] for the investigation.

Merged the commit 4f7ebbb4f91ad27aaf89c8c035031efe91b84171 with extending timeout and adding logs. I will keep an eye on the following runs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RankHarnessTest. testUpdateRankWithRowNumberSortKeyDropsToNotLast test failed with result mismatch,FLINK-27691,13445763,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxbks2ks,hxbks2ks,19/May/22 06:11,29/Dec/22 02:01,13/Jul/23 08:08,29/Dec/22 02:01,1.16.0,1.17.0,,,,,1.16.1,1.17.0,,,Table SQL / Runtime,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-05-19T04:34:04.2677138Z May 19 04:34:04 [ERROR] RankHarnessTest.testUpdateRankWithRowNumberSortKeyDropsToNotLast
2022-05-19T04:34:04.2689553Z May 19 04:34:04 [ERROR]   Run 1: [result mismatch] 
2022-05-19T04:34:04.2690614Z May 19 04:34:04 expected: [+I(a,1,100,1),
2022-05-19T04:34:04.2691128Z May 19 04:34:04     +I(b,1,90,2),
2022-05-19T04:34:04.2691552Z May 19 04:34:04     +I(c,1,90,3),
2022-05-19T04:34:04.2692235Z May 19 04:34:04     +I(d,1,80,4),
2022-05-19T04:34:04.2692634Z May 19 04:34:04     +I(e,1,80,5),
2022-05-19T04:34:04.2693060Z May 19 04:34:04     +I(f,1,70,6),
2022-05-19T04:34:04.2693468Z May 19 04:34:04     +U(b,1,80,5),
2022-05-19T04:34:04.2693874Z May 19 04:34:04     +U(c,1,90,2),
2022-05-19T04:34:04.2694282Z May 19 04:34:04     +U(d,1,80,3),
2022-05-19T04:34:04.2694670Z May 19 04:34:04     +U(e,1,80,4),
2022-05-19T04:34:04.2696097Z May 19 04:34:04     -U(b,1,90,2),
2022-05-19T04:34:04.2696718Z May 19 04:34:04     -U(c,1,90,3),
2022-05-19T04:34:04.2697298Z May 19 04:34:04     -U(d,1,80,4),
2022-05-19T04:34:04.2698102Z May 19 04:34:04     -U(e,1,80,5)]
2022-05-19T04:34:04.2698758Z May 19 04:34:04  but was: [+I(a,1,100,1),
2022-05-19T04:34:04.2699189Z May 19 04:34:04     +I(b,1,90,1),
2022-05-19T04:34:04.2699607Z May 19 04:34:04     +I(c,1,90,2),
2022-05-19T04:34:04.2700017Z May 19 04:34:04     +I(d,1,80,3),
2022-05-19T04:34:04.2712164Z May 19 04:34:04     +I(e,1,80,4),
2022-05-19T04:34:04.2712777Z May 19 04:34:04     +I(f,1,70,5),
2022-05-19T04:34:04.2713191Z May 19 04:34:04     +U(b,1,80,4),
2022-05-19T04:34:04.2713621Z May 19 04:34:04     +U(c,1,90,1),
2022-05-19T04:34:04.2714029Z May 19 04:34:04     +U(d,1,80,2),
2022-05-19T04:34:04.2714435Z May 19 04:34:04     +U(e,1,80,3),
2022-05-19T04:34:04.2715272Z May 19 04:34:04     -U(b,1,90,1),
2022-05-19T04:34:04.2715847Z May 19 04:34:04     -U(c,1,90,2),
2022-05-19T04:34:04.2716420Z May 19 04:34:04     -U(d,1,80,3),
2022-05-19T04:34:04.2716990Z May 19 04:34:04     -U(e,1,80,4)]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35815&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10445
",,godfreyhe,hxb,hxbks2ks,kevin.cyj,leonard,lincoln.86xy,lzljs3620320,mapohl,martijnvisser,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 28 09:50:30 UTC 2022,,,,,,,,,,"0|z12i1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 07:31;godfreyhe;cc [~lzljs3620320] could you help fixing this?;;;","31/May/22 08:13;lzljs3620320;Ah... I try to re-produce it, but can not work, It appears to be a non-random test and should not be unstable.

[~lincoln.86xy] Do you have some comments? I don't know FLINK-24704 is related.;;;","31/May/22 09:09;lincoln.86xy;[~lzljs3620320] Yes, this is a deterministic test and shouldn't get random result. I can't reproduce it locally yet that done a 2000 times repeat testing. And I'll try to  analyze other possibilities.;;;","31/May/22 09:15;godfreyhe;Thanks [~lincoln.86xy];;;","16/Jun/22 12:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36764&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","05/Jul/22 07:23;martijnvisser;Downgrading to Major due to lack of occurrences ;;;","21/Jul/22 01:52;kevin.cyj;[https://dev.azure.com/kevin-flink/flink/_build/results?buildId=582&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f];;;","26/Jul/22 06:39;renqs;An instance on my own Azure pipeline:
https://dev.azure.com/renqs/Apache%20Flink/_build/results?buildId=345&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&l=10284;;;","09/Aug/22 03:47;leonard;https://dev.azure.com/leonardBang/Azure_CI/_build/results?buildId=713&view=ms.vss-test-web.build-test-results-tab&runId=20042&resultId=111227&paneView=debug;;;","09/Aug/22 08:18;yunfengzhou;https://dev.azure.com/yurizhouyunfeng/Flink/_build/results?buildId=50&view=ms.vss-test-web.build-test-results-tab&runId=2840&resultId=111281&paneView=debug;;;","15/Aug/22 06:10;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39961&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","15/Aug/22 06:11;hxbks2ks;Hi [~lincoln.86xy], could you help take an another look? Thanks.;;;","19/Aug/22 10:17;lincoln.86xy;[~hxbks2ks] seems happened when under some workload in ci machine, still cannot reproduce it locally, I'll continue looking into it.;;;","13/Sep/22 03:15;hxb;Given that it hasn't appeared for a month, I will close this issue. Please reopen the ticket if the case appears again. ;;;","02/Dec/22 10:56;mapohl;Reopening the issue. We've seen another test failure of that kind:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43664&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13092;;;","22/Dec/22 08:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44160&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12641;;;","22/Dec/22 08:29;mapohl;[~lzljs3620320] [~godfreyhe] Could you have a look at it?;;;","22/Dec/22 12:49;lincoln.86xy;[~mapohl] I think it is most probably caused by the unnecessary state ttl of RankHarnessTest which will lead unstable result when execution gets slow (and I checked recent failures, all of their execution duration is greater than 2 second while state ttl in the test is 1 second). I submitted a pr, hope this can fix this unstable case.

;;;","23/Dec/22 07:48;mapohl;Thanks [~lincoln.86xy]. Could you find someone to review this PR to double-check whether that's reasonable?;;;","23/Dec/22 08:10;leonard;I'll help review this PR, [~mapohl];;;","28/Dec/22 09:50;leonard;master(1.17):c07167b6f01296da11176818382fa9cdf6f985da
1.16:31f1e235f878a3dcbcf3358a2be92e95c1cef4b6;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Insert into (column1, column2) Values(.....) fails with SQL hints",FLINK-27683,13445610,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuanyu66,xuanyu66,xuanyu66,18/May/22 10:28,17/Jun/22 13:04,13/Jul/23 08:08,13/Jun/22 08:31,1.14.0,1.15.1,1.16.0,,,,1.14.6,1.15.1,1.16.0,,Table SQL / Planner,,,,,0,pull-request-available,,,"When I try to use statement `Insert into (column1, column2) Values(.....)` with SQL hints, it throw some exception, which is certainly a bug.

 
 * Sql 1
{code:java}
INSERT INTO `tidb`.`%s`.`%s` /*+ OPTIONS('tidb.sink.update-columns'='c2, c13')*/   (c2, c13) values(11111, 12.12) {code}

 * 
 ** result 1
!screenshot-1.png!

 * Sql 2
{code:java}
INSERT INTO `tidb`.`%s`.`%s` (c2, c13) /*+ OPTIONS('tidb.sink.update-columns'='c2, c13')*/  values(11111, 12.12)
{code}

 * 
 ** result 2
!screenshot-2.png!

 * Sql 3
{code:java}
INSERT INTO `tidb`.`%s`.`%s` (c2, c13)  values(11111, 12.12)
{code}

 * 
 ** result3 : success",,fsk119,godfreyhe,qingyue,xuanyu66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/22 10:29;xuanyu66;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13043841/screenshot-1.png","18/May/22 10:31;xuanyu66;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13043842/screenshot-2.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 08:31:01 UTC 2022,,,,,,,,,,"0|z12h3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 02:25;fsk119;The main reason is that it tries to cast the SqlTabelRef to SqlIdentifier. ;;;","20/May/22 03:02;xuanyu66;[~fsk119] It's a bug, right?;;;","20/May/22 03:11;fsk119;Yes. I think we can fix this. Are you interested in this issue?;;;","20/May/22 08:29;xuanyu66;[~fsk119] Yes, sure;;;","24/May/22 04:35;fsk119;You can open a PR and ping me when you are ready.;;;","30/May/22 13:08;xuanyu66;[~fsk119] Would you tell me where can I add an integration test for this sql?

`JdbcDynamicTableSinkITCase`?;;;","31/May/22 02:56;xuanyu66;https://github.com/apache/flink/pull/19847;;;","06/Jun/22 06:19;godfreyhe;Thanks for the fix, could you create a PR for release-1.14 since there are some conflicts [~xuanyu66];;;","06/Jun/22 06:21;godfreyhe;Fixed in 
master: 9bcc7fd20420bbf90f4b67d98c521a8ddf775d4e
1.15.1: e0af037d9910b6cfd4cc3fd8937289f939bb6d9b;;;","06/Jun/22 06:46;xuanyu66;[~godfreyhe] OK;;;","07/Jun/22 06:34;xuanyu66;[https://github.com/apache/flink/pull/19892] [~godfreyhe] ;;;","13/Jun/22 08:31;godfreyhe;Fixed in 1.14.6: ca16d28a741434f78fba508d8050786cd3281793;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output records from on_timer are behind the triggering watermark in PyFlink,FLINK-27676,13445574,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,18/May/22 07:44,19/May/22 12:24,13/Jul/23 08:08,19/May/22 12:24,1.14.4,1.15.0,,,,,1.14.5,1.15.1,1.16.0,,API / Python,,,,,0,pull-request-available,,,"Currently, when dealing with watermarks in AbstractPythonFunctionOperator, super.processWatermark(mark) is called, which advances watermark in timeServiceManager thus triggering timers and then emit current watermark. However, timer triggering is not synchronous in PyFlink (processTimer only put data into beam buffer), and when remote bundle is closed and output records produced by on_timer function finally arrive at Java side, they are already behind the triggering watermark.",,hxbks2ks,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 12:24:17 UTC 2022,,,,,,,,,,"0|z12gvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 12:24;hxbks2ks;Merged into master via 124e4adb04196e5d56974aead02e61a9bd5bf2cf
Merged into release-1.15 via f26831b85f90324ce9097f3d18d220f1071d0868
Merged into release-1.14 via a5eee994e6417a9e2f1bb6709b768bf4a039d658;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Older jackson-databind found in /flink-kubernetes-shaded-1.0-SNAPSHOT.jar,FLINK-27654,13445276,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,jbusche,jbusche,16/May/22 22:57,22/May/22 23:15,13/Jul/23 08:08,22/May/22 23:15,kubernetes-operator-0.1.0,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"A twistlock security scan of the latest kubernetes flink operator is showing an older version of jackson-databind in the /flink-kubernetes-shaded-1.0-SNAPSHOT.jar file.  I don't know how to control/update the contents of this snapshot file.  

I see this in the report (Otherwise, everything else looks good!):

======
severity: High

cvss: 7.5 

riskFactors: Attack complexity: low,Attack vector: network,DoS,Has fix,High severity

cve: CVE-2020-36518

Link: [https://nvd.nist.gov/vuln/detail/CVE-2020-36518]

packageName: com.fasterxml.jackson.core_jackson-databind

packagePath: /flink-kubernetes-operator-1.0-SNAPSHOT-shaded.jar

description: jackson-databind before 2.13.0 allows a Java StackOverflow exception and denial of service via a large depth of nested objects.

=====

I'd be glad to try to fix it, I'm just not sure how the jackson-databind versions are controlled in this /flink-kubernetes-operator-1.0-SNAPSHOT-shaded.jar ",,gyfora,jbusche,martijnvisser,mbalassi,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 22 23:15:16 UTC 2022,,,,,,,,,,"0|z12f20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 02:52;wangyang0918;Thanks [~jbusche] for reporting this ticket.

It seems that we are bundling the com.fasterxml.jackson.core:jackson-databind:jar:2.13.2.2:compile in the flink-kubernetes-operator-1.0-SNAPSHOT-shaded.jar. And this version does not have vulnerability.

 

The dependency is introduced by the flink-kubernetes module in the upstream project Flink. You could find the pom here[1].

 

[1]. https://github.com/apache/flink/blob/master/flink-kubernetes/pom.xml#L63;;;","19/May/22 23:54;jbusche;Thanks [~wangyang0918] 

It looks like the kubernetes client version [5.5.0|https://github.com/fabric8io/kubernetes-client/blob/master/CHANGELOG.md#550-2021-06-30] is pretty old.
[5.12.2|https://github.com/fabric8io/kubernetes-client/blob/master/CHANGELOG.md#5122-2022-04-06] is the latest v5 client version...  I can try putting a PR in the upstream flink repo, but I'm not sure how to test it to ensure I'm not breaking something there.  I'd feel badly if I ended up unintentionally breaking both products, and not certain that the 5.12.2 version will update the jackson-databind in the end anyway.

Any suggestions on the best way to proceed? I'm happy to try!

 ;;;","20/May/22 02:39;wangyang0918;[~jbusche] You are right. The kubernetes-client in Flink project is a little old and I am not against with bumping the version. The reason we are lazy to update the kubernetes-client is that Flink only depends some core features(e.g. creating deployment/pod/configmap/service, leader election, watch/informer) and they are stable enough now. Currently, these functionalities has already been covered by the e2e tests in Flink project[1]. It is not a burden to bump the version. If you want to do this, we could create a new dedicated ticket.

I have to clarify one more thing. In Flink project, we do not need to bump the kubernetes-client version to update the jackson-databind. Actually, the version is managed by parent pom[2] via maven dependencyManagement.

This ticket also inspires me to verify the bundled the jackson-databind in the flink-kubernetes-operator module. The version is ""com.fasterxml.jackson.core:jackson-databind:jar:2.13.1:compile"". It is introduced by ""io.fabric8:kubernetes-client:jar:5.12.1:compile"". From the maven repository, 2.13.1 has one known vulnerability[3].

Would you like to create a PR to fix this? I believe it is simple since we could use dependencyManagement in the parent pom to pin the jackson version just like Flink project.


[1]. https://github.com/apache/flink/blob/release-1.15/flink-end-to-end-tests/test-scripts
[2]. https://github.com/apache/flink/blob/release-1.15/pom.xml#L563
[3]. https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind;;;","20/May/22 03:07;wangyang0918;Of cause, we could also bump the kubernetes-client in flink-kubernetes-operator/pom.xml from 5.12.1 to 5.12.2[1]. But it still does not fix the vulnerability and we still need the above dependencyManagement solution.

[1]. https://mvnrepository.com/artifact/io.fabric8/kubernetes-client/5.12.2;;;","20/May/22 03:10;wangyang0918;I am making this ticket as a blocker now since we should fix the known vulnerability before releasing.;;;","22/May/22 03:18;wangyang0918;I am assigning this ticket to myself so that we could prepare the first release candidate on next Monday.;;;","22/May/22 23:10;mbalassi;Thanks for the report [~jbusche] and the fix [~wangyang0918].;;;","22/May/22 23:15;mbalassi;Fixed as 3fee956b628373f2939bf7e0d30fa747da441b92 in main and as 269a964371b52af66441459c3806bd1b89260da4 in release-1.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactManager.Rewriter cannot handle different partition keys invoked compaction,FLINK-27652,13445214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,16/May/22 15:03,29/May/22 13:25,13/Jul/23 08:08,19/May/22 02:09,table-store-0.2.0,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,"h3. Issue Description

When enabling {{commit.force-compact}} for the partitioned managed table, there had a chance that the successive synchronized
writes got failure.  {{rewrite}} method messing up with the wrong data file with the {{partition}} and {{{}bucket{}}}.
{code:java}
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.FileNotFoundException: File file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit5920507275110651781/junit4163667468681653619/default_catalog.catalog/default_database.db/T1/f1=Autumn/bucket-0/data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not exist or the user running Flink ('jane.cjm') has insufficient permissions to access it. at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:172)
{code}
However, data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not belong to partition Autumn. It seems like the rewriter found the wrong partition/bucket with the wrong file.
h3. How to Reproduce
{code:java}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.store.connector;

import org.junit.Test;

import java.util.Collections;
import java.util.List;
import java.util.concurrent.ExecutionException;

/** A reproducible case. */
public class ForceCompactionITCase extends FileStoreTableITCase {

    @Override
    protected List<String> ddl() {
        return Collections.singletonList(
                ""CREATE TABLE IF NOT EXISTS T1 (""
                        + ""f0 INT, f1 STRING, f2 STRING) PARTITIONED BY (f1)"");
    }

    @Test
    public void test() throws ExecutionException, InterruptedException {
        bEnv.executeSql(""ALTER TABLE T1 SET ('num-levels' = '3')"");
        bEnv.executeSql(""ALTER TABLE T1 SET ('commit.force-compact' = 'true')"");
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(1, 'Winter', 'Winter is Coming')""
                                + "",(2, 'Winter', 'The First Snowflake'), ""
                                + ""(2, 'Spring', 'The First Rose in Spring'), ""
                                + ""(7, 'Summer', 'Summertime Sadness')"")
                .await();
        bEnv.executeSql(""INSERT INTO T1 VALUES(12, 'Winter', 'Last Christmas')"").await();
        bEnv.executeSql(""INSERT INTO T1 VALUES(11, 'Winter', 'Winter is Coming')"").await();
        bEnv.executeSql(""INSERT INTO T1 VALUES(10, 'Autumn', 'Refrain')"").await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(6, 'Summer', 'Watermelon Sugar'), ""
                                + ""(4, 'Spring', 'Spring Water')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(66, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(6666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(66666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(666666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(6666666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
    }
}

{code}",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27515,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 02:09:15 UTC 2022,,,,,,,,,,"0|z12eoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 15:04;qingyue;h3. Full Stacktrace
{code:java}
org.apache.flink.table.store.connector.ForceCompactionITCase.test(ForceCompactionITCase.java:65)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
	at akka.dispatch.OnComplete.internal(Future.scala:300)
	at akka.dispatch.OnComplete.internal(Future.scala:297)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.FileNotFoundException: File file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit5920507275110651781/junit4163667468681653619/default_catalog.catalog/default_database.db/T1/f1=Autumn/bucket-0/data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not exist or the user running Flink ('jane.cjm') has insufficient permissions to access it.
	at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:172)
	at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:51)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emitCommittables(SinkWriterOperator.java:196)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.endInput(SinkWriterOperator.java:183)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.FileNotFoundException: File file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit5920507275110651781/junit4163667468681653619/default_catalog.catalog/default_database.db/T1/f1=Autumn/bucket-0/data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not exist or the user running Flink ('jane.cjm') has insufficient permissions to access it.
		at org.apache.flink.table.store.connector.sink.StoreSinkWriter.closeWriter(StoreSinkWriter.java:217)
		at org.apache.flink.table.store.connector.sink.StoreSinkWriter.close(StoreSinkWriter.java:226)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
		at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.close(SinkWriterOperator.java:216)
		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163)
		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
		at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916)
		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930)
		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930)
		... 3 more
{code};;;","17/May/22 02:39;lzljs3620320;There is indeed a bug here, the reason is that the partition object being reused should be copied inside the StoreSinkWriter:
{code:java}
private RecordWriter getWriter(BinaryRowData partition, int bucket) {
    Map<Integer, RecordWriter> buckets = writers.get(partition);
    if (buckets == null) {
        buckets = new HashMap<>();
        writers.put(partition.copy(), buckets);
    }

    return buckets.computeIfAbsent(
            bucket,
            k ->
                    overwrite
                            ? fileStoreWrite.createEmptyWriter(
                                    partition.copy(), bucket, compactExecutor)
                            : fileStoreWrite.createWriter(partition.copy(), bucket, compactExecutor));
} {code};;;","19/May/22 02:09;lzljs3620320;master: bc46116bef59be033db8b85655e53e479113f462;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink not compiling, flink-connector-hive_2.12 is missing jhyde pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde ",FLINK-27640,13445158,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,foxss,pnowojski,pnowojski,16/May/22 12:07,13/Apr/23 09:43,13/Jul/23 08:08,12/Jan/23 12:24,1.16.0,,,,,,1.16.2,1.17.0,,,Build System,Connectors / Hive,,,,0,pull-request-available,stale-assigned,,"When clean installing whole project after cleaning local {{.m2}} directory I encountered the following error when compiling flink-connector-hive_2.12:
{noformat}
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.16-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> [Help 1]
{noformat}
I've solved this by adding 
{noformat}
        <repository>
            <id>spring-repo-plugins</id>
            <url>https://repo.spring.io/ui/native/plugins-release/</url>
        </repository>
{noformat}
to ~/.m2/settings.xml file. ",,foxss,jingge,knaufk,liuml07,luoyuxia,martijnvisser,pnowojski,rmetzger,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27894,FLINK-29201,FLINK-30362,FLINK-28406,FLINK-29404,,,,,,,,FLINK-31658,BIGTOP-3929,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 19:31:47 UTC 2023,,,,,,,,,,"0|z12ec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 14:32;chesnay;FYI this is because you are using a newer maven version than recommended, which blocks http repositories.

The conjars repo is defined in the hive poms. We could override those, but it's not really ideal because this is only required for hive 2.X, and not 3.X that we also support.

;;;","19/May/22 11:23;chesnay;You could add this to your maven settings.xml:

{code}
<settings>
    <mirrors>
        <mirror>
            <id>conjars-https</id>
            <url>https://conjars.org/repo/</url>
            <mirrorOf>conjars</mirrorOf>
        </mirror>
    </mirrors>
</settings>
{code}

This has the advantage that we don't have to sprinkle the repo setting into different modules, and we'll continue to use conjars only for pentaho and nothing else. (If we were to set it in the poms then Maven would try to download everything from there first in the respective module).;;;","21/May/22 06:58;rmetzger;I just ran into this too. It is a bit annoying for first time contributors I guess.

In FLINK-26034, we added a maven wrapper. I'll raise a PR to update the build instructions in the README;;;","05/Jul/22 15:21;jingge;[~rmetzger] have you fired the PR to update the README? Would you like to link it to this Jira? Thanks!;;;","04/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Aug/22 19:04;liuml07;It failed to build with the same error when I used the mvn wrapper as well.
{code:java}
$ ./mvnw -v
Apache Maven 3.2.5...
$ ./mvnw clean package -DskipTests{code}
Adding the {{conjars}} mirror in settings.xml did not resolve the problem. I use ARM64 macOS but I do not think that matters.;;;","11/Dec/22 16:51;foxss;shall we exclude org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde instead, I don't think this package is used.

here is how hudi community fix  https://github.com/apache/hudi/pull/3034;;;","20/Dec/22 15:19;martijnvisser;[~luoyuxia] Do you know if this package is needed in the Hive connector, or can it safely be excluded?;;;","21/Dec/22 01:53;luoyuxia;I think it can safely be excluded since org.pentaho:pentaho-aggdesigner-algorithm is just a third level transitive dependency (hive-exec - > calcite-core -> pentaho-aggdesigner-algorithm) and I think we won't need it.;;;","11/Jan/23 14:07;martijnvisser;Fixed in master: 78ab08bfc8e28f36df77fdb1be06831a98bcc43d;;;","11/Jan/23 16:22;martijnvisser;Re-opening because pentaho still appears in the dependency tree, probably an exclusion was missed;;;","12/Jan/23 12:24;martijnvisser;Fixed in master: 881627dcda9f5cdea9e095d7aff854fbf9508785;;;","29/Mar/23 19:31;Sergey Nuyanzin;Fixed in 1.16: 36f39712d289b6b7b0b957a33bd1891266e3f992;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink JOIN uses the now() function when inserting data, resulting in data that cannot be deleted",FLINK-27639,13445157,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lvycc,lvycc,16/May/22 12:05,25/Aug/22 07:20,13/Jul/23 08:08,25/Aug/22 07:20,1.14.4,,,,,,1.16.0,,,,Table SQL / API,,,,,0,,,,"I use the now() function as the field value when I insert data using SQL ，but I can't delete the inserted data，here is my sql：
{code:java}
//代码占位符
CREATE TABLE t_order (
    order_id INT,
    order_name STRING,
    product_id INT,
    user_id INT,
    PRIMARY KEY(order_id) NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'localhost',
    'port' = '3306',
    'username' = 'root',
    'password' = 'ycc123',
    'database-name' = 'wby_test',
    'table-name' = 't_order'
);
CREATE TABLE t_logistics (
    logistics_id INT,
    logistics_target STRING,
    logistics_source STRING,
    logistics_time TIMESTAMP(0),
    order_id INT,
    PRIMARY KEY(logistics_id) NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'localhost',
    'port' = '3306',
    'username' = 'root',
    'password' = 'ycc123',
    'database-name' = 'wby_test',
    'table-name' = 't_logistics'
);
CREATE TABLE t_join_sink (
    order_id INT,
    order_name STRING,
    logistics_id INT,
    logistics_target STRING,
    logistics_source STRING,
    logistics_time timestamp,
    PRIMARY KEY(order_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/wby_test?characterEncoding=utf8&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai',
    'table-name' = 't_join_sink',
    'username' = 'root',
    'password' = 'ycc123'
);
INSERT INTO t_join_sink
SELECT ord.order_id,
ord.order_name,
logistics.logistics_id,
logistics.logistics_target,
logistics.logistics_source,
now()
FROM t_order AS ord
LEFT JOIN t_logistics AS logistics ON ord.order_id=logistics.order_id; {code}
The debug finds that SinkUpsertMaterializer causes the problem ，the result of the now() function changes when I delete the data，therefore, the delete operation is ignored

But what can I do to avoid this problem？",,fsk119,lincoln.86xy,lvycc,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27849,,,,,,,,,,,FLINK-27849,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 07:08:40 UTC 2022,,,,,,,,,,"0|z12ec0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 03:19;fsk119;Hi. Could you also share the test data with us and we can also test in our local environment?;;;","25/May/22 03:47;lvycc;Run  init  sql in Mysql:
{code:java}
//代码占位符
CREATE TABLE `t_order`  (
  `order_id` int NOT NULL AUTO_INCREMENT COMMENT '订单id',
  `order_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL COMMENT '订单名称',
  `product_id` int NULL DEFAULT NULL COMMENT '商品id',
  `user_id` int NULL DEFAULT NULL COMMENT '用户id',
  PRIMARY KEY (`order_id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 7 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of t_order
-- ----------------------------
INSERT INTO `t_order` VALUES (1, '李四买吹风机4', 1, 2);
INSERT INTO `t_order` VALUES (2, '王二买电饭煲4', 2, 1);
INSERT INTO `t_order` VALUES (3, '小明买口罩4', 3, 4);
INSERT INTO `t_order` VALUES (4, '张三买吹风机3', 1, 3);

CREATE TABLE `t_logistics`  (
  `logistics_id` int NOT NULL AUTO_INCREMENT,
  `logistics_target` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_source` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_time` datetime(0) NULL DEFAULT NULL,
  `order_id` int NULL DEFAULT NULL,
  PRIMARY KEY (`logistics_id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 6 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of t_logistics
-- ----------------------------
INSERT INTO `t_logistics` VALUES (1, '湖南省长沙市', '广东省揭阳市', '2022-04-07 11:46:52', 1);
INSERT INTO `t_logistics` VALUES (2, '广东省深圳市', '浙江省杭州市', '2022-04-06 11:48:09', 2);
INSERT INTO `t_logistics` VALUES (3, '北京市', '山东省青岛市', '2022-04-06 11:48:50', 3);
INSERT INTO `t_logistics` VALUES (4, '上海市', '广东省揭阳市', '2022-04-07 11:49:24', 4);

CREATE TABLE `t_join_sink`  (
  `order_id` int NOT NULL,
  `order_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_id` int NULL DEFAULT NULL,
  `logistics_target` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_source` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_time` datetime(0) NULL DEFAULT NULL,
  PRIMARY KEY (`order_id`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic; {code}
Then start the flink  job, in the end, run delete sql
{code:java}
//代码占位符
delete from t_order where id = 1;{code}
the table `t_join_sink` not delete;;;","31/May/22 07:00;lincoln.86xy;Hi [~lvycc], thanks for reporting this!  

For now, a fast solution is remove the temporal function 'now()' before join operation.

It's a long-standing issue in streaming, and we're planning to solve it in FLINK-27849.;;;","31/May/22 07:08;lvycc;OK, Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store throws NullPointerException when pushing down NotEqual predicate to a column consisting of nulls,FLINK-27629,13445089,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,16/May/22 06:39,16/May/22 06:58,13/Jul/23 08:08,16/May/22 06:58,table-store-0.2.0,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,"Run the following Flink SQL to reproduce this issue.
{code}
Flink SQL> create table S ( a double ) with ( 'path' = '/tmp/store' );
[INFO] Execute statement succeed.

Flink SQL> insert into S values (cast(null as double)), (cast(null as double));
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: edb2ce383c00b2f635759dee70add73d


Flink SQL> select * from S where a <> 1;
[ERROR] Could not execute SQL statement. Reason:
java.util.concurrent.ExecutionException: java.lang.NullPointerException
{code}

The exception stack is
{code}
Caused by: java.lang.NullPointerException
	at java.lang.Double.compareTo(Double.java:978) ~[?:1.8.0_151]
	at java.lang.Double.compareTo(Double.java:49) ~[?:1.8.0_151]
	at org.apache.flink.table.store.file.predicate.Literal.compareValueTo(Literal.java:60) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at org.apache.flink.table.store.file.predicate.NotEqual.test(NotEqual.java:50) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at org.apache.flink.table.store.file.operation.FileStoreScanImpl.filterManifestEntry(FileStoreScanImpl.java:287) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174) ~[?:1.8.0_151]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[?:1.8.0_151]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721) ~[?:1.8.0_151]
	at java.util.stream.AbstractTask.compute(AbstractTask.java:316) ~[?:1.8.0_151]
	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_151]
	at org.apache.flink.table.store.file.operation.FileStoreScanImpl.lambda$plan$3(FileStoreScanImpl.java:221) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_151]
{code}",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 06:58:39 UTC 2022,,,,,,,,,,"0|z12dww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 06:58;lzljs3620320;master: c88da6c39877f30bb4a7b4dc1ca5710f93a188eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store records and fetches incorrect results with NaN,FLINK-27628,13445071,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,TsReaper,TsReaper,16/May/22 04:48,29/Mar/23 01:52,13/Jul/23 08:08,29/Mar/23 01:52,table-store-0.2.0,,,,,,,,,,Table Store,,,,,0,,,,"Use the following test data and SQL to reproduce this issue.

gao.csv:
{code}
1.0,2.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
1.0,1.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
1.0,0.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
-1.0,0.0,aaaaaaaaaaaaaaa
1.0,-1.0,aaaaaaaaaaaaaaa
1.0,-2.0,aaaaaaaaaaaaaaa
{code}

Flink SQL:
{code}
Flink SQL> create table T ( a double, b double, c string ) WITH ( 'connector' = 'filesystem', 'path' = '/tmp/gao.csv', 'format' = 'csv' );
[INFO] Execute statement succeed.

Flink SQL> create table S ( a string, b double ) WITH ( 'path' = '/tmp/store' );
[INFO] Execute statement succeed.

Flink SQL> insert into S select c, a / b from T;
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 851d7b3c233061733bdabbf30f20d16f


Flink SQL> select c, a / b from T;
+-----------------+-----------+
|               c |    EXPR$1 |
+-----------------+-----------+
| aaaaaaaaaaaaaaa |       0.5 |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |       1.0 |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |  Infinity |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa |      -1.0 |
| aaaaaaaaaaaaaaa |      -0.5 |
+-----------------+-----------+
9 rows in set

Flink SQL> select * from S;
+-----------------+-----------+
|               a |         b |
+-----------------+-----------+
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa |      -1.0 |
| aaaaaaaaaaaaaaa |      -0.5 |
+-----------------+-----------+
9 rows in set
{code}

Note that this issue may also affect {{FieldStatsCollector}}.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27627,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 04:48:59.0,,,,,,,,,,"0|z12dsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during Flink-Pulsar checkpoint notification,FLINK-27611,13444936,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,longtimer,longtimer,14/May/22 04:38,16/Sep/22 01:36,13/Jul/23 08:08,16/Sep/22 01:36,1.15.0,,,,,,1.14.6,1.15.3,1.16.0,,Connectors / Pulsar,,,,,0,pull-request-available,,,"When attempting to run a job that was working in 1.12.7, but upgraded to 1.15.0, the following exception is occurring outside of the control of my own code:

 
java.util.ConcurrentModificationException
    at java.base/java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1208)
    at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1244)
    at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1239)
    at org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.notifyCheckpointComplete(PulsarUnorderedSourceReader.java:129)
    at org.apache.flink.streaming.api.operators.SourceOperator.notifyCheckpointComplete(SourceOperator.java:511)
    at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1384)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$14(StreamTask.java:1325)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1364)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.base/java.lang.Thread.run(Thread.java:829)",,longtimer,martijnvisser,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 16 01:36:35 UTC 2022,,,,,,,,,,"0|z12cyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 11:26;martijnvisser;[~syhily] Can you take a look?;;;","16/May/22 21:42;syhily;I can. This should be a bug on the Pulsar connector. It affects all the available releases.;;;","24/May/22 14:00;longtimer;Does the most recent comment mean that it should go to the Pulsar project or is the issue with code in the Flink project?;;;","24/May/22 14:17;chesnay;It is an issue on the Flink side.;;;","24/May/22 14:20;chesnay;This looks a bit like a basic modify-collection-within-a-loop issue. May be trivial to fix by working against an {{Iterator}}.;;;","23/Jun/22 19:44;longtimer;Is there a possibility of this getting into the 1.15.1 patch release? I do know the process for inclusion but since this affects all available releases, it really blocks usage with Pulsar.;;;","24/Jun/22 06:39;martijnvisser;[~longtimer] The voting on Flink 1.15.1 is already happening, so that won't be possible. This can potentially go in Flink 1.15.2 but that also depends on the overall Pulsar stability (which is currently not in the state where it should be);;;","18/Aug/22 12:36;longtimer;May I ask why this fix was not included in the 1.15.2 release? It really blocks usage of 1.15 for us.;;;","29/Aug/22 08:28;syhily;[~longtimer] Sorry. we are working on the test stable. This would be included in next release.;;;","29/Aug/22 12:21;martijnvisser;[~longtimer] Most simply said because it wasn't completed yet. It's being worked on as [~syhily] mentioned, but releases are not bound to open tickets unless they are considered Blockers (which this ticket isn't). ;;;","30/Aug/22 20:15;longtimer;Understood, unfortunately, it is a blocker for us, just not one for the larger Flink ecosystem.;;;","16/Sep/22 01:36;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompileException when using UDAF with merge() method,FLINK-27606,13444841,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,13/May/22 12:28,17/Jun/22 13:03,13/Jul/23 08:08,08/Jun/22 09:56,1.15.0,1.16.0,,,,,1.15.1,1.16.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,"If the SQL contains a UDAF with {{merge()}} method, the job will throw an exception like following in cluster mode. 

Flink generates code to use {{SingleElementIterator}} class to call merge() of UDAF. However, the {{SingleElementIterator}} is in flink-table-planner module and planner scala-free was introduced since release 1.15. Therefore, user classloader can't find {{SingleElementIterator}} class. 

We can move {{SingleElementIterator}} to flink-table-runtime to fix this problem. 

{code}
2022-05-10 17:41:46,621 WARN  org.apache.flink.table.runtime.generated.GeneratedClass      [] - Failed to compile split code, falling back to original code
  org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
      at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.open(MiniBatchGlobalGroupAggFunction.java:142) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.open(AbstractMapBundleOperator.java:86) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at java.lang.Thread.run(Thread.java:834) [?:?]
  Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      ... 14 more
  Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
      at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      ... 14 more
  Caused by: org.codehaus.commons.compiler.CompileException: Line 11, Column 28: Cannot determine simple type name ""org""
      at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
{code}",,jark,lsy,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27968,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 03:03:47 UTC 2022,,,,,,,,,,"0|z12cds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 03:03;jark;Fixed in 
 - master: 24cb706c20559e1921d95d8137183bf931f62911
 - 1.15.1: ca05ad9f0911500d90d5e3b8e8e9fb9fed60482e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink sql read hive on hbase throw NPE,FLINK-27604,13444817,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,18579099920@163.com,18579099920@163.com,13/May/22 10:45,11/Oct/22 02:54,13/Jul/23 08:08,11/Oct/22 02:54,1.13.6,,,,,,,,,,Connectors / Hive,,,,,0,,,,"I have some table data on hbase, I usually read the hbase data by loading external tables through hive, I want to read the data through flink sql by reading hive tables, when I try with sql-client I get an error. I don't know if there is any way to solve this problem, but I can read the data using the spark engine.
----
Environment：
flink:1.13.6
hive:2.1.1-cdh6.2.0
hbase:2.1.0-cdh6.2.0
flinksql Execution tools：flink sql client 
sql submit mode：yarn-per-job
----
flink lib directory
antlr-runtime-3.5.2.jar
flink-csv-1.13.6.jar
flink-dist_2.11-1.13.6.jar
flink-json-1.13.6.jar
flink-shaded-zookeeper-3.4.14.jar
flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar
flink-table_2.11-1.13.6.jar
flink-table-blink_2.11-1.13.6.jar
guava-14.0.1.jar
hadoop-mapreduce-client-core-3.0.0-cdh6.2.0.jar
hbase-client-2.1.0-cdh6.2.0.jar
hbase-common-2.1.0-cdh6.2.0.jar
hbase-protocol-2.1.0-cdh6.2.0.jar
hbase-server-2.1.0-cdh6.2.0.jar
hive-exec-2.1.1-cdh6.2.0.jar
hive-hbase-handler-2.1.1-cdh6.2.0.jar
htrace-core4-4.1.0-incubating.jar
log4j-1.2-api-2.17.1.jar
log4j-api-2.17.1.jar
log4j-core-2.17.1.jar
log4j-slf4j-impl-2.17.1.jar
protobuf-java-2.5.0.jar
----
step:
hive create table stament:
{code:java}
CREATE EXTERNAL TABLE `ods`.`student`(
  `row_key` string, 
  `name` string,
  `age` int,
  `addr` string 
) 
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key,FINAL:NAME,FINAL:AGE,FINAL:ADDR,'serialization.format'='1')
TBLPROPERTIES (
  'hbase.table.name'='ODS:STUDENT'); {code}
catalog：hive catalog 
sql: select * from ods.student;
----
error:
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Could not execute SQL statement.
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:215) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:235) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:479) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:412) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.lambda$executeStatement$0(CliClient.java:327) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_191]
    at org.apache.flink.table.client.cli.CliClient.executeStatement(CliClient.java:327) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:297) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:221) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
Caused by: org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:100) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:212) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:207) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:123) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:96) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:114) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:70) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:791) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1225) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:90) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    ... 12 more
Caused by: java.lang.NullPointerException
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_191]
    at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_191]
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:94) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:212) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:207) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:123) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:96) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:114) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:70) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:791) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1225) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:90) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    ... 12 more
{code}",,18579099920@163.com,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28342,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 11 02:53:10 UTC 2022,,,,,,,,,,"0|z12c8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 02:53;luoyuxia;[~18579099920@163.com] Thanks for reporting it. Currently, it's not supported to hbase data via Hive in Flink.  But I think we may need to support it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only recover JM deployment if HA metadata available,FLINK-27594,13444616,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,12/May/22 13:05,16/May/22 09:37,13/Jul/23 08:08,16/May/22 09:37,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,,,,"This ticket is related to https://issues.apache.org/jira/browse/FLINK-27572

The deployment recovery logic for list jobmanager deployments simply performs a restoreFromLasteSavepoint operation currently.

This is incorrect in cases where the HA metadata is not available as it might lead to accidentally restoring from an older state.

We should verify that HA metadata is present and simply perform a deployOperation. Once we have this we can actually make the recovery default true for all versions.",,gyfora,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 09:37:41 UTC 2022,,,,,,,,,,"0|z12azs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 13:06;gyfora;cc [~wangyang0918] ;;;","12/May/22 17:54;thw;[~gyfora] are you saying that when no HA metadata is available and the upgrade mode is LAST_STATE then the operator should keep the deployment in error state? I think that would be correct. When the upgrade mode is SAVEPOINT, then it can go back to that savepoint?

I also think that with LAST_STATE we should pick either last checkpoint or savepoint, whichever is more recent.;;;","13/May/22 08:57;gyfora;Yes, [~thw] what you are saying is part of that. All in all I am working on hardening and at the same type simplifying the logic with these sanity checks around HA data.

The ultimate goal is to avoid situations where you accidentally restore from empty state or a stale savepoint bacause the HA data disappeared for some reason.;;;","16/May/22 09:37;gyfora;merged to main a0aca64bff2c9dc355ed2f9a907f861ffaf556df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The param client.timeout can not be set by dynamic properties when stopping the job ,FLINK-27579,13444545,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,Jiangang,Jiangang,12/May/22 08:23,27/Oct/22 09:36,13/Jul/23 08:08,26/Jul/22 09:18,1.16.0,,,,,,1.15.3,1.16.0,,,Client / Job Submission,,,,,0,pull-request-available,,,"The default client.timeout value is one minute which may be too short when stop-with-savepoint for big state jobs.

When we stop the job by dynamic properties(-D or -yD for yarn), the client.timeout is not effective.

From the code, we can see that the dynamic properties are only effect for run command. We should support it for stop command.",,Jiangang,paul8263,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29749,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 02:36:16 UTC 2022,,,,,,,,,,"0|z12ak0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 06:30;paul8263;Hi [~Jiangang] ,

The option `client.timeout` is read only from flink configuration file. The dynamic properties take no effect.

I would like to solve this problem.;;;","25/May/22 01:59;Jiangang;[~paul8263] Thanks for the pr. We need a flink committer to verify the problem. [~wangyang0918] , could you please have a look. Thanks.;;;","25/May/22 03:52;wangyang0918;The root cause is we do not get the {{clientTimeout}} from effective configuration when doing stop/list/etc. operations. Right?;;;","25/May/22 09:16;Jiangang;[~wangyang0918] Yes, you are right. The method getEffectiveConfiguration is not called for some actions, like stop and cancel. Although most of configs take effect when the job starts, there exist some configs in other actions. The config clientTimeout is just one.

In this case, I think that the simplest way to fix is to fetch clientTimeout from the effectiveConfiguration. If we consider more, all the actions should get configs from effectiveConfiguration instead of the initial configuration in the constructor. 

What do you think?

 ;;;","25/May/22 13:07;wangyang0918;Yes. I lean to fix this issue by get the clientTimeout from effective configuration, not the original conf.;;;","26/May/22 07:36;Jiangang;[~paul8263] Would you like to solve it? If so, you can refer to the discussion. If not, I would like to solve it. Thanks.;;;","27/May/22 00:28;paul8263;Hi [~Jiangang] ,

I would like to do this. Please have a look at PR  [#19722|https://github.com/apache/flink/pull/19772].Thanks.;;;","01/Jun/22 13:22;Jiangang;[~wangyang0918] Can you review the code? Thanks.;;;","05/Jul/22 06:12;paul8263;Hi [~wangyang0918] ，

Could you help review the code please? Thanks.;;;","06/Jul/22 09:25;wangyang0918;Really sorry for the late response. I will get to this PR in this week.;;;","12/Jul/22 01:05;paul8263;Hi [~wangyang0918] ,

Thank you very much for your suggestions and code review.

If there is something need to update in this PR, please let me know.;;;","26/Jul/22 09:18;wangyang0918;Fixed via:

master: c4b107835c8bdae0667efcbdfe52aa3a34aec894;;;","26/Jul/22 09:19;wangyang0918;Thanks [~paul8263] and [~Jiangang] for your contribution.;;;","27/Oct/22 02:36;wangyang0918;When working for FLINK-29749, I realize that this ticket also needs to be backported to release-1.15. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify HA Metadata present before performing last-state restore,FLINK-27572,13444345,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,11/May/22 08:28,19/May/22 10:21,13/Jul/23 08:08,16/May/22 09:37,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"When we restore a job using the last-state logic we need to verify that the HA metadata has not been deleted. And if it's not there we need to simply throw an error because this requires manual user intervention.

This only applies when the FlinkDeployment is not already in a suspended state with recorded last state information.

The problem be reproduced easily in 1.14 by triggering a fatal job error. (turn of restart-strategy and kill TM for example). In these cases HA metadata will be removed, and the next last-state upgrade should throw an error instead of restoring from a completely empty state. ",,aitozi,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 09:37:06 UTC 2022,,,,,,,,,,"0|z129c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 08:28;gyfora;cc [~wangyang0918] [~matyas] ;;;","12/May/22 03:34;wangyang0918;This is only necessary for Flink 1.14 and previous versions since FLINK-27495 will cover the 1.15 and later. Right?

Not considering the ZK HA, maybe we could simply verify the existence of HA ConfigMaps.

Another question is how could the users fix this manually? They need to find out the latest external checkpoint and specify it via {{{}execution.savepoint.path{}}}.;;;","12/May/22 05:29;gyfora;[~wangyang0918] It is most useful in 1.14 but due to the ResultStore limitations as we have seen there are cases when its required also in 1.15 (job completes/fails and jobmanager pod dies before the first subsequent observe).

I think verifying the existence of the configmaps should be enough yes.

Yes manual fix is to find the latest external checkpoint/savepoint manually but I think you need to delete the flinkdeployment resource completely and recreate while specifying initialSavepointPath. The savepoint config that you mentioned is basically ignored the way we use it.;;;","16/May/22 09:37;gyfora;merged to main a0aca64bff2c9dc355ed2f9a907f861ffaf556df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Recognize ""less is better"" benchmarks in regression detection script",FLINK-27571,13444342,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,roman,roman,11/May/22 08:22,08/Feb/23 10:45,13/Jul/23 08:08,08/Feb/23 10:45,1.16.0,,,,,,1.17.0,,,,Benchmarks,,,,,0,pull-request-available,,,"Example benchmark:

[http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=schedulingDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200]

 

[Proposed solution|https://issues.apache.org/jira/browse/FLINK-27555?focusedCommentId=17534423&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17534423]:
{quote}
I think #2 is the correct way.
Maybe we can modify the save_jmh_result.py to correctly set the 'units' and the 'lessisbetter' fields of benchmark results. The 'units' is already contained in the jmh result and the 'lessisbetter' can be derived from the mode(false if it is 'thrpt' mode, otherwise true). An example of the jmh result format can be found at https://i.stack.imgur.com/vB3fV.png.
This can fix the web UI as well as the REST result, and then the regression_report.py will be able to identify which benchmarks are ""less is better"" and treat them differently.
{quote}

 ",,martijnvisser,pnowojski,roman,smattheis,Thesharing,Yanfei Lei,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29825,,FLINK-27555,,,,,,,,,,,,,,,,,,,,,,"11/May/22 08:22;roman;Screenshot_2022-05-09_10-33-11.png;https://issues.apache.org/jira/secure/attachment/13043515/Screenshot_2022-05-09_10-33-11.png","29/Dec/22 06:40;Yanfei Lei;image-2022-12-29-14-39-59-976.png;https://issues.apache.org/jira/secure/attachment/13054203/image-2022-12-29-14-39-59-976.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 08 10:45:07 UTC 2023,,,,,,,,,,"0|z129bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 06:56;Yanfei Lei;Hi [~roman] , could you please help take a look [pull/63|https://github.com/apache/flink-benchmarks/pull/63] in your free time? thanks :)

 

Thanks to [pull/55|https://github.com/apache/flink-benchmarks/pull/55], the information for future new benchmarks will be correctly stored.

But for the old existing benchmarks, their information cannot be changed only through ‘http://url/result/add/json/’.  Because *codespeed* stores the basic information of benchmark when a result is added {*}for the first time{*}, and will not update it later( related code is [here|https://github.com/tobami/codespeed/blob/263860bc298fd970c8466b3161de386582e4f801/codespeed/results.py#L61]).  So I logged into the Jenkins master and modified the `codespeed_benchmark` table in SQLite, currently ""less is better"" can be displayed normally on the [timeline|http://codespeed.dak8s.net:8000/timeline/#/?ben=schedulingDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200&exe=1,3,5,6,8,9]. ;;;","29/Dec/22 23:20;roman;Thanks for picking this up [~Yanfei Lei]!
I'll take a look.;;;","08/Feb/23 10:45;pnowojski;merged commit b21a946 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint path error does not cause the job to stop,FLINK-27570,13444341,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Underwood,Underwood,Underwood,11/May/22 08:12,12/Aug/22 07:43,13/Jul/23 08:08,12/Aug/22 07:41,1.14.4,1.15.0,1.16.0,,,,1.15.2,1.16.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"I configured the wrong checkpoint path when starting the job, and set：
{code:java}
conf.set (executioncheckpointingoptions. Tolerable_failure_number, 0);
env setRestartStrategy(RestartStrategies.noRestart());
{code}
The job is expected to stop due to a checkpoint error, but the job is still running.

Here is my job configuration and environment：

!image-2022-05-11-16-13-20-709.png!

!image-2022-05-11-16-12-11-818.png!

!image-2022-05-11-16-12-22-157.png!",,roman,Underwood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/22 08:12;Underwood;image-2022-05-11-16-12-11-818.png;https://issues.apache.org/jira/secure/attachment/13043512/image-2022-05-11-16-12-11-818.png","11/May/22 08:12;Underwood;image-2022-05-11-16-12-22-157.png;https://issues.apache.org/jira/secure/attachment/13043513/image-2022-05-11-16-12-22-157.png","11/May/22 08:13;Underwood;image-2022-05-11-16-13-20-709.png;https://issues.apache.org/jira/secure/attachment/13043514/image-2022-05-11-16-13-20-709.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 07:41:14 UTC 2022,,,,,,,,,,"0|z129b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 07:43;Zsigner;Hi 
Can you provide the details of the checkpoint page? If you enable checkpoint and fill in the wrong path, the checkpoint should not be successful and a hdfs file should be generated.
 ;;;","11/Aug/22 14:48;Underwood;[~martijnvisser] [~MartijnVisser] 

I submitted a PR to solve this problem, could you please take a review?

[https://github.com/apache/flink/pull/20091];;;","12/Aug/22 07:41;roman;Merged into master as 88b309b7dcad269ad084eab5e2944724daf6dee4,
into 1.15 as b24c51611628807ecb78337dc7adae70d359c5b5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Resource Providers - Yarn doc page has minor display error,FLINK-27563,13444141,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wanglijie,zhongwei,zhongwei,10/May/22 09:45,11/May/22 07:01,13/Jul/23 08:08,11/May/22 07:01,1.15.0,,,,,,1.15.1,1.16.0,,,Documentation,,,,,0,pull-request-available,,,"doc link: [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/yarn/]

screen shot:

!image-2022-05-10-17-44-37-241.png|width=811,height=301!",,wanglijie,wangyang0918,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/22 09:44;zhongwei;image-2022-05-10-17-44-37-241.png;https://issues.apache.org/jira/secure/attachment/13043458/image-2022-05-10-17-44-37-241.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 07:01:56 UTC 2022,,,,,,,,,,"0|z12834:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 11:03;wanglijie;This was introduced in [PR18450|https://github.com/apache/flink/pull/18450] , I will fix it.

 ;;;","11/May/22 07:01;wangyang0918;Fixed via:

master: 061f596096156231b7f899583cb26b678896594a

release-1.15: e779f3c7cc7b84e0deb09a5f605607a64a6e28e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in checkpointSingleInput.UNALIGNED on 29.04.2022,FLINK-27556,13443935,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,09/May/22 08:35,08/Jul/22 15:00,13/Jul/23 08:08,08/Jul/22 15:00,1.16.0,,,,,,1.16.0,,,,Benchmarks,Runtime / Checkpointing,,,,0,pull-request-available,,,http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200,,akalashnikov,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25511,,,,,,,,,,,,,,"09/May/22 08:36;roman;Screenshot_2022-05-09_10-35-57.png;https://issues.apache.org/jira/secure/attachment/13043408/Screenshot_2022-05-09_10-35-57.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 08 15:00:42 UTC 2022,,,,,,,,,,"0|z126ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 08:44;roman;[~akalashnikov] , [~dwysakowicz] could you please take a look at this?;;;","09/May/22 13:04;akalashnikov;I will take a look;;;","19/May/22 11:38;akalashnikov;[~roman], as I can see the cause of this regression is FLINK-25511(the first commit 5b58ef66a8835bf22db23e1d9836a1e9f4d94045). More precisely, the reason is a new field in ByteStreamStateHandle. As I understand, our microbenchmark is too sensitive to even such a small extra data which we write to each checkpoint now. So we need to think about how it is important for us.
Btw, right now we write 'handleName' twice - directly in 'ByteStreamStateHandle' and inside 'physicalID'. Was it the target? or we can avoid duplication?;;;","25/May/22 10:01;roman;Thanks for looking into it [~akalashnikov].
I'm assuming by writing you mean Java serialization, not MetadataV2V3SerializerBase. That wasn't intended.
I could try to revert https://github.com/apache/flink/pull/19550#discussion_r858603609 and see whether it helps;;;","08/Jul/22 15:00;roman;Merged into master as 231e96b205edadcf9cbccf19d5cd1b414c2eadea.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update examples in PyFlink shell,FLINK-27545,13443877,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,09/May/22 02:31,09/May/22 06:15,13/Jul/23 08:08,09/May/22 06:15,1.14.0,1.15.0,,,,,1.14.5,1.15.1,1.16.0,,API / Python,Examples,,,,0,pull-request-available,,,The examples in pyflink.shell is outdated and we should update it.,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 06:15:40 UTC 2022,,,,,,,,,,"0|z126hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 06:15;dianfu;Fixed in:
- master via da0e2587a92d60098bc76f55bbc95d5de55e5a40
- release-1.15 via bc0cdc5d63068543c0a91b6977e02de659ebd080
- release-1.14 via 23c11ac50931f86b7040111592af7a9b497b517f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor migration tests to support version update automatically,FLINK-27518,13443518,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,06/May/22 04:12,12/Jul/23 07:04,13/Jul/23 08:08,12/May/23 07:02,1.16.0,,,,,,1.18.0,,,,Test Infrastructure,,,,,0,pull-request-available,,,"Currently on releasing each version, we need to manually generate the snapshots for every migration tests and update the current versions. With more and more migration tests are added, this has been more and more intractable. It is better if we could make it happen automatically on cutting new branches. ",,gaoyunhaii,leonard,mapohl,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29485,,,,,,,FLINK-32582,,,,,FLINK-31567,FLINK-31593,,,,FLINK-30944,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 12 07:01:52 UTC 2023,,,,,,,,,,"0|z124a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 11:04;mapohl;FYI: I closed the [PR draft for FLINK-29485|https://github.com/apache/flink/pull/20954]. Feel free to use code segments from there if it's of any help.;;;","25/Nov/22 08:00;mapohl;[~gaoyunhaii] any updates on this effort?;;;","30/Nov/22 06:28;gaoyunhaii;Hi [~mapohl] sorry I'm a bit bound in recent weeks and I'll try to finish this issue as soon as possible. ;;;","30/Nov/22 06:29;mapohl;That's alright. Let me know if you need help.;;;","02/Jan/23 12:34;mapohl;Any updates on that topic, [~gaoyunhaii]? It would be nice to have that in before 1.17 is released.;;;","03/Jan/23 06:54;gaoyunhaii;Got that, I'm now a bit freed and will improve the priority of this issue. I'll open the PR before early next week. ;;;","03/Jan/23 09:23;mapohl;Awesome, thanks~;;;","18/Jan/23 10:34;gaoyunhaii;Sorry for it takes more time than I thought to finish the PR, and I'll open the PR later today. ;;;","12/May/23 07:01;gaoyunhaii;Merged on master via 82fb74e23b1dfa46fa98c89a58d7f3126aeaec6c. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation error handling inside controller blocks reconciliation,FLINK-27500,13443349,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,05/May/22 10:38,09/May/22 05:12,13/Jul/23 08:08,08/May/22 10:55,kubernetes-operator-1.0.0,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Currently when using the operator without the Webhook (validating only within the controller) , the way we handle validation errors completely blocks reconciliation.

The reason for this is that validation happens between observe and reconciliation and an error short-circuits the controller flow thus skipping the reconciler which would be able to execute actions such as rollbacks, deployment-recovery etc.

We also return an UpdateControl without reschedule after an error which makes this even worse.

There are a few ways to get around this some are more complex than the other. One possible solution:

If a validation error occurs simply use the ""old"" FlinkDeployment option in the rest of the controller loop. We can restore the old valid deployment from the lastReconciledSpec field, we just need to make sure to only update the status at the end. This would work from the observer/reconciler's perspective as if the new broken spec was never submitted.

Going this way we have to avoid repeatedly reporting the error caused by validation as we reschedule again and again.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 08 10:55:01 UTC 2022,,,,,,,,,,"0|z12394:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/22 10:55;gyfora;merged to main 52447287b17e8864066e65a0f1bbacf2184dbf2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation is unavailable,FLINK-27494,13443305,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,soberChina0@gamil.com,soberChina0@gamil.com,05/May/22 08:38,05/May/22 13:36,13/Jul/23 08:08,05/May/22 13:36,1.15.0,,,,,,1.15.1,,,,Documentation,,,,,0,,,,"!image-2022-05-05-16-38-01-827.png!

!image-2022-05-05-16-38-21-499.png!",,martijnvisser,soberChina0@gamil.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/22 08:38;soberChina0@gamil.com;image-2022-05-05-16-38-01-827.png;https://issues.apache.org/jira/secure/attachment/13043245/image-2022-05-05-16-38-01-827.png","05/May/22 08:38;soberChina0@gamil.com;image-2022-05-05-16-38-21-499.png;https://issues.apache.org/jira/secure/attachment/13043244/image-2022-05-05-16-38-21-499.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 05 13:36:49 UTC 2022,,,,,,,,,,"0|z122zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/22 12:42;martijnvisser;[~soberChina0@gamil.com] Thanks for reporting. This is because the release notes were merged after the documentation build has passed, I'll trigger it now manually to resolve this. ;;;","05/May/22 13:36;martijnvisser;Documentation rebuild, problem fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaMetricWrappers do incorrect cast,FLINK-27487,13443093,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,chesnay,chesnay,04/May/22 11:32,22/Aug/22 15:51,13/Jul/23 08:08,10/May/22 11:31,1.15.0,,,,,,1.15.1,1.16.0,,,Connectors / Kafka,Runtime / Metrics,,,,1,pull-request-available,,,"In FLINK-24765 the kafka metric wrappers that bridge kafka metrics into our metric system were migrated from the deprecated {{KafkaMetric#value}} to {{#metricValue}}.

This migration was done incorrectly. It was assumed that #metricValue behaves similar to #value in that it always returns a double, but this is not the case, as they may also return longs or strings.

This results in these metrics throwing exceptions, which generally breaks the reporter mechanism completely.",,alexeyt820,eskabetxe,fpaul,mason6345,tashoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27552,FLINK-29064,,,,FLINK-24765,,,,FLINK-27493,,,FLINK-28488,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 11 10:46:21 UTC 2022,,,,,,,,,,"0|z121o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/22 11:33;chesnay;[~arvid] [~fpaul];;;","04/May/22 12:00;fpaul;[~chesnay] thanks for the followup I'll take a look at it.;;;","10/May/22 11:31;fpaul;Merged in

master: 0d32cbd3f4d4c41b0a8e54b2d2872f0a360efd0d

release-1.15: addfc8cd92c863032f9da60b30a82d84322ea134;;;","30/Jun/22 06:49;tashoyan;Hi [~fpaul]
Should we apply the same fix to KafkaMetricWrapper?
https://github.com/apache/flink/blob/dfdf7afb047d1b8af581883bf208e4d8a30a116f/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/metrics/KafkaMetricWrapper.java#L35;;;","11/Jul/22 10:46;chesnay;[~tashoyan] yes; I created a separate ticket: FLINK-28488;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSource refreshes availability future,FLINK-27479,13442920,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,mason6345,mason6345,03/May/22 07:45,09/Oct/22 16:27,13/Jul/23 08:08,09/Oct/22 16:27,1.14.4,,,,,,1.17.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"HybridSourceReader needs to refresh the availability future according to the underlying reader. It currently maintains its own future and completes it after the sub-reader's availability future is complete. However, the implementation does not refresh the future again until the reader receives a switch event. This can cause a tight loop with the Flink runtime repeatedly invoking pollNext() and high CPU utilization.

 

To solve this, we can reuse the MultipleFuturesAvailabilityHelper to manage the lifecycle of the availability future.",,aitozi,maguowei,martijnvisser,mason6345,SelimAbidin,straw,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/22 06:11;mason6345;hybrid-source-with-fix.html;https://issues.apache.org/jira/secure/attachment/13046434/hybrid-source-with-fix.html","08/Jul/22 06:11;mason6345;hybrid-source-without-fix.html;https://issues.apache.org/jira/secure/attachment/13046436/hybrid-source-without-fix.html","08/Jul/22 06:11;mason6345;kafka-source.html;https://issues.apache.org/jira/secure/attachment/13046435/kafka-source.html",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 09:15:25 UTC 2022,,,,,,,,,,"0|z120ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/22 07:46;mason6345;[~thw@apache.org] can I take a look at solving this issue?;;;","03/May/22 22:15;thw;Of course, thanks for reporting the issue!;;;","08/Jul/22 06:15;mason6345;I was able to reproduce the user's reported issue in https://lists.apache.org/thread/l6m9p4cnmm4dzcm7nr285g2xyhnw96c1. Please see the PR description for more details. Sorry for the delay!;;;","04/Aug/22 09:15;SelimAbidin;This problem exists in 1.13.6 versions too. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Observing JobManager deployment. Previous status: MISSING,FLINK-27468,13442763,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,morhidi,,02/May/22 08:46,24/Nov/22 01:03,13/Jul/23 08:08,07/May/22 09:18,kubernetes-operator-0.1.0,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,The operator keeps looping if the JM Deployment gets deleted ( and probably when the job is in terminal Flink state such as FAILED). We need to agree on how to handle such cases and fix it.,,bgeng777,gyfora,tedhtchang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 07 09:18:23 UTC 2022,,,,,,,,,,"0|z11zn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/22 08:49;morhidi;cc [~mbalassi]  [~gyfora] [~aitozi] [~wangyang0918] ;;;","03/May/22 06:18;gyfora;First of all I think we need to send an event in these cases signaling to the user that this has happened.

What we can do in this situation depends on the upgrade mode.

For stateless/last-state upgrade modes (or even for savepoint when HA is enabled) if the deployment goes missing after it was observed in another state (deploying/running etc) we could try to recover it by resubmitting. For savepoint upgrade mode this probably means a terminal error state that requires user intervention. 

The main problem I see here is that this won't handle the FAILED scenario really nicely which might lead to an unintended a failure loop. We might have to accept the failure loop for 1.14 but we can fix this properly for 1.15 where we should anyways always disable
h5. execution.shutdown-on-application-finish

 ;;;","03/May/22 15:05;wangyang0918;I agree with [~gyfora] that we could not handle the FAILED scenario properly in 1.14 since the ResourceManager will delete all the K8s resources once the job reached a globally terminal state.;;;","07/May/22 09:18;gyfora;Merged to main b280822b6915ba0d03ac23da18c2543a67ffe15f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC metaspace leak fix is misleading,FLINK-27466,13442745,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,02/May/22 06:52,23/Jun/22 11:38,13/Jul/23 08:08,23/Jun/22 11:38,,,,,,,1.16.0,,,,Connectors / JDBC,Documentation,,,,0,pull-request-available,,,"{code}
To ensure that these classes are only loaded once you should either add the driver jars to Flink’s lib/ folder, or add the driver classes to the list of parent-first loaded class via classloader.parent-first-patterns-additional.
{code}

This reads as if adding the driver to classloader.parent-first-patterns-additional can solve the issue in all cases, but this only works if the driver is already in lib/.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 23 11:38:10 UTC 2022,,,,,,,,,,"0|z11zj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 11:38;chesnay;master: 99c9c2ee3d4cf15319ca9f97b61b18f3f3b23210;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroRowDeserializationSchema.convertToTimestamp fails with negative nano seconds,FLINK-27465,13442737,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,02/May/22 04:49,15/Jun/22 12:54,13/Jul/23 08:08,04/May/22 18:06,1.15.0,,,,,,1.15.1,1.16.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"The issue is exposed due to time zone dependency in AvroRowDeSerializationSchemaTest.
 
The root cause is that convertToTimestamp attempts to set negative value with java.sql.Timestamp.setNanos",,danderson,martijnvisser,matriv,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 12:54:51 UTC 2022,,,,,,,,,,"0|z11zhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/22 11:29;matriv;Do you have an example?

As far as I know Avro doesn't currently support nanos (not even micros), only millis.;;;","02/May/22 11:57;matriv;Nevermind, I saw the thread in the mailing list.;;;","03/May/22 04:43;thw;https://lists.apache.org/thread/bq8v1vrsq6p6mogtk5ffpp7pffog7w0n

[~matriv] thanks, I'm going to open a PR shortly.

 ;;;","15/Jun/22 12:54;danderson;Fixed in master via 9ecdc5fa10caf671c6fa823bff9c00b3a2dd1500

Fixed in 1.15.1 via 85c9612daae5c56f16de41fbd0ce2e5958d1762c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Module flink-sql-avro-confluent-registry does not configure Confluent repo,FLINK-27442,13442323,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,Nicolaus Weidner,Nicolaus Weidner,28/Apr/22 14:17,05/May/22 08:28,13/Jul/23 08:08,05/May/22 08:28,1.15.0,,,,,,1.14.5,1.15.1,1.16.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"flink-sql-avro-confluent-registry depends on org.apache.kafka:kafka-clients, which is not available in Maven Central, but only in the Confluent repo. However, it does not configure this repo. This causes the build to fail for me locally with the following exception:

{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (process-resource-bundles) on project flink-sql-avro-confluent-registry: Error resolving project artifact: Could not transfer artifact org.apache.kafka:kafka-clients:pom:6.2.2-ccs from/to <other repo>: Not authorized , ReasonPhrase: . for project org.apache.kafka:kafka-clients:jar:6.2.2-ccs -> [Help 1]
{code}

This may be build order dependent, but the module should probably configure the repo to be safe, like done elsewhere: https://github.com/apache/flink/blob/dd48d058c6b745f505870836048284a76a23f7cc/flink-end-to-end-tests/flink-confluent-schema-registry/pom.xml#L36-L41

Looks like this is the case since 1.12 at least.
",,martijnvisser,Nicolaus Weidner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 05 08:28:40 UTC 2022,,,,,,,,,,"0|z11wy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/22 08:28;martijnvisser;Fixed in master: a112465efe0e32f6c6c5e5e433b8d4b9f90dfd79
Fixed in release-1.15: db9f8e10816ca02d3f212b3912df1efffdbd72d9
Fixed in release-1.14: c6a97239ec8b6ff6a9fbe5be827d3028d57fd54c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scrollbar is missing for particular UI elements (Accumulators, Backpressure, Watermarks)",FLINK-27441,13442313,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,28/Apr/22 13:31,02/May/22 21:44,13/Jul/23 08:08,02/May/22 19:59,1.14.3,1.15.0,,,,,1.16.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,"The angular version bump introduced a bug, where for {{nzScroll}} does not support percentage in CSS calc, so the scrollbar will be invisible. There is an easy workaround, the linked Angular discussion covers it.

Angular issue: https://github.com/NG-ZORRO/ng-zorro-antd/issues/3090",,ferenc-csaky,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 02 19:59:27 UTC 2022,,,,,,,,,,"0|z11wvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/22 13:43;ferenc-csaky;Since this is a trivial fix, I opened a PR for it: https://github.com/apache/flink/pull/19606;;;","02/May/22 19:59;mbalassi;Fixed via [{{f3d4ceb}}|https://github.com/apache/flink/commit/f3d4cebb6516f1f35d4ff9fbb9d3ff941f92243e] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ClassNotFoundException when using array type in table store,FLINK-27439,13442273,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,28/Apr/22 09:58,28/Apr/22 12:33,13/Jul/23 08:08,28/Apr/22 10:54,table-store-0.1.0,,,,,,table-store-0.1.0,,,,Table Store,,,,,0,pull-request-available,,,"When using array data type with table store a {{ClassNotFound}} exception will be thrown. This is because FLINK-27408 packaged codegen related classes into table store itself but missed out some classes.

To make sure that such bug will not happen again we also need an e2e test to check all supported data types.",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27408,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 28 10:54:26 UTC 2022,,,,,,,,,,"0|z11wmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/22 10:54;lzljs3620320;master:

78159c5152927b5d336a3ec7ba27fbe920475a3f

a73435031431c0d2eccf9f346c0f68c9892ed8ab

release-0.1:

c90e583a350b327539b5958b7d36be232d9da045

945c315f031cd10eb258f487f71b212219786df1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suspended SlotManager fails to re-register metrics when started again,FLINK-27420,13441896,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,baugarten,baugarten,baugarten,26/Apr/22 20:17,20/Jun/22 09:55,13/Jul/23 08:08,20/Jun/22 09:55,1.13.5,,,,,,1.14.5,1.15.1,1.16.0,,Runtime / Coordination,Runtime / Metrics,,,,0,pull-request-available,,,"The symptom is that SlotManager metrics are missing (taskslotsavailable and taskslotstotal) when a SlotManager is suspended and then restarted. We noticed this issue when running 1.13.5, but I believe this impacts 1.14.x, 1.15.x, and master.

 

When a SlotManager is suspended, the [metrics group is closed|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/DeclarativeSlotManager.java#L214]. When the SlotManager is [started again|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/DeclarativeSlotManager.java#L181], it makes an attempt to [reregister metrics|[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/DeclarativeSlotManager.java#L199-L202],] but that fails because the underlying metrics group [is still closed|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/groups/AbstractMetricGroup.java#L393] 

 

I was able to trace through this issue by restarting zookeeper nodes in a staging environment and watching the JM with a debugger. 

 

A concise test, which currently fails, shows the expected behavior – [https://github.com/apache/flink/compare/master...baugarten:baugarten/slot-manager-missing-metrics?expand=1]

 

I am happy to provide a PR to fix this issue, but first would like to verify that this is not intended.",,baugarten,danderson,gaoyunhaii,maguowei,Nicolaus Weidner,Thesharing,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 09:55:43 UTC 2022,,,,,,,,,,"0|z11ubk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/22 01:48;maguowei;Thanks [~baugarten] for reporting this. cc [~xtsong] ;;;","27/Apr/22 02:55;xtsong;Thanks for reporting this, [~baugarten].

This is indeed a valid issue. I'd like to add a bit more clarification.
* For 1.13, I don't think it is supported for the JM process to live through multiple leader sessions, i.e. being revoked and re-granted leadership without failing the process. I know some of the codes look like it is supported, but unfortunately it never really worked until FLINK-23240 which is fixed in 1.14.4.
* For 1.14 & 1.15, yes, the issue still exist. Since 1.14, for each leader session we create a new ResourceManager instance. However, some of the components and services are preserved in {{ResourceManagerProcessContext}} and are reused across multiple RM instances. If these components / services are closed, they need to be restarted properly. I've checked the current implementation, and it seems the only things that are affected are {{resourceManagerMetricGroup}} and {{slotManagerMetricGroup}}. I think the easiest way to fix this is probably to store {{metricRegistry}} rather than the metric groups in {{ResourceManagerProcessContext}}, so that we can create new metric group instances for each leader session.

WDYT?;;;","27/Apr/22 20:26;baugarten;Thanks for adding that context!

For 1.14 & 1.15, I looked through the implementation of `ResourceManagerServiceImpl`, which seems to be new in 1.14, and I follow what you're saying. I agree that the `resourceManagerMetricGroup` and `slotManagerMetricGroup` are the only metrics affected and that storing the `metricRegistry` (and `hostname`, which is required to create the metric group) and creating the metric group with each new leader session makes the most sense. I can start on this change and could post a PR tomorrow during US working hours.

 

I'm not sure I follow your point about 1.13 though. I currently run 1.13 in standalone, session mode and the JM process does seem to live through multiple leader sessions. Regardless, my understanding is that Flink only supports the latest two versions, which would be 1.14 and 1.15, so a patch for 1.13 is not desired – is that correct?;;;","28/Apr/22 02:17;xtsong;[~baugarten],

Thanks for volunteering to fix this. I've assigned you to this ticket.

Regarding 1.13, I'm surprised that you have the JM process live through multiple leader sessions. IIRC, we had tried it before making the changes in 1.14, and JM process was terminated after losing the leadership. Unfortunately I cannot recall more details about how it was terminated. Anyway, if that works for you, I'd be fine with also fixing this for 1.13.

According to the [Update Policy for old releases|https://flink.apache.org/downloads.html#update-policy-for-old-releases], the Flink community provides supports for the latest 2 versions, but is also open to discussing bugfix releases for older versions. Actually, it is not rare that we ship bugfix release for the 3rd latest version. To sum up, although I don't know whether (and when) there will be a next bugfix release for 1.13.x, I would not consider a patch for 1.13 is not desired.;;;","29/Apr/22 03:32;baugarten;[~xtsong] – thanks, I posted a PR against master here: [https://github.com/apache/flink/pull/19607]. Let me know if you'd like any changes to the PR and I'm happy to make them. I can post PRs against release-1.14 and release-1.15 after.

That's good to know that there might be another bugfix release for 1.13.x. I'm going to work on a patch for 1.13 as well because most of our applications run on 1.13.;;;","29/Apr/22 08:48;xtsong;Fixed via
- master (1.16): 525e3170c622da65becfab3e8afe97303b07b7db
- release-1.15: -e0c82d6d52871dbbea70c9b41384d2d33179bec0- fbc8e460cd0f80ecb4387855ab982d896e95af3b
- release-1.14: -054b59bb97d14e453745e18f8d9cc90b109bb33a- ff060543bc8867bdc97e4c3138c4999894f2909e

Leave the ticket open to fix for 1.13;;;","03/May/22 05:04;gaoyunhaii;Hi, I'll first revert the commits on 1.15 and 1.14 since they fails the compilations. Let's have a double check and re-merge the commit. ;;;","03/May/22 14:25;Nicolaus Weidner;Looks fine on master, but in the backports, a test was backported without checking that the same variables are available. E.g. on release-1.15,  delegationTokenManager is undefined: https://github.com/apache/flink/blob/e0c82d6d52871dbbea70c9b41384d2d33179bec0/flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerServiceImplTest.java#L520. It was added in this commit: https://github.com/apache/flink/commit/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb

I didn't check for the specific issue on release-1.14;;;","05/May/22 03:27;xtsong;Thanks [~gaoyunhaii] for fixing the broken branches, and [~Nicolaus Weidner] for looking into this.

I noticed that [~baugarten] has already opened a new PR for the 1.15 & 1.14 branches. I'll help with finalizing the PR.;;;","15/Jun/22 12:52;danderson;[~xtsong] What's the status here? Do you want to finalize this for 1.15.1? I don't intend to block the release on it, but we are waiting on something else at the moment.;;;","15/Jun/22 13:06;xtsong;[~danderson],

This has already been fixed for:
- master (1.16.0): 525e3170c622da65becfab3e8afe97303b07b7db
- 1.15.1: fbc8e460cd0f80ecb4387855ab982d896e95af3b

The ticket is kept open for 1.13 & 1.14, where the current patch does not apply.;;;","15/Jun/22 13:13;xtsong;[~baugarten],

Are you still going to work on this? If you don't have the time, I can take it over and fix for 1.14. I'll probably leave it as is for 1.13.;;;","20/Jun/22 09:55;xtsong;Fixed via
- master (1.16): 525e3170c622da65becfab3e8afe97303b07b7db
- release-1.15: fbc8e460cd0f80ecb4387855ab982d896e95af3b
- release-1.14: ff060543bc8867bdc97e4c3138c4999894f2909e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL TopN result is wrong,FLINK-27418,13441830,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rovboyko,zhangbinzaifendou,zhangbinzaifendou,26/Apr/22 14:10,09/Jun/22 08:38,13/Jul/23 08:08,09/Jun/22 08:38,1.12.2,1.14.3,,,,,1.14.6,1.15.1,1.16.0,,Table SQL / API,,,,,0,pull-request-available,,,"Flink SQL TopN is executed multiple times with different results, sometimes with correct results and sometimes with incorrect results.

Example:
{code:java}
@Test
    public void flinkSqlJoinRetract() {
        EnvironmentSettings settings = EnvironmentSettings.newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();

        StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();
        streamEnv.setParallelism(1);
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv, settings);
        tableEnv.getConfig().setIdleStateRetention(Duration.ofSeconds(10000));

        RowTypeInfo waybillTableTypeInfo = buildWaybillTableTypeInfo();
        RowTypeInfo itemTableTypeInfo = buildItemTableTypeInfo();
        SourceFunction<Row> waybillSourceFunction = buildWaybillStreamSource(waybillTableTypeInfo);
        SourceFunction<Row> itemSourceFunction = buildItemStreamSource(itemTableTypeInfo);
        String waybillTable = ""waybill"";
        String itemTable = ""item"";

        DataStreamSource<Row> waybillStream = streamEnv.addSource(
                waybillSourceFunction,
                waybillTable,
                waybillTableTypeInfo);
        DataStreamSource<Row> itemStream = streamEnv.addSource(
                itemSourceFunction,
                itemTable,
                itemTableTypeInfo);

        Expression[] waybillFields = ExpressionParser
                .parseExpressionList(String.join("","", waybillTableTypeInfo.getFieldNames())
                        + "",proctime.proctime"").toArray(new Expression[0]);
        Expression[] itemFields = ExpressionParser
                .parseExpressionList(
                        String.join("","", itemTableTypeInfo.getFieldNames()) + "",proctime.proctime"")
                .toArray(new Expression[0]);

        tableEnv.createTemporaryView(waybillTable, waybillStream, waybillFields);
        tableEnv.createTemporaryView(itemTable, itemStream, itemFields);

        String sql =
                ""select \n""
                + ""    city_id, \n""
                + ""    count(*) as cnt\n""
                + ""from (\n""
                + ""    select id,city_id\n""
                + ""    from (\n""
                + ""        select \n""
                + ""            id,\n""
                + ""            city_id,\n""
                + ""            row_number() over(partition by id order by utime desc ) as rno \n""
                + ""        from (\n""
                + ""            select \n""
                + ""                waybill.id as id,\n""
                + ""                coalesce(item.city_id, waybill.city_id) as city_id,\n""
                + ""                waybill.utime as utime \n""
                + ""            from waybill left join item \n""
                + ""            on waybill.id = item.id \n""
                + ""        ) \n""
                + ""    )\n""
                + ""    where rno =1\n""
                + "")\n""
                + ""group by city_id"";

        StatementSet statementSet = tableEnv.createStatementSet();
        Table table = tableEnv.sqlQuery(sql);
        DataStream<Tuple2<Boolean, Row>> rowDataStream = tableEnv.toRetractStream(table, Row.class);
        rowDataStream.printToErr();
        try {
            streamEnv.execute();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static RowTypeInfo buildWaybillTableTypeInfo() {
        TypeInformation[] types = new TypeInformation[]{Types.INT(), Types.STRING(), Types.LONG(), Types.LONG()};
        String[] fields = new String[]{""id"", ""city_id"", ""rider_id"", ""utime""};
        return new RowTypeInfo(types, fields);
    }

    private static RowTypeInfo buildItemTableTypeInfo() {
        TypeInformation[] types = new TypeInformation[]{Types.INT(), Types.STRING(), Types.LONG()};
        String[] fields = new String[]{""id"", ""city_id"", ""utime""};
        return new RowTypeInfo(types, fields);
    }

    //id,rider_id,city_id,utime
    private static SourceFunction<Row> buildWaybillStreamSource(RowTypeInfo rowTypeInfo) {
        return new SourceFunction<Row>() {
            private volatile boolean stopped = false;
            int count = 0;
            int[] ids = {111, 222, 333, 111};
            String[] cityIds = {""A"", ""A"", ""B"", ""A""};

            @Override
            public void run(SourceContext<Row> ctx) throws Exception {
                while (!stopped) {
                    int id = ids[count % ids.length];
                    String cityId = cityIds[count % cityIds.length];
                    Row row = new Row(4);
                    row.setField(0, id);
                    row.setField(1, cityId);
                    row.setField(2, (long) RandomUtils.nextInt(1000, 2000));
                    row.setField(3, System.currentTimeMillis());
                    printRow(rowTypeInfo, row);
                    ctx.collect(row);
                    if (++count > 3) {
                        stopped = true;
                    }
                }
            }

            @Override
            public void cancel() {
                stopped = true;
            }
        };
    }

    //id,city_id,utime
    private static SourceFunction<Row> buildItemStreamSource(RowTypeInfo rowTypeInfo) {
        return new SourceFunction<Row>() {
            private volatile boolean stopped = false;
            int count = 0;
            int[] ids = {111, 333};
            String[] cityIds = {""C"", ""D""};

            @Override
            public void run(SourceContext<Row> ctx) throws Exception {
                while (!stopped) {
                    Thread.sleep(RandomUtils.nextInt(1000, 2000));
                    int id = ids[count % ids.length];
                    String cityId = cityIds[count % cityIds.length];
                    Row row = new Row(3);
                    row.setField(0, id);
                    row.setField(1, cityId);
                    //row.setField(2, System.currentTimeMillis());
                    printRow(rowTypeInfo, row);
                    ctx.collect(row);
                    if (++count >= 2) {
                        stopped = true;
                    }

                }
            }

            @Override
            public void cancel() {
                stopped = true;
            }
        };
    }

    public static void printRow(RowTypeInfo rowTypeInfo, Row row) {
        String prefix = """";
        for (int i = 0; i < rowTypeInfo.getArity(); ++i) {
            prefix = i > 0 ? "","" : """";
            System.out.print(prefix + rowTypeInfo.getFieldNames()[i] + "":"" + row.getField(i));
        }
        System.out.println();
    }

{code}
------------------------------------------------------------
|*wrong result*||right result||
|id:111,city_id:A,rider_id:1137,utime:1650979957702
id:222,city_id:A,rider_id:1976,utime:1650979957725
id:333,city_id:B,rider_id:1916,utime:1650979957725
id:111,city_id:A,rider_id:1345,utime:1650979957725
(true,A,1)
(false,A,1)
(true,A,2)
(true,B,1)
(false,A,2)
(true,A,1)
(false,A,1)
(true,A,2)
id:111,city_id:C,utime:null
(false,A,2)
(true,A,1)
(true,C,1)
(false,A,1)
(false,C,1)
(true,C,2)
id:333,city_id: D,utime:null
(false,B,1)
(true,D,1)
The final result:
C,2
D,1
is wrong.| 
id:111,city_id:A,rider_id:1155,utime:1650980662019
id:222,city_id:A,rider_id:1875,utime:1650980662042
id:333,city_id:B,rider_id:1430,utime:1650980662042
id:111,city_id:A,rider_id:1308,utime:1650980662042
(true,A,1)
(false,A,1)
(true,A,2)
(true,B,1)
(false,A,2)
(true,A,1)
(false,A,1)
(true,A,2)
id:111,city_id:C,utime:null
(false,A,2)
(true,A,1)
(false,A,1)
(true,A,2)
(false,A,2)
(true,A,1)
(true,C,1)
id:333,city_id: D,utime:null
(false,B,1)
(true,D,1)
The final result:
A,1
C,1
D,1
is right.|

 

 ",Flink 1.12.2 and Flink 1.14.3 test results are sometimes wrong,jark,martijnvisser,rovboyko,zhangbinzaifendou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 15:06:17 UTC 2022,,,,,,,,,,"0|z11tww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/22 14:25;zhangbinzaifendou;Hi,[~jark] ，I see you've fixed several topn bugs.
I have found the cause of this wrong result, and I hope to discuss it with you.

Thanks.;;;","26/Apr/22 16:36;martijnvisser;[~zhangbinzaifendou] Are you sure that you've tested this in Flink 1.14? Your code refers to Blink Planner, but that doesn't exist in Flink 1.14. ;;;","27/Apr/22 05:09;zhangbinzaifendou;Thank you for your reply, the useBlinkPlanner method still exists in Flink 1.14 and the result is sometimes wrong in Flink 1.14. You will get different results if you execute it several times;;;","04/May/22 11:08;martijnvisser;[~jark] What do you think? ;;;","20/May/22 07:37;rovboyko;The bug exists indeed. The simplest way to reproduce it is add this to RetractableTopNFunctionTest:
{code:java}
@Test
public void testRetractRowNumber() throws Exception {
    AbstractTopNFunction func =
            new RetractableTopNFunction(
                    ttlConfig,
                    InternalTypeInfo.ofFields(
                            VarCharType.STRING_TYPE,
                            new BigIntType(),
                            new IntType(),
                            new IntType()),
                    comparableRecordComparator,
                    sortKeySelector,
                    RankType.ROW_NUMBER,
                    new ConstantRankRange(1, 1),
                    generatedEqualiser,
                    true,
                    false);

    OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
    testHarness.open();
    testHarness.processElement(insertRecord(""a"", 1L, 10, 0));
    testHarness.processElement(insertRecord(""a"", 1L, 9, 0));
    testHarness.processElement(deleteRecord(""a"", 1L, 10, 0));
    testHarness.processElement(deleteRecord(""a"", 1L, 9, 0));
    testHarness.processElement(insertRecord(""a"", 1L, 10, 1));
    testHarness.processElement(insertRecord(""a"", 1L, 9, 1));
    testHarness.close();

    List<Object> expectedOutput = new ArrayList<>();
    expectedOutput.add(insertRecord(""a"", 1L, 10, 0));
    expectedOutput.add(deleteRecord(""a"", 1L, 10, 0));
    expectedOutput.add(insertRecord(""a"", 1L, 9, 0));
    expectedOutput.add(deleteRecord(""a"", 1L, 9, 0));
    expectedOutput.add(insertRecord(""a"", 1L, 10, 1));
    expectedOutput.add(deleteRecord(""a"", 1L, 10, 1));
    expectedOutput.add(insertRecord(""a"", 1L, 9, 1));

    assertorWithRowNumber.assertOutputEquals(
            ""output wrong."", expectedOutput, testHarness.getOutput());
} {code}
I'll take it.

[~zhangbinzaifendou] , [~martijnvisser] could you please assign it to me?;;;","20/May/22 09:24;martijnvisser;[~rovboyko] Thanks, I've assigned it to you;;;","21/May/22 04:02;rovboyko;The PR is ready - [https://github.com/apache/flink/pull/19778]

[~zhangbinzaifendou] , [~jark] , could you please have a look?;;;","08/Jun/22 15:06;jark;Fixed in 
 - master: 99f4511de90052f182085f943337d86778bc8488
 - 1.15.1: ce23d6331c448b401066006a794c7cf91aeb5fda
 - 1.14.5: 80abb30a7217d4cee034aa375c1a72a07e0c5768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read empty csv file throws exception in FileSystem table connector,FLINK-27415,13441783,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sonice_lj,lzljs3620320,lzljs3620320,26/Apr/22 10:54,06/Jul/23 13:40,13/Jul/23 08:08,06/Jul/23 13:40,1.15.0,,,,,,1.16.3,1.17.2,1.18.0,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,stale-assigned,,"{code:java}
// in CsvFilesystemBatchITCase
@Test
public void testEmpty() throws Exception {
    String path = new URI(resultPath()).getPath();
    new File(path).mkdirs();
    File file = new File(path, ""test_file"");
    file.createNewFile();

    check(""select * from nonPartitionedTable"", Collections.emptyList());
} {code}
Throws:
{code:java}
Caused by: java.lang.IllegalArgumentException
    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter$TrackingFsDataInputStream.<init>(StreamFormatAdapter.java:264)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.lambda$openStream$3(StreamFormatAdapter.java:180)
    at org.apache.flink.connector.file.src.util.Utils.doWithCleanupOnException(Utils.java:45)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.openStream(StreamFormatAdapter.java:172)
    at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.createReader(StreamFormatAdapter.java:70)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:112)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:65)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
    ... 6 more {code}
This may be introduced by FLINK-24703 ",,dannycranmer,jackwangcs,lzljs3620320,sonice_lj,wanglijie,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32539,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 04 18:36:26 UTC 2023,,,,,,,,,,"0|z11tmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 02:52;sonice_lj;[~lzljs3620320] If you're not working on it, could you assign this ticket to me?;;;","17/Jun/22 03:46;lzljs3620320;[~sonice_lj] Assigned;;;","20/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","04/Jul/23 18:36;dannycranmer;Merged commit [{{0015fd3}}|https://github.com/apache/flink/commit/0015fd3d0b7f226f31354c5792df055bc8e97d74] into master (1.18)

Merged commit [{{3061dee}}|https://github.com/apache/flink/commit/3061dee843b69eba62775e96f65b7f9a51f82bb5] into release-1.17

Merged commit [{{ad1b739}}|https://github.com/apache/flink/commit/ad1b739358efa3c58187f80ead80a81377029c48] into release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup stale slot allocation record when the resource requirement of a job is empty,FLINK-27409,13441715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,26/Apr/22 06:15,27/Apr/22 03:45,13/Jul/23 08:08,27/Apr/22 03:45,1.14.4,1.15.0,,,,,1.14.5,1.15.1,1.16.0,,Runtime / Coordination,,,,,0,pull-request-available,,,"We need to clean up the stale slot allocation record when the resource requirement of a job is empty, in case the pending slot of this job is incorrectly allocated when registered. This only affects the `FineGrainedSlotManager`.",,guoyangze,maguowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 27 03:44:46 UTC 2022,,,,,,,,,,"0|z11t7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/22 03:44;guoyangze;master: fd4e52ba4d6292c02c8b5192a8679c1bb666a218
1.15: 39377d6c5c64734f043851087849b4278715e45c
1.14: b5d77c0cdb1519163acad92084a4f3f79b24b012;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector subscribed the system topic when using the regex,FLINK-27400,13441656,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,25/Apr/22 22:16,16/Sep/22 01:36,13/Jul/23 08:08,16/Sep/22 01:35,1.14.4,1.15.2,1.16.0,,,,1.14.7,1.15.3,1.16.0,,Connectors / Pulsar,,,,,0,pull-request-available,stale-assigned,,Pulsar has a lot of internal topics which is used for metadata. It couldn't be consumed directly by the user. We accidentally exposed these topics to end-users when using the regex topics.	,,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 16 01:36:02 UTC 2022,,,,,,,,,,"0|z11su8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Sep/22 01:36;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector didn't set start consuming position correctly,FLINK-27399,13441654,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,syhily,syhily,syhily,25/Apr/22 22:11,06/Sep/22 13:02,13/Jul/23 08:08,14/Aug/22 14:55,1.14.4,1.15.0,1.16.0,,,,1.14.6,1.15.2,1.16.0,,Connectors / Pulsar,,,,,1,pull-request-available,,,"The Pulsar connector didn't use the consuming position from the checkpoint. They just commit the position to Pulsar after the checkpoint is complete. And the connector starts to consume messages from Pulsar directly by the offset stored on the Pulsar subscription.

This causes the test could be failed in some situations. The start cursor (position on Pulsar) would be reset to the wrong position, which caused the results didn't match the desired records.

h2. How to fix this issue

Change the start position seeking mechanism from Pulsar consumer API to Pulsar admin API. Don't reset the start position when the topic has a subscription.

h2. This issue fixes

# FLINK-23944
# FLINK-24872
# FLINK-25815
# FLINK-25884
# FLINK-26177
# FLINK-26721
# FLINK-27833",,cyc,longtimer,syhily,tison,,,,,,,,,,,,,,,,,,,,,FLINK-23944,FLINK-25884,FLINK-26177,FLINK-26721,FLINK-27833,FLINK-25815,FLINK-24872,,,,,,,,,,,,,,,FLINK-28972,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 14 14:55:28 UTC 2022,,,,,,,,,,"0|z11sts:",9223372036854775807,"A number of breaking changes were made to the Pulsar Connector cursor APIs:

- CursorPosition#seekPosition() has been removed.
- StartCursor#seekPosition() has been removed.
- StopCursor#shouldStop now returns a StopCondition instead of a boolean.",,,,,,,,,,,,,,,,,,,"13/Aug/22 19:02;syhily;[~tison] I have backported this fix to 1.14 and 1.15 branches. Can you check it and get this resolved?;;;","14/Aug/22 14:55;tison;master via 18d21a0618f2195d4279828f610094ffccd052b3
1.15 via 5f842cb8665e831d6acb978a57b896accd1c0928
1.14 via 95d14edbc8dc16fe420ec12fbd5d7f61dc873699;;;","14/Aug/22 14:55;tison;[~syhily] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the Hive dimension table, when the data is changed on the original partition, the create_time configuration does not take effect",FLINK-27384,13441437,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,empcl,empcl,empcl,25/Apr/22 07:52,12/Oct/22 08:41,13/Jul/23 08:08,12/Oct/22 08:41,1.14.4,1.15.1,,,,,1.14.7,1.15.3,1.16.0,,Connectors / Hive,,,,,0,pull-request-available,stale-assigned,,"In the Hive dimension table, when the data is changed on the original partition, the create_time configuration does not take effect.

!image-2022-04-25-15-46-01-833.png!

The current table structure directory is as follows:

!image-2022-04-25-15-47-54-213.png!

From the above figure, we can know that when hive is a dimension table, it will load the data of dt=2021-04-22, hr=27.

If a new partition arrives now, the data of the latest partition can be read smoothly. However, if the data is modified on the original partition, theoretically, the data of the modified partition is read, because of the create_time and latest configuration, but this is not the case in practice. The data that was originally loaded is still read.",,empcl,leonard,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/22 07:46;empcl;image-2022-04-25-15-46-01-833.png;https://issues.apache.org/jira/secure/attachment/13042870/image-2022-04-25-15-46-01-833.png","25/Apr/22 07:47;empcl;image-2022-04-25-15-47-54-213.png;https://issues.apache.org/jira/secure/attachment/13042869/image-2022-04-25-15-47-54-213.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 11 03:01:53 UTC 2022,,,,,,,,,,"0|z11ri8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 03:13;leonard;Fixed in 
master(1.16): d97b94d4b723aaa403f5849ceaa76f59f4dd3b5a
release-1.15: bcff5af0b5fa8a4e21913329ad8e182bedf228ad
release-1.14: 824d341255a4b6f26d6ca55566710bceb248a14b;;;","03/Aug/22 07:48;leonard;[~empcl] Could you also open PRs for release-1.4&1.15 branches?;;;","03/Aug/22 07:51;empcl;[~leonard] OK, let me do it.;;;","02/Sep/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Oct/22 03:01;luoyuxia;[~leonard] Seems the prs for release-1.4&1.15 branches are ready. Could you please help merge it when you're free.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSource split should use Arrays.hashcode,FLINK-27381,13441431,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taoran,taoran,taoran,25/Apr/22 07:36,26/Apr/22 04:24,13/Jul/23 08:08,26/Apr/22 04:24,1.14.4,,,,,,1.16.0,,,,Connectors / Common,,,,,0,pull-request-available,,," 

HybridSourceSplit's wrappedSplit  has been changed from SourceSplit to serialized wrappedSplitsBytes array. but hashcode methods did not match the byte array. however here we should use Arrays.hashcode for serialized wrappedSplitsBytes.

 
{code:java}
public int hashCode(){
   return Objects.hash(wrappedSplitBytes, sourceIndex);
} {code}
 
{code:java}
// old
public class HybridSourceSplit implements SourceSplit {   
    private final SourceSplit wrappedSplit;
    private final int sourceIndex; 
    public HybridSourceSplit(int sourceIndex, SourceSplit wrappedSplit) {
        this.sourceIndex = sourceIndex;
        this.wrappedSplit = wrappedSplit;
    }
    public int sourceIndex() {
        return this.sourceIndex;
    }
    public SourceSplit getWrappedSplit() {
        return wrappedSplit;
    }
    @Override
    public int hashCode() {
        return Objects.hash(wrappedSplit, sourceIndex);
    }    ...
}     {code}
 
{code:java}
// current(master branch)
public class HybridSourceSplit implements SourceSplit {
    private final byte[] wrappedSplitBytes;
    private final int wrappedSplitSerializerVersion;
    private final int sourceIndex;
    private final String splitId;
    public HybridSourceSplit(
            int sourceIndex, byte[] wrappedSplit, int serializerVersion, String splitId) {
        this.sourceIndex = sourceIndex;
        this.wrappedSplitBytes = wrappedSplit;
        this.wrappedSplitSerializerVersion = serializerVersion;
        this.splitId = splitId;
    }
    public int sourceIndex() {
        return this.sourceIndex;
    }
    public byte[] wrappedSplitBytes() {
        return wrappedSplitBytes;
    }
    @Override
    public int hashCode() {
        return Objects.hash(wrappedSplitBytes, sourceIndex);
    }
    ...
 }   {code}
detail diff: 

[https://github.com/apache/flink/pull/17143/files#diff-cbf6e2386d7457d9084faa23053f1f96ba5f173f5531d0e4a94205497e08df4c]

 

detail issue:

https://issues.apache.org/jira/browse/FLINK-24064

 ",,taoran,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 26 03:24:48 UTC 2022,,,,,,,,,,"0|z11rgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/22 08:11;taoran;[~thw]  Hi, If you have time, please have a look about this issue. thanks ~;;;","26/Apr/22 02:26;thw;[~lemonjing] thanks for the fix, I can help with your PR.;;;","26/Apr/22 03:24;taoran;[~thw] thanks for your reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make DataFileMeta and ManifestFileMeta schemaless,FLINK-27372,13441395,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/Apr/22 02:51,12/May/22 03:37,13/Jul/23 08:08,12/May/22 03:37,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,"Currently these meta need key and value schema to serialize and deserialize
 * schema binding makes the construction of serializers difficult and prone to problems
 * Not conducive to schema evolution, different schema can not be mixed together, will increase the difficulty of management.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27365,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 03:37:11 UTC 2022,,,,,,,,,,"0|z11r8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 03:37;lzljs3620320;master: d28856e5092a0117fa203834651d505e6d944dbf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"COALESCE('1', CAST(NULL as varchar)) throws expression type mismatch",FLINK-27369,13441367,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,jark,jark,24/Apr/22 12:02,26/Apr/22 06:49,13/Jul/23 08:08,26/Apr/22 02:25,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"{code}
Flink SQL> SELECT
>     COALESCE('1', cast(NULL as varchar)),
>     COALESCE('4', cast(NULL as varchar), cast(NULL as varchar), cast(NULL as varchar));


Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
Caused by: java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$0, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$1) NOT NULL
expression type is RecordType(CHAR(1) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$0, CHAR(1) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$1) NOT NULL
set is rel#910:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#909,exprs=[COALESCE(_UTF-16LE'1', null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), COALESCE(_UTF-16LE'4', null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")])
expression is LogicalProject(EXPR$0=[_UTF-16LE'1'], EXPR$1=[_UTF-16LE'4'])
  LogicalValues(tuples=[[{ 0 }]])

	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:58)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.flink.table.planner.plan.rules.logical.RemoveUnreachableCoalesceArgumentsRule.onMatch(RemoveUnreachableCoalesceArgumentsRule.java:71)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:164)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:82)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:303)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:179)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:231)
	at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:561)
	at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:446)
	at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:373)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:330)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
{code}",,godfreyhe,jark,jingzhang,martijnvisser,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 26 06:49:22 UTC 2022,,,,,,,,,,"0|z11r2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/22 07:02;godfreyhe;The reason UnreachableCoalesceArgumentsRemoveRexShuttle will return the first non-null operand of RexCall, which type may be different from the type of the original RexCall. The solution is UnreachableCoalesceArgumentsRemoveRexShuttle should make sure the type of new RexNode must be same with the type of original RexCall.;;;","26/Apr/22 02:25;godfreyhe;Fixed in
master: 2e632af6a75ae8db8b1dd8b2b8af9fa5a514af36
1.15.0: a75cef353b3be230a3815e17d69f871dde9c4460;;;","26/Apr/22 06:49;martijnvisser;[~godfreyhe] Is this really fixed in 1.15.0? I believe it's now for 1.15.1, correct? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL CAST(' 1 ' as BIGINT) returns wrong result,FLINK-27368,13441362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,jark,jark,24/Apr/22 11:04,17/Jun/22 13:00,13/Jul/23 08:08,04/May/22 09:20,1.15.0,,,,,,1.15.1,1.16.0,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,"{code:sql}
Flink SQL> select
>                     cast(' 1 ' as tinyint),
>                     cast(' 1 ' as smallint),
>                     cast(' 1 ' as int),
>                     cast(' 1 ' as bigint),
>                     cast(' 1 ' as float),
>                     cast(' 1 ' as double);
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
| op | EXPR$0 | EXPR$1 |      EXPR$2 |               EXPR$3 |                         EXPR$4 |                         EXPR$5 |
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: ' 1 '. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toByte(BinaryStringDataUtil.java:568)
	at StreamExecCalc$392.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}

Setting CAST behavior to legacy but got null result :

{code}
Flink SQL> set table.exec.legacy-cast-behaviour=enabled;
[INFO] Session property has been set.

Flink SQL> select
>                     cast(' 1 ' as tinyint),
>                     cast(' 1 ' as smallint),
>                     cast(' 1 ' as int),
>                     cast(' 1 ' as bigint),
>                     cast(' 1 ' as float),
>                     cast(' 1 ' as double);
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
| op | EXPR$0 | EXPR$1 |      EXPR$2 |               EXPR$3 |                         EXPR$4 |                         EXPR$5 |
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Column 'EXPR$0' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='DROP' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processNotNullConstraint(ConstraintEnforcer.java:261)
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:241)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at StreamExecCalc$591.processElement_split1(Unknown Source)
	at StreamExecCalc$591.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}


In 1.14 the result should be {{[1, 1, 1, 1, 1.0, 1.0]}}. 

In Postgres:
{code}
postgres=# select cast(' 1 ' as int), cast(' 1 ' as bigint), cast(' 1 ' as float);
 int4 | int8 | float8
------+------+--------
    1 |    1 |      1
(1 row)
{code}",,jark,jingzhang,martijnvisser,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 04 09:20:07 UTC 2022,,,,,,,,,,"0|z11r1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/22 07:16;twalthr;+1 for performing trimming again.;;;","04/May/22 09:20;twalthr;Fixed in master: 7128a6006546254abc2e80da9621c97bc45c77d5
Fixed in 1.15: b1b58ee4c2e61bc6f8e9c4d356708c1e080b3ee9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL CAST between INT and DATE is broken,FLINK-27367,13441360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,jark,jark,24/Apr/22 10:40,17/Jun/22 12:59,13/Jul/23 08:08,29/Apr/22 07:31,1.15.0,,,,,,1.15.1,1.16.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,"{code:sql}
SELECT CASE WHEN true THEN cast(1 as int) ELSE cast('2017-12-11 09:30:00' as date) END;
{code}

The above code throws the following exception which is supported in the previous version:

{code}
org.apache.flink.table.api.ValidationException: SQL validation failed. Unsupported cast from 'INT NOT NULL' to 'DATE NOT NULL'.

	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:395)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:112)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:680)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.parseQuery(BatchTestBase.scala:297)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:139)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.test2(CalcITCase.scala:92)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast from 'INT NOT NULL' to 'DATE NOT NULL'.
	at org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens$.generateCast(ScalarOperatorGens.scala:955)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:711)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:495)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:60)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:159)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.$anonfun$reduce$2(ExpressionReducer.scala:79)
	at scala.collection.immutable.List.map(List.scala:282)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:79)
	at org.apache.calcite.rex.RexSimplify.simplifyCast(RexSimplify.java:2101)
	at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:326)
	at org.apache.calcite.rex.RexSimplify.simplifyCase(RexSimplify.java:1006)
	at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:322)
	at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:287)
	at org.apache.calcite.rex.RexSimplify.simplifyPreservingType(RexSimplify.java:226)
	at org.apache.calcite.rex.RexSimplify.simplifyPreservingType(RexSimplify.java:221)
	at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1456)
	at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1311)
	at org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1565)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4248)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:689)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:646)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3464)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:572)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.java:336)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.java:329)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1370)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1318)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:371)
	... 42 more
{code}

It seems that the FLINK-24780 drops this support when porting CAST to the new structure.",,jark,jingzhang,martijnvisser,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 26 08:59:08 UTC 2022,,,,,,,,,,"0|z11r14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/22 10:41;jark;Hi [~matriv], is it on purpose that dropped this feature in FLINK-24780?;;;","25/Apr/22 07:12;twalthr;I would say this is on purpose. It is similar to numeric timestamp conversions, we should introduce dedicated {{CONVERT}} methods for those arbitrary semantics. Neither Oracle nor MySQL support his cast.;;;","25/Apr/22 10:59;jark;Could we keep compatible for this behavior when legacy cast behavior is enabled? [~twalthr]
;;;","25/Apr/22 12:46;twalthr;Sure, this we can do.;;;","26/Apr/22 08:59;jark;Fixed in 
 - 1.15.1: 3b790d16b731bf6455a793207501a9bd5f0ee2a0
 - master: 631f6891b2d04bd485a6879fa6f424d040eb6610;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator throws NPE when testing with Flink 1.15,FLINK-27358,13441257,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,23/Apr/22 04:03,25/Apr/22 02:11,13/Jul/23 08:08,25/Apr/22 02:10,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"{code:java}
2022-04-22 10:19:18,307 o.a.f.k.o.c.FlinkDeploymentController [WARN ][default/flink-example-statemachine] Attempt count: 5, last attempt: true
2022-04-22 10:19:18,329 i.j.o.p.e.ReconciliationDispatcher [ERROR][default/flink-example-statemachine] Error during event processing ExecutionScope{ resource id: CustomResourceID{name='flink-example-statemachine', namespace='default'}, version: 4979543} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.NullPointerException
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:110)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:53)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:101)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:76)
    at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:75)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:143)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:109)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:74)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.NullPointerException
    at org.apache.flink.kubernetes.operator.utils.FlinkUtils.lambda$deleteJobGraphInKubernetesHA$0(FlinkUtils.java:253)
    at java.base/java.util.ArrayList.forEach(Unknown Source)
    at org.apache.flink.kubernetes.operator.utils.FlinkUtils.deleteJobGraphInKubernetesHA(FlinkUtils.java:248)
    at org.apache.flink.kubernetes.operator.service.FlinkService.submitApplicationCluster(FlinkService.java:130)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deployFlinkJob(ApplicationReconciler.java:205)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.restoreFromLastSavepoint(ApplicationReconciler.java:218)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.reconcile(ApplicationReconciler.java:117)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.reconcile(ApplicationReconciler.java:56)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:106)
    ... 13 more {code}
The root cause is that the Kubernetes HA implementation has changed from 1.15. When the job is cancelled, the data of leader ConfigMap will be cleared. ",,aitozi,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27359,,,,,,FLINK-24038,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 25 02:10:09 UTC 2022,,,,,,,,,,"0|z11qew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/22 11:14;aitozi;Do we need to link the related upstream ticket here ? ;;;","25/Apr/22 02:10;wangyang0918;Fixed via:

main: 707102be6c0af3964a2c9f105639042fc0f2a34d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManager running together with JobManager are bind to 127.0.0.1,FLINK-27341,13440933,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,gaoyunhaii,gaoyunhaii,21/Apr/22 13:44,02/Dec/22 02:56,13/Jul/23 08:08,02/Dec/22 02:56,1.15.0,1.16.0,,,,,1.15.4,1.16.1,1.17.0,,Runtime / Coordination,,,,,0,pull-request-available,,,"If some TaskManagers running with JobManager on the same machine while some other TaskManager not, the TaskManagers running together with JobManager would bind to localhost or 127.0.01, which makes the Netty connections across the TaskManagers fail.
",,gaoyunhaii,huwh,nobleyd,Thesharing,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29572,,,,,FLINK-24474,,FLINK-28115,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 02 02:56:19 UTC 2022,,,,,,,,,,"0|z11ofc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/22 13:52;chesnay;We're currently checking if the loopback address resolution strategy is still necessary; if not we may be able to remove it or hide it behind a feature flag. The recent fixes around yarn related to FLINK-24474 may have removed the ""need"" for it. (Quotation marks because I think we were only fighting symptoms).;;;","26/Aug/22 06:22;nobleyd;Hi, I want to known which release will fix this issue, is there any plan?;;;","06/Sep/22 07:34;gaoyunhaii;Hi [~nobleyd] If currently no other progress, I'll open a PR soon and try to make it be included in 1.16. ;;;","30/Oct/22 04:45;nobleyd;Hi, this issue is not resolve in flink1.16, whilte I think it is important. [~gaoyunhaii] ;;;","31/Oct/22 09:58;gaoyunhaii;Hi [~nobleyd] sorry after I checked the code, it looks more complicated than I initially thought, thus I'm not very confident in changing this part of code right before the deadline without causing other issues and we then moved the fix after the release.

I also still think the issue is critical and cause a lot of confusions, we are still working on this issue and will fix it soon. ;;;","01/Nov/22 12:10;chesnay;[~gaoyunhaii] What makes this so complicated? I thought we just have to drop the loopback address resolution strategy again.;;;","07/Nov/22 06:58;huwh;Hi, [~chesnay] , [~gaoyunhaii]  I think this was introduced by FLINK-24474. It uses the loopback address as the default address. In these configs, it only supports flink clusters running on a single host, and taskmanager must use the loopback interface to connect with the jobmanager, since the jobmanager only binds the loopback interface. But if we don't set the bind-address to localhost, taskmanager should not use the loopback interface to find its external address. otherwise, this will cause other TaskManagers to not connect with it.

IMO, we can determine whether to use the loopback interface by whether the taskmanager.bind-host is loopback address. 

If this change is acceptable, you can assign this to me.;;;","10/Nov/22 00:06;gaoyunhaii;Hi [~chesnay]  sorry for missing the comment, I also think we should drop the lookback address, but it may need some tests for different deployment targets. 

And very thanks [~huwh] for helping tracking this issue! I have assigned the issue to you. And for the method, do you think we might directly remove the LOOPBACK? I previously have some try with [https://github.com/gaoyunhaii/flink/pull/new/remove_loopback] and it should works in standalone session. ;;;","10/Nov/22 08:10;chesnay;It should be safe to just remove it imo.;;;","10/Nov/22 09:12;huwh;[~gaoyunhaii] yes, we can directly remove the LOOPBACK, i will do more tests in different deployment targets.;;;","16/Nov/22 04:07;huwh;[~gaoyunhaii] I tested it in Kubernetes/Docker/Standalone. Rremoving the loopback works fine. Could you help review this PR.;;;","16/Nov/22 05:59;gaoyunhaii;Very thanks [~huwh] for the checking! I'll have a look.;;;","02/Dec/22 02:56;gaoyunhaii;Merged on master via fc5e8bacef34119579defca6256476482da523f9

Merged on 1.15 via 84e8806af16f81a4295ca1f9b0c711c210884b1d

Merged on 1.16 via 8a47420adfe9af7f1c303874c7a32dab3229ea66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add license headers and spotless checks for them,FLINK-27322,13440653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,20/Apr/22 10:21,21/Apr/22 10:35,13/Jul/23 08:08,21/Apr/22 10:35,1.14.4,,,,,,1.15.0,,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,"It looks as if there are a couple of files that are missing their appropriate license headers, e.g. https://github.com/apache/flink-training/blob/0b1c83b16065484200564402bef2ca10ef19cb30/common/src/main/java/org/apache/flink/training/exercises/common/datatypes/RideAndFare.java

We should fix that by:
# adding the missing license headers
# adding spotless checks to ensure this doesn't happen again

Potential downside: if a user doing the training exercises creates files on their own, these would need the license header as well. On the other hand, a simple `./gradlew spotlessApply` can fix that easily",,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 21 10:35:20 UTC 2022,,,,,,,,,,"0|z11mq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/22 10:35;nkruber;Fixed on master via:
- a47f38e7b3e7fe0e1bf372d97852da3b997df461
- bc18cea87ef713fb8e103751c588b0703b08ee22
- 108c8b35dc294c511dc5e57b500d4641d89752ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Duplicated ""-t"" option for savepoint format and deployment target",FLINK-27319,13440643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,20/Apr/22 09:50,20/Apr/22 12:33,13/Jul/23 08:08,20/Apr/22 12:33,1.15.0,,,,,,1.15.0,,,,Command Line Client,,,,,1,pull-request-available,,,"The two options savepoint format and deployment target have the same short option which causes a clash and the CLI to fail.

I suggest to drop the short ""-t"" for savepoint format.",,dwysakowicz,Feifan Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26353,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 12:33:10 UTC 2022,,,,,,,,,,"0|z11mns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/22 12:33;dwysakowicz;Fixed:
* master
** ed302be6566436ca3878a121683ac3b3df8b01bd
* 1.15.0
** bacce4e25ea8293005f189591f1a7393f47110f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the demo of MemoryStateBackendMigration,FLINK-27315,13440551,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Echo Lee,Echo Lee,Echo Lee,20/Apr/22 00:34,22/Apr/22 02:41,13/Jul/23 08:08,21/Apr/22 03:59,1.14.4,,,,,,1.14.5,1.15.0,1.16.0,,Documentation,,,,,0,pull-request-available,,,"There is a problem with the memorystatebackendmigration demo under [state backends doc|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/#code-configuration]

JobManagerStateBackend should be changed to JobManagerCheckpointStorage",,Echo Lee,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 21 03:59:38 UTC 2022,,,,,,,,,,"0|z11m3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/22 01:05;Echo Lee;CC [~yunta] ;;;","20/Apr/22 02:55;yunta;[~Echo Lee]  Thanks for creating this ticket, already assigned to you.;;;","21/Apr/22 03:59;yunta;merged in master: f5b2e6177bf61097a497f0f6040131e17ab32a62
release-1.15: eb0f5c37ce34f95415fe012355a4f33d032c23cd
release-1.14: ee75ae34a5c85e914314405f3aeb0a955c56c6e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkOperatorITCase failure due to JobManager replicas less than one,FLINK-27310,13440346,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bgeng777,usamj,usamj,19/Apr/22 09:52,20/Apr/22 08:36,13/Jul/23 08:08,20/Apr/22 08:34,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"The FlinkOperatorITCase test is currently failing, even in the CI pipeline 

 
{code:java}
INFO] Running org.apache.flink.kubernetes.operator.FlinkOperatorITCase

  

  
    
    Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.178 s <<< FAILURE! - in org.apache.flink.kubernetes.operator.FlinkOperatorITCase

  

  
    
    Error:  org.apache.flink.kubernetes.operator.FlinkOperatorITCase.test  Time elapsed: 2.664 s  <<< ERROR!

  

  
    
    io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/apis/flink.apache.org/v1beta1/namespaces/flink-operator-test/flinkdeployments. Message: Forbidden! User minikube doesn't have permission. admission webhook ""vflinkdeployments.flink.apache.org"" denied the request: JobManager replicas should not be configured less than one..

  

  
    
    	at flink.kubernetes.operator@1.0-SNAPSHOT/org.apache.flink.kubernetes.operator.FlinkOperatorITCase.test(FlinkOperatorITCase.java:86){code}
 

While the test is failing the CI test run is passing which also should be fixed then to fail on the test failure.",,bgeng777,usamj,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 08:35:44 UTC 2022,,,,,,,,,,"0|z11l5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/22 12:59;bgeng777;It seems that this ITCase is not really executed due to its naming(not follow **Test * pattern). 
For the error in the posted log, I believe it is because we have not added {{jm.setReplicas(1);}} to set the the replica of JM.;;;","19/Apr/22 13:06;usamj;What do you mean?

If I look at a recent CI run (e.g. [https://github.com/apache/flink-kubernetes-operator/runs/6077821022?check_suite_focus=true)] I see this in the logs
{code:java}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.flink.kubernetes.operator.FlinkOperatorITCase
Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.744 s <<< FAILURE! - in org.apache.flink.kubernetes.operator.FlinkOperatorITCase
Error:  org.apache.flink.kubernetes.operator.FlinkOperatorITCase.test  Time elapsed: 3.232 s  <<< ERROR!io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/apis/flink.apache.org/v1beta1/namespaces/flink-operator-test/flinkdeployments. Message: Forbidden! User minikube doesn't have permission. admission webhook ""vflinkdeployments.flink.apache.org"" denied the request: JobManager replicas should not be configured less than one..  
  at flink.kubernetes.operator@1.0-SNAPSHOT/org.apache.flink.kubernetes.operator.FlinkOperatorITCase.test(FlinkOperatorITCase.java:86)
[INFO] 
[INFO] Results:
[INFO] Error:  Errors: Error:    FlinkOperatorITCase.test:86 » KubernetesClient Failure executing: POST at: htt...
[INFO] Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------
[INFO] Total time:  43.296 s
[INFO] Finished at: 2022-04-19T12:38:49Z
[INFO] ------------------------------------------------------------------------{code};;;","19/Apr/22 13:45;bgeng777;[~usamj] oh, I tried to find the error in my local machine but ignored that the IT case is only executed in CI pipeline :(

cc [~wangyang0918] I created a [PR|https://github.com/apache/flink-kubernetes-operator/pull/173https://github.com/apache/flink-kubernetes-operator/pull/173] for fixing this IT case.
I think we should improve the ci.yml as well.;;;","20/Apr/22 08:34;wangyang0918;Fixed via:

main:

e0e34cb481af597c2126760a0f4c81bacb7ea284

6e1f0e8c28986b515f7becb9968b9af25c8f4b6a;;;","20/Apr/22 08:35;wangyang0918;Thanks [~usamj] for reporting this issue and [~bgeng777] for fixing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the used table name in the SQL Client documentation to the correct name,FLINK-27298,13440275,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,empcl,empcl,empcl,19/Apr/22 03:23,20/Apr/22 12:12,13/Jul/23 08:08,20/Apr/22 12:12,,,,,,,1.16.0,,,,Documentation,,,,,0,pull-request-available,,,"in sqlClient.md, table name inconsistency problem.

!image-2022-04-19-11-23-46-235.png!",,empcl,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/22 03:23;empcl;image-2022-04-19-11-23-46-235.png;https://issues.apache.org/jira/secure/attachment/13042599/image-2022-04-19-11-23-46-235.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 12:12:22 UTC 2022,,,,,,,,,,"0|z11kq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/22 12:12;martijnvisser;Fixed in master: 13c45ec3866e41f90905e6a268bd1d3d488df05e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of wrong positions mapping in RowCoder,FLINK-27282,13440119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Apr/22 07:13,13/Oct/22 14:03,13/Jul/23 08:08,27/Apr/22 12:59,1.13.6,1.14.4,1.15.0,,,,1.15.1,1.16.0,,,API / Python,,,,,0,pull-request-available,,," !image-2022-04-18-15-12-42-795.png! 
 !image-2022-04-18-15-13-15-045.png! 
 !image-2022-04-18-15-12-58-695.png! ",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/22 07:12;hxbks2ks;image-2022-04-18-15-12-42-795.png;https://issues.apache.org/jira/secure/attachment/13042561/image-2022-04-18-15-12-42-795.png","18/Apr/22 07:13;hxbks2ks;image-2022-04-18-15-12-58-695.png;https://issues.apache.org/jira/secure/attachment/13042560/image-2022-04-18-15-12-58-695.png","18/Apr/22 07:13;hxbks2ks;image-2022-04-18-15-13-15-045.png;https://issues.apache.org/jira/secure/attachment/13042559/image-2022-04-18-15-13-15-045.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 27 12:59:02 UTC 2022,,,,,,,,,,"0|z11jso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/22 12:59;dianfu;Fixed in:
- master via f877a91558ff23f7284599d24c6988c4b6b07746
- release-1.15 via ef4500c3d38e54a673302f7d36c2fad521e97eff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong indentation in doc dev/datastream/operators/windows.md,FLINK-27278,13440063,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,17/Apr/22 18:54,19/Apr/22 07:03,13/Jul/23 08:08,19/Apr/22 07:03,1.15.0,,,,,,1.16.0,,,,Documentation,,,,,0,pull-request-available,,,"Due to special char the current indentation of {{ProcessWindowFunction}} in doc {{dev/datastream/operators/windows.md}} is wrong. It's not very visible in the source code. However, if you visit the formatted doc ([link|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/#processwindowfunction]), you will see the wrong indentation. It can be fixed by using spaces in the example code.",,liuml07,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/22 18:53;liuml07;after.png;https://issues.apache.org/jira/secure/attachment/13042545/after.png","17/Apr/22 18:53;liuml07;before.png;https://issues.apache.org/jira/secure/attachment/13042546/before.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Tue Apr 19 07:03:36 UTC 2022,,,,,,,,,,"0|z11jg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/22 07:03;yunta;merged in master: d3623c9fccbf0a7bee59b5ffdc67c3f7fa0cdf7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The plan for query with local sort is incorrect if adaptive batch scheduler is enabled,FLINK-27272,13439989,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,17/Apr/22 03:41,18/Apr/22 02:42,13/Jul/23 08:08,18/Apr/22 02:42,1.15.0,,,,,,1.15.0,1.16.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,"Add the following test case in ForwardHashExchangeTest


{code:java}
  @Test
    public void testRankWithHashShuffle() {
        util.verifyExecPlan(
                ""SELECT * FROM (SELECT a, b, RANK() OVER(PARTITION BY a ORDER BY b) rk FROM T) WHERE rk <= 10"");
    }
{code}

The result plan is:

{code:java}
Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[true], select=[a, b, w0$o0])
+- Exchange(distribution=[forward])
   +- Sort(orderBy=[a ASC, b ASC])
      +- Exchange(distribution=[hash[a]])
         +- Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[false], select=[a, b])
            +- Sort(orderBy=[a ASC, b ASC])
                +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a, b], metadata=[]]], fields=[a, b])
{code}

There should be an additional {{Exchange(distribution=[forward])}} node between local {{Rank}} and {{Sort}}, other wise if adaptive batch scheduler is enabled but operator chain is disabled, the result may be wrong. Because the parallelism for local {{Rank}} and {{Sort}} should be same, otherwise the adaptive batch scheduler may change their parallelism.

 Local sort agg has the similar problem.
",,aitozi,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 18 02:42:49 UTC 2022,,,,,,,,,,"0|z11izs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/22 02:42;godfreyhe;Fixed in 
master: 456ceb299d0601dee283f718f5d3d0a9d108196e
1.15.0: 1556c3c75535cbb653e4bd8251cb4f0b3ca2e8f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink comes with five built-in BulkWriter factories instead of the mentioned four in filesystem.md.,FLINK-27265,13439832,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,empcl,empcl,empcl,15/Apr/22 12:26,19/Apr/22 08:57,13/Jul/23 08:08,19/Apr/22 08:57,,,,,,,1.16.0,,,,Documentation,,,,,0,pull-request-available,,,"in filesystem.md, flink comes with four built-in BulkWriter factories, in fact, the list has five.",,empcl,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/22 12:25;empcl;image-2022-04-15-20-25-26-150.png;https://issues.apache.org/jira/secure/attachment/13042506/image-2022-04-15-20-25-26-150.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 19 08:57:28 UTC 2022,,,,,,,,,,"0|z11i1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/22 12:41;empcl;[https://github.com/apache/flink/pull/19489]

[~MartijnVisser] Hi, Could you merge this PR? Thanks.;;;","19/Apr/22 08:57;martijnvisser;Fixed in master: 6f704215fa6ed04124b09dfbe5835c903b206b45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator triggers savepoint failed because of not all tasks running,FLINK-27257,13439748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,wangyang0918,wangyang0918,15/Apr/22 02:52,07/Jun/22 09:33,13/Jul/23 08:08,07/Jun/22 09:33,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"{code:java}
2022-04-15 02:38:56,551 o.a.f.k.o.s.FlinkService       [INFO ][default/flink-example-statemachine] Fetching savepoint result with triggerId: 182d7f176496856d7b33fe2f3767da18
2022-04-15 02:38:56,690 o.a.f.k.o.s.FlinkService       [ERROR][default/flink-example-statemachine] Savepoint error
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Source: Custom Source (1/2) of job 00000000000000000000000000000000 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
    at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:143)
    at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:105)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-04-15 02:38:56,693 o.a.f.k.o.o.SavepointObserver  [ERROR][default/flink-example-statemachine] Checkpoint triggering task Source: Custom Source (1/2) of job 00000000000000000000000000000000 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running. {code}
How to reproduce?

Update arbitrary fields(e.g. parallelism) along with {{{}savepointTriggerNonce{}}}.

 

The root cause might be the running state return by {{ClusterClient#listJobs()}} does not mean all the tasks are running.",,aitozi,gyfora,nicholasjiang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 09:33:04 UTC 2022,,,,,,,,,,"0|z11hio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/22 03:21;wangyang0918;The expected behavior is the subsequent reconciliation should trigger the savepoint again.;;;","16/Apr/22 13:40;aitozi;I also encountered this problem, I think it is caused by the {{ApplicationReconciler#isJobRunning}} results is not exact ;;;","18/Apr/22 08:11;wangyang0918;Instead of triggering the savepoint again after failure, now I prefer to improve the {{isJobRunning}} implementation which could actually reflect the job status. If it still fails to trigger the savepoint, we need to ensure the error in status is not cleared accidently.;;;","10/May/22 13:54;gyfora;[~nicholasjiang] are you still working on this?;;;","18/May/22 01:44;nicholasjiang;I have offline discussed with [~wangyang0918]. It's hard to improve the isJobRunning implementation because the returned JobStatusMessages of ClusterClient#listJobs() don't contain the tasksPerState of the JobDetail, which cause that there is no way to judge whether all the ExecutionState of tasks are running in the job and for batch tasks how to judge whether the job is really running.

The current idea is that if the error is found to be not all required tasks are currently running, then continue to trigger savepoint in the next reconciliation until it is successfully triggered.

[~gyfora], [~wangyang0918] WDYT?;;;","18/May/22 06:18;gyfora;Sounds good :);;;","18/May/22 09:09;wangyang0918;Then FLINK-27675 might be related.;;;","31/May/22 09:09;gyfora;I will take over this issue [~nicholasjiang] if you dont mind. I have been working on some improvements on the general savepoint triggering/management area and have some concrete ideas how to solve this in a robust way.;;;","01/Jun/22 06:21;nicholasjiang;[~gyfora], I don't mind. I will take a look at the implementation of savepoint triggering/management.  Thanks.;;;","07/Jun/22 09:33;gyfora;Merged to main 4f2f0d392959f70bbb7b012403f1d6c9c16477cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-avro does not support serialization and deserialization of avro schema longer than 65535 characters,FLINK-27255,13439733,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Keathalin21,haizhouz,haizhouz,15/Apr/22 00:15,11/Apr/23 13:19,13/Jul/23 08:08,10/May/22 23:25,1.14.4,,,,,,1.14.5,1.15.1,1.16.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,2,pull-request-available,,,"The underlying serialization of avro schema uses string serialization method of ObjectOutputStream.class, however, the default string serialization by ObjectOutputStream.class does not support handling string of more than 66535 characters (64kb). As a result, constructing flink operators that input/output Avro Generic Record with huge schema is not possible.

 

The purposed fix is two change the serialization and deserialization method of these following classes so that huge string could also be handled.

 

[GenericRecordAvroTypeInfo|https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/GenericRecordAvroTypeInfo.java#L107]

[SerializableAvroSchema|https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/SerializableAvroSchema.java#L55]

 ",,danderson,elkhand,fsk119,haizhouz,jinyius,stevenz3wu,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22727,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Wed Jun 15 12:42:24 UTC 2022,,,,,,,,,,"0|z11hfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/22 00:30;jinyius;has this issue existed for awhile now (1.11+)?  or, is it just constrained to 1.14.4?;;;","15/Apr/22 20:16;stevenz3wu;[~jinyius] this issue existed for a while now. not sth new.;;;","12/May/22 16:32;thw;[~Keathalin21] please also open a PR for 1.15.x;;;","13/May/22 18:03;haizhouz;Here it is: https://github.com/apache/flink/pull/19719;;;","15/Jun/22 12:42;danderson;master:

fd6571c33e998a9c7c0786e189569f918441a71d

1389f6818272d2277c64f3146ff8c9711c5db6a0

531cc90831960a978672e521ee09437d4b804993

1.15:

b9e4b181102292f7b4a164066cee125801853e3b

1.14:

e1572620b570e6a90ca2913d30c7930700ede699

4cefc6b5fa2c9fc95de4d614dd4442a616ca14a7

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalarOperatorGens.numericCasting is not compatible with legacy behavior,FLINK-27247,13439603,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,14/Apr/22 08:06,21/Apr/22 06:15,13/Jul/23 08:08,21/Apr/22 06:15,,,,,,,1.15.1,1.16.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,"Add the following test cases in ScalarFunctionsTest:
{code:java}
// code placeholder
@Test
def test(): Unit ={
  testSqlApi(""rand(1) + 1"","""")
} {code}
it will throw the following exception:
{code:java}
// code placeholder
org.apache.flink.table.planner.codegen.CodeGenException: Unsupported casting from DOUBLE to DOUBLE NOT NULL.
    at org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens$.numericCasting(ScalarOperatorGens.scala:1734)
    at org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens$.generateBinaryArithmeticOperator(ScalarOperatorGens.scala:85)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:507)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:481)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:57)
    at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.$anonfun$visitCall$1(ExprCodeGenerator.scala:478)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:469)
... {code}
This is because in ScalarOperatorGens#numericCasting,  FLINK-24779  lost the logic that in some cases there is no need to casting the left and right type.",,matriv,twalthr,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 21 06:15:41 UTC 2022,,,,,,,,,,"0|z11gmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/22 08:10;xuyangzhong;I'll try to fix it;;;","14/Apr/22 15:01;matriv;Commented on the PR as well, I think we need to fix the root of the issue which is the implicit cast introduced to cast from a nullable to a not nullable type, and not just allow this in the code gen.;;;","15/Apr/22 02:55;xuyangzhong;Hi, [~matriv], I think currently this is not the problem with casting. Because this part of code gen doesn't not care of the nullable with the type and other part of code gen will avoid the nullable exists. For example, you can see the following code generated:
{code:java}
// the result rand(...) will be null only if the arg in rand is nullable, like ""rand(cast (null as int))""
// so if the sql is ""rand() + 1"", the generated code is:
        
    isNull$3 = false;
    result$4 = random$2.nextDouble();

// and if the sql is ""rand(cast(null as int))"":

    isNull$3 = true;
    result$4 = -1.0d;
    if (!isNull$3) {
          result$4 = random$2.nextDouble();
    }{code}
This part that i fixed is only about the code : ""result$4 = random$2.nextDouble();""  and this should just ignore the nullable between DOUBLE and DOUBLE NOT NULL. And actually the logic of the legacy code does this by pre-checking the same type not necessary to cast before casting this different types.

I strongly agree with you that the logic in casting should throw an exception if it meets casting a type from nullable to not nullable. But the problem of this issue is before casting logic.

By the way, I think converting casting logic to different rules is a good improvement but should not affect the base logic when change the code. You can see the code before and after rewriting casting rules:

before:
{code:java}
// no casting necessary
if (isInteroperable(operandType, resultType)) {
  operandTerm => s""$operandTerm""
}
// decimal to decimal, may have different precision/scale
else if (isDecimal(resultType) && isDecimal(operandType)) {
  val dt = resultType.asInstanceOf[DecimalType]
  operandTerm =>
    s""$DECIMAL_UTIL.castToDecimal($operandTerm, ${dt.getPrecision}, ${dt.getScale})""
}
// non_decimal_numeric to decimal
else if ...{code}
after:
{code:java}
// All numeric rules are assumed to be instance of AbstractExpressionCodeGeneratorCastRule
val rule = CastRuleProvider.resolve(operandType, resultType)
rule match {
  case codeGeneratorCastRule: ExpressionCodeGeneratorCastRule[_, _] =>
    operandTerm =>
      codeGeneratorCastRule.generateExpression(
        toCodegenCastContext(ctx),
        operandTerm,
        operandType,
        resultType
      )
  case _ =>
    throw new CodeGenException(s""Unsupported casting from $operandType to $resultType."")
} {code}
Looking forward to your reply :)

 

 ;;;","19/Apr/22 11:20;matriv;We can fix the issue with the quick fix in your PR.

We have to open an issue, so that these kind of explicit casting is introduced with a Rule during the analysis/planning phase, and not in the code generation.

This way, it will be visible what kind of implicit cast is introduced, by looking at the plan, and not just generate code for it under the hood.;;;","20/Apr/22 03:22;xuyangzhong;+1;;;","21/Apr/22 06:15;twalthr;Fixed in master: fcbbcf9bf91dc15ac5fc6d37a0b1445aa897f0d4
Fixed in 1.15: b9727d5b1914b64395e58cd6783f2f3a2f4957b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""HashAggregateWithKeys$9211"" grows beyond 64 KB",FLINK-27246,13439590,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,KristoffSC,maver1ck,maver1ck,14/Apr/22 07:32,09/Feb/23 09:32,13/Jul/23 08:08,09/Feb/23 09:32,1.14.3,1.15.3,1.16.1,,,,1.16.2,1.17.0,,,Table SQL / Runtime,,,,,1,pull-request-available,,,"I think this bug should get fixed in https://issues.apache.org/jira/browse/FLINK-23007
Unfortunately I spotted it on Flink 1.14.3


{code}

java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$9211'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:85) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:198) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:666) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	... 11 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	... 11 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	... 11 more
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""HashAggregateWithKeys$9211"": Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""HashAggregateWithKeys$9211"" grows beyond 64 KB
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	... 11 more
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""HashAggregateWithKeys$9211"" grows beyond 64 KB
	at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.CodeContext.write(CodeContext.java:940) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:12282) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11941) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11926) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$8000(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4408) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:4274) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4461) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$7500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4403) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4120) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3957) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4448) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5004) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$8500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4417) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5057) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$8100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4409) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$ParenthesizedExpression.accept(Java.java:4924) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1]
	... 11 more
{code}",,aitozi,godfrey,jingge,KristoffSC,leonard,libenchao,liuleng,mason6345,maver1ck,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23007,,,,,,,"02/Nov/22 10:33;KristoffSC;endInput_falseFilter9123_split9704.txt;https://issues.apache.org/jira/secure/attachment/13051709/endInput_falseFilter9123_split9704.txt",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 09 09:32:03 UTC 2023,,,,,,,,,,"0|z11gjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/22 10:02;twalthr;Can you provide a reproducible example? Maybe [~TsReaper] knows how to fix it then. The pure exception might not provide enough information.;;;","10/Jun/22 06:24;jingge;Hi [~maver1ck] could you share the SQL that we could reproduce the issue?;;;","10/Jun/22 08:37;maver1ck;I'm trying to build it using datagen and removing UDFs. 
Will be ready next week.;;;","30/Jun/22 12:57;maver1ck;[~jingge] [~twalthr]
Please find following example from our production (with change to datagen)
Tested on 1.14.4 sql-client

{code:java}
SET 'execution.runtime-mode' = 'batch';


CREATE TABLE `SA_NSN5G_PM.GSITE_PDCP2_R` (
  `DAY_WH_ID` DECIMAL(10, 0),
  `GSITE_WH_ID` DECIMAL(10, 0),
  `START_TIME` TIMESTAMP(6),
  `INSERTION_TIME` TIMESTAMP(6),
  `MRBTS_ID` DECIMAL(10, 0),
  `NRBTS_ID` DECIMAL(10, 0),
  `DURATION` DECIMAL(10, 0),
  `MEAS_PERIOD_NR_PDCP_REPORT2` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_01` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_02` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_03` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_04` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_05` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_06` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_07` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_08` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_09` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_10` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_11` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_12` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_13` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_14` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_15` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_16` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_17` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_18` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_19` DECIMAL(10, 0),
  `DL_VOL_DISC_REL_QOS_GRP_20` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_01` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_02` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_03` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_04` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_05` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_06` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_07` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_08` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_09` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_10` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_11` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_12` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_13` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_14` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_15` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_16` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_17` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_18` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_19` DECIMAL(10, 0),
  `DL_SDU_DISC_REL_QOS_GRP_20` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_01` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_02` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_03` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_04` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_05` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_06` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_07` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_08` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_09` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_10` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_11` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_12` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_13` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_14` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_15` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_16` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_17` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_18` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_19` DECIMAL(10, 0),
  `DL_VOL_DISC_OTHER_QOS_GRP_20` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_01` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_02` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_03` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_04` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_05` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_06` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_07` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_08` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_09` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_10` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_11` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_12` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_13` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_14` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_15` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_16` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_17` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_18` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_19` DECIMAL(10, 0),
  `DL_SDU_DISC_OTHER_QOS_GRP_20` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_01` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_02` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_03` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_04` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_05` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_06` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_07` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_08` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_09` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_10` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_11` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_12` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_13` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_14` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_15` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_16` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_17` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_18` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_19` DECIMAL(10, 0),
  `DL_VOL_DISC_BUFTH_QOS_GRP_20` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_01` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_02` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_03` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_04` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_05` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_06` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_07` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_08` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_09` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_10` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_11` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_12` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_13` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_14` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_15` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_16` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_17` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_18` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_19` DECIMAL(10, 0),
  `DL_SDU_DISC_BUFTH_QOS_GRP_20` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_01` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_02` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_03` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_04` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_05` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_06` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_07` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_08` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_09` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_10` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_11` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_12` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_13` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_14` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_15` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_16` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_17` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_18` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_19` DECIMAL(10, 0),
  `UL_VOL_DISC_OTHER_QOS_GRP_20` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_01` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_02` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_03` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_04` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_05` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_06` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_07` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_08` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_09` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_10` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_11` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_12` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_13` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_14` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_15` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_16` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_17` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_18` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_19` DECIMAL(10, 0),
  `UL_SDU_DISC_OTHER_QOS_GRP_20` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_01` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_02` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_03` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_04` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_05` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_06` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_07` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_08` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_09` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_10` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_11` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_12` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_13` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_14` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_15` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_16` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_17` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_18` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_19` DECIMAL(10, 0),
  `DL_SDU_DISC_QUEDLY_QOS_GRP_20` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_01` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_02` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_03` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_04` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_05` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_06` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_07` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_08` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_09` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_10` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_11` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_12` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_13` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_14` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_15` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_16` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_17` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_18` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_19` DECIMAL(10, 0),
  `DL_VOL_DISC_QUEDLY_QOS_GRP_20` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_01` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_02` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_03` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_04` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_05` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_06` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_07` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_08` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_09` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_10` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_11` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_12` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_13` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_14` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_15` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_16` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_17` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_18` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_19` DECIMAL(10, 0),
  `DL_DELAY_DISC_QOS_GRP_20` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_01` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_02` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_03` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_04` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_05` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_06` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_07` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_08` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_09` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_10` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_11` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_12` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_13` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_14` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_15` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_16` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_17` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_18` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_19` DECIMAL(10, 0),
  `NUM_RETRANS_QOS_GRP_20` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_01` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_02` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_03` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_04` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_05` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_06` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_07` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_08` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_09` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_10` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_11` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_12` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_13` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_14` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_15` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_16` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_17` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_18` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_19` DECIMAL(10, 0),
  `VOL_RETRANS_QOS_GRP_20` DECIMAL(10, 0),
  `MAX_NUM_OF_DRB` DECIMAL(10, 0),
  `DL_NUM_ROHC_COMP_HEADER` DECIMAL(10, 0),
  `DL_VOL_ROHC_COMP_HEADER` DECIMAL(10, 0),
  `UL_NUM_ROHC_DECOMP_HEADER` DECIMAL(10, 0),
  `UL_VOL_ROHC_DECOMP_HEADER` DECIMAL(10, 0),
  `KAFKA_PARTID` DECIMAL(10, 0),
  `KAFKA_OFFSET` INT
)
COMMENT ''
WITH (
  'connector' = 'datagen',
  'number-of-rows' = '100'
);

SELECT
    MINUTE(T1.START_TIME) as `DAY_WH_ID`,
    HOUR(T1.START_TIME) as `START_TIME`,
    GSITE_WH_ID as `GSITE_WH_ID`,
    T1.MRBTS_ID as `MRBTS_ID`,
    T1.NRBTS_ID as `NRBTS_ID`,
    SUM(T1.DURATION) as `DURATION`, SUM(MEAS_PERIOD_NR_PDCP_REPORT2) as `MEAS_PERIOD_NR_PDCP_REPORT2`, SUM(DL_VOL_DISC_REL_QOS_GRP_01) as `DL_VOL_DISC_REL_QOS_GRP_01`, SUM(DL_VOL_DISC_REL_QOS_GRP_02) as `DL_VOL_DISC_REL_QOS_GRP_02`, SUM(DL_VOL_DISC_REL_QOS_GRP_03) as `DL_VOL_DISC_REL_QOS_GRP_03`, SUM(DL_VOL_DISC_REL_QOS_GRP_04) as `DL_VOL_DISC_REL_QOS_GRP_04`, SUM(DL_VOL_DISC_REL_QOS_GRP_05) as `DL_VOL_DISC_REL_QOS_GRP_05`, SUM(DL_VOL_DISC_REL_QOS_GRP_06) as `DL_VOL_DISC_REL_QOS_GRP_06`, SUM(DL_VOL_DISC_REL_QOS_GRP_07) as `DL_VOL_DISC_REL_QOS_GRP_07`, SUM(DL_VOL_DISC_REL_QOS_GRP_08) as `DL_VOL_DISC_REL_QOS_GRP_08`, SUM(DL_VOL_DISC_REL_QOS_GRP_09) as `DL_VOL_DISC_REL_QOS_GRP_09`, SUM(DL_VOL_DISC_REL_QOS_GRP_10) as `DL_VOL_DISC_REL_QOS_GRP_10`, SUM(DL_VOL_DISC_REL_QOS_GRP_11) as `DL_VOL_DISC_REL_QOS_GRP_11`, SUM(DL_VOL_DISC_REL_QOS_GRP_12) as `DL_VOL_DISC_REL_QOS_GRP_12`, SUM(DL_VOL_DISC_REL_QOS_GRP_13) as `DL_VOL_DISC_REL_QOS_GRP_13`, SUM(DL_VOL_DISC_REL_QOS_GRP_14) as `DL_VOL_DISC_REL_QOS_GRP_14`, SUM(DL_VOL_DISC_REL_QOS_GRP_15) as `DL_VOL_DISC_REL_QOS_GRP_15`, SUM(DL_VOL_DISC_REL_QOS_GRP_16) as `DL_VOL_DISC_REL_QOS_GRP_16`, SUM(DL_VOL_DISC_REL_QOS_GRP_17) as `DL_VOL_DISC_REL_QOS_GRP_17`, SUM(DL_VOL_DISC_REL_QOS_GRP_18) as `DL_VOL_DISC_REL_QOS_GRP_18`, SUM(DL_VOL_DISC_REL_QOS_GRP_19) as `DL_VOL_DISC_REL_QOS_GRP_19`, SUM(DL_VOL_DISC_REL_QOS_GRP_20) as `DL_VOL_DISC_REL_QOS_GRP_20`, SUM(DL_SDU_DISC_REL_QOS_GRP_01) as `DL_SDU_DISC_REL_QOS_GRP_01`, SUM(DL_SDU_DISC_REL_QOS_GRP_02) as `DL_SDU_DISC_REL_QOS_GRP_02`, SUM(DL_SDU_DISC_REL_QOS_GRP_03) as `DL_SDU_DISC_REL_QOS_GRP_03`, SUM(DL_SDU_DISC_REL_QOS_GRP_04) as `DL_SDU_DISC_REL_QOS_GRP_04`, SUM(DL_SDU_DISC_REL_QOS_GRP_05) as `DL_SDU_DISC_REL_QOS_GRP_05`, SUM(DL_SDU_DISC_REL_QOS_GRP_06) as `DL_SDU_DISC_REL_QOS_GRP_06`, SUM(DL_SDU_DISC_REL_QOS_GRP_07) as `DL_SDU_DISC_REL_QOS_GRP_07`, SUM(DL_SDU_DISC_REL_QOS_GRP_08) as `DL_SDU_DISC_REL_QOS_GRP_08`, SUM(DL_SDU_DISC_REL_QOS_GRP_09) as `DL_SDU_DISC_REL_QOS_GRP_09`, SUM(DL_SDU_DISC_REL_QOS_GRP_10) as `DL_SDU_DISC_REL_QOS_GRP_10`, SUM(DL_SDU_DISC_REL_QOS_GRP_11) as `DL_SDU_DISC_REL_QOS_GRP_11`, SUM(DL_SDU_DISC_REL_QOS_GRP_12) as `DL_SDU_DISC_REL_QOS_GRP_12`, SUM(DL_SDU_DISC_REL_QOS_GRP_13) as `DL_SDU_DISC_REL_QOS_GRP_13`, SUM(DL_SDU_DISC_REL_QOS_GRP_14) as `DL_SDU_DISC_REL_QOS_GRP_14`, SUM(DL_SDU_DISC_REL_QOS_GRP_15) as `DL_SDU_DISC_REL_QOS_GRP_15`, SUM(DL_SDU_DISC_REL_QOS_GRP_16) as `DL_SDU_DISC_REL_QOS_GRP_16`, SUM(DL_SDU_DISC_REL_QOS_GRP_17) as `DL_SDU_DISC_REL_QOS_GRP_17`, SUM(DL_SDU_DISC_REL_QOS_GRP_18) as `DL_SDU_DISC_REL_QOS_GRP_18`, SUM(DL_SDU_DISC_REL_QOS_GRP_19) as `DL_SDU_DISC_REL_QOS_GRP_19`, SUM(DL_SDU_DISC_REL_QOS_GRP_20) as `DL_SDU_DISC_REL_QOS_GRP_20`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_01) as `DL_VOL_DISC_OTHER_QOS_GRP_01`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_02) as `DL_VOL_DISC_OTHER_QOS_GRP_02`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_03) as `DL_VOL_DISC_OTHER_QOS_GRP_03`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_04) as `DL_VOL_DISC_OTHER_QOS_GRP_04`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_05) as `DL_VOL_DISC_OTHER_QOS_GRP_05`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_06) as `DL_VOL_DISC_OTHER_QOS_GRP_06`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_07) as `DL_VOL_DISC_OTHER_QOS_GRP_07`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_08) as `DL_VOL_DISC_OTHER_QOS_GRP_08`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_09) as `DL_VOL_DISC_OTHER_QOS_GRP_09`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_10) as `DL_VOL_DISC_OTHER_QOS_GRP_10`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_11) as `DL_VOL_DISC_OTHER_QOS_GRP_11`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_12) as `DL_VOL_DISC_OTHER_QOS_GRP_12`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_13) as `DL_VOL_DISC_OTHER_QOS_GRP_13`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_14) as `DL_VOL_DISC_OTHER_QOS_GRP_14`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_15) as `DL_VOL_DISC_OTHER_QOS_GRP_15`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_16) as `DL_VOL_DISC_OTHER_QOS_GRP_16`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_17) as `DL_VOL_DISC_OTHER_QOS_GRP_17`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_18) as `DL_VOL_DISC_OTHER_QOS_GRP_18`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_19) as `DL_VOL_DISC_OTHER_QOS_GRP_19`, SUM(DL_VOL_DISC_OTHER_QOS_GRP_20) as `DL_VOL_DISC_OTHER_QOS_GRP_20`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_01) as `DL_SDU_DISC_OTHER_QOS_GRP_01`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_02) as `DL_SDU_DISC_OTHER_QOS_GRP_02`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_03) as `DL_SDU_DISC_OTHER_QOS_GRP_03`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_04) as `DL_SDU_DISC_OTHER_QOS_GRP_04`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_05) as `DL_SDU_DISC_OTHER_QOS_GRP_05`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_06) as `DL_SDU_DISC_OTHER_QOS_GRP_06`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_07) as `DL_SDU_DISC_OTHER_QOS_GRP_07`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_08) as `DL_SDU_DISC_OTHER_QOS_GRP_08`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_09) as `DL_SDU_DISC_OTHER_QOS_GRP_09`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_10) as `DL_SDU_DISC_OTHER_QOS_GRP_10`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_11) as `DL_SDU_DISC_OTHER_QOS_GRP_11`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_12) as `DL_SDU_DISC_OTHER_QOS_GRP_12`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_13) as `DL_SDU_DISC_OTHER_QOS_GRP_13`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_14) as `DL_SDU_DISC_OTHER_QOS_GRP_14`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_15) as `DL_SDU_DISC_OTHER_QOS_GRP_15`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_16) as `DL_SDU_DISC_OTHER_QOS_GRP_16`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_17) as `DL_SDU_DISC_OTHER_QOS_GRP_17`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_18) as `DL_SDU_DISC_OTHER_QOS_GRP_18`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_19) as `DL_SDU_DISC_OTHER_QOS_GRP_19`, SUM(DL_SDU_DISC_OTHER_QOS_GRP_20) as `DL_SDU_DISC_OTHER_QOS_GRP_20`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_01) as `DL_VOL_DISC_BUFTH_QOS_GRP_01`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_02) as `DL_VOL_DISC_BUFTH_QOS_GRP_02`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_03) as `DL_VOL_DISC_BUFTH_QOS_GRP_03`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_04) as `DL_VOL_DISC_BUFTH_QOS_GRP_04`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_05) as `DL_VOL_DISC_BUFTH_QOS_GRP_05`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_06) as `DL_VOL_DISC_BUFTH_QOS_GRP_06`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_07) as `DL_VOL_DISC_BUFTH_QOS_GRP_07`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_08) as `DL_VOL_DISC_BUFTH_QOS_GRP_08`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_09) as `DL_VOL_DISC_BUFTH_QOS_GRP_09`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_10) as `DL_VOL_DISC_BUFTH_QOS_GRP_10`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_11) as `DL_VOL_DISC_BUFTH_QOS_GRP_11`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_12) as `DL_VOL_DISC_BUFTH_QOS_GRP_12`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_13) as `DL_VOL_DISC_BUFTH_QOS_GRP_13`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_14) as `DL_VOL_DISC_BUFTH_QOS_GRP_14`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_15) as `DL_VOL_DISC_BUFTH_QOS_GRP_15`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_16) as `DL_VOL_DISC_BUFTH_QOS_GRP_16`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_17) as `DL_VOL_DISC_BUFTH_QOS_GRP_17`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_18) as `DL_VOL_DISC_BUFTH_QOS_GRP_18`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_19) as `DL_VOL_DISC_BUFTH_QOS_GRP_19`, SUM(DL_VOL_DISC_BUFTH_QOS_GRP_20) as `DL_VOL_DISC_BUFTH_QOS_GRP_20`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_01) as `DL_SDU_DISC_BUFTH_QOS_GRP_01`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_02) as `DL_SDU_DISC_BUFTH_QOS_GRP_02`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_03) as `DL_SDU_DISC_BUFTH_QOS_GRP_03`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_04) as `DL_SDU_DISC_BUFTH_QOS_GRP_04`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_05) as `DL_SDU_DISC_BUFTH_QOS_GRP_05`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_06) as `DL_SDU_DISC_BUFTH_QOS_GRP_06`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_07) as `DL_SDU_DISC_BUFTH_QOS_GRP_07`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_08) as `DL_SDU_DISC_BUFTH_QOS_GRP_08`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_09) as `DL_SDU_DISC_BUFTH_QOS_GRP_09`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_10) as `DL_SDU_DISC_BUFTH_QOS_GRP_10`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_11) as `DL_SDU_DISC_BUFTH_QOS_GRP_11`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_12) as `DL_SDU_DISC_BUFTH_QOS_GRP_12`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_13) as `DL_SDU_DISC_BUFTH_QOS_GRP_13`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_14) as `DL_SDU_DISC_BUFTH_QOS_GRP_14`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_15) as `DL_SDU_DISC_BUFTH_QOS_GRP_15`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_16) as `DL_SDU_DISC_BUFTH_QOS_GRP_16`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_17) as `DL_SDU_DISC_BUFTH_QOS_GRP_17`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_18) as `DL_SDU_DISC_BUFTH_QOS_GRP_18`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_19) as `DL_SDU_DISC_BUFTH_QOS_GRP_19`, SUM(DL_SDU_DISC_BUFTH_QOS_GRP_20) as `DL_SDU_DISC_BUFTH_QOS_GRP_20`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_01) as `UL_VOL_DISC_OTHER_QOS_GRP_01`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_02) as `UL_VOL_DISC_OTHER_QOS_GRP_02`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_03) as `UL_VOL_DISC_OTHER_QOS_GRP_03`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_04) as `UL_VOL_DISC_OTHER_QOS_GRP_04`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_05) as `UL_VOL_DISC_OTHER_QOS_GRP_05`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_06) as `UL_VOL_DISC_OTHER_QOS_GRP_06`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_07) as `UL_VOL_DISC_OTHER_QOS_GRP_07`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_08) as `UL_VOL_DISC_OTHER_QOS_GRP_08`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_09) as `UL_VOL_DISC_OTHER_QOS_GRP_09`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_10) as `UL_VOL_DISC_OTHER_QOS_GRP_10`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_11) as `UL_VOL_DISC_OTHER_QOS_GRP_11`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_12) as `UL_VOL_DISC_OTHER_QOS_GRP_12`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_13) as `UL_VOL_DISC_OTHER_QOS_GRP_13`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_14) as `UL_VOL_DISC_OTHER_QOS_GRP_14`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_15) as `UL_VOL_DISC_OTHER_QOS_GRP_15`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_16) as `UL_VOL_DISC_OTHER_QOS_GRP_16`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_17) as `UL_VOL_DISC_OTHER_QOS_GRP_17`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_18) as `UL_VOL_DISC_OTHER_QOS_GRP_18`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_19) as `UL_VOL_DISC_OTHER_QOS_GRP_19`, SUM(UL_VOL_DISC_OTHER_QOS_GRP_20) as `UL_VOL_DISC_OTHER_QOS_GRP_20`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_01) as `UL_SDU_DISC_OTHER_QOS_GRP_01`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_02) as `UL_SDU_DISC_OTHER_QOS_GRP_02`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_03) as `UL_SDU_DISC_OTHER_QOS_GRP_03`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_04) as `UL_SDU_DISC_OTHER_QOS_GRP_04`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_05) as `UL_SDU_DISC_OTHER_QOS_GRP_05`,
    SUM(UL_SDU_DISC_OTHER_QOS_GRP_06) as `UL_SDU_DISC_OTHER_QOS_GRP_06`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_07) as `UL_SDU_DISC_OTHER_QOS_GRP_07`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_08) as `UL_SDU_DISC_OTHER_QOS_GRP_08`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_09) as `UL_SDU_DISC_OTHER_QOS_GRP_09`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_10) as `UL_SDU_DISC_OTHER_QOS_GRP_10`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_11) as `UL_SDU_DISC_OTHER_QOS_GRP_11`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_12) as `UL_SDU_DISC_OTHER_QOS_GRP_12`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_13) as `UL_SDU_DISC_OTHER_QOS_GRP_13`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_14) as `UL_SDU_DISC_OTHER_QOS_GRP_14`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_15) as `UL_SDU_DISC_OTHER_QOS_GRP_15`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_16) as `UL_SDU_DISC_OTHER_QOS_GRP_16`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_17) as `UL_SDU_DISC_OTHER_QOS_GRP_17`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_18) as `UL_SDU_DISC_OTHER_QOS_GRP_18`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_19) as `UL_SDU_DISC_OTHER_QOS_GRP_19`, SUM(UL_SDU_DISC_OTHER_QOS_GRP_20) as `UL_SDU_DISC_OTHER_QOS_GRP_20`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_01) as `DL_SDU_DISC_QUEDLY_QOS_GRP_01`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_02) as `DL_SDU_DISC_QUEDLY_QOS_GRP_02`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_03) as `DL_SDU_DISC_QUEDLY_QOS_GRP_03`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_04) as `DL_SDU_DISC_QUEDLY_QOS_GRP_04`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_05) as `DL_SDU_DISC_QUEDLY_QOS_GRP_05`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_06) as `DL_SDU_DISC_QUEDLY_QOS_GRP_06`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_07) as `DL_SDU_DISC_QUEDLY_QOS_GRP_07`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_08) as `DL_SDU_DISC_QUEDLY_QOS_GRP_08`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_09) as `DL_SDU_DISC_QUEDLY_QOS_GRP_09`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_10) as `DL_SDU_DISC_QUEDLY_QOS_GRP_10`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_11) as `DL_SDU_DISC_QUEDLY_QOS_GRP_11`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_12) as `DL_SDU_DISC_QUEDLY_QOS_GRP_12`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_13) as `DL_SDU_DISC_QUEDLY_QOS_GRP_13`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_14) as `DL_SDU_DISC_QUEDLY_QOS_GRP_14`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_15) as `DL_SDU_DISC_QUEDLY_QOS_GRP_15`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_16) as `DL_SDU_DISC_QUEDLY_QOS_GRP_16`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_17) as `DL_SDU_DISC_QUEDLY_QOS_GRP_17`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_18) as `DL_SDU_DISC_QUEDLY_QOS_GRP_18`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_19) as `DL_SDU_DISC_QUEDLY_QOS_GRP_19`, SUM(DL_SDU_DISC_QUEDLY_QOS_GRP_20) as `DL_SDU_DISC_QUEDLY_QOS_GRP_20`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_01) as `DL_VOL_DISC_QUEDLY_QOS_GRP_01`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_02) as `DL_VOL_DISC_QUEDLY_QOS_GRP_02`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_03) as `DL_VOL_DISC_QUEDLY_QOS_GRP_03`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_04) as `DL_VOL_DISC_QUEDLY_QOS_GRP_04`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_05) as `DL_VOL_DISC_QUEDLY_QOS_GRP_05`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_06) as `DL_VOL_DISC_QUEDLY_QOS_GRP_06`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_07) as `DL_VOL_DISC_QUEDLY_QOS_GRP_07`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_08) as `DL_VOL_DISC_QUEDLY_QOS_GRP_08`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_09) as `DL_VOL_DISC_QUEDLY_QOS_GRP_09`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_10) as `DL_VOL_DISC_QUEDLY_QOS_GRP_10`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_11) as `DL_VOL_DISC_QUEDLY_QOS_GRP_11`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_12) as `DL_VOL_DISC_QUEDLY_QOS_GRP_12`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_13) as `DL_VOL_DISC_QUEDLY_QOS_GRP_13`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_14) as `DL_VOL_DISC_QUEDLY_QOS_GRP_14`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_15) as `DL_VOL_DISC_QUEDLY_QOS_GRP_15`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_16) as `DL_VOL_DISC_QUEDLY_QOS_GRP_16`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_17) as `DL_VOL_DISC_QUEDLY_QOS_GRP_17`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_18) as `DL_VOL_DISC_QUEDLY_QOS_GRP_18`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_19) as `DL_VOL_DISC_QUEDLY_QOS_GRP_19`, SUM(DL_VOL_DISC_QUEDLY_QOS_GRP_20) as `DL_VOL_DISC_QUEDLY_QOS_GRP_20`, SUM(DL_DELAY_DISC_QOS_GRP_01) as `DL_DELAY_DISC_QOS_GRP_01`, SUM(DL_DELAY_DISC_QOS_GRP_02) as `DL_DELAY_DISC_QOS_GRP_02`, SUM(DL_DELAY_DISC_QOS_GRP_03) as `DL_DELAY_DISC_QOS_GRP_03`, SUM(DL_DELAY_DISC_QOS_GRP_04) as `DL_DELAY_DISC_QOS_GRP_04`, SUM(DL_DELAY_DISC_QOS_GRP_05) as `DL_DELAY_DISC_QOS_GRP_05`, SUM(DL_DELAY_DISC_QOS_GRP_06) as `DL_DELAY_DISC_QOS_GRP_06`, SUM(DL_DELAY_DISC_QOS_GRP_07) as `DL_DELAY_DISC_QOS_GRP_07`, SUM(DL_DELAY_DISC_QOS_GRP_08) as `DL_DELAY_DISC_QOS_GRP_08`, SUM(DL_DELAY_DISC_QOS_GRP_09) as `DL_DELAY_DISC_QOS_GRP_09`, SUM(DL_DELAY_DISC_QOS_GRP_10) as `DL_DELAY_DISC_QOS_GRP_10`, SUM(DL_DELAY_DISC_QOS_GRP_11) as `DL_DELAY_DISC_QOS_GRP_11`, SUM(DL_DELAY_DISC_QOS_GRP_12) as `DL_DELAY_DISC_QOS_GRP_12`, SUM(DL_DELAY_DISC_QOS_GRP_13) as `DL_DELAY_DISC_QOS_GRP_13`, SUM(DL_DELAY_DISC_QOS_GRP_14) as `DL_DELAY_DISC_QOS_GRP_14`, SUM(DL_DELAY_DISC_QOS_GRP_15) as `DL_DELAY_DISC_QOS_GRP_15`, SUM(DL_DELAY_DISC_QOS_GRP_16) as `DL_DELAY_DISC_QOS_GRP_16`, SUM(DL_DELAY_DISC_QOS_GRP_17) as `DL_DELAY_DISC_QOS_GRP_17`, SUM(DL_DELAY_DISC_QOS_GRP_18) as `DL_DELAY_DISC_QOS_GRP_18`, SUM(DL_DELAY_DISC_QOS_GRP_19) as `DL_DELAY_DISC_QOS_GRP_19`, SUM(DL_DELAY_DISC_QOS_GRP_20) as `DL_DELAY_DISC_QOS_GRP_20`, SUM(NUM_RETRANS_QOS_GRP_01) as `NUM_RETRANS_QOS_GRP_01`, SUM(NUM_RETRANS_QOS_GRP_02) as `NUM_RETRANS_QOS_GRP_02`, SUM(NUM_RETRANS_QOS_GRP_03) as `NUM_RETRANS_QOS_GRP_03`, SUM(NUM_RETRANS_QOS_GRP_04) as `NUM_RETRANS_QOS_GRP_04`, SUM(NUM_RETRANS_QOS_GRP_05) as `NUM_RETRANS_QOS_GRP_05`, SUM(NUM_RETRANS_QOS_GRP_06) as `NUM_RETRANS_QOS_GRP_06`, SUM(NUM_RETRANS_QOS_GRP_07) as `NUM_RETRANS_QOS_GRP_07`, SUM(NUM_RETRANS_QOS_GRP_08) as `NUM_RETRANS_QOS_GRP_08`, SUM(NUM_RETRANS_QOS_GRP_09) as `NUM_RETRANS_QOS_GRP_09`, SUM(NUM_RETRANS_QOS_GRP_10) as `NUM_RETRANS_QOS_GRP_10`, SUM(NUM_RETRANS_QOS_GRP_11) as `NUM_RETRANS_QOS_GRP_11`, SUM(NUM_RETRANS_QOS_GRP_12) as `NUM_RETRANS_QOS_GRP_12`, SUM(NUM_RETRANS_QOS_GRP_13) as `NUM_RETRANS_QOS_GRP_13`, SUM(NUM_RETRANS_QOS_GRP_14) as `NUM_RETRANS_QOS_GRP_14`, SUM(NUM_RETRANS_QOS_GRP_15) as `NUM_RETRANS_QOS_GRP_15`, SUM(NUM_RETRANS_QOS_GRP_16) as `NUM_RETRANS_QOS_GRP_16`, SUM(NUM_RETRANS_QOS_GRP_17) as `NUM_RETRANS_QOS_GRP_17`, SUM(NUM_RETRANS_QOS_GRP_18) as `NUM_RETRANS_QOS_GRP_18`, SUM(NUM_RETRANS_QOS_GRP_19) as `NUM_RETRANS_QOS_GRP_19`, SUM(NUM_RETRANS_QOS_GRP_20) as `NUM_RETRANS_QOS_GRP_20`, SUM(VOL_RETRANS_QOS_GRP_01) as `VOL_RETRANS_QOS_GRP_01`, SUM(VOL_RETRANS_QOS_GRP_02) as `VOL_RETRANS_QOS_GRP_02`, SUM(VOL_RETRANS_QOS_GRP_03) as `VOL_RETRANS_QOS_GRP_03`, SUM(VOL_RETRANS_QOS_GRP_04) as `VOL_RETRANS_QOS_GRP_04`, SUM(VOL_RETRANS_QOS_GRP_05) as `VOL_RETRANS_QOS_GRP_05`, SUM(VOL_RETRANS_QOS_GRP_06) as `VOL_RETRANS_QOS_GRP_06`, SUM(VOL_RETRANS_QOS_GRP_07) as `VOL_RETRANS_QOS_GRP_07`, SUM(VOL_RETRANS_QOS_GRP_08) as `VOL_RETRANS_QOS_GRP_08`, SUM(VOL_RETRANS_QOS_GRP_09) as `VOL_RETRANS_QOS_GRP_09`, SUM(VOL_RETRANS_QOS_GRP_10) as `VOL_RETRANS_QOS_GRP_10`, SUM(VOL_RETRANS_QOS_GRP_11) as `VOL_RETRANS_QOS_GRP_11`, SUM(VOL_RETRANS_QOS_GRP_12) as `VOL_RETRANS_QOS_GRP_12`, SUM(VOL_RETRANS_QOS_GRP_13) as `VOL_RETRANS_QOS_GRP_13`, SUM(VOL_RETRANS_QOS_GRP_14) as `VOL_RETRANS_QOS_GRP_14`, SUM(VOL_RETRANS_QOS_GRP_15) as `VOL_RETRANS_QOS_GRP_15`, SUM(VOL_RETRANS_QOS_GRP_16) as `VOL_RETRANS_QOS_GRP_16`, SUM(VOL_RETRANS_QOS_GRP_17) as `VOL_RETRANS_QOS_GRP_17`, SUM(VOL_RETRANS_QOS_GRP_18) as `VOL_RETRANS_QOS_GRP_18`, SUM(VOL_RETRANS_QOS_GRP_19) as `VOL_RETRANS_QOS_GRP_19`, SUM(VOL_RETRANS_QOS_GRP_20) as `VOL_RETRANS_QOS_GRP_20`, MAX(MAX_NUM_OF_DRB) as `MAX_NUM_OF_DRB`, SUM(DL_NUM_ROHC_COMP_HEADER) as `DL_NUM_ROHC_COMP_HEADER`, SUM(DL_VOL_ROHC_COMP_HEADER) as `DL_VOL_ROHC_COMP_HEADER`, SUM(UL_NUM_ROHC_DECOMP_HEADER) as `UL_NUM_ROHC_DECOMP_HEADER`, SUM(UL_VOL_ROHC_DECOMP_HEADER) as `UL_VOL_ROHC_DECOMP_HEADER`, sum(`DURATION`) * 60 as __duration
    FROM `SA_NSN5G_PM.GSITE_PDCP2_R` T1
    GROUP BY MINUTE(T1.START_TIME), HOUR(T1.START_TIME), GSITE_WH_ID, T1.MRBTS_ID, T1.NRBTS_ID;
{code}
;;;","28/Oct/22 19:40;KristoffSC;Hi,
I would like to try to fix this problem but I would appreciate any guidance. FYI I've verified that it still occurs on latest master branch.

From what I've debugged the problem is caused by one rewrited method created from in:
[JavaCodeSplitter -> new FunctionSplitter(text, maxMethodLength).rewrite()|https://github.com/apache/flink/blob/master/flink-table/flink-table-code-splitter/src/main/java/org/apache/flink/table/codesplit/JavaCodeSplitter.java#:~:text=new%20FunctionSplitter(text%2C%20maxMethodLength).rewrite()]

The FunctionSplitter::visitMethodDeclaration method iterates through JavaParser.BlockStatementContext elements from ctx.methodBody().block().blockStatement() - > [code|https://github.com/apache/flink/blob/87c33711fa3a4844598772ceafd66dd4a776eea9/flink-table/flink-table-code-splitter/src/main/java/org/apache/flink/table/codesplit/FunctionSplitter.java#L100:~:text=for%20(JavaParser.BlockStatementContext%20blockStatementContext]

For every element we get ContextString from it and add to the splitFuncBodies list. Later we iterated through this list to Merged them into [Code Blocks|https://github.com/apache/flink/blob/87c33711fa3a4844598772ceafd66dd4a776eea9/flink-table/flink-table-code-splitter/src/main/java/org/apache/flink/table/codesplit/FunctionSplitter.java#L100:~:text=getMergedCodeBlocks(List%3CString%3E%20codeBlock)] respectively to  maxMethodLength.

However the problem is that for our case the single JavaParser.BlockStatementContext element from ctx.methodBody().block().blockStatement() by it self is larger than maxMethodLength. Its entire body is converted to the method by FunctionSplitter::getMergedCodeBlocks and this causes the exception.

The code block is a big while loop that contains a bunch of calls to rewrite$XXXX methods like so:

{code:java}
        rewrite$9722[10418] = ((org.apache.flink.table.data.DecimalData) null);
        rewrite$9729[10431] = true;
        rewrite$9722[10419] = ((org.apache.flink.table.data.DecimalData) null);
        rewrite$9729[10432] = true;
        rewrite$9722[10420] = ((org.apache.flink.table.data.DecimalData) null);
        rewrite$9729[10433] = true;
{code}

Which in my opinion could be easily extracted to separate methods which will solve the problem.

I would like to ask:
1. if my understanding and proposed high level solution for splitting the problematic code block into smaller chunks is correct?
2. should this be done by FunctionSplitter or this should be implemented in Scala code for code generation?

I'm quite familiar with Antlr4 so I do understand what is happening there. 


;;;","31/Oct/22 02:13;TsReaper;Hi [~KristoffSC]!

Thanks for your interest in solving this issue. I'm the author of {{JavaCodeSplitter}} and I'd like to offer my thoughts.

*1. if my understanding and proposed high level solution for splitting the problematic code block into smaller chunks is correct?*

Your understanding is quite correct. However it might not be that easy to solve this issue due to some corner cases. For example we may have {{continue}}'s and {{break}}'s in a {{while}} loop, and we also need to deal with the loop variable in a {{for}} loop.

Due to these cases I choose not to recursively split most of the looping code blocks when implementing the first version of {{JavaCodeSplitter}}. However for {{if}} code blocks I implement a special {{IfStatementRewriter}} to split them because they are quite common in generated code and they don't suffer from those corner cases.

I'm not sure if the generated code in this particular ticket contains {{continue}}'s or {{break}}'s. If not we may choose to split code blocks without these keywords.

*2. should this be done by FunctionSplitter or this should be implemented in Scala code for code generation?*

It depends. If this issue is only caused by a single operator I'd like to change the code generation Scala code in that operator. However if other operators may also cause this issue it would be better to fix the {{FunctionSplitter}}.

To me this issue seems like a common one. I'd prefer fixing the {{FunctionSplitter}} if possible.;;;","02/Nov/22 10:33;KristoffSC;Hi [~TsReaper]
Thanks for replaying.

The extracted while loop method contains many, many referneces to rewrite$index[xxxx] methods and a lot of self contained if else blocks. 
There is no break, return, continue or break statements. I've attached the method body to this ticket.  [^endInput_falseFilter9123_split9704.txt] 

So it looks like that body of this method was already reprocessed and rewrite by Spliter.
Having this I was thinking that maybe if we would ""rewrite"" it again, to further split it into smaller groups, we could fix the problem.

What do you think?;;;","03/Nov/22 06:31;TsReaper;Hi [~KristoffSC].

You talked about ""maybe if we would 'rewrite' it again, to further split it into smaller groups, we could fix the problem"". What does it mean? Do you mean using {{JavaCodeSplitter}} again to further split the code?

From my understanding there is no use to apply {{JavaCodeSplitter}} multiple times because each rewriter has tried its best to split the code. The main reason of this issue is that we've generated a very long while loop and {{JavaCodeSplitter}} currently does not deal with loops. What we should do is to change the implementation of {{FunctionSplitter}} so that it can recursively split each code block, given that there is no break, continue, return or loop variables.;;;","03/Nov/22 09:20;KristoffSC;[~TsReaper]
I see why my comment was causing confusion, apologize for that. 

For a moment I though that FunctionSplitter is rewriting this while block but now I see clearly it does not. 
At the same time I saw that some ""rewrite"" process is applyed to this block but also now I see it was MemberFieldRewriter logic. 

Long story short, I did not want to run JavaCodeSplitter again but just enhance current logic to handle this ""while"" case.;;;","08/Nov/22 02:30;TsReaper;[~KristoffSC]

Thanks for the clarification. The general approach looks good to me.

Could you please sketch out your intended solution a bit more? Like which class are you going to modify and how are you going to modify it?;;;","25/Nov/22 10:07;KristoffSC;Hi [~TsReaper]
Im sory for a long delay. I was actyally trying to develop a PoC fix for this problem. I think I managed to at least proof a concept. You can find my raft PR here -> https://github.com/apache/flink/pull/21393  The code from this PR made the SQL from this ticket to compile and execute which is at least something :) 

The idea is to enhance FunctionSplitter that for every codeBlock (getMergedCodeBlocks method) that is bigger than ""maxMethodLength"" try to further split it by calling two new splitters that I've created:
1. BlockStatementSplitter
2. BlockStatementGrouper

The BlockStatementSplitter splits body of WHILE, IF/ELSE statements to new methods. The original statement is rewritten that will call those new methods. 

Next ,the *BlockStatementGrouper* is groping calls created by *BlockStatementSplitter*  to blocks with lengths <  ""maxMethodLength"" and extracting those to another new method. Finally *BlockStatementGrouper* rewrites original code block to call methods created by *BlockStatementGrouper*.

For example, an input statement:
{code:java}
while (
            (kvPair$9687 = (org.apache.flink.api.java.tuple.Tuple2<org.apache.flink.table.data.binary.BinaryRowData, org.apache.flink.table.data.binary.BinaryRowData>) iterator.next()) != null) {
            key$6207 = (org.apache.flink.table.data.binary.BinaryRowData) kvPair$9687.f0;
            val$9688 = (org.apache.flink.table.data.binary.BinaryRowData) kvPair$9687.f1;
            local$5912.replace(key$6207, val$9688);
    if (lastKey$6208 == null) {
              lastKey$6208 = key$6207.copy();
              agg0_sumIsNull = true;
        agg0_sum = ((org.apache.flink.table.data.DecimalData) null);
        agg1_sumIsNull = true;
        agg1_sum = ((org.apache.flink.table.data.DecimalData) null);
        agg3_sum = ((org.apache.flink.table.data.DecimalData) null);
        agg3_sum = ((org.apache.flink.table.data.DecimalData) null);
      } else if (lastKey$6209 == null) { 
        agg2_sum = ((org.apache.flink.table.data.DecimalData) null);
        agg3_sumIsNull = true;
} else {
        agg2_sumIsNull = true;
}};
{code}

will be converted by BlockStatementSplitter to:

{code:java}
while (
            (kvPair$9687 = (org.apache.flink.api.java.tuple.Tuple2<org.apache.flink.table.data.binary.BinaryRowData, org.apache.flink.table.data.binary.BinaryRowData>) iterator.next()) != null) {
            top_whileBody0_0();
    if (lastKey$6208 == null) {
              top_whileBody0_0_ifBody0();
      } else if (lastKey$6209 == null) { 
        top_whileBody0_0_ifBody1_ifBody0();
} else {
        top_whileBody0_0_ifBody1_ifBody1();
}};
{code}

Further this will be converted by BlockStatementGrouper with maxMethodLength parameter set to 4000 to:
{code:java}
while (
            (kvPair$9687 = (org.apache.flink.api.java.tuple.Tuple2<org.apache.flink.table.data.binary.BinaryRowData, org.apache.flink.table.data.binary.BinaryRowData>) iterator.next()) != null) {

            top_rewriteGroup_0();
}
{code}


Body for the new methods would be:
{code:java}
private void top_whileBody0_0_ifBody1_ifBody0 {
agg2_sum = ((org.apache.flink.table.data.DecimalData) null);
agg3_sumIsNull = true;
}

private void top_whileBody0_0_ifBody1_ifBody1 {
agg2_sumIsNull = true;
}

private void top_whileBody0_0 {
key$6207 = (org.apache.flink.table.data.binary.BinaryRowData) kvPair$9687.f0;
val$9688 = (org.apache.flink.table.data.binary.BinaryRowData) kvPair$9687.f1;
local$5912.replace(key$6207, val$9688);
}

private void top_whileBody0_0_ifBody0 {
lastKey$6208 = key$6207.copy();
agg0_sumIsNull = true;
agg0_sum = ((org.apache.flink.table.data.DecimalData) null);
agg1_sumIsNull = true;
agg1_sum = ((org.apache.flink.table.data.DecimalData) null);
agg3_sum = ((org.apache.flink.table.data.DecimalData) null);
agg3_sum = ((org.apache.flink.table.data.DecimalData) null);
}

void top_rewriteGroup_0() {
top_whileBody0_0();
if (lastKey$6208 == null) {
              top_whileBody0_0_ifBody0();
      } else if (lastKey$6209 == null) {         
        top_whileBody0_0_ifBody1_ifBody0();
  } else {       
     top_whileBody0_0_ifBody1_ifBody1();
  }
}
{code}


What do you think [~TsReaper]?

;;;","01/Dec/22 17:42;KristoffSC;[~TsReaper] I would love for your feedbeck on my PoC fix above.

thanks.;;;","26/Dec/22 02:42;TsReaper;Hi [~KristoffSC]!

Sorry for the long delay as I'm mainly working on the table store module.

The idea looks good to me and you can continue working on your fix.

Your fix seems to rewrite both the {{WHILE}} statements and the {{IF}} statements. For {{IF}} statements we already have {{IfStatementRewriter}}. Can we remove the special {{IfStatementRewriter}} after your fix is implemented? Or are they working on different things? If so, what's the difference?;;;","27/Dec/22 16:38;KristoffSC;Hi [~TsReaper]
Thanks for the feedback.

Regarding IfStatementRewriter its a little bit tricky for me.
I think my new thing might handle more cases than IfStatementRewriter did. Plus fStatementRewriter and existing rewrites seems to expect a method declaration + body, where my BlocksStatementGrouper and Splitter are processing individual block statements. They are called from FunctionSplitter::FunctionSplitVisitor where while processing block statements from method's body.

Now It seems that after my change,  FunctionSplitter is also rewriting the code, similar to IfStatementRewriter and maybe this is not the best thing to do from the clean code/architecture perspective.

The problem with IfStatementRewriter  is that it will not rewrite the If/Else branch if the branch contains ""while"" statement in it or when entire if/else statement is inside while statement, which was the original problem.

So now I'm wonder should we have this extracted from FunctionSplitter into new BlocksStatementRewriter that whill handle while/If/else statements in combination or can this be inside Function Splitter as it is now.

;;;","30/Dec/22 16:13;KristoffSC;Hi [~TsReaper]
I have modified my PR.

I reverted all chanegs from FunctionSplitter and I introduced totally new BlockStatementRewriter that is using new classses from original PR, BlockStatementSplitter and BlockStatementGrouper.

The new BlockStatementRewriter  can handle If/Else/While statemets. For IF/ESLE statemets if produces similiar but not exact result as IfStatementRewriter did.

However to make the original problem gone I had to use both rewriters, so JavaCodeSplitter now has this in splitImpl

{code:java}
        return Optional.ofNullable(
                        new DeclarationRewriter(returnValueRewrittenCode, maxMethodLength)
                                .rewrite())
                .map(text -> new IfStatementRewriter(text, maxMethodLength).rewrite())
                .map(text -> new BlockStatementRewriter(text, maxMethodLength).rewrite())
                .map(text -> new FunctionSplitter(text, maxMethodLength).rewrite())
                .map(text -> new MemberFieldRewriter(text, maxClassMemberCount).rewrite())
                .orElse(code);
    }
{code}

The good news is that all tests on CI/CD are passing. 
Still I have to investigate more. My goal would be to drop IfStatementRewriter and use only BlockStatementRewriter   unless I will find some reason not to. I will keep you posted on this.;;;","06/Jan/23 20:17;KristoffSC;Hi [~TsReaper], [~twalthr] and [~jingge]

I have finally finished working on my PR for this issue.
The PR is here: https://github.com/apache/flink/pull/21393

In short I've created new Rewritter that can rewrite IF/ESLE and WHILE blocks including combination and nested statements. I also removed IfStatementRewriter since its logic is covered by my new BlockStatementRewriter.

I ran the new Code Splitter against SQL query attached to this ticket and it works perfectly.

I would appreciate if you could take a look and do the review. I added detailed description of my change/solution to the PR.
Please let me know what do you think.

Cheers.

;;;","06/Feb/23 08:12;KristoffSC;master commit: af9a1128f728c691b896bc9c591e9be1327601c4 (included in 1.17 branch)

1.16 backport PR https://github.com/apache/flink/pull/21860 (contains bugfix https://github.com/apache/flink/pull/21871);;;","09/Feb/23 09:32;TsReaper;master: af9a1128f728c691b896bc9c591e9be1327601c4
release-1.16: f8bf7cc0c26773eecc928401f0c9ca5c714cd7bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary entries in connector-elasticsearch7 in NOTICE file,FLINK-27233,13439493,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,dwysakowicz,dwysakowicz,13/Apr/22 15:33,20/Apr/22 03:01,13/Jul/23 08:08,20/Apr/22 03:01,1.15.0,,,,,,1.15.0,,,,Connectors / ElasticSearch,,,,,0,,,,"{{flink-sql-connector-elasticsearch7}} lists following dependencies in the NOTICE file, which are not bundled in the jar:

{code}
- com.fasterxml.jackson.core:jackson-databind:2.13.2.2
- com.fasterxml.jackson.core:jackson-annotations:2.13.2
- org.apache.lucene:lucene-spatial:8.7.0
- org.elasticsearch:elasticsearch-plugin-classloader:7.10.2
- org.lz4:lz4-java:1.8.0
{code}",,dwysakowicz,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 03:01:04 UTC 2022,,,,,,,,,,"0|z11fy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/22 03:01;gaoyunhaii;Fix on master via 2b9560c8e0d0d569a3d30b8f2beb0bdff54fa16d
Fix on 1.15 via 15d409b158bc293520437e0e898b0132098d82e8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL pulsar connector lists dependencies under wrong license,FLINK-27231,13439477,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,dwysakowicz,dwysakowicz,13/Apr/22 14:51,20/Apr/22 02:59,13/Jul/23 08:08,20/Apr/22 02:59,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Connectors / Pulsar,,,,,0,pull-request-available,,,"Pulsar sql connector lists following dependencies under ASL2 license while they are licensed with Bouncy Castle license (variant of MIT?).

{code}
- org.bouncycastle:bcpkix-jdk15on:1.69
- org.bouncycastle:bcprov-ext-jdk15on:1.69
- org.bouncycastle:bcprov-jdk15on:1.69
- org.bouncycastle:bcutil-jdk15on:1.69
{code}",,dwysakowicz,Feifan Wang,gaoyunhaii,leonard,,,,,,,,,,,,,,,,,,,,,FLINK-27199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 02:59:40 UTC 2022,,,,,,,,,,"0|z11fug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/22 02:59;gaoyunhaii;Fix on master via a208c5b9785ab658f53317af2dfea2b5ee0766ab
Fix on 1.15 via 9d996ebd2aed6cb0b575dbddb674f55492106231;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary entries in connector-kinesis NOTICE file,FLINK-27230,13439473,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaoyunhaii,dwysakowicz,dwysakowicz,13/Apr/22 14:39,20/Apr/22 03:00,13/Jul/23 08:08,20/Apr/22 03:00,1.15.0,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,,,,"flink-connector-kinesis lists but does not bundle:

{code}
- commons-logging:commons-logging:1.1.3
- com.fasterxml.jackson.core:jackson-core:2.13.2
{code}

{code}
[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-core:jar:2.13.2 from the shaded jar.
{code}",,dwysakowicz,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 03:00:24 UTC 2022,,,,,,,,,,"0|z11ftk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/22 03:00;gaoyunhaii;Fix on master via 9a5dc22eece105a92befb2e903db19000fce6b67
Fix on 1.15 via 1ebfe8558fe82679d25836f4cd5979a4e6917358
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State access doesn't work as expected when cache size is set to 0,FLINK-27223,13439418,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,13/Apr/22 10:58,14/Apr/22 04:11,13/Jul/23 08:08,14/Apr/22 04:11,1.14.0,,,,,,1.14.5,1.15.1,,,API / Python,,,,,0,pull-request-available,,,"For the following job:
{code}
import json
import logging
import sys

from pyflink.common import Types, Configuration
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.util.java_utils import get_j_env_configuration

if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    env = StreamExecutionEnvironment.get_execution_environment()
    config = Configuration(
        j_configuration=get_j_env_configuration(env._j_stream_execution_environment))
    config.set_integer(""python.state.cache-size"", 0)
    env.set_parallelism(1)

    # define the source
    ds = env.from_collection(
        collection=[
            (1, '{""name"": ""Flink"", ""tel"": 123, ""addr"": {""country"": ""Germany"", ""city"": ""Berlin""}}'),
            (2, '{""name"": ""hello"", ""tel"": 135, ""addr"": {""country"": ""China"", ""city"": ""Shanghai""}}'),
            (3, '{""name"": ""world"", ""tel"": 124, ""addr"": {""country"": ""USA"", ""city"": ""NewYork""}}'),
            (4, '{""name"": ""PyFlink"", ""tel"": 32, ""addr"": {""country"": ""China"", ""city"": ""Hangzhou""}}')
        ],
        type_info=Types.ROW_NAMED([""id"", ""info""], [Types.INT(), Types.STRING()])
    )

    # key by
    ds = ds.map(lambda data: (json.loads(data.info)['addr']['country'],
                              json.loads(data.info)['tel'])) \
           .key_by(lambda data: data[0]).sum(1)
    ds.print()
    env.execute()
{code}

The expected result should be:
{code}
('Germany', 123)
('China', 135)
('USA', 124)
('China', 167)
{code}

However, the actual result is:
{code}
('Germany', 123)
('China', 135)
('USA', 124)
('China', 32)
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 04:11:24 UTC 2022,,,,,,,,,,"0|z11fhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/22 04:11;dianfu;Fixed in:
- master via eee8804bb8db97864ecff6b27853570e75b15807
- release-1.15 via b070a7bb424da40ce0839a1151b110701de48a2c
- release-1.14 via d4de83b4dc66d18acc4355d1edafa95fd1adca7a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execution history limit can lead to eviction of critical local-recovery information,FLINK-27222,13439417,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,13/Apr/22 10:48,05/May/22 07:39,13/Jul/23 08:08,15/Apr/22 07:21,1.15.0,,,,,,1.15.0,1.16.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,"Local recovery relies on knowing the allocation id of the last deployment. To that end we iterate over all previous execution attempts and use the last {{assignedAllocationID}}, if any.
However, since the execution history is bounded (to, by default, 16 entries) this can lead this information being evicted.

In other words, with the default configuration (history limit = 16, restart delay = 1s) local recovery can only kick if the TM is restarted within 16 seconds.

We should decouple this information from the execution (history).",,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27127,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 15 07:17:29 UTC 2022,,,,,,,,,,"0|z11fh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/22 07:17;chesnay;master: 07bab2f72add9ecdffd8ffa038b552797ff72579
1.15: 4727a20922bbab81faf4247a2a89e820ba122c60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientITCase.testSqlStatements failed on azure with jdk11,FLINK-27219,13439388,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,gaoyunhaii,gaoyunhaii,13/Apr/22 09:15,08/Feb/23 03:40,13/Jul/23 08:08,08/Feb/23 03:40,1.15.0,1.16.0,,,,,1.15.1,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,"
{code:java}
Apr 13 04:56:44 [ERROR] Could not execute SQL statement. Reason:
Apr 13 04:56:44 java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')
Apr 13 04:56:44 !error
Apr 13 04:56:44 
Apr 13 04:56:44 # test ""ctas"" only supported in Hive Dialect
Apr 13 04:56:44 CREATE TABLE foo as select 1;
Apr 13 04:56:44 [ERROR] Could not execute SQL statement. Reason:
Apr 13 04:56:44 java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')
Apr 13 04:56:44 !error
Apr 13 04:56:44 
Apr 13 04:56:44 # list the configured configuration
Apr 13 04:56:44 set;
Apr 13 04:56:44 [ERROR] Could not execute SQL statement. Reason:
Apr 13 04:56:44 java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')
Apr 13 04:56:44 !error
Apr 13 04:56:44 
Apr 13 04:56:44 # reset the configuration
Apr 13 04:56:44 reset;
Apr 13 04:56:44 [ERROR] Could not execute SQL statement. Reason:
Apr 13 04:56:44 java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')
Apr 13 04:56:44 !error
Apr 13 04:56:44 
Apr 13 04:56:44 set;
Apr 13 04:56:44 [ERROR] Could not execute SQL statement. Reason:
Apr 13 04:56:44 java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')
Apr 13 04:56:44 !error

...

Apr 13 04:56:44 [ERROR] Could not execute SQL statement. Reason:
Apr 13 04:56:44 org.apache.flink.sql.parser.impl.ParseException: Encountered ""STRING"" at line 10, column 27.
Apr 13 04:56:44 Was expecting one of:
Apr 13 04:56:44     "")"" ...
Apr 13 04:56:44     "","" ...
Apr 13 04:56:44 
Apr 13 04:56:44 !error

...

Apr 13 04:56:44 SHOW JARS;
Apr 13 04:56:44 Empty set
Apr 13 04:56:44 !ok
Apr 13 04:56:44 ""
Apr 13 04:56:44 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Apr 13 04:56:44 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Apr 13 04:56:44 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Apr 13 04:56:44 	at org.apache.flink.table.client.cli.CliClientITCase.testSqlStatements(CliClientITCase.java:139)
Apr 13 04:56:44 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 13 04:56:44 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 13 04:56:44 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 13 04:56:44 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Apr 13 04:56:44 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Apr 13 04:56:44 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 13 04:56:44 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Apr 13 04:56:44 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 13 04:56:44 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 13 04:56:44 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 13 04:56:44 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 13 04:56:44 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Apr 13 04:56:44 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Apr 13 04:56:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Apr 13 04:56:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Apr 13 04:56:44 	at org.junit.runners.Suite.runChild(Suite.java:128)
Apr 13 04:56:44 	at org.junit.runners.Suite.runChild(Suite.java:27)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Apr 13 04:56:44 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)

{code}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34596&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11357

This seems to only occur on jdk11",,fsk119,gaoyunhaii,hxbks2ks,jingzhang,luoyuxia,mapohl,martijnvisser,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 10 14:47:13 UTC 2023,,,,,,,,,,"0|z11fao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/22 07:07;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34745&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11362;;;","18/Apr/22 07:26;gaoyunhaii;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34743&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11336;;;","19/May/22 07:40;martijnvisser;release-1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35817&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11777;;;","19/May/22 07:41;martijnvisser;[~jark] any thoughts on this? ;;;","20/May/22 08:31;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35858&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f;;;","20/May/22 08:35;fsk119;I will take a look at the issue. ;;;","20/May/22 08:36;hxbks2ks;Thanks [~fsk119]. I have assigned it to you.;;;","23/May/22 02:30;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35908&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12208;;;","23/May/22 12:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35945&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f;;;","24/May/22 02:07;fsk119;Hi, all. I still can't reproduce the exception in my local environment or in the azure pipelines(in [~hxbks2ks]'s environment). In my local environment, I just use the command as follows(it's almost same comparing to the command that starts the tests):

```
mvn -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B -Dhadoop.version=2.8.5 -Dinclude_hadoop_aws -Dscala-2.12 -Djdk11 -Pjava11-target -Duse-alibaba-mirror  -Dflink.forkCount=2 -Dflink.forkCountTestPackage=2 -Dfast -Pskip-webui-build -Dlog.dir=/__w/_temp/debug_files -Dlog4j.configurationFile=file:///__w/2/s/tools/ci/log4j.properties -Dflink.tests.with-openssl -Dflink.tests.check-segment-multiple-free -Darchunit.freeze.store.default.allowStoreUpdate=false -Dhadoop.version=2.8.5 -Dinclude_hadoop_aws -Dscala-2.12 -Djdk11 -Pjava11-target -pl flink-table/flink-sql-client verify
```

So I think we can print the exception stack when getting errors in the sql client test. At least, we can know what is the problem.

;;;","24/May/22 02:57;hxbks2ks;[~fsk119] Thanks a lot for the investigation. As far as I know, the machines run in our private Azure pipeline are provided by Azure, but the tests of flink Azure Pipeline run on Alibaba Cloud  machines. I am not sure if this is related, but I did encounter this difference before, which an unstable test only failed on Azure machines.  Let's merge this commit into the master branch firstly to see if we can get more specific stack information.

Merged the ""print stack info"" commit in 3c5d3bacda580de1dc4986909a0aa5fc30e37885;;;","24/May/22 09:27;fsk119;Hi, all.

After discussion with the [~hxbks2ks] and [~luoyuxia] offline, we think the main cause is the FLINK-26681 adds tests about the HiveCatalog in the sql client. However, Hive in the old version is not able to work in the JDK11 unless we upgrade the hive to the version 2.3.9. In the FLINK-27450, we upgraded the version to 2.3.9. That is, the test should pass after the merge of the FLINK-27450 in the master. Just as our assumption, the failed module doesn't fail any more. Therefore, we should cherry pick the PR in the FLINK-27450 to the release-1.15 to fix the broken tests in JDK11.

You can see the failed module recently([5.23|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35943&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=ce3801ad-3bd5-5f06-d165-34d37e757d90], [5.22|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35925&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=ce3801ad-3bd5-5f06-d165-34d37e757d90], [5.21|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35906&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=ce3801ad-3bd5-5f06-d165-34d37e757d90]) in the master passes. Actually after the upgrading the Hive version in the 5.11([5.14|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35656&view=results], [5.15|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35664&view=results], [5.16|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35682&view=results]), the test module cron_jdk11_table is stable. Before 5.11([5.11|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35561&view=results], [5.10|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35529&view=results], [5.9|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35492&view=results]), it fails.;;;","24/May/22 09:31;luoyuxia;The backport for release-1.15 is in [https://github.com/apache/flink/pull/19806.|https://github.com/apache/flink/pull/19806]

Appreciated if  someone can help merge it to relase-1.15;;;","24/May/22 09:34;hxbks2ks;Thanks a lot for your work [~fsk119] [~luoyuxia]. I can help merge the PR once the tests passes.;;;","26/May/22 10:08;hxbks2ks;Merged into release-1.15 via 177e8a78a33a449bdeab4aafb1ff60ee169cd134;;;","07/Dec/22 04:26;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43772&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14992]

 

Looks like this unstable case popped up again. Any ideas [~fsk119] ?;;;","14/Dec/22 03:00;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43917&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14190;;;","14/Dec/22 03:14;luoyuxia;[~renqs] I don't think this one [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43917&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14190 |https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43917&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14190] shares same reason with this original Jira according to the error message. Is it also fail for jdk 11?

 ;;;","27/Dec/22 07:29;renqs;[~luoyuxia] Ah sorry for the mistake! It's not related to JDK11. I created FLINK-30508 for tracking the issue. ;;;","10/Jan/23 14:47;mapohl;[~renqs] can we close this issue then again, considering that FLINK-30508 deals with the new test instability?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Serializer in OperatorState has not been updated when new Serializers are NOT incompatible,FLINK-27218,13439367,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mayuehappy,mayuehappy,mayuehappy,13/Apr/22 07:11,22/Apr/22 14:44,13/Jul/23 08:08,22/Apr/22 14:44,1.15.1,,,,,,1.15.1,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,"OperatorState such as *BroadcastState* or *PartitionableListState*  can only be constructed via {*}DefaultOperatorStateBackend{*}. But when *BroadcastState* or *PartitionableListState* Serializer changes after we restart the job , it seems to have the following problems .

As an example, we can see how PartitionableListState is initialized.

First, RestoreOperation will construct a restored PartitionableListState based on the information in the snapshot.

Then StateMetaInfo in partitionableListState will be updated  as the following code
{code:java}
TypeSerializerSchemaCompatibility<S> stateCompatibility =
                restoredPartitionableListStateMetaInfo.updatePartitionStateSerializer(newPartitionStateSerializer);

partitionableListState.setStateMetaInfo(restoredPartitionableListStateMetaInfo);{code}
The main problem is that there is also an *internalListCopySerializer* in *PartitionableListState* that is built using the previous Serializer and it has not been updated. 

Therefore, when we update the StateMetaInfo, the *internalListCopySerializer* also needs to be updated.

 ",,mayuehappy,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/22 06:50;mayuehappy;image-2022-04-13-14-50-10-921.png;https://issues.apache.org/jira/secure/attachment/13042348/image-2022-04-13-14-50-10-921.png","18/Apr/22 13:48;mayuehappy;image-2022-04-18-21-48-30-519.png;https://issues.apache.org/jira/secure/attachment/13042584/image-2022-04-18-21-48-30-519.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 22 14:44:33 UTC 2022,,,,,,,,,,"0|z11f60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/22 12:20;mayuehappy;[~yunta] [~roman] could you please take a look at this ticket ? ;;;","15/Apr/22 03:22;yunta;[~mayuehappy] I think you're right, this would affect the correctness when executing copy during snapshot. Do you like to take this ticket to fix this bug?;;;","15/Apr/22 10:29;mayuehappy;[~yunta] ok i'm glad to do this fix, i'll submit a pr soon;;;","18/Apr/22 02:43;yunta;[~mayuehappy] Already assigned to you, please go ahead.;;;","18/Apr/22 13:48;mayuehappy;[~yunta] hi , I drafted a pr [https://github.com/apache/flink/pull/19508]  , can you help with a review  ? thx  ;;;","22/Apr/22 14:44;roman;Merged into master as 4033ddc5fa682a1619f8f22348e2ee38afcc1c85,

into 1.15 as 3d4b3a495b273c3a15ce7d35ba5a5b2e4ddc4c20.
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Failed on state backend benchmark,FLINK-27214,13439340,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Yanfei Lei,ym,ym,13/Apr/22 04:38,27/Dec/22 04:50,13/Jul/23 08:08,14/Apr/22 12:43,1.16.0,,,,,,1.16.0,,,,Benchmarks,,,,,0,pull-request-available,,,"Failed build 72 of flink-statebackend-benchmarks-java11 ([Open|http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java11/72/]): hudson.AbortException: script returned exit code 1
 
Failed build 214 of flink-statebackend-benchmarks-java8 ([Open|http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java8/214/]): hudson.AbortException: script returned exit code 1",,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 12:43:13 UTC 2022,,,,,,,,,,"0|z11f08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/22 04:41;ym;After a quick glance, it may relate to the commit

[https://github.com/apache/flink-benchmarks/commit/57cc8064db4a9f934f77a5fa5038712cb01e5183]

 

[~yunta] , could you please double check?;;;","13/Apr/22 07:21;yunta;I think the direct cause is chaning from {{Level.Invocation}} to {{Level.Iteration}} in this comment of review, however, we still need time to figure out, and [~Yanfei Lei] will create a PR to resovle this ticket.;;;","14/Apr/22 12:43;yunta;Merged in flink-benchmark master: 989e4fae452ecdd3c749d9f4493c34e63e0c41c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failed to CAST('abcde', VARBINARY)",FLINK-27212,13439322,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,matriv,fsk119,fsk119,13/Apr/22 03:00,20/Apr/22 03:37,13/Jul/23 08:08,19/Apr/22 15:33,,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"Please add test in the CalcITCase


{code:scala}
@Test
  def testCalc(): Unit = {
    val sql =
      """"""
        |SELECT CAST('abcde' AS VARBINARY(6))
        |"""""".stripMargin
    val result = tEnv.executeSql(sql)
    print(result.getResolvedSchema)
    result.print()
  }
{code}


The exception is 

{code:java}
Caused by: org.apache.flink.table.api.TableException: Odd number of characters.
	at org.apache.flink.table.utils.EncodingUtils.decodeHex(EncodingUtils.java:203)
	at StreamExecCalc$33.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)

{code}
",,fsk119,godfreyhe,jark,maguowei,martijnvisser,matriv,qingyue,twalthr,wenlong.lwl,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 03:37:27 UTC 2022,,,,,,,,,,"0|z11ew8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/22 03:03;fsk119;cc [~matriv] [~slinkydeveloper] ;;;","13/Apr/22 08:12;matriv;You cannot CAST an odd number of chars to a binary type (BYTES/BINARY/VARBINARY) as you need pairs, each one of them representing one byte. ;;;","14/Apr/22 07:02;fsk119;[~matriv]  I think it's very strange for users. It works in release-1.13 but when upgrading to the latest version it fails. 

VARBINARY/BINARY represents binary string and I think it should not be limited by the input length. We also test the behaviour in other databases:

* In PostgreSQL, it allows users to CAST('abcde' AS bytea) the return results is [97,98,99,100,101].
* In MySQL, it also allow users  to CAST(""abcde"" AS BINARY) the return results is [97,98,99,100,101].

You can test in the https://www.db-fiddle.com/
;;;","14/Apr/22 07:09;martijnvisser;[~matriv] Let me know if [~fsk119] is correct and if this ticket should be re-opened? ;;;","14/Apr/22 08:33;jark;I think this is a feature regression and should block 1.16 release. I re-opened the issue and set to blocker. We can continue to discuss whether it is or not. 
;;;","14/Apr/22 09:26;jark;It seems PG will inspect the format of the character string. If it is hex representation, PG will decode the string into bytes using hex encoding. Otherwise, PG will decode the string using UTF8 encoding. 

{code}
postgres=# create table t (a varchar(20));
CREATE TABLE
postgres=#
postgres=# insert into t values ('\x00011c57'), ('00011c57');
INSERT 0 2
postgres=#
postgres=# select a, pg_typeof(a), a::bytea from t;
     a      |     pg_typeof     |         a
------------+-------------------+--------------------
 \x00011c57 | character varying | \x00011c57
 00011c57   | character varying | \x3030303131633537
(2 rows)
{code};;;","14/Apr/22 09:38;matriv;If we use the legacy cast behaviour flag, and set it to ENABLED then we support the previous behaviour that we don't require a hex string, but simply use UTF-8 char encoding.;;;","14/Apr/22 13:46;matriv;We chose for the new behavior (Legacy=DISABLED) to accept only valid hex strings to be consistent with literals like: {{x'ABC0df2E'}}

 
{noformat}
tableEnv.executeSql(""SELECT x'abcde'"").print();{noformat}
throws:

 

 
{noformat}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 8 to line 1, column 15: Binary literal string must contain an even number of hexits
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:184)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:109)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:237)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:105)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:695)
    at org.apache.flink.table.examples.java.basics.WordCountSQLExample.main(WordCountSQLExample.java:43)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 8 to line 1, column 15: Binary literal string must contain an even number of hexits
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4867)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateLiteral(SqlValidatorImpl.java:2960)
    at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateLiteral(FlinkCalciteSqlValidator.java:86)
    at org.apache.calcite.sql.SqlLiteral.validate(SqlLiteral.java:554)
    at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:273)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateExpr(SqlValidatorImpl.java:4112)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4088)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3347)
    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:180)
    ... 5 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Binary literal string must contain an even number of hexits
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
    ... 23 more{noformat}
which comes from calcite.

 

In my opinion having consistency with literals and columns is very important.

 

I would vote to keep the new behaviour of CAST(<String> AS BYTES/BINARY/VARBINARY) as is currently in master and 1.15 and introduce special function that converts a string to a byte[] with an optional arg that defines the desired charset (if omitted, defaults to UTF-8), i.e.:
{noformat}
to_binary(<src::string>, [<charset::string>]){noformat}
 

 ;;;","15/Apr/22 02:26;wenlong.lwl;[~matriv] I think you may have some misunderstanding here. Regarding x'XXX', it means a hexdecimal literal(https://dev.mysql.com/doc/refman/8.0/en/hexadecimal-literals.html), it requires even number of values, the error in calcite means that the literal is illegal. it is irrelevant to the casting behavior I think.

BTW, we may need a FLIP for such kind of change to collect more feedbacks from devs and users. I think it is better to keep it the same as former versions, and make the decision later.;;;","15/Apr/22 12:13;jark;Hi [~matriv], {{x'45F0AB'}} is a {{BINARY}} not {{CHAR/VARCHAR}} type [1]. So, {{CAST(x'45F0AB' AS VARBINARY(6))}} is a CAST BINARY to BINRY,  conversion (the hex string of course should be in hex digits). Therefore, what we are missing is CAST CHAR to BINARY, and this is supported in previous versions and other database systems. 

I would still vote this to be a blocker issue. 

cc [~twalthr]

[1]: https://calcite.apache.org/docs/reference.html#scalar-types;;;","18/Apr/22 09:00;matriv;Ok, I agree, I can revert the change and just produce the UTF-8 byte[] from any string when casting a CHAR/VARCHAR/BYTES to a STRING/CHAR/VARCHAR

We can introduce a dedicated function if we want a hex cast similar to a {{x'AB0cDF28' literal}};;;","18/Apr/22 10:23;matriv;https://github.com/apache/flink/pull/19507;;;","19/Apr/22 07:43;twalthr;Thanks for fixing this so quickly [~matriv]. Let's merge this fix today. I also feel safer with the old behavior for CAST. However, I would still vote for changing the printing representation. Otherwise, byte arrays will be printing as UTF-8 string which is not the desired behavior usually. The current master is more consistent in this regard.;;;","19/Apr/22 08:45;matriv;I propose, when we do {{CAST(<some binary> AS STRING)}} to also use UTF-8 decoding, so we can have a seamless roundtrip: {{CAST(CAST('<some string>' AS BYTES) AS STRING)}} would return `<some string>`.

But when we do a {{SELECT CAST('<some string>' AS BYTES}} and we print the result, we will print in a {{x'ABC324Fd32'}} format, which can then be easily copy pasted as a binary literal in another SQL query:
{noformat}
tableEnv.executeSql(""SELECT CAST('Marios Timo' AS BYTES)"").print();{noformat}
will print:
{noformat}
+--------------------------------+
|                         EXPR$0 |
+--------------------------------+
|      x'4d6172696f732054696d6f' |
+--------------------------------+{noformat}
and then:
{noformat}
tableEnv.executeSql(""SELECT x'4d6172696f732054696d6f'""){noformat}
which creates the desired binary column.

 

[~jark] [~wenlong.lwl] what do you think?;;;","19/Apr/22 09:13;matriv;Here is the commit for this printing change: https://github.com/apache/flink/pull/19507/commits/58f60562fb2745437c5938332698967550c4ee1c;;;","19/Apr/22 15:33;twalthr;Fixed in master:

commit 75007f6ab601c285a2dbfc4f1c41952269979967
[table-planner] Change printing of binary columns to use `x'ab3234f0'` format

commit 4cdafffa6cee1c428e23dae616c98cc57ee3e20b
[table-planner] Use UTF-8 encoding when casting between binary/character string

Fixed in 1.15:

commit 5f280f2eca4ad80ee981c44f114b549af5f5da71
[table-planner] Change printing of binary columns to use `x'ab3234f0'` format

commit 5175087052f66e3d66d02685787d1e5def9a50e9
[table-planner] Use UTF-8 encoding when casting between binary/character string;;;","20/Apr/22 03:37;wenlong.lwl;+1 for printing binary in hexadecimal, thanks for the quick update!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBAC deployments/finalizers missing for OpenShift Deployment,FLINK-27211,13439304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,12/Apr/22 23:25,26/Apr/22 11:11,13/Jul/23 08:08,26/Apr/22 11:11,kubernetes-operator-0.1.0,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,,,,"On Openshift 4.8 when applying the basic.yaml, we see in the operator logs:

 

??2022-04-12 23:11:56,290 i.j.o.p.e.ReconciliationDispatcher *[ERROR][default/basic-example] Error during event processing ExecutionScope{ resource id*??

??*: CustomResourceID\{name='basic-example', namespace='default'}, version: 680939} failed.*??

??{*}org.apache.flink.kubernetes.operator.exception.ReconciliationException: org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes clus{*}{*}ter ""basic-example"".{*}??
??{*}....{*}{*}{*}??

??*Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at:* [*https://172.30.0.1/api/v1/namespaces/*]??

??{*}default/services. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. services ""basic-example"" is forbidden: cann{*}{*}ot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , <nil>.{*}??

Manually, this can be fixed by adding to the flink role under apps apiGroups:

  - deployments/finalizers

 

and to add to the flink-operator clusterrole under apps apiGrups:

  - deployments/finalizers

 ",,jbusche,mbalassi,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 26 11:11:21 UTC 2022,,,,,,,,,,"0|z11es0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/22 08:21;mbalassi;https://github.com/apache/flink-kubernetes-operator/pull/166;;;","13/Apr/22 09:09;wangyang0918;I am curious why we do not have this issue in the Kubernetes cluster, but only not work for Openshift.;;;","13/Apr/22 14:25;mbalassi;Merged [{{c33f7e1}}|https://github.com/apache/flink-kubernetes-operator/commit/c33f7e1a0dd05ae38826a42f2fa605b4c9663eee] to main. [~jbusche] could you please answer [~wangyang0918]'s question before we close the issue?;;;","13/Apr/22 16:02;jbusche;Hi [~wangyang0918], [~mbalassi], OpenShift has more security enabled by default.  For example, in the security section of this [Kubernetes-vs-openshift|https://thechief.io/c/editorial/kubernetes-vs-openshift-what-you-need-know/] doc it mentions ""OpenShift offers strict security policies and is more restrictive than Kubernetes.""  So the RBAC permissions needed some additional finalizers permissions added in order to be able to launch and later cleanup pods that the Flink operator is managing.;;;","14/Apr/22 02:14;wangyang0918;[~jbusche] Thanks for the explanation.;;;","26/Apr/22 11:11;wangyang0918;Already merged to main via:

c33f7e1a0dd05ae38826a42f2fa605b4c9663eee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Concepts -> Glossary"" page into Chinese",FLINK-27205,13439201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liu zhuang,liu zhuang,liu zhuang,12/Apr/22 13:18,12/Jul/22 07:28,13/Jul/23 08:08,12/Jul/22 07:28,1.14.4,,,,,,1.16.0,,,,chinese-translation,Documentation,,,,0,pull-request-available,stale-assigned,,"Translate Glossary page into Chinese: https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/concepts/glossary/.
The markdown file is located in docs/concepts/glossary.md.
In https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/concepts/glossary/, most of them have been translated, but a small part has not been translated into Chinese. Details See FLINK-13037 for information.
",,liu zhuang,martijnvisser,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 12 07:28:42 UTC 2022,,,,,,,,,,"0|z11e54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/22 13:30;liu zhuang;[~yunta],[~jark],Hi,I think I could help to translate this page. Could you assign it to me ? I will complete it  in today. Thanks.;;;","13/Apr/22 02:59;yunta;[~liu zhuang] Already assigned to you, please go ahead.;;;","05/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","06/Jul/22 00:07;liu zhuang;[~yunta],Sorry to bother you, can you help me review this pr?;;;","12/Jul/22 07:28;martijnvisser;Merged in master: 326b4b638afece388bf23b4cc192c0536fedc6aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on stop-with-savepoint with AsyncWaitOperator followed by FlinkKafkaProducer ,FLINK-27202,13439137,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,pnowojski,pnowojski,12/Apr/22 08:55,12/Apr/22 13:02,13/Jul/23 08:08,12/Apr/22 09:30,1.12.7,1.13.6,,,,,1.14.0,,,,Connectors / Kafka,Runtime / Task,,,,0,,,,"Some lingering mails from {{AsyncWaitOperator}} (or other operators using mailbox, or maybe even processing time timers), that are chained with {{FlinkKafkaProducer}} can cause the following exceptions when using stop-with-savepoint:

{noformat}
2022-04-11 15:46:19,781 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - static enrichment -> Map -> Sink: enriched events sink (179/256) (3fefa588ad05fa8d2a10a6ad4a740cc6) switched from RUNNING to FAILED on 10.239.104.67:38149-12df6c @ 10.239.104.67 (dataPort=35745).
java.lang.NullPointerException: null
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction$TransactionHolder.access$000(TwoPhaseCommitSinkFunction.java:591) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:223) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:64) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueue$Segment.emitCompleted(UnorderedStreamElementQueue.java:272) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueue.emitCompletedElement(UnorderedStreamElementQueue.java:159) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:287) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:78) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:356) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:337) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.drain(MailboxProcessor.java:170) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:647) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:591) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:758) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:573) ~[flink-dist_2.12-1.12.7-stream2.jar:1.12.7-stream2]
	at java.lang.Thread.run(Thread.java:829) ~[?:?]
{noformat}

This happens since {{FlinkKafkaProducer}} can be closed, without quiescing the mailbox. This issue might have been fixed by either FLINK-23532
or FLINK-23408.
",,dwysakowicz,pnowojski,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22972,FLINK-2491,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 12 09:30:40 UTC 2022,,,,,,,,,,"0|z11dqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/22 09:30;dwysakowicz;I think it should be fixed in 1.14 already. It is not necessarily fixed by the way we handle stop-with-savepoint per se. It should be fixed by the way we call {{close/(and/or previously dispose)}}. I'd say it should rather be fixed with e.g. FLINK-22972 and other parts of the FLINK-2491 effort. We call close only once the mailbox is closed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kyro Serialisation and Deserialisation returns a different object ,FLINK-27193,13439092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,akshayhazari,akshayhazari,12/Apr/22 05:35,12/Apr/22 07:41,13/Jul/23 08:08,12/Apr/22 07:41,,,,,,,,,,,,,,,,0,,,,"We have a unit test to check if Kyro serialisation and deserialisation results in the same value but it fails

The KyroSerializer and Deserializer is used like this
{code:java}
import kotlin.reflect.KClass
import org.apache.flink.api.common.ExecutionConfig
import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer
import org.apache.flink.core.memory.DataInputDeserializer
import org.apache.flink.core.memory.DataOutputSerializer
class KryoSerializerExtension {
    fun <T : Any> serde(t: T): T
    { val bytes = serialize(t) return deserialize(bytes, t::class) }
    fun serialize(any: Any): ByteArray {
    val config = ExecutionConfig()
    config.registerKryoType(any.javaClass)
    val serializer = KryoSerializer(any.javaClass, config)
    val output = DataOutputSerializer(1)
    serializer.serialize(any, output)
    return output.sharedBuffer
}
fun <T : Any> deserialize(bytes: ByteArray, kClass: KClass<T>): T {
    val config = ExecutionConfig()
    config.registerKryoType(kClass.java)
    val serializer = KryoSerializer(kClass.java, config)
    val input = DataInputDeserializer(bytes)
    return serializer.deserialize(input)
    }
}
{code}
 

The Unit test simply looks like this
{code:java}
@Test
fun fieldRecord() {
    val record = getFieldRecord()
    val result = kryo.serde(record)
    assertThat(result).isEqualTo(record)
}{code}
This is the actual vs expected assertion error.

The record is huge all the components hash, result in a different value. I am not sure hot the record is modified.
{code:java}
org.opentest4j.AssertionFailedError: 
expected: ""FieldRecord(name=foo, type=string, fieldCounter=FieldCounter(populated=1, missing=3), countDistinctCalculator=### CPD SKETCH - PREAMBLE:
  Flavor         : SPARSE
  LgK            : 10
  Merge Flag     : false
  Error Const    : 0.5887050112577373
  RSE            : 0.01839703160180429
  Seed Hash      : 93cc | 37836
  Num Coupons    : 2
  Num Pairs (SV) : 2
  First Inter Col: 0
  Valid Window   : false
  Valid PairTable: true
  Window Offset  : 0
  KxP            : 1023.375
  HIP Accum      : 2.00012208521548
### END CPC SKETCH, categoryHistogramCalculator=maxNumberOfStoredValues: 20, frequencies: {test=1}, numericalRangeHistogramCalculator=
### Quantiles HeapUpdateDoublesSketch SUMMARY: 
   Empty                        : false
   Direct, Capacity bytes       : false, 
   Estimation Mode              : false
   K                            : 128
   N                            : 2
   Levels (Needed, Total, Valid): 0, 0, 0
   Level Bit Pattern            : 0
   BaseBufferCount              : 2
   Combined Buffer Capacity     : 4
   Retained Items               : 2
   Compact Storage Bytes        : 48
   Updatable Storage Bytes      : 64
   Normalized Rank Error        : 1.406%
   Normalized Rank Error (PMF)  : 1.711%
   Min Value                    : 1.000000e+00
   Max Value                    : 3.000000e+00
### END SKETCH SUMMARY
, numberOfElementsCalculator=NumberOfElementsCalculator(statsList=[SummaryStatistics:
n: 1
min: 3.0
max: 3.0
sum: 3.0
mean: 3.0
geometric mean: 3.0000000000000004
variance: 0.0
population variance: 0.0
second moment: 0.0
sum of squares: 9.0
standard deviation: 0.0
sum of logs: 1.0986122886681098
]), decimalDataCalculator=DoubleDistributionForCollections(min=1.0, max=3.0, mean=2.0, stdDev=1.0, sum=6.0)) (FieldRecord@3249a1ce)""
but was : ""FieldRecord(name=foo, type=string, fieldCounter=FieldCounter(populated=1, missing=3), countDistinctCalculator=### CPD SKETCH - PREAMBLE:
  Flavor         : SPARSE
  LgK            : 10
  Merge Flag     : false
  Error Const    : 0.5887050112577373
  RSE            : 0.01839703160180429
  Seed Hash      : 93cc | 37836
  Num Coupons    : 2
  Num Pairs (SV) : 2
  First Inter Col: 0
  Valid Window   : false
  Valid PairTable: true
  Window Offset  : 0
  KxP            : 1023.375
  HIP Accum      : 2.00012208521548
### END CPC SKETCH, categoryHistogramCalculator=maxNumberOfStoredValues: 20, frequencies: {test=1}, numericalRangeHistogramCalculator=
### Quantiles HeapUpdateDoublesSketch SUMMARY: 
   Empty                        : false
   Direct, Capacity bytes       : false, 
   Estimation Mode              : false
   K                            : 128
   N                            : 2
   Levels (Needed, Total, Valid): 0, 0, 0
   Level Bit Pattern            : 0
   BaseBufferCount              : 2
   Combined Buffer Capacity     : 4
   Retained Items               : 2
   Compact Storage Bytes        : 48
   Updatable Storage Bytes      : 64
   Normalized Rank Error        : 1.406%
   Normalized Rank Error (PMF)  : 1.711%
   Min Value                    : 1.000000e+00
   Max Value                    : 3.000000e+00
### END SKETCH SUMMARY
, numberOfElementsCalculator=NumberOfElementsCalculator(statsList=[SummaryStatistics:
n: 1
min: 3.0
max: 3.0
sum: 3.0
mean: 3.0
geometric mean: 3.0000000000000004
variance: 0.0
population variance: 0.0
second moment: 0.0
sum of squares: 9.0
standard deviation: 0.0
sum of logs: 1.0986122886681098
]), decimalDataCalculator=DoubleDistributionForCollections(min=1.0, max=3.0, mean=2.0, stdDev=1.0, sum=6.0)) (FieldRecord@769a58e5)""
  
{code}
 

Whether there is any issue with the way we are serialising deserialising this ?  

Any help is appreciated",,akshayhazari,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-04-12 05:35:10.0,,,,,,,,,,"0|z11dh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-null check for bootstrapServers field is incorrect in KafkaSink,FLINK-27174,13438915,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Tony Giao,Tony Giao,Tony Giao,11/Apr/22 10:24,24/May/22 11:43,13/Jul/23 08:08,24/May/22 11:43,1.14.4,1.15.0,1.16.0,,,,1.14.5,1.15.1,1.16.0,,Connectors / Kafka,,,,,0,easyfix,pull-request-available,,"If the user-supplied kafkaProducerConfig contains bootstrapServers information, there is no need to define the value of this field separately through the setBootstrapServers method. Obviously, the current code doesn't notice this.

!image-2022-04-11-18-11-18-576.png|width=859,height=261!

 

Perhaps we can check bootstrapServers as follows:

!image-2022-04-11-18-17-48-514.png|width=861,height=322!

 

{color:#172b4d}Or check bootstrapServers like KafkaSourceBuilder.{color}

 

 ",,fpaul,Tony Giao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/22 10:11;Tony Giao;image-2022-04-11-18-11-18-576.png;https://issues.apache.org/jira/secure/attachment/13042234/image-2022-04-11-18-11-18-576.png","11/Apr/22 10:17;Tony Giao;image-2022-04-11-18-17-48-514.png;https://issues.apache.org/jira/secure/attachment/13042233/image-2022-04-11-18-17-48-514.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 11:43:32 UTC 2022,,,,,,,,,,"0|z11ce8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/22 10:07;Tony Giao;[~fpaul] , please take a look;;;","22/Apr/22 11:07;fpaul;I think that is a valid point. We introduced the  setBootstrapServers method to keep in consistent with the KafkaSource. Do you want to work on relaxing the check that it also allows the bootstrap servers as part of the properties?;;;","22/Apr/22 11:56;Tony Giao;Yes. In the current code, if the user does not use the setBootstrapServers method to set bootstrapServers, even if he provides it in a separate property, the non-null check on bootstrapServers will fail, which is obviously unreasonable. In fact, we can just check bootstrapServers in the final property.;;;","01/May/22 08:58;Tony Giao;[~fpaul], If you think this proposal works, can you give me a ticket and I'll be happy to try and fix it.;;;","19/May/22 14:06;fpaul;Merged in master: b4bb9c8bffe1e37ad6912348d8b3bef89af42286;;;","24/May/22 09:25;fpaul;Merged in release-1.14: c81a0512c7ade2ec4f2a91e98ec5ee4d09a66d3b;;;","24/May/22 11:43;fpaul;Merged in release-1.15: c028c00b5ddfdf2903de07e1415c7c855007c663;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase.test hangs on azure,FLINK-27169,13438883,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,gaoyunhaii,gaoyunhaii,11/Apr/22 08:15,08/Mar/23 17:55,13/Jul/23 08:08,03/Mar/23 20:12,1.15.3,1.16.0,,,,,1.15.4,1.16.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,"
{code:java}
Apr 10 08:32:18 ""main"" #1 prio=5 os_prio=0 tid=0x00007f553400b800 nid=0x8345 waiting on condition [0x00007f553be60000]
Apr 10 08:32:18    java.lang.Thread.State: TIMED_WAITING (sleeping)
Apr 10 08:32:18 	at java.lang.Thread.sleep(Native Method)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
Apr 10 08:32:18 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
Apr 10 08:32:18 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
Apr 10 08:32:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 10 08:32:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 10 08:32:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 10 08:32:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 10 08:32:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Apr 10 08:32:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 10 08:32:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 10 08:32:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Apr 10 08:32:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 10 08:32:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Apr 10 08:32:18 	at org.junit.runners.Suite.runChild(Suite.java:128)
Apr 10 08:32:18 	at org.junit.runners.Suite.runChild(Suite.java:27)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=6757",,akalashnikov,gaoyunhaii,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26685,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 03 20:12:24 UTC 2023,,,,,,,,,,"0|z11c74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/22 08:16;gaoyunhaii;I'll have a look;;;","20/May/22 07:57;chesnay;What seems to happen is that the job keeps checkpointing and eventually runs out of disk space.
This only seems to happen when the changelog statebackend is used.

I don't know why the test doesn't exit earlier, so there may be 2 separate issues here.

[~roman] Could you have a look?;;;","25/May/22 13:10;roman;Thanks for looking into the issue [~chesnay].
This is what I believe leads to test hanging up:
 # Checkpoint 1 completes
 # Several subsequent checkpoints fail due to a timeout while writing changelog segments (all 3 configured attempts exhausted)
 # Job graph gets restarted due to failure
 # FINISH_SOURCES command gets lost as a result
 # TestJobExecutor hangs in waitForSubtasksToFinish as a result

I suppose the root cause is an intermittent failure of the local disk. I'm going to increase the timeout and the number of attempts in test. Increasing environment.tolerable_declined_checkpoint_number won't help because a failed upload fails all subsequent checkpoint (until the segment materialized).


To prevent the test from hanging up, I'm going to add a timeout (restarts can not be disabled because they are required by the test scenario; and can not be detected easily)
To ease debugging I'm going to raise the log level TestJobExecutor to INFO.

 

I'll open a PR with the above changes.;;;","26/May/22 12:16;roman;Merged into master as 0b139a12c2c24e07cedb43bbad6ee1ff634bcfcb..f0ec84a36c5751ab5375702b1c147fd417b729da.;;;","25/Feb/23 22:33;roman;Reopening to backport the fix to 1.15 (FLINK-31133);;;","03/Mar/23 20:12;roman;Backported to 1.15 as ddec8d8e144c9cc9adb0a04f41c9667cdd68aabb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState failed on azure,FLINK-27162,13438856,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,gaoyunhaii,gaoyunhaii,11/Apr/22 06:58,19/Jul/22 02:15,13/Jul/23 08:08,19/Jul/22 02:15,1.16.0,,,,,,1.16.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-04-08T16:21:37.7382295Z Apr 08 16:21:37 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.21 s <<< FAILURE! - in org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase
2022-04-08T16:21:37.7383825Z Apr 08 16:21:37 [ERROR] org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState  Time elapsed: 9.642 s  <<< ERROR!
2022-04-08T16:21:37.7385362Z Apr 08 16:21:37 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was canceled because a barrier from newer checkpoint was received.
2022-04-08T16:21:37.7386479Z Apr 08 16:21:37 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-04-08T16:21:37.7387206Z Apr 08 16:21:37 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-04-08T16:21:37.7388026Z Apr 08 16:21:37 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.runJobAndGetCheckpoint(RescaleCheckpointManuallyITCase.java:196)
2022-04-08T16:21:37.7389054Z Apr 08 16:21:37 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingKeyedState(RescaleCheckpointManuallyITCase.java:137)
2022-04-08T16:21:37.7390072Z Apr 08 16:21:37 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState(RescaleCheckpointManuallyITCase.java:115)
2022-04-08T16:21:37.7391320Z Apr 08 16:21:37 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-04-08T16:21:37.7392401Z Apr 08 16:21:37 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-04-08T16:21:37.7393916Z Apr 08 16:21:37 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-04-08T16:21:37.7394662Z Apr 08 16:21:37 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-04-08T16:21:37.7395293Z Apr 08 16:21:37 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-04-08T16:21:37.7396038Z Apr 08 16:21:37 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-04-08T16:21:37.7396749Z Apr 08 16:21:37 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-04-08T16:21:37.7397458Z Apr 08 16:21:37 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-04-08T16:21:37.7398164Z Apr 08 16:21:37 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-04-08T16:21:37.7398844Z Apr 08 16:21:37 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-04-08T16:21:37.7399505Z Apr 08 16:21:37 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-04-08T16:21:37.7400182Z Apr 08 16:21:37 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-04-08T16:21:37.7400804Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-04-08T16:21:37.7401492Z Apr 08 16:21:37 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-04-08T16:21:37.7402605Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-04-08T16:21:37.7403783Z Apr 08 16:21:37 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-04-08T16:21:37.7404514Z Apr 08 16:21:37 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-04-08T16:21:37.7405180Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-04-08T16:21:37.7405784Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-04-08T16:21:37.7406537Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-04-08T16:21:37.7407256Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-04-08T16:21:37.7407889Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-04-08T16:21:37.7408752Z Apr 08 16:21:37 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-04-08T16:21:37.7409367Z Apr 08 16:21:37 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-04-08T16:21:37.7409948Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-04-08T16:21:37.7410562Z Apr 08 16:21:37 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-04-08T16:21:37.7411239Z Apr 08 16:21:37 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-04-08T16:21:37.7411801Z Apr 08 16:21:37 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-04-08T16:21:37.7412623Z Apr 08 16:21:37 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-04-08T16:21:37.7413727Z Apr 08 16:21:37 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-04-08T16:21:37.7414618Z Apr 08 16:21:37 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-04-08T16:21:37.7415569Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-04-08T16:21:37.7416796Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-04-08T16:21:37.7418037Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-04-08T16:21:37.7418969Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-04-08T16:21:37.7420010Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-04-08T16:21:37.7420785Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-04-08T16:21:37.7421658Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-04-08T16:21:37.7422449Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-04-08T16:21:37.7423442Z Apr 08 16:21:37 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-04-08T16:21:37.7424331Z Apr 08 16:21:37 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-04-08T16:21:37.7425171Z Apr 08 16:21:37 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-04-08T16:21:37.7426159Z Apr 08 16:21:37 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-04-08T16:21:37.7426927Z Apr 08 16:21:37 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-04-08T16:21:37.7427641Z Apr 08 16:21:37 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-04-08T16:21:37.7428304Z Apr 08 16:21:37 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-04-08T16:21:37.7428971Z Apr 08 16:21:37 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-04-08T16:21:37.7429908Z Apr 08 16:21:37 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was canceled because a barrier from newer checkpoint was received.
2022-04-08T16:21:37.7430794Z Apr 08 16:21:37 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:549)
2022-04-08T16:21:37.7431598Z Apr 08 16:21:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2078)
2022-04-08T16:21:37.7432613Z Apr 08 16:21:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1047)
2022-04-08T16:21:37.7433691Z Apr 08 16:21:37 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
2022-04-08T16:21:37.7434782Z Apr 08 16:21:37 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
2022-04-08T16:21:37.7435602Z Apr 08 16:21:37 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-04-08T16:21:37.7436566Z Apr 08 16:21:37 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-04-08T16:21:37.7437190Z Apr 08 16:21:37 	at java.lang.Thread.run(Thread.java:748)
2022-04-08T16:21:37.7437875Z Apr 08 16:21:37 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was canceled because a barrier from newer checkpoint was received.
2022-04-08T16:21:37.7438791Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.abortInternal(SingleCheckpointBarrierHandler.java:376)
2022-04-08T16:21:37.7439802Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.cancelSubsumedCheckpoint(SingleCheckpointBarrierHandler.java:463)
2022-04-08T16:21:37.7440817Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.checkNewCheckpoint(SingleCheckpointBarrierHandler.java:347)
2022-04-08T16:21:37.7441817Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:228)
2022-04-08T16:21:37.7442730Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
2022-04-08T16:21:37.7443851Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
2022-04-08T16:21:37.7444753Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:112)
2022-04-08T16:21:37.7445652Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
2022-04-08T16:21:37.7446419Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
2022-04-08T16:21:37.7447189Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:379)
2022-04-08T16:21:37.7448034Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:341)
2022-04-08T16:21:37.7448892Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:225)
2022-04-08T16:21:37.7450230Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:841)
2022-04-08T16:21:37.7451146Z Apr 08 16:21:37 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:767)
2022-04-08T16:21:37.7451885Z Apr 08 16:21:37 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
2022-04-08T16:21:37.7452583Z Apr 08 16:21:37 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
2022-04-08T16:21:37.7453450Z Apr 08 16:21:37 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
2022-04-08T16:21:37.7454150Z Apr 08 16:21:37 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
2022-04-08T16:21:37.7454615Z Apr 08 16:21:37 	... 1 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34459&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5658",,akalashnikov,fanrui,gaoyunhaii,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28328,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 02:15:07 UTC 2022,,,,,,,,,,"0|z11c14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 10:01;akalashnikov;From FLINK-28328: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37433&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9225;;;","14/Jul/22 02:01;hxbks2ks;another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38110&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","19/Jul/22 02:15;hxbks2ks;Merged into master via bf983ab1f9bdc7ef1daed15397abf0b032ad2919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase fails on AZP,FLINK-27148,13438638,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,roman,roman,08/Apr/22 16:21,26/Aug/22 07:58,13/Jul/23 08:08,19/Apr/22 02:43,1.16.0,,,,,,1.16.0,,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34394&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5812]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34394&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=6018]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34448&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=41655]

Relevant error message:
 
{code:java}
Caused by: java.lang.IllegalStateException: Cannot mark for checkpoint 12, already marked for checkpoint 11
	at org.apache.flink.runtime.operators.coordination.OperatorEventValve.markForCheckpoint(OperatorEventValve.java:113)
 {code}
 
{code:java}
[ERROR] Tests run: 22, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 174.732 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointITCase
[ERROR] UnalignedCheckpointITCase.execute  Time elapsed: 6.408 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:184)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:287)
	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.lang.IllegalStateException: Cannot mark for checkpoint 12, already marked for checkpoint 11
	at org.apache.flink.runtime.operators.coordination.OperatorEventValve.markForCheckpoint(OperatorEventValve.java:113)
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.checkpointCoordinatorInternal(OperatorCoordinatorHolder.java:302)
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinator$0(OperatorCoordinatorHolder.java:230)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}",,akalashnikov,dwysakowicz,gaoyunhaii,pltbkd,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 19 02:43:23 UTC 2022,,,,,,,,,,"0|z11apk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/22 16:25;roman;So far I've only seen it on master (1.16), but it looks like the same changes are also present in 1.15.

Could you please take a look [~pnowojski], [~akalashnikov], [~dwysakowicz] ?

;;;","08/Apr/22 16:34;roman;Here's KafkaSourceE2ECase.testScaleDown failure

with the same error message: Cannot mark for checkpoint 40, already marked for checkpoint 39:

https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1548&view=logs&j=0e31ee24-31a6-528c-a4bf-45cde9b2a14e&t=ff03a8fa-e84e-5199-efb2-5433077ce8e2&l=13854;;;","08/Apr/22 17:02;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34454&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=6256;;;","11/Apr/22 06:42;gaoyunhaii;With some checks I think this issue is caused by the modification in https://issues.apache.org/jira/browse/FLINK-26394, [~pltbkd] could you have a look? 

The scenarios of this issue is
# The timer thread starts triggering a checkpoint
# in createPendingCheckpoint, after putting the checkpoint into the pendingCheckpoints, the timer thread given up the CPU.
# The JM main thread start stopping the CheckpointCoordinator, which further aborts all the pending checkpoints. 

The story was changed from here. Previously it would
# The JM main thread abort the pending checkpoint.
# The timer thread resume. it does not check whether the pending checkpoint is aborted, thus it continues to triggering the operator coordinators. It would mark the valve currentCheckpointId = 11.
# After that it starts triggering the tasks. However, a check here found that the pending checkpoint is aborted, thus it called onTriggerFailure, which further reset the valve currentCheckpointId to None.


After the change
# The JM main thread abort the pending checkpoint. However, with the newly introduced masterTriggerCompletePromise to be canceled, it would insert a new onTriggerFailure task to the timer thread.
# The timer thread executes onTriggerFailure and reset the valve currentCheckpointId to None.
# The timer thread resume the triggering as before, it continues to triggering the operator coordinators. It would mark the valve currentCheckpointId = 11.
# However, since now masterTriggerCompletePromise has been completed, the triggering stop after the source coordinator acknowledged. There won't be onTriggerFailure get executed after that, thus the valve currentCheckpointId = 11 is kept.

As a whole, currently the trigger is split into multiple steps. Between any two steps the pending checkpoints might be aborted. We need to check this state carefully in the start of any step to avoid inconsistency. 

Since this PR is only merged onto master, I think the release-1.15 is still ok. I'll first remove 1.15 from the affected and fixed versions. ;;;","11/Apr/22 08:13;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=15209
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=40325
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=41200 (This has a different stack, might need a double check)
;;;","13/Apr/22 06:59;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34582&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=40641;;;","13/Apr/22 07:03;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34585&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=6293;;;","13/Apr/22 07:12;pltbkd;I'm working on the issue now. The solution seems to be simple, but I'm thinking about how to verify the change with an UT, which may take some time.;;;","15/Apr/22 06:41;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34701&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=40845;;;","15/Apr/22 06:44;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34689&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5847;;;","15/Apr/22 06:56;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34659&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=16294;;;","18/Apr/22 07:02;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34756&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36048;;;","18/Apr/22 07:26;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34743&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=40807;;;","19/Apr/22 02:43;gaoyunhaii;Fix on master via 2c608b8efa18e4633ba4cee308ea71a4c54ab955;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The code generated for agg function accepts empty parameters can't be compiled,FLINK-27145,13438588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,08/Apr/22 12:27,21/Apr/22 02:27,13/Jul/23 08:08,21/Apr/22 02:27,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"In batch mode, when I try to call Hive's `count` function(GenericUDAFCount) using the following sql:
{code:java}
select count(*) from src;
{code}
It'll throw the exception ""Unexpected token "")"" in primary"" while compling the generated code.

It happens in the following generated code:
{code:java}
function_org$apache$flink$table$planner$utils$xxx.accumulate(
            acc$7,); {code}
 

The reason is the following code in  method `AggCodeGenHelper#genAccumulateFlatAggregateBuffer`
{code:java}
s""""""
  |$externalAccTypeTerm $externalAccTerm = $externalAccCode;
  |${functionIdentifiers(function)}.accumulate(
  |  $externalAccTerm,
  |  ${operandTerms.mkString("", "")}); // but operandTerms can be empty list
  |$aggBufferName = ${genToInternalConverter(ctx, externalAccType)(externalAccTerm)};
  |${aggBufferExpr.nullTerm} = false;
"""""".stripMargin {code}
In this case, for count(\*), it'll be regarded as an agg function with empty parameter in Hive.

So, the exception happens.

Although the agg function with empty parameter is rare, it may happens. We should make the code more rubst.",,luoyuxia,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 21 02:27:33 UTC 2022,,,,,,,,,,"0|z11aeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/22 12:30;luoyuxia;[~lzljs3620320] Could you please have a look? If it's a problem that should be fixed, I would like to take it.;;;","11/Apr/22 04:16;lzljs3620320;[~luoyuxia] Thanks;;;","21/Apr/22 02:27;lzljs3620320;master: 27c3f47781f3a9f0b804219c7b8037fe4a00dc75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move JobResultStore dirty entry creation into ioExecutor,FLINK-27140,13438558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,08/Apr/22 10:14,14/Apr/22 14:37,13/Jul/23 08:08,14/Apr/22 07:57,1.15.0,1.16.0,,,,,1.15.1,1.16.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,"The {{FileSystemJobResultStore}} is thread-safe and, therefore, we can move the dirty entry creation into the {{ioExecutor}}",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27204,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 07:57:01 UTC 2022,,,,,,,,,,"0|z11a7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/22 07:57;chesnay;master: 8d006224d209c981c563ef6625bc6f617d40684a
1.15: 2d5bebb91df23a3c4dd60506984f1b0cfb539db5 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hardcoded namespace in FlinkDeployment manifests may fail to deploy,FLINK-27129,13438492,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tedhtchang,tedhtchang,tedhtchang,08/Apr/22 01:43,01/Nov/22 09:39,13/Jul/23 08:08,02/May/22 10:30,0.1.0,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,starter,,"When the Flink operator is installed to non-default namespace these FlinkDeployment manifests [1] may fail to deploy with the following error in the Flink operator log [2]. We may want to remove the `namespace: default` from these manifests and let user specify different one with the --namespace flag in kubectl.
[1][https://github.com/apache/flink-kubernetes-operator/tree/main/examples]
[2]
{code:java}
2022-04-08 00:42:02,803 o.a.f.k.o.c.FlinkDeploymentController [ERROR][default/basic-example] Flink Deployment failed
org.apache.flink.kubernetes.operator.exception.DeploymentFailedException: pods ""basic-example-5cc7894895-"" is forbidden: error looking up service account default/flink: serviceaccount ""flink"" not found
    at org.apache.flink.kubernetes.operator.observer.BaseObserver.checkFailedCreate(BaseObserver.java:135)
    at org.apache.flink.kubernetes.operator.observer.BaseObserver.observeJmDeployment(BaseObserver.java:102)
    at org.apache.flink.kubernetes.operator.observer.JobObserver.observe(JobObserver.java:51)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:122)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:101)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:76)
    at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:75)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:143)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:109)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:74)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source) {code}
 

 ","Client Version: version.Info\{Major:""1"", Minor:""22"", GitVersion:""v1.22.2"", GitCommit:""8b5a19147530eaac9476b0ab82980b4088bbc1b2"", GitTreeState:""clean"", BuildDate:""2021-09-15T21:38:50Z"", GoVersion:""go1.16.8"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info\{Major:""1"", Minor:""22"", GitVersion:""v1.22.8+IKS"", GitCommit:""0d0ff1cc1dbe76cf96c33e7510b25c283ac29943"", GitTreeState:""clean"", BuildDate:""2022-03-17T14:47:39Z"", GoVersion:""go1.16.15"", Compiler:""gc"", Platform:""linux/amd64""}",mbalassi,tedhtchang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 02 10:30:55 UTC 2022,,,,,,,,,,"0|z119t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/22 02:22;wangyang0918;I believe it is the user's responsibility to create a proper service account in expected namespace if the {{watchNamespaces}} is not configured.

But just like you said, I think we could remove the namespace from all examples so that user could specify the namespace when applying.
{code:java}
kubectl -n flink-operator apply -f examples/basic.yaml {code}
 

[1]. https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-0.1/docs/operations/helm/#watching-only-specific-namespaces;;;","13/Apr/22 10:16;tedhtchang;[~wangyang0918] Thanks! Could you assign to me.;;;","14/Apr/22 02:05;wangyang0918;[~tedhtchang] Thanks for working on this.

Except for removing the namespace in the examples, I think we also need to document that users need to manually create a proper service account in expected namespace if the {{watchNamespaces}} is not configured.;;;","28/Apr/22 19:47;tedhtchang;[~wangyang0918] Please review the PR [https://github.com/apache/flink-kubernetes-operator/pull/188]
Thanks;;;","02/May/22 10:30;mbalassi;Merged to main in [1cb3ecc|https://github.com/apache/flink-kubernetes-operator/commit/1cb3eccc450eba5e68551c6713a5fe1537c87a78];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect the state cache size configurations in Python DataStream API operators,FLINK-27126,13438399,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,07/Apr/22 12:58,08/Apr/22 06:24,13/Jul/23 08:08,08/Apr/22 06:24,,,,,,,1.15.0,,,,API / Python,,,,,0,pull-request-available,,,"Currently, the state cache size configurations are not handled in Python DataStream API operators. See https://github.com/apache/flink/blob/master/flink-python/pyflink/fn_execution/beam/beam_operations.py#L188 for more details.",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 08 06:24:36 UTC 2022,,,,,,,,,,"0|z1198g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/22 06:24;dianfu;Fixed in:
- master via 388dac810182428a73f7b17a67aca3d2e21d57f5
- release-1.15 via 63521977fca5e05128ad3720724f6be550b3098d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kubernetes operator prints starting logs with wrong version,FLINK-27124,13438379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,wangyang0918,wangyang0918,07/Apr/22 11:51,18/Apr/22 05:09,13/Jul/23 08:08,18/Apr/22 05:09,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,starter,,"{code:java}
2022-04-07 11:48:01,357 o.a.f.k.o.FlinkOperator       [INFO ]  Starting Flink Kubernetes Operator (Version: 1.14.4, Scala: 2.12, Rev:895c609, Date:2022-02-25T11:57:14+01:00) {code}
The version information comes from Flink binary, not the flink-kubernetes-operator.",,nicholasjiang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 18 05:09:00 UTC 2022,,,,,,,,,,"0|z11940:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/22 15:11;nicholasjiang;[~wangyang0918], I have pushed a pull request for this ticket. Please help to assign this ticket to me.;;;","18/Apr/22 05:09;wangyang0918;Fixed via:

main: 06dd78676e4ece0bbbcbd75de3d69eb37e3a6331;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink on Yarned failed with the default flink-conf,FLINK-27118,13438351,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,gaoyunhaii,gaoyunhaii,07/Apr/22 10:25,09/Apr/22 03:06,13/Jul/23 08:08,09/Apr/22 03:06,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,"We changed the default bind-host of TaskManager to localhost in https://issues.apache.org/jira/browse/FLINK-24474. For one part, in the PR we want to remove this configuration for the Yarn mode so that the Yarn mode does not be affected, however, it seems the removing not work properly. ",,Feifan Wang,gaoyunhaii,jingzhang,Thesharing,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24474,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 09 03:06:14 UTC 2022,,,,,,,,,,"0|z118xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/22 10:28;gaoyunhaii;[~chesnay] [~nsemmler] could you have a look at this issue~?;;;","08/Apr/22 02:23;jingzhang;[~gaoyunhaii]Thanks for reporting this bug, I've meet with the same problem today.
The second vertex of the topology would fail because it could not get result partition from the upstream vertex.
;;;","09/Apr/22 03:06;gaoyunhaii;Merged on master via 6e55d223efdc2563f0b7600a393ff9a3d8767942
Merged on 1.15 via 920e81b6478be1bb605d46bac800d4af675f8e79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State cache clean up doesn't work as expected,FLINK-27108,13438312,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,07/Apr/22 07:26,07/Apr/22 11:04,13/Jul/23 08:08,07/Apr/22 11:04,1.13.0,1.14.0,1.15.0,,,,1.13.7,1.14.5,1.15.0,,API / Python,,,,,0,pull-request-available,,,"The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine [whether a key & namespace exists in state cache is wrong|https://github.com/apache/flink/blob/master/flink-python/pyflink/fn_execution/state_impl.py#L1183] is wrong. It causes the state cache isn't clean up when it becomes invalidate. ",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 07 11:04:58 UTC 2022,,,,,,,,,,"0|z118p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/22 11:04;dianfu;Fixed in:
- master via ead6db797923d665edcbfdb94e2e1a847a356ff7
- release-1.15 via a833196ce58e9ce7a42306bc4ae44c6ed66283d5
- release-1.14 via c372ddd3ea1d0239a9177cade2207bf9ba2b24a2
- release-1.13 via 66d896081ae9c9572972ec13cef8169d57391444;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong metric type of ChangelogStorage.uploadQueueSize in document,FLINK-27105,13438293,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,07/Apr/22 04:29,07/Apr/22 17:13,13/Jul/23 08:08,07/Apr/22 17:13,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Documentation,,,,,0,pull-request-available,,,"Hi [~roman] , metric type of ChangelogStorage.uploadQueueSize is {color:#de350b}Gauge{color}, not Meter.

see : [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/#state-changelog]",,Feifan Wang,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 07 17:13:42 UTC 2022,,,,,,,,,,"0|z118kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/22 17:13;roman;Fix merged into master as 04b136340c2fc73f605f4143c2352bb7a657b7bb,

into release-1.15 as a2eb826e1fc4ae6018cc3e3ccfbcc8a642b68f96.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The session job controller should create FlinkDeployment informer event source with namespace definition,FLINK-27098,13438206,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,haoxin,haoxin,haoxin,06/Apr/22 15:25,07/Apr/22 07:02,13/Jul/23 08:08,07/Apr/22 06:58,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"The below error will occur if we deploy the operator with a namespaced scope and submit a session job.

 
{code:java}
[WARN ] Error starting org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController$1@96a75da
io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.16.0.1/apis/flink.apache.org/v1alpha1/namespaces/flink-operator/flinkdeployments. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. flinkdeployments.flink.apache.org is forbidden: User ""system:serviceaccount:flink-operator:flink-operator"" cannot list resource ""flinkdeployments"" in API group ""flink.apache.org"" in the namespace ""flink-operator"". {code}
This error comes from the creation of the FlinkDeployment informer event source.

I just submitted a PR to fix this, https://github.com/apache/flink-kubernetes-operator/pull/157",,haoxin,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 07 06:58:29 UTC 2022,,,,,,,,,,"0|z1181k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/22 06:58;wangyang0918;Fixed via:

main: a7b8a7679ee44141d9cf5a97d49d8afe8308532c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling TRY_CAST with an invalid value throws IndexOutOfBounds Exception.,FLINK-27089,13438123,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,matriv,TsReaper,TsReaper,06/Apr/22 09:13,08/Apr/22 08:16,13/Jul/23 08:08,08/Apr/22 08:16,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"Add the following test to {org.apache.flink.table.planner.runtime.batch.sql.CalcITCase} to reproduce this issue.
{code:scala}
@Test
def myTest(): Unit = {
  checkResult(""SELECT TRY_CAST('invalid' AS INT)"", Seq(row(null)))
}
{code}

The exception stack is
{code}
java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)

	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:1345)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:1327)
	at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:43)
	at org.apache.calcite.rex.RexCallBinding.getOperandType(RexCallBinding.java:136)
	at org.apache.calcite.sql.fun.SqlCastFunction.getMonotonicity(SqlCastFunction.java:205)
	at org.apache.flink.table.planner.functions.sql.BuiltInSqlFunction.getMonotonicity(BuiltInSqlFunction.java:141)
	at org.apache.calcite.rel.metadata.RelMdCollation.project(RelMdCollation.java:291)
	at org.apache.calcite.rel.logical.LogicalProject.lambda$create$0(LogicalProject.java:122)
	at org.apache.calcite.plan.RelTraitSet.replaceIfs(RelTraitSet.java:242)
	at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:121)
	at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)
	at org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:177)
	at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1516)
	at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1311)
	at org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1565)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4222)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:687)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:644)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3438)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:570)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:198)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:190)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1240)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1188)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:345)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:105)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:675)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.parseQuery(BatchTestBase.scala:297)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:139)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest(CalcITCase.scala:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}",,matriv,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 08 08:16:42 UTC 2022,,,,,,,,,,"0|z117jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/22 09:40;twalthr;CC [~slinkydeveloper];;;","06/Apr/22 11:06;matriv;The issue is because *SqlTryCastFunction* is of kind *SqlKind.OTHER_FUNCTION* then in 

*RexCallBinding*
{noformat}
public static RexCallBinding create(RelDataTypeFactory typeFactory,
    RexCall call,
    List<RelCollation> inputCollations) {
  switch (call.getKind()) {
  case CAST:
    return new RexCastCallBinding(typeFactory, call.getOperator(),
        call.getOperands(), call.getType(), inputCollations);
  }
  return new RexCallBinding(typeFactory, call.getOperator(),
      call.getOperands(), inputCollations);
}{noformat}
we don't use the {*}RexCastCallBinding{*},but the regular one, for which we don't get the code:

 
{noformat}
@Override public RelDataType getOperandType(int ordinal) {
  if (ordinal == 1) {
    return type;
  }
  return super.getOperandType(ordinal);
}{noformat}
 

but the one from the regular *RexCallBinding* (superclass)
{noformat}
public RelDataType getOperandType(int ordinal) {
  return operands.get(ordinal).getType();
}{noformat}
which uses the preconditions of *SingletonImmutableList* and fails (since indeed there is only one operand, given the special syntax of CAST)

 ;;;","06/Apr/22 14:44;matriv;With the help of [~twalthr] we have tried out workarounds,

like creating a call with a dummy second operand in *FlinkConvertletTable* where we make the call, which solves the issues, but changes our *EXPLAIN* output (and also the json plan output)

or changing the *TRY_CAST* to be of kind *CAST* instead of {*}OTHER_FUNCTION{*}, which creates other issues, since calcite will then apply rules and optimizations and end up with *CAST* instead of {*}TRY_CAST{*}.

 

So finally, we decided to just not implement the *getMonotonicity()* for {*}TRY_CAST{*}, which is the only way to ""cleanly"" solve the bug currently, but with the drawback to many looses some optimizations in batch mode.;;;","08/Apr/22 08:16;twalthr;Fixed in master: 637d08e97f40f7a4d2836fcd0e697eab8b2fe324
Fixed in 1.15: c8e37d9d850f76093fecf68fbfb81065bfc6f914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perround mode recreating operator fails,FLINK-27084,13438088,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,06/Apr/22 07:09,01/Jul/22 02:12,13/Jul/23 08:08,01/Jul/22 02:07,ml-2.0.0,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,test-stability,,"When I was trying to submit a job containing Flink ML KMeans operator to a Flink cluster, the following exception is thrown out.
{code:java}
The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 8ba9d3173d1c83eb4803298f81349aea)
at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 8ba9d3173d1c83eb4803298f81349aea)
at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironment.java:123)
at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:80)
at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1898)
at org.apache.flink.ml.benchmark.BenchmarkUtils.runBenchmark(BenchmarkUtils.java:127)
at org.apache.flink.ml.benchmark.BenchmarkUtils.runBenchmark(BenchmarkUtils.java:84)
at org.apache.flink.ml.benchmark.Benchmark.main(Benchmark.java:50)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
... 8 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 8ba9d3173d1c83eb4803298f81349aea)
at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:125)
at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$26(RestClusterClient.java:698)
at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403)
at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:123)
... 24 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
at akka.actor.Actor.aroundReceive(Actor.scala:537)
at akka.actor.Actor.aroundReceive$(Actor.scala:535)
at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
at akka.actor.ActorCell.invoke(ActorCell.scala:548)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
at akka.dispatch.Mailbox.run(Mailbox.scala:231)
at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.flink.ml.clustering.kmeans.KMeans$CountAppender
at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316)
at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:176)
at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:146)
at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:68)
at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
at org.apache.flink.iteration.proxy.ProxyOutput.collect(ProxyOutput.java:92)
at org.apache.flink.iteration.proxy.ProxyOutput.collect(ProxyOutput.java:35)
at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
at org.apache.flink.ml.clustering.kmeans.KMeans$SelectNearestCentroidOperator.onEpochWatermarkIncremented(KMeans.java:303)
at org.apache.flink.iteration.operator.AbstractWrapperOperator.notifyEpochWatermarkIncrement(AbstractWrapperOperator.java:129)
at org.apache.flink.iteration.operator.allround.AbstractAllRoundWrapperOperator.lambda$onEpochWatermarkIncrement$1(AbstractAllRoundWrapperOperator.java:105)
at org.apache.flink.iteration.operator.OperatorUtils.processOperatorOrUdfIfSatisfy(OperatorUtils.java:79)
at org.apache.flink.iteration.operator.allround.AbstractAllRoundWrapperOperator.onEpochWatermarkIncrement(AbstractAllRoundWrapperOperator.java:102)
at org.apache.flink.iteration.progresstrack.OperatorEpochWatermarkTracker.tryUpdateLowerBound(OperatorEpochWatermarkTracker.java:79)
at org.apache.flink.iteration.progresstrack.OperatorEpochWatermarkTracker.onEpochWatermark(OperatorEpochWatermarkTracker.java:63)
at org.apache.flink.iteration.operator.AbstractWrapperOperator.onEpochWatermarkEvent(AbstractWrapperOperator.java:121)
at org.apache.flink.iteration.operator.allround.TwoInputAllRoundWrapperOperator.processElement(TwoInputAllRoundWrapperOperator.java:77)
at org.apache.flink.iteration.operator.allround.TwoInputAllRoundWrapperOperator.processElement2(TwoInputAllRoundWrapperOperator.java:59)
at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.processRecord2(StreamTwoInputProcessorFactory.java:225)
at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.lambda$create$1(StreamTwoInputProcessorFactory.java:194)
at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:266)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.ml.clustering.kmeans.KMeans$CountAppender
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1984)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1848)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2158)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2403)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2403)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:501)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:459)
at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)
at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)
at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)
at org.apache.flink.util.InstantiationUtil.clone(InstantiationUtil.java:704)
at org.apache.flink.util.InstantiationUtil.clone(InstantiationUtil.java:682)
at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:161)
... 35 more{code}
The location that throws the exception above is within `PerRoundSubBody`, when `InstantiationUtil.clone(operatorFactory)` is invoked.

If the submitted job contains `KMeansModel` instead of `KMeans`, then the job can run successfully.

If the Flink ML dependencies are added to`$FLINK_HOME/lib` folder, instead of included in the submitted jar, the KMeans operator can also be executed successfully.

Thus there might be a bug with the implementation of `PerRoundSubBody`, that it cannot find the correct classpath when recreating operators inside the per-round body.

 ","*strong text*Flink 1.14.0, Flink ML 2.0.0",yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-04-06 07:09:35.0,,,,,,,,,,"0|z117bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the potential memory corruption in Thread Mode,FLINK-27069,13438056,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,06/Apr/22 06:15,07/Apr/22 07:17,13/Jul/23 08:08,06/Apr/22 08:39,1.15.0,,,,,,1.15.0,,,,API / Python,,,,,0,pull-request-available,,,"
{code:java}
Apr 02 12:24:54 *** Error in `/usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/bin/java': malloc(): memory corruption: 0x00007ff7c43bb820 ***
Apr 02 12:24:54 ======= Backtrace: =========
Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7ff7f90be7f5]
Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(+0x8215e)[0x7ff7f90c915e]
Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7ff7f90cb1d4]
Apr 02 12:24:54 /root/flink/flink-python/dev/.conda/envs/3.8/lib/libpython3.8.so.1.0(PyObject_Malloc+0x166)[0x7ff78c16c636]
Apr 02 12:24:54 /root/flink/flink-python/dev/.conda/envs/3.8/lib/libpython3.8.so.1.0(PyBytes_FromStringAndSize+0x76)[0x7ff78c1b9316]
Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyBytes_FromJByteArray+0x46)[0x7ff7f400c706]
Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyObject_FromJObject+0x3db)[0x7ff7f400d2ab]
Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyObject_SetJObject+0x2e)[0x7ff7f400ee6e]
Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(Java_pemja_core_PythonInterpreter_set__JLjava_lang_String_2Ljava_lang_Object_2+0x35)[0x7ff7f400a765]
Apr 02 12:24:54 [0x7ff7d8887630]
{code}
",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,FLINK-27075,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 06 08:39:03 UTC 2022,,,,,,,,,,"0|z1174g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/22 08:39;hxbks2ks;Merged into master via 495be7178481ec17082928f49610ff0559302733
Merged into release-1.15 via 0cfee81048b1d56e18b1c7aa80f78bb7be92c2bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"test_keyed_min_and_max and test_keyed_min_by_and_max_by failed in py36,37",FLINK-27068,13438044,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,javacaoyu,hxbks2ks,hxbks2ks,06/Apr/22 03:50,06/Apr/22 12:17,13/Jul/23 08:08,06/Apr/22 12:17,1.16.0,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-04-05T06:34:52.3441929Z Apr 05 06:34:52 =================================== FAILURES ===================================
2022-04-05T06:34:52.3442672Z Apr 05 06:34:52 _____________ StreamingModeDataStreamTests.test_keyed_min_and_max ______________
2022-04-05T06:34:52.3447439Z Apr 05 06:34:52 
2022-04-05T06:34:52.3448183Z Apr 05 06:34:52 self = <pyflink.datastream.tests.test_data_stream.StreamingModeDataStreamTests testMethod=test_keyed_min_and_max>
2022-04-05T06:34:52.3448809Z Apr 05 06:34:52 
2022-04-05T06:34:52.3449290Z Apr 05 06:34:52     def test_keyed_min_and_max(self):
2022-04-05T06:34:52.3472285Z Apr 05 06:34:52         ds = self.env.from_collection([('a', 3, 0), ('a', 1, 1), ('b', 5, 1), ('b', 3, 1)],
2022-04-05T06:34:52.3473027Z Apr 05 06:34:52                                       type_info=Types.ROW_NAMED(
2022-04-05T06:34:52.3473569Z Apr 05 06:34:52                                           [""v1"", ""v2"", ""v3""],
2022-04-05T06:34:52.3474130Z Apr 05 06:34:52                                           [Types.STRING(), Types.INT(), Types.INT()])
2022-04-05T06:34:52.3474677Z Apr 05 06:34:52                                       )
2022-04-05T06:34:52.3475478Z Apr 05 06:34:52         # 1th operator min: ('a', 3, 0), ('a', 1, 0), ('b', 5, 1), ('b', 3, 1)
2022-04-05T06:34:52.3476325Z Apr 05 06:34:52         # 2th operator max: ('a', 3, 0), ('a', 3, 0), ('b', 5, 1), ('b', 5, 1)
2022-04-05T06:34:52.3477137Z Apr 05 06:34:52         # 3th operator max: ('a', 1), ('a', 1), ('b', 1), ('b', 1)
2022-04-05T06:34:52.3477893Z Apr 05 06:34:52         # 4th operator min: ('a', 'a', 'b', 'b')
2022-04-05T06:34:52.3478429Z Apr 05 06:34:52         ds.key_by(lambda x: x[0]) \
2022-04-05T06:34:52.3478915Z Apr 05 06:34:52             .min(""v2"") \
2022-04-05T06:34:52.3479403Z Apr 05 06:34:52             .map(lambda x: (x[0], x[1], x[2]),
2022-04-05T06:34:52.3480001Z Apr 05 06:34:52                  output_type=Types.TUPLE([Types.STRING(), Types.INT(), Types.INT()])) \
2022-04-05T06:34:52.3481663Z Apr 05 06:34:52             .key_by(lambda x: x[2]) \
2022-04-05T06:34:52.3482134Z Apr 05 06:34:52             .max(1) \
2022-04-05T06:34:52.3482587Z Apr 05 06:34:52             .map(lambda x: (x[0], 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \
2022-04-05T06:34:52.3483270Z Apr 05 06:34:52             .key_by(lambda x: x[1]) \
2022-04-05T06:34:52.3483606Z Apr 05 06:34:52             .max() \
2022-04-05T06:34:52.3484001Z Apr 05 06:34:52             .map(lambda x: x[0], output_type=Types.STRING()) \
2022-04-05T06:34:52.3484425Z Apr 05 06:34:52             .key_by(lambda x: x) \
2022-04-05T06:34:52.3484770Z Apr 05 06:34:52             .min() \
2022-04-05T06:34:52.3485123Z Apr 05 06:34:52             .add_sink(self.test_sink)
2022-04-05T06:34:52.3485446Z Apr 05 06:34:52     
2022-04-05T06:34:52.3485822Z Apr 05 06:34:52         self.env.execute(""key_by_min_max_test_stream"")
2022-04-05T06:34:52.3486275Z Apr 05 06:34:52         results = self.test_sink.get_results(False)
2022-04-05T06:34:52.3486953Z Apr 05 06:34:52         expected = ['a', 'a', 'b', 'b']
2022-04-05T06:34:52.3487388Z Apr 05 06:34:52 >       self.assert_equals_sorted(expected, results)
2022-04-05T06:34:52.3487878Z Apr 05 06:34:52 
2022-04-05T06:34:52.3488255Z Apr 05 06:34:52 pyflink/datastream/tests/test_data_stream.py:1123: 
2022-04-05T06:34:52.3488735Z Apr 05 06:34:52 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-04-05T06:34:52.3489250Z Apr 05 06:34:52 pyflink/datastream/tests/test_data_stream.py:56: in assert_equals_sorted
2022-04-05T06:34:52.3489736Z Apr 05 06:34:52     self.assertEqual(expected, actual)
2022-04-05T06:34:52.3490406Z Apr 05 06:34:52 E   AssertionError: Lists differ: ['a', 'a', 'b', 'b'] != ['b', 'b', 'b', 'b']
2022-04-05T06:34:52.3490899Z Apr 05 06:34:52 E   
2022-04-05T06:34:52.3491242Z Apr 05 06:34:52 E   First differing element 0:
2022-04-05T06:34:52.3491718Z Apr 05 06:34:52 E   'a'
2022-04-05T06:34:52.3492148Z Apr 05 06:34:52 E   'b'
2022-04-05T06:34:52.3492450Z Apr 05 06:34:52 E   
2022-04-05T06:34:52.3492900Z Apr 05 06:34:52 E   - ['a', 'a', 'b', 'b']
2022-04-05T06:34:52.3493646Z Apr 05 06:34:52 E   + ['b', 'b', 'b', 'b']
2022-04-05T06:34:52.3494138Z Apr 05 06:34:52 __________ StreamingModeDataStreamTests.test_keyed_min_by_and_max_by ___________
2022-04-05T06:34:52.3494576Z Apr 05 06:34:52 
2022-04-05T06:34:52.3495077Z Apr 05 06:34:52 self = <pyflink.datastream.tests.test_data_stream.StreamingModeDataStreamTests testMethod=test_keyed_min_by_and_max_by>
2022-04-05T06:34:52.3495573Z Apr 05 06:34:52 
2022-04-05T06:34:52.3495919Z Apr 05 06:34:52     def test_keyed_min_by_and_max_by(self):
2022-04-05T06:34:52.3496607Z Apr 05 06:34:52         ds = self.env.from_collection([('a', 3, 0), ('a', 1, 1), ('b', 5, 0), ('b', 3, 1)],
2022-04-05T06:34:52.3497104Z Apr 05 06:34:52                                       type_info=Types.ROW_NAMED(
2022-04-05T06:34:52.3497500Z Apr 05 06:34:52                                           [""v1"", ""v2"", ""v3""],
2022-04-05T06:34:52.3497928Z Apr 05 06:34:52                                           [Types.STRING(), Types.INT(), Types.INT()])
2022-04-05T06:34:52.3498330Z Apr 05 06:34:52                                       )
2022-04-05T06:34:52.3498937Z Apr 05 06:34:52         # 1th operator min_by: ('a', 3, 0), ('a', 1, 1), ('b', 5, 0), ('b', 3, 1)
2022-04-05T06:34:52.3499641Z Apr 05 06:34:52         # 2th operator max_by: ('a', 3, 0), ('a', 3, 0), ('b', 5, 0), ('b', 5, 0)
2022-04-05T06:34:52.3500343Z Apr 05 06:34:52         # 3th operator min_by: ('a', 3, 0), ('a', 3, 0), ('a', 3, 0), ('a', 3, 0)
2022-04-05T06:34:52.3500981Z Apr 05 06:34:52         # 4th operator max_by: ('a', 'a', 'a', 'a')
2022-04-05T06:34:52.3501400Z Apr 05 06:34:52         ds.key_by(lambda x: x[0]) \
2022-04-05T06:34:52.3501765Z Apr 05 06:34:52             .min_by(""v2"") \
2022-04-05T06:34:52.3502126Z Apr 05 06:34:52             .map(lambda x: (x[0], x[1], x[2]),
2022-04-05T06:34:52.3502598Z Apr 05 06:34:52                  output_type=Types.TUPLE([Types.STRING(), Types.INT(), Types.INT()])) \
2022-04-05T06:34:52.3503064Z Apr 05 06:34:52             .key_by(lambda x: x[2]) \
2022-04-05T06:34:52.3503421Z Apr 05 06:34:52             .max_by(1) \
2022-04-05T06:34:52.3503782Z Apr 05 06:34:52             .key_by(lambda x: x[2]) \
2022-04-05T06:34:52.3504234Z Apr 05 06:34:52             .min_by() \
2022-04-05T06:34:52.3504633Z Apr 05 06:34:52             .map(lambda x: x[0], output_type=Types.STRING()) \
2022-04-05T06:34:52.3505055Z Apr 05 06:34:52             .key_by(lambda x: x) \
2022-04-05T06:34:52.3505406Z Apr 05 06:34:52             .max_by() \
2022-04-05T06:34:52.3505766Z Apr 05 06:34:52             .add_sink(self.test_sink)
2022-04-05T06:34:52.3506103Z Apr 05 06:34:52     
2022-04-05T06:34:52.3506477Z Apr 05 06:34:52         self.env.execute(""key_by_min_by_max_by_test_stream"")
2022-04-05T06:34:52.3506946Z Apr 05 06:34:52         results = self.test_sink.get_results(False)
2022-04-05T06:34:52.3507533Z Apr 05 06:34:52         expected = ['a', 'a', 'a', 'a']
2022-04-05T06:34:52.3507964Z Apr 05 06:34:52 >       self.assert_equals_sorted(expected, results)
2022-04-05T06:34:52.3508332Z Apr 05 06:34:52 
2022-04-05T06:34:52.3508702Z Apr 05 06:34:52 pyflink/datastream/tests/test_data_stream.py:1151: 
2022-04-05T06:34:52.3509277Z Apr 05 06:34:52 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-04-05T06:34:52.3509791Z Apr 05 06:34:52 pyflink/datastream/tests/test_data_stream.py:56: in assert_equals_sorted
2022-04-05T06:34:52.3510278Z Apr 05 06:34:52     self.assertEqual(expected, actual)
2022-04-05T06:34:52.3510943Z Apr 05 06:34:52 E   AssertionError: Lists differ: ['a', 'a', 'a', 'a'] != ['b', 'b', 'b', 'b']
2022-04-05T06:34:52.3511358Z Apr 05 06:34:52 E   
2022-04-05T06:34:52.3511685Z Apr 05 06:34:52 E   First differing element 0:
2022-04-05T06:34:52.3512150Z Apr 05 06:34:52 E   'a'
2022-04-05T06:34:52.3512578Z Apr 05 06:34:52 E   'b'
2022-04-05T06:34:52.3512880Z Apr 05 06:34:52 E   
2022-04-05T06:34:52.3513342Z Apr 05 06:34:52 E   - ['a', 'a', 'a', 'a']
2022-04-05T06:34:52.3513670Z Apr 05 06:34:52 E   ?   ^    ^    ^    ^
2022-04-05T06:34:52.3513970Z Apr 05 06:34:52 E   
2022-04-05T06:34:52.3514428Z Apr 05 06:34:52 E   + ['b', 'b', 'b', 'b']
2022-04-05T06:34:52.3514777Z Apr 05 06:34:52 E   ?   ^    ^    ^    ^
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=26123",,dianfu,gaoyunhaii,hxbks2ks,javacaoyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 06 12:17:41 UTC 2022,,,,,,,,,,"0|z11728:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/22 04:11;javacaoyu;It may be because the order in which the data is passed to the downstream operator is not fixed

 

eg, the 2th operation return result: ('a', 3, 0), ('a', 3, 0), ('b', 5, 1), ('b', 5, 1)

May the order in which the data stream is passed to the downstream operators is not passed in the order \{a a b b }

It may be passed to the downstream operator in the order of \{b b a a}

If so, the result of the 3th operator is no \{('a', 1), ('a', 1), ('b', 1), ('b', 1)} but \{('b', 1), ('b', 1), ('b', 1), ('b', 1)}

So the end result is \{b b b b }instead of \{a a b b}

 

Test cases need to be redesigned and are not affected by the data order.

 ;;;","06/Apr/22 04:13;javacaoyu;This problem may arise with a very small probability. But in any case, it is possible

 

Would you assign the task to me, I tried to fix it.

 

Thanks.

 ;;;","06/Apr/22 06:07;dianfu;[~javacaoyu] Have assigned it to you~. Besides, I think we should set the parallelism to 1 to avoid the deterministic introduced of order.;;;","06/Apr/22 06:39;javacaoyu;Thanks [~dianfu] 

Sure, i agree, set parallelism to 1 will fix this bug easy.

 

I have summit a pr, would you take a look.

 ;;;","06/Apr/22 06:57;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24779
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24813
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=25499
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=26088
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=25195;;;","06/Apr/22 12:17;dianfu;Merged to master via 11510c726c4688ec4ceb97f3ade507b810ca8e12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestingDispatcher.Builder instantiates a RPCSystem without shutting it down,FLINK-27050,13437619,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,04/Apr/22 12:45,14/Apr/22 07:05,13/Jul/23 08:08,14/Apr/22 07:05,1.15.0,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,{{TestingDispatcher.Builder}} provides a default RpcSystem that isn't shutdown causing leaking of threads.,,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25953,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 07:05:09 UTC 2022,,,,,,,,,,"0|z114hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/22 12:47;mapohl;This was introduced by {{9cac58227d2c28e7fb5262cc76b3f5b15515a73c}} on {{master}} when refactoring the {{TestingDispatcher}} initialization.;;;","14/Apr/22 07:05;mapohl;master: ccf4d24ee71264eb32d1bf2fcc61c2a8aef7b617
1.15: bafaeec66bd770e32227c179fe6dd1c76241f610;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSource in batch mode failing if any topic partition is empty,FLINK-27041,13437528,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,terxor,terxor,03/Apr/22 19:59,17/Jun/22 12:58,13/Jul/23 08:08,17/Jun/22 02:48,1.14.4,,,,,,1.14.6,1.15.1,,,Connectors / Kafka,,,,,0,pull-request-available,,,"First let's take the case of consuming from a Kafka topic with a single partition having 0 messages. Execution in batch mode, with bounded offsets set to latest, is expected to finish gracefully. However, it fails with an exception.

Consider this minimal working example (assume that test_topic exists with 1 partition and 0 messages):

{code:java}
		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		env.setRuntimeMode(RuntimeExecutionMode.BATCH);

		KafkaSource<String> kafkaSource = KafkaSource
				.<String>builder()
				.setBootstrapServers(""localhost:9092"")
				.setTopics(""test_topic"")
				.setValueOnlyDeserializer(new SimpleStringSchema())
				.setBounded(OffsetsInitializer.latest())
				.build();

		DataStream<String> stream = env.fromSource(
				kafkaSource,
				WatermarkStrategy.noWatermarks(),
				""Kafka Source""
		);
		stream.print();

		env.execute(""Flink KafkaSource test job"");
{code}

This produces exception:

{code}
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
  ... [omitted for readability]
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
  ... [omitted for readability]
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:351)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: java.lang.IllegalStateException: Consumer is not subscribed to any topics or assigned any partitions
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1223)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:97)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
	... 6 more
{code}

In our actual use case, we have a Kafka topic with many partitions. Some of them have no messages. This causes our batch job to fail.
","Kafka cluster version: 3.1.0

Flink version 1.14.4",Echo Lee,leonard,martijnvisser,mason6345,renqs,terxor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,,Tue Jun 14 07:35:25 UTC 2022,,,,,,,,,,"0|z113xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/22 11:52;martijnvisser;[~renqs] Can you have a look?;;;","07/Apr/22 08:30;renqs;Thanks for the ticket [~terxor] !

I checked the code and this is indeed a bug. If an empty or invalid partition (starting offset > stopping offset) is assigned to the reader, it won't be added to the Kafka consumer, so it's possible that no partition is added to consumer if the consuming topic is empty.

[~martijnvisser] Could you assign the ticket to me? Thanks;;;","07/Apr/22 09:14;martijnvisser;[~renqs] Done!;;;","14/Jun/22 07:35;leonard;Fixed in
master(1.16):  2b89dfe0d4c5c71a958fb1f07c5e6861e4bbe6a6
release-1.15:   5439b425e97c7889dca6fb327022a0a95be9e4a7
release-1.14:   a4f83353fe623b3f09363e964d6a9aeb7af0bf11
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNITCase failed due to OOM,FLINK-27033,13437496,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,mapohl,mapohl,03/Apr/22 09:23,26/Aug/22 12:15,13/Jul/23 08:08,26/Aug/22 12:15,1.16.0,,,,,,,,,,Deployment / YARN,,,,,0,auto-deprioritized-major,test-stability,,"We experienced a 137 exit code in [this build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34124&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=33678] while executing {{YARNITCase}}:
{code}
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1004  -w /home/agent05_azpcontainer bb00bf8c80330d042d18da617194edc1ff1a8bf5f73851d8786eb6675d13b5f2 /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
{code}",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 12 22:38:01 UTC 2022,,,,,,,,,,"0|z113qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Jul/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRescalingITCase.test failed due to IllegalStateException,FLINK-27031,13437493,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,mapohl,mapohl,03/Apr/22 09:02,12/May/23 11:14,13/Jul/23 08:08,30/Jun/22 12:09,1.15.0,1.16.0,,,,,1.15.2,1.16.0,,,Runtime / Network,Runtime / State Backends,,,,0,pull-request-available,,,"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=923&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=12961] failed in {{ChangelogRescalingITCase.test}}:
{code}
Apr 01 20:26:53 Caused by: java.lang.IllegalArgumentException: Key group 94 is not in KeyGroupRange{startKeyGroup=96, endKeyGroup=127}. Unless you're directly using low level state access APIs, this is most likely caused by non-deterministic shuffle key (hashCode and equals implementation).
Apr 01 20:26:53 	at org.apache.flink.runtime.state.KeyGroupRangeOffsets.newIllegalKeyGroupException(KeyGroupRangeOffsets.java:37)
Apr 01 20:26:53 	at org.apache.flink.runtime.state.heap.StateTable.getMapForKeyGroup(StateTable.java:305)
Apr 01 20:26:53 	at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:261)
Apr 01 20:26:53 	at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:143)
Apr 01 20:26:53 	at org.apache.flink.runtime.state.heap.HeapListState.add(HeapListState.java:94)
Apr 01 20:26:53 	at org.apache.flink.state.changelog.ChangelogListState.add(ChangelogListState.java:78)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:404)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
Apr 01 20:26:53 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
Apr 01 20:26:53 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
Apr 01 20:26:53 	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:531)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:227)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:841)
Apr 01 20:26:53 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:767)
Apr 01 20:26:53 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
Apr 01 20:26:53 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
Apr 01 20:26:53 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
Apr 01 20:26:53 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
Apr 01 20:26:53 	at java.lang.Thread.run(Thread.java:748)
{code}",,akalashnikov,gaoyunhaii,mapohl,pnowojski,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31963,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 30 12:09:24 UTC 2022,,,,,,,,,,"0|z113q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/22 09:04;mapohl;[~roman]  may you have a look;;;","04/Apr/22 07:36;roman;Sure [~mapohl], I'll take a look.;;;","04/Apr/22 14:42;roman;The test fails reliably when rescaling from 1 to 3 or higher DoP, ~6 runs out of 10.  ACCUMULATE_TIME_MILLIS doesn't affect it and can be zero.

Furthermore, it only fails when both Changelog and Unaligned checkpoints are enabled.

While debugging, I can see that:
- the problematic record comes from ResultSubpartition state
- according to its key, it should be assigned to subtask 1 or 2
- it is produced by upstream 0 and  consumed by downstream 0 (the downstream should filter it out)
- however, the downstream 0 doesn't setup Virtual Channels that could filter the record; when it does, the test passes

I didn't look further why Virtual Channels aren't being setup.

From the above, the failure seems to be caused by Unaligned checkpoints rather than changelog.
[~pnowojski] could you please take a look?;;;","08/Apr/22 16:30;roman;[https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1548&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=41041];;;","11/Apr/22 07:33;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34494&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5380;;;","19/May/22 11:24;akalashnikov;As I can see we setup Virtual Channels inside `StreamTaskNetworkInputFactory#create` when `InflightDataRescalingDescriptor` is not equal to 'NO_RESCALE'. It happens only if the subtask receives the 'JobManagerTaskRestore'(see 'TaskStateManagerImpl#getInputRescalingDescriptor').  So if the subtask doesn't receive  'JobManagerTaskRestore' but it receives the old state which should be filtered then we have an error described in this ticket. 
As I understand, It is only possible when the subtask doesn't have any states for restoring, but this task's upstream has output channel states. According to current logic(see last 'for' in 'StateAssignmentOperation#assignStates'), we assign states only based on states of current subtask and ignore its upstream states which actually leads to the described problem. 
So I think we can expand the condition for assigning the states by including upstream output channel states check.(which I have done in https://github.com/apache/flink/pull/19702).  Or we can think of another fix that should somehow notify the subtask about creating the Virtual Channels. ;;;","30/Jun/22 12:09;roman;Merged as d98a34be120c1906dc3f01e9c48ab847a75b5822 (master),
012a3602a9188b9d4b147ec8779aab5f1349a2dd (1.15);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeploymentValidator should take default flink config into account during validation,FLINK-27029,13437483,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,gyfora,gyfora,03/Apr/22 06:13,20/Apr/22 08:12,13/Jul/23 08:08,20/Apr/22 08:12,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,starter,,"Currently the DefaultDeploymentValidator only takes the FlinkDeployment object into account.

However in places where we validate the presence of config keys we should also consider the default flink config which might already provide default values for the required configs even if the deployment itself doesnt.

We should make sure this works correctly both in the operator and the webhook",,gyfora,nicholasjiang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 08:12:14 UTC 2022,,,,,,,,,,"0|z113o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/22 17:11;nicholasjiang;[~gyfora], do you mean that in DefaultValidator#validateFlinkConfig, we should validate the default Flink config?;;;","05/Apr/22 18:24;gyfora;What I mean is that we should merge the default config with the userProvided config before doing validation.

Example:

Default config:

high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
high-availability.storageDir: [file:///flink-data/ha]

User config:

// config without ha settings

If the user enables last-state currently validation fails as no HA settings have been provided. This logic is broken because HA configs would actually come already from defaults. So we should not validate the user flinkConf directly, but first copy the default + apply user conf on top of it, then validate;;;","06/Apr/22 03:27;wangyang0918;I think what you mean is we need to verify the effective flink configs.;;;","06/Apr/22 05:29;gyfora;Yes;;;","20/Apr/22 08:12;gyfora;merged to main: 0c0ae05a4a2b73618e270f72a4a52a98fa4850c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove log-related configuration from predefined options in RocksDB state backend ,FLINK-26998,13437362,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zakelly,Zakelly,Zakelly,02/Apr/22 03:16,03/Apr/22 12:01,13/Jul/23 08:08,03/Apr/22 12:01,1.15.0,,,,,,1.15.0,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,"After FLINK-23791, the RocksDB log should be enabled by default. However, the log-related configuration remains in _PredefinedOptions_, which will override the default value provided in _RocksDBConfigurableOptions_.
We could remove the values in _PredefinedOptions_ and let _RocksDBConfigurableOptions_ take control of the default value.",,yunta,Zakelly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23791,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 03 12:01:42 UTC 2022,,,,,,,,,,"0|z112xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/22 12:01;yunta;merged in master: 428bb6584ff867e5b121185fac604f71ad0afbdd
release-1.15: 8f502b808eea89cc53f6aed917c12f42e0661280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Break the reconcile after first create session cluster,FLINK-26996,13437355,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aitozi,aitozi,02/Apr/22 01:34,06/Apr/22 17:39,13/Jul/23 08:08,06/Apr/22 17:39,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"When I test session cluster, I found that it will always start twice for the session cluster. ",,aitozi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-04-02 01:34:55.0,,,,,,,,,,"0|z112w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit operator docs build to main repository,FLINK-26991,13437244,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mbalassi,mbalassi,mbalassi,01/Apr/22 12:36,01/Apr/22 20:02,13/Jul/23 08:08,01/Apr/22 20:02,kubernetes-operator-1.0.0,,,,,,kubernetes-operator-1.0.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,"Currently the nightly scheduled docs run tries running for all forks and it fails on repos other than the main one, we need to add a filter for this.",,martijnvisser,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 01 20:02:42 UTC 2022,,,,,,,,,,"0|z1127c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 20:02;mbalassi;Fixed via [{{df5229e}}|https://github.com/apache/flink-kubernetes-operator/commit/df5229eecd42203e947d24a73bd33bf914e5c5fe] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ZooKeeperStateHandleStore.getAllAndLock ends up in a infinite loop if there's an entry marked for deletion that's not cleaned up, yet",FLINK-26987,13437227,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,01/Apr/22 11:26,04/Apr/22 05:38,13/Jul/23 08:08,04/Apr/22 05:38,1.15.0,1.16.0,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"{{ZooKeeperStateHandleStore.getAllAndLock}} is used when recovering {{CompletedCheckpoints}}. It iterates over all childs and retries until it reaches a stable and consistent version (i.e. no entries are subject for deletion and no child nodes were added while accessing the ZK instance).

Additionally, {{ZooKeeperStateHandleStore}} marks entries for deletion internally before actually deleting them. This can lead to a state where an entry is marked for deletion but the discard failed causing the cleanup to fail. The entry will be left marked for deletion and another cleanup will be tried. This works infinitely. But the users has the ability to limit the amount of retries. In that case, the entry might be left marked.

Restarting Flink cluster will now try to access this ZooKeeperStateHandleStore recovering the checkpoints with this entry still being marked for deletion which will cause an error in [ZooKeeperStateHandleStore.getAllAndLock|https://github.com/apache/flink/blob/c3df4c3f1f868d40e1e70404bea41b7a007e8b08/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L413] which results in a retry loop that's not desired.

We actually don't need to retry in that case because the child can be ignored, as far as I can see.",,dmvk,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26284,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 04 05:38:51 UTC 2022,,,,,,,,,,"0|z1123k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 12:59;dmvk;The behavior has changed due too introduction of the nested structure for storing ""deletion holds"" / ""locks"";;;","04/Apr/22 05:38;mapohl;master: cda343349f5d2b080218b7fe1993794a5a16c272
1.15: aa3bb951db745f94070f2ef6ecb62ce207bda520;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated string expressions in Python Table API,FLINK-26986,13437226,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,01/Apr/22 11:22,02/Apr/22 02:02,13/Jul/23 08:08,01/Apr/22 15:09,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,"In FLINK-26704, it has removed the string expressions in Table API. However, there are still some APIs still using string expressions in Python Table API, however, they should not work any more as the string expressions have already been removed in the Java Table API. ",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 02 02:02:57 UTC 2022,,,,,,,,,,"0|z1123c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 15:09;dianfu;Fixed in:
- master via 98ad8a7271b4ebc689f35b5a2da75cff3c2872cb
- release-1.15 via 02c5e4136c809eac7b5d723be0d043b639ddf477;;;","02/Apr/22 02:02;dianfu;Just found that FLINK-26704 is resolved in 1.16 and so revert the changes in release-1.15 via 1bfbdc9c76de3b3fab4d70058257b7a307269570;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"With legacy restore mode, incremental checkpoints would be deleted by mistake",FLINK-26985,13437214,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,yunta,yunta,01/Apr/22 10:19,09/Apr/22 08:02,13/Jul/23 08:08,09/Apr/22 08:02,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"Before flink-1.15, the restored checkpoint would be regsiterd and not discard on subsume, which means the restored incremental checkpoint would have one more reference counting to avoid discard.

However, after state registry refactored, we could delete artificats in the restored incremental checkpoint with legacy restore mode.

The error could be reproduced via {{ResumeCheckpointManuallyITCase#testExternalizedIncrementalRocksDBCheckpointsStandalone}} in my [local branch|https://github.com/Myasuka/flink/tree/legacy-error].


Thanks for [~masteryhx] who found this problem in the manual test.",,dmvk,liyu,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27132,,,FLINK-27114,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 09 08:02:13 UTC 2022,,,,,,,,,,"0|z1120o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 10:21;yunta;I think this error is caused by the refactoring introduced in FLINK-24611, [~roman] would you please take a look?;;;","09/Apr/22 08:02;roman;Merged into master as c0936deaf99390fc727acc8633e3be22e62f4bf5,
into release-1.15 as 5afff68c89c5e51a55f135109089c22d073b507f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate failed on azure,FLINK-26977,13437148,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,gaoyunhaii,gaoyunhaii,01/Apr/22 07:12,14/Jul/22 11:08,13/Jul/23 08:08,13/Apr/22 08:01,1.15.0,1.16.0,,,,,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-03-31T06:11:52.2333685Z Mar 31 06:11:52 [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 35.288 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase
2022-03-31T06:11:52.2336004Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate  Time elapsed: 15.008 s  <<< FAILURE!
2022-03-31T06:11:52.2336907Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2337353Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2337876Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2338631Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2339436Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2340599Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate(JobMasterStopWithSavepointITCase.java:136)
2022-03-31T06:11:52.2342251Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2342896Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2343608Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2344234Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2344873Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2345590Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2346498Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2347221Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2347922Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2348580Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2349222Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2349860Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2350502Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2351172Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2352095Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2352949Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2353643Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2354298Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2354909Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2355535Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2356505Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2357142Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2357771Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2358400Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2359014Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2359614Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2360221Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2371694Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2372907Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2373992Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2375195Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2376592Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2377778Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2379338Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2380786Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2382151Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2383487Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2384979Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2386341Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2387454Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2389081Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2390447Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2391930Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2393389Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2394759Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2395544Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2396673Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2397347Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2397932Z Mar 31 06:11:52 
2022-03-31T06:11:52.2398639Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend  Time elapsed: 15.004 s  <<< FAILURE!
2022-03-31T06:11:52.2399342Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2399793Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2400311Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2400837Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2401633Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2402751Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend(JobMasterStopWithSavepointITCase.java:130)
2022-03-31T06:11:52.2403623Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2404247Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2404961Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2405936Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2406676Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2407520Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2408242Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2409245Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2409940Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2410604Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2411358Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2412174Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2412786Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2413640Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2414856Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2416140Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2417502Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2418495Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2419110Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2419737Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2420361Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2420986Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2421601Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422359Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422969Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2423569Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2424331Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2424922Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2425464Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2426334Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2427379Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2428432Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2429538Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2430713Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2431900Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2433166Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2434372Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2435500Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2436771Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2437877Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2439206Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2440452Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2441694Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2442881Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2443999Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2445104Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2446367Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2447434Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2448170Z Mar 31 06:11:52 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5183",,dwysakowicz,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27918,,,FLINK-27083,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 13 08:01:03 UTC 2022,,,,,,,,,,"0|z111m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 07:20;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33999&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","06/Apr/22 06:48;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34243&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5242;;;","06/Apr/22 07:05;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34241&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5238;;;","11/Apr/22 07:29;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34485&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5183;;;","11/Apr/22 07:41;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34494&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5185;;;","11/Apr/22 08:07;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34493&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5108;;;","11/Apr/22 08:20;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5167;;;","12/Apr/22 08:18;gaoyunhaii;The problem seems to related to adaptive scheduler, perhaps cc [~dmvk]~;;;","12/Apr/22 11:11;dwysakowicz;I'll take over as I missed this test when working on FLINK-26783 and FLINK-26923.;;;","13/Apr/22 08:01;dwysakowicz;Fixed in:
* master
** 3b7a64e3cddcdbe7aa61b7bd8804ad5635ef96f7..31579621e16c6cdad3d965a91bb048282bf7942b
* 1.15.0
** 55fa1cf56dd8633aa0a04fc19e2c86d9ae2c2617..1fe11253572a1b1d1224de638e313666da346f62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystemJobResultStore calls flush on an already closed OutputStream ,FLINK-26957,13436863,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,31/Mar/22 10:30,13/Apr/22 12:51,13/Jul/23 08:08,04/Apr/22 12:33,1.15.0,1.16.0,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"We experienced problems with some FileSystems when creating the dirty JRS entries (see initial discussion in FLINK-26555). The {{writeValue}} method closes the {{OutputStream}} by default which causes the subsequent {{flush}} call to fail.

It didn't appear in the unit tests because {{LocalDataOutputStream.flush}} is a no-op operation. We still have to investigate why it didn't appear when doing the tests with the presto and hadoop S3 filesystems.",,mapohl,tanyuxin,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26555,,FLINK-27133,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 13 12:51:12 UTC 2022,,,,,,,,,,"0|z1108g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/22 12:33;mapohl;* master:
 ** c50b0706237114adec195b84202b969a148ccece
 ** caa296b813b8a719910f4e1337e011c772a12868
 * 1.15:
 ** fafeb7f9534c684b76db14b5cbd26c44251c8647
 ** cb0da8f2817bb51a01d168b70fdac99e7f34d94f;;;","13/Apr/22 12:51;mapohl;[~tanyuxin] did you do some testing with the fix?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSource recovery from savepoint fails When flink parallelism is greater than the number of Kafka partitions,FLINK-26938,13436802,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,wenbao,wenbao,31/Mar/22 05:34,15/Aug/22 03:25,13/Jul/23 08:08,15/Aug/22 03:25,1.14.0,,,,,,1.16.0,,,,API / DataStream,,,,,0,,,,"HybridSource recovery from savepoint fails When flink parallelism is greater than the number of Kafka partitions

First test

Flink job before savePoint
    flink parallelism =16
    kafka partition=3
Flink after savePoint
case 1：
    flink parallelism =16
    kafka partition=3

HybridSource recovery from savepoint fails 
!image-2022-03-31-13-36-45-686.png!

case 2：
    flink parallelism =3
    kafka partition=3
HybridSource recovery from savepoint  successful

case 3：
    flink parallelism =8
    kafka partition=3
HybridSource recovery from savepoint fails  the same NullPointerException: Source for index=0 not available

case 4：
    flink parallelism =4
    kafka partition=3
HybridSource recovery from savepoint fails  the same NullPointerException: Source for index=0 not available

case 5：
    flink parallelism =1
    kafka partition=3
HybridSource recovery from savepoint  successful

Second test

Flink job before savePoint
    flink parallelism =3
    kafka partition=3
Flink after savePoint
case 1：
    flink parallelism =3
    kafka partition=3
HybridSource recovery from savepoint  successful

case 2：
    flink parallelism =1
    kafka partition=3
HybridSource recovery from savepoint  successful

case 3：
    flink parallelism =4
    kafka partition=3
HybridSource recovery from savepoint fails  the same NullPointerException: Source for index=0 not available

Specific code see the attached test code HybridSourceTest

 ",Flink 1.14.0,hackergin,martijnvisser,mason6345,nicholasjiang,straw,wenbao,yesorno,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/22 05:38;wenbao;HybridSourceTest.java;https://issues.apache.org/jira/secure/attachment/13041815/HybridSourceTest.java","31/Mar/22 05:36;wenbao;image-2022-03-31-13-36-45-686.png;https://issues.apache.org/jira/secure/attachment/13041814/image-2022-03-31-13-36-45-686.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 31 07:29:15 UTC 2022,,,,,,,,,,"0|z10zuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/22 05:58;nicholasjiang;[~MartijnVisser], could you please assign this ticket to me?  I would like to own the issues about the Hybrid Sources.;;;","31/Mar/22 06:59;straw;I have reproduce it locally.

I found that switchedSources in HybridSourceSplitEnumerator only add lastest source when it start().

But if flink parallelism is greater than the number of Kafka partitions, there will be a circumstance that currentSourceIndex of some source readers is 0, others are 1. So when the reader restore from savepoint(the real current index is 1 in HybridSourceSplitEnumerator), it will fetch source from switchedSources in HybridSourceSplitEnumerator. It will cause NPE due to the absence of source 0.;;;","31/Mar/22 07:10;martijnvisser;[~nicholasjiang] I've assigned it to you. Would this only happen with Kafka or also with a similar source like Pulsar? ;;;","31/Mar/22 07:17;straw;I think it will happen with other sources only if the num of readers is greater than the num of SourceSplit.;;;","31/Mar/22 07:29;nicholasjiang;[~martijnvisser], IMO, the problem could happen with other sources not only Kafka.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar sink's producer name should be unique,FLINK-26931,13436641,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,30/Mar/22 12:16,05/Jul/22 14:12,13/Jul/23 08:08,05/Jul/22 14:12,1.15.0,1.16.0,,,,,1.15.2,1.16.0,,,Connectors / Pulsar,,,,,0,pull-request-available,,,Pulsar's new sink interface didn't make the producer name unique. Which would make the pulsar fail to consume messages.,,martijnvisser,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 01 11:35:30 UTC 2022,,,,,,,,,,"0|z10yvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 18:40;arvid;Merged into master as https://github.com/apache/flink/commit/495685970e31085815ba0435322ab44e4504cd55.;;;","25/Apr/22 21:59;syhily;[~arvid] The backport PR for 1.15 is ready, can you merge it?;;;","01/Jul/22 11:35;martijnvisser;Fixed in release-1.15: 56f8c0ea1f05d4829188b5213fea2d7567e7ae94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testStopWithSavepointFailsOverToSavepoint  failed on azure,FLINK-26923,13436588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,gaoyunhaii,gaoyunhaii,30/Mar/22 07:40,09/Jun/22 10:37,13/Jul/23 08:08,30/Mar/22 19:33,1.15.0,1.16.0,,,,,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-03-29T05:50:18.9604940Z Mar 29 05:50:18 [ERROR] Tests run: 20, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 28.306 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
2022-03-29T05:50:18.9609713Z Mar 29 05:50:18 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointFailsOverToSavepoint  Time elapsed: 3.363 s  <<< ERROR!
2022-03-29T05:50:18.9611347Z Mar 29 05:50:18 org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.
2022-03-29T05:50:18.9613057Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onLeave(StopWithSavepoint.java:124)
2022-03-29T05:50:18.9614629Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1181)
2022-03-29T05:50:18.9616369Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToRestarting(AdaptiveScheduler.java:858)
2022-03-29T05:50:18.9618273Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.FailureResultUtil.restartOrFail(FailureResultUtil.java:28)
2022-03-29T05:50:18.9619815Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onFailure(StopWithSavepoint.java:149)
2022-03-29T05:50:18.9621464Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.updateTaskExecutionState(StateWithExecutionGraph.java:367)
2022-03-29T05:50:18.9623122Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$updateTaskExecutionState$4(AdaptiveScheduler.java:496)
2022-03-29T05:50:18.9624528Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.State.tryCall(State.java:137)
2022-03-29T05:50:18.9626318Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.updateTaskExecutionState(AdaptiveScheduler.java:493)
2022-03-29T05:50:18.9627831Z Mar 29 05:50:18 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-03-29T05:50:18.9629329Z Mar 29 05:50:18 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-03-29T05:50:18.9630643Z Mar 29 05:50:18 	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
2022-03-29T05:50:18.9632127Z Mar 29 05:50:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-29T05:50:18.9633394Z Mar 29 05:50:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-29T05:50:18.9634943Z Mar 29 05:50:18 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-03-29T05:50:18.9636737Z Mar 29 05:50:18 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-03-29T05:50:18.9638234Z Mar 29 05:50:18 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-03-29T05:50:18.9639920Z Mar 29 05:50:18 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-03-29T05:50:18.9641506Z Mar 29 05:50:18 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-03-29T05:50:18.9643007Z Mar 29 05:50:18 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-03-29T05:50:18.9644379Z Mar 29 05:50:18 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-03-29T05:50:18.9645829Z Mar 29 05:50:18 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-03-29T05:50:18.9647316Z Mar 29 05:50:18 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-03-29T05:50:18.9648648Z Mar 29 05:50:18 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-03-29T05:50:18.9650044Z Mar 29 05:50:18 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-03-29T05:50:18.9651437Z Mar 29 05:50:18 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-03-29T05:50:18.9652830Z Mar 29 05:50:18 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-03-29T05:50:18.9654205Z Mar 29 05:50:18 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-03-29T05:50:18.9655489Z Mar 29 05:50:18 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-03-29T05:50:18.9656976Z Mar 29 05:50:18 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-03-29T05:50:18.9658156Z Mar 29 05:50:18 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-03-29T05:50:18.9659677Z Mar 29 05:50:18 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-03-29T05:50:18.9660662Z Mar 29 05:50:18 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-03-29T05:50:18.9661444Z Mar 29 05:50:18 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-03-29T05:50:18.9662014Z Mar 29 05:50:18 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-03-29T05:50:18.9663168Z Mar 29 05:50:18 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-03-29T05:50:18.9668230Z Mar 29 05:50:18 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-03-29T05:50:18.9669554Z Mar 29 05:50:18 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-03-29T05:50:18.9670718Z Mar 29 05:50:18 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-03-29T05:50:18.9671952Z Mar 29 05:50:18 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-03-29T05:50:18.9673252Z Mar 29 05:50:18 Caused by: org.apache.flink.runtime.operators.testutils.ExpectedTestException: Expected Test Exception
2022-03-29T05:50:18.9674967Z Mar 29 05:50:18 	at org.apache.flink.test.checkpointing.SavepointITCase$FailingOnCompletedSavepointMapFunction.notifyCheckpointComplete(SavepointITCase.java:367)
2022-03-29T05:50:18.9677056Z Mar 29 05:50:18 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:126)
2022-03-29T05:50:18.9678734Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104)
2022-03-29T05:50:18.9680432Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145)
2022-03-29T05:50:18.9682031Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409)
2022-03-29T05:50:18.9683810Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343)
2022-03-29T05:50:18.9685416Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1421)
2022-03-29T05:50:18.9687234Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$17(StreamTask.java:1362)
2022-03-29T05:50:18.9689195Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$20(StreamTask.java:1401)
2022-03-29T05:50:18.9690889Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
2022-03-29T05:50:18.9692407Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
2022-03-29T05:50:18.9693862Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:362)
2022-03-29T05:50:18.9695483Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:348)
2022-03-29T05:50:18.9697230Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:225)
2022-03-29T05:50:18.9698683Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:871)
2022-03-29T05:50:18.9699947Z Mar 29 05:50:18 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:773)
2022-03-29T05:50:18.9701220Z Mar 29 05:50:18 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
2022-03-29T05:50:18.9702487Z Mar 29 05:50:18 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
2022-03-29T05:50:18.9703639Z Mar 29 05:50:18 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
2022-03-29T05:50:18.9704731Z Mar 29 05:50:18 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
2022-03-29T05:50:18.9705865Z Mar 29 05:50:18 	at java.lang.Thread.run(Thread.java:748)
2022-03-29T05:50:18.9706814Z Mar 29 05:50:18 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33863&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5661",,dwysakowicz,gaoyunhaii,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27972,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 30 19:33:15 UTC 2022,,,,,,,,,,"0|z10yjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/22 07:45;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33865&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5738;;;","30/Mar/22 08:08;dwysakowicz;Leaving it as a comment. It happens with adaptive scheduler;;;","30/Mar/22 11:36;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33932&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5666
(1.15);;;","30/Mar/22 19:33;dwysakowicz;Fixed in:
* master
** 37dcaa96249ec28c736b1602c2ad0c25a170512f
* 1.15.0
** 575a0083e73736df35b0986fb3487aa01b6f8351;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job executes failed with ""The configured managed memory fraction for Python worker process must be within (0, 1], was: %s.""",FLINK-26920,13436574,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,30/Mar/22 06:39,01/Apr/22 11:42,13/Jul/23 08:08,01/Apr/22 11:42,1.14.0,,,,,,1.14.5,1.15.0,,,API / Python,,,,,0,pull-request-available,,,"For the following code:
{code}
import numpy as np
from pyflink.common import Row
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext
from pyflink.table import StreamTableEnvironment
from sklearn import svm, datasets

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(stream_execution_environment=env)

# Table Source
t_env.execute_sql(""""""
    CREATE TABLE my_source (
        a FLOAT,
        key STRING
    ) WITH (
        'connector' = 'datagen',
        'rows-per-second' = '1',
        'fields.a.min' = '4.3',
        'fields.a.max' = '7.9',
        'fields.key.length' = '10'
    )
"""""")


def process_type():
    return Types.ROW_NAMED(
        [""a"", ""key""],
        [Types.FLOAT(), Types.STRING()]
    )


# append only datastream
ds = t_env.to_append_stream(
    t_env.from_path('my_source'),
    process_type())


class MyKeyedProcessFunction(KeyedProcessFunction):

    def open(self, runtime_context: RuntimeContext):
        clf = svm.SVC()
        X, y= datasets.load_iris(return_X_y=True)
        clf.fit(X, y)

        self.model = clf


    def process_element(self, value: Row, ctx: 'KeyedProcessFunction.Context'):

        # 根据role_id + space去redis查询回合结算日志

        features = np.array([value['a'], 3.5, 1.4, 0.2]).reshape(1, -1)
        predict = int(self.model.predict(features)[0])

        yield Row(predict=predict, role_id=value['key'])


        
ds = ds.key_by(lambda a: a['key'], key_type=Types.STRING()) \
    .process(
        MyKeyedProcessFunction(), 
        output_type=Types.ROW_NAMED(
            [""hit"", ""role_id""],
            [Types.INT(), Types.STRING()]
    ))


# 采用table sink
t_env.execute_sql(""""""
        CREATE TABLE my_sink (
          hit INT,
          role_id STRING
        ) WITH (
          'connector' = 'print'
        )
    """""")

t_env.create_temporary_view(""predict"", ds)
t_env.execute_sql(""""""
    INSERT INTO my_sink
    SELECT * FROM predict
"""""").wait()
{code}

It reported the following exception:
{code}
Caused by: java.lang.IllegalArgumentException: The configured managed memory fraction for Python worker process must be within (0, 1], was: %s. It may be because the consumer type ""Python"" was missing or set to 0 for the config option ""taskmanager.memory.managed.consumer-weights"".0.0
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:233)
	at org.apache.flink.streaming.api.operators.python.AbstractExternalPythonFunctionOperator.open(AbstractExternalPythonFunctionOperator.java:56)
	at org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator.open(AbstractOneInputPythonFunctionOperator.java:116)
	at org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.open(PythonKeyedProcessOperator.java:121)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:712)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:688)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:655)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:748)
{code}",,dianfu,syntomic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 01 11:42:30 UTC 2022,,,,,,,,,,"0|z10ygg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/22 11:42;dianfu;Fixed in:
- master via 39854d1b70c2234f4079f2e7c846eef81902aec4
- release-1.15 via 4a3d6e52fe820e53b7f3d9bf301b4a1c7d14ab41
- release-1.14 via 73d140e3544f65f759bf6c7ca3a7163702386bfc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The Operator ignores job related changes (jar, parallelism) during last-state upgrades",FLINK-26916,13436467,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,morhidi,,29/Mar/22 15:42,24/Nov/22 01:01,13/Jul/23 08:08,30/Mar/22 11:43,kubernetes-operator-0.1.0,kubernetes-operator-1.0.0,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,RC: The old jobgraph is being reused when resuming,,aitozi,dmvk,gyfora,thw,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26930,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 30 17:38:21 UTC 2022,,,,,,,,,,"0|z10xso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 15:53;gyfora;One way to solve the problem would be to change the current last-state shutdown and restore logic as follows:

SUSPEND
1. Delete deployment
2. Extract last checkpoint pointer from HA configmap -> store in lastSavepoint status

DEPLOY
3. Delete HA configmaps
4. Restore from pointer in lastSavepoint;;;","29/Mar/22 17:39;gyfora;After some discussion with [~thw] it seems there is an inherent limitation in the Kubernetes HA service that makes this nearly impossible to implement.

The Kuberntes HA service only stores a file pointer within the kubernetes HA storage dir. In order to get the actual checkpoint pointer one would actually need to read the CompletedCheckpoint object from the HA storage dir and get it from that.

This would require access to the HA storage from within the operator which is completely unfeasible. 

We suggest to change the Flink Kubernetes HA Service implementation to store the external checkpoint pointer also in the same configmap. This could be a minimal backward compatible change that we should aim to get in for 1.15 and if simple enough backport for the next 1.14 release.

Due to these inherent limitations we propose to add a big fat warning to the last-state upgrade mode and point out that job changes are not possible and accept this as a limitation for the preview release.

 [~wangyang0918]  : Yang you are familiar with the Kubernetes HA implementation, do you think we can reasonably make this change for 1.15? What is your gut feeling?;;;","30/Mar/22 07:12;gyfora;After discussing with [~wangyang0918] there seems to be a simple fix for the Kubernetes HA case by simply deleting the jobgraph data before deployment. This will cause flink to always regenerate the job. promoting this to blocker with this in mind;;;","30/Mar/22 08:01;wangyang0918;I have got myself assigned and will attach a PR.;;;","30/Mar/22 09:14;dmvk;Hi, I think this whole thing boils down to how the LAST_STATE upgrade is implemented. Few observations:
- The operator is not shutting down the cluster properly. It simply deletes the underlying k8s resources.`FlinkUtils#deleteCluster(java.lang.String, java.lang.String, io.fabric8.kubernetes.client.KubernetesClient, boolean)`
- The whole implementation seems more or less like a JM failover scenario. Basically from the Flink standpoint JM disappears for no obvious reason, which leaves job in a ""SUSPENDED"" state. This also implies that all the HA data must remain untouched so Flink can restore to the previous state.
- This code path is not designed for application upgrades. Upgrade should be always done by a new job submission.
- ClusterId shouldn't be reused. This guarantee is needed for example for implementing reliable cluster shutdown. (see JobId semantics section of https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=195726435)

Could this be addressed by using native savepoints instead?;;;","30/Mar/22 09:46;wangyang0918;For safety and compatibility, I agree with you that upgrade should be done by a new job submission, especially when the Flink version is changed. However, if the continuously increased checkpoint could be easily obtained by external tools(e.g. flink-kubernetes-operator), I also lean to not simply delete the Flink cluster and have HA data retained. I do not find a more appropriate way.

Note: We could not get the latest checkpoint from the RestAPI since JobManager might crash backoff when we want to upgrade a Flink application.

 

I am afraid I cannot agree with you about that clusterId shouldn't be reused. Users just need to do the manual clean-up for job result store if they want to reuse the same cluster-id.

 

I am not familiar with native savepoint and will have a look about this solution.;;;","30/Mar/22 10:00;dmvk;As long as the operator has access to user credentials (s3), can you simply get the latest checkpoint directly from the filesystem?

> I am afraid I cannot agree with you about that clusterId shouldn't be reused. Users just need to do the manual clean-up for job result store if they want to reuse the same cluster-id.

Having a clean entry in JRS implies that all cluster resources (including HA) have been cleaned up (this is not what happens here). Then yes, the id should be safe to reuse after cleaning up the entry.;;;","30/Mar/22 10:15;wangyang0918;> As long as the operator has access to user credentials (s3), can you simply get the latest checkpoint directly from the filesystem?

 

I have also considered the solution to scan the DFS checkpoint directory to get the latest checkpoint. It seems that we do not have a ready-made utility method for this. I guess it will be a little heavy and we need to force users to enable external checkpoint. Do I miss something?;;;","30/Mar/22 10:24;gyfora;[~dmvk] The operator does not have access to credentials or even libraries necessary to acces arbitrary storage systems. This would not work most prod environments. ;;;","30/Mar/22 10:25;gyfora;Also users can freely change their checkpoint stroage directory / Filesystem etc, so you would have to check a bunch of places even if the operator could check. But it cant :);;;","30/Mar/22 10:36;dmvk;This has been a known struggle for quite some time. In the past this has been worked around by custom HA services, which is very inconvenient because it brings an additional dependency to the Flink runtime. One approach that we were thinking of recently was extending the JRS so the stored job result contains list of retained checkpoints. This of course implies that cluster gets shut down / job gets terminated properly (other cases should be used for fail-over scenarios only).;;;","30/Mar/22 10:40;gyfora;I think as soon as there is a straightforward way of accessing the last checkpoint, we can easily modify the last-state upgrade implementation and make it nicer by properly shutting down clusters.

Unfortunately this is not possible at the moment hence the current mechanism. We also need to consider supporting older flink versions (1.14 and up) so the current mechanism might need to stick around for some time;;;","30/Mar/22 11:41;wangyang0918;Thanks for the fruitful discussion. I will close this ticket and create a new one to track the future potential improvement.;;;","30/Mar/22 11:43;wangyang0918;Fixed via:

main: 48aded1b2fba62b1e339f761627d338b1bea07c2

release-0.1: 08183effa8b6af67e05cf4271cf016a5c5c7ca91;;;","30/Mar/22 17:38;thw;I think this is a good intermediate solution that works for all scenarios, including that of a failing JM deployment. As discussed, there should also be a best effort to gracefully stop a deployment though. Is there a ticket to make the most recent checkpoint path available in the HA store?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-add FlinkDeploymentList and FlinkSessionJobList classes,FLINK-26905,13436321,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zeus1ammon,gyfora,gyfora,29/Mar/22 06:25,11/Apr/22 15:28,13/Jul/23 08:08,11/Apr/22 15:28,kubernetes-operator-1.0.0,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,Seems like removing these was a bad idea as it breaks the fabric8 java client when using the POJO classes,,gyfora,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 15:28:40 UTC 2022,,,,,,,,,,"0|z10wwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/22 15:28;mbalassi;Implemented via [bd835f8|https://github.com/apache/flink-kubernetes-operator/commit/bd835f8f70b73004a73321225e5298408ae3e0e9];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update load(...) of all Stage subclasses to use StreamTableEnvironment,FLINK-26904,13436317,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,lindong,lindong,29/Mar/22 06:13,24/Jun/22 09:09,13/Jul/23 08:08,30/Mar/22 10:12,,,,,,,ml-2.1.0,,,,,,,,,0,pull-request-available,,,"Currently every Stage subclass uses static `load(StreamExecutionEnvironment, String)` to load model data from the given path. Algorithm developers are expected to use StreamExecutionEnvironment.create(env) to instantiate a new StreamTableEnvironment and uses it to create Table instances for model data.

This approach is problematic. Use KMeansModel as example. Users will use KMeansModel::load(env, path) to instantiate the model and call model.transform(inputDataTable) to do inference, where modelDataTable (created from load(...)) and inputDataTable are created using different StreamTableEnvironment instances. 

Having multiple Table instances in the same job where instances are created from different StreamTableEnvironment instances are in general error prone, as they can not share information such as table catalog.

In order to fix this problem, we will need to consistently use StreamTableEnvironment for load(...) and similar public APIs in Flink ML.
",,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-03-29 06:13:59.0,,,,,,,,,,"0|z10wvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add correct NOTICE file to flink-kubernetes-operator,FLINK-26903,13436311,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,29/Mar/22 05:31,29/Mar/22 09:40,13/Jul/23 08:08,29/Mar/22 09:24,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 09:24:31 UTC 2022,,,,,,,,,,"0|z10wu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 06:04;wangyang0918;I will work on this and attach a PR soon.;;;","29/Mar/22 06:21;gyfora;Let me know if you need any help with checking a list of licenses or whatever :);;;","29/Mar/22 09:24;wangyang0918;Fixed via:

main:

fff31272c2cdf4024cd02df4002e02140fc64f39

627cd656b9fdf6e8fac3431b70a198a833c34db4

 

release-0.1:

600f487e76ab01bccd33de2ec56a67b222d83f99

3f43f3a8dec732f242380621c3ee43b9298ba4b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamoDB consumer error consuming partitions close to retention,FLINK-26890,13436195,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,elphastori,dannycranmer,dannycranmer,28/Mar/22 15:13,04/Nov/22 12:33,13/Jul/23 08:08,04/Nov/22 12:33,1.15.2,1.16.0,,,,,1.15.3,1.16.1,1.17.0,,Connectors / Kinesis,,,,,0,pull-request-available,,,"*Background*

The Amazon Kinesis Data Streams consumer supports consuming from Amazon DynamoDB via the [DynamoDB Streams Kinesis Adapter|https://github.com/awslabs/dynamodb-streams-kinesis-adapter]. 

*Problem*

We have seen instances of consumer throwing {{ResouceNotFoundException}} when attempting to invoke {{GetShardIterator}}.

{code}
com.amazonaws.services.kinesis.model.ResourceNotFoundException: Requested resource not found: Shard does not exist 
{code}

According to the DynamoDB team, the {{DescribeStream}} call may return shard IDs that are no longer valid, and this exception needs to be handled by the client. 

*Solution*

Modify the DynamoDB consumer to treat {{ResourceNotFoundException}} as a shard closed signal.",,dannycranmer,elphastori,mason6345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 03 15:51:54 UTC 2022,,,,,,,,,,"0|z10w5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 14:54;elphastori;I would like to take this.;;;","11/Oct/22 14:55;dannycranmer;Thanks [~elphastori] , I have assigned to you;;;","25/Oct/22 12:43;elphastori;[~dannycranmer] The PR is ready for review;;;","03/Nov/22 15:51;dannycranmer;Merged:
 - [{{68adbcc}}|https://github.com/apache/flink/commit/68adbcc8823a1f42e1911ddfd8ab5d4ea111043b] into master
 - [{{f410434}}|https://github.com/apache/flink/commit/f410434e83558191bed2e9f0874f0634ca56dfa1] into release-1.16
 - [{{17bc1a0}}|https://github.com/apache/flink/commit/17bc1a0cab397f49edcef7cbaba306a35daaa263] into release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unaligned checkpoint with 0s timeout would fail RescaleCheckpointManuallyITCase,FLINK-26882,13436114,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,yunta,yunta,28/Mar/22 08:52,31/Mar/22 07:40,13/Jul/23 08:08,30/Mar/22 09:34,1.16.0,,,,,,1.16.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,,,"Once we make {{execution.checkpointing.unaligned: true}} and {{execution.checkpointing.alignment-timeout: PT0S}}, the RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState would fail then.

Borken instances:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33776&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5623 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33787&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5626
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33787&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12409
",,akalashnikov,gaoyunhaii,guoyangze,pnowojski,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26789,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 31 07:40:03 UTC 2022,,,,,,,,,,"0|z10vnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 15:37;nsemmler;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33748&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","29/Mar/22 07:18;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33795&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5685
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33795&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12412

And some other tests in the same build;;;","29/Mar/22 07:27;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33810&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5685
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33810&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12414;;;","29/Mar/22 07:34;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33854&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5762;;;","29/Mar/22 17:24;akalashnikov;Some conclusions:

First of all, this test didn't start to fail, this test has never worked at all(it didn't work before FLINK-26789 and after FLINK-26789 as well). So since it is not a degradation we can easily revert commits or ignore tests([https://github.com/apache/flink/pull/19271). [~pnowojski], [~gaoyunhaii]?

Secondly, this test doesn't work because it validates the state incorrectly. More precisely, the static variable *CollectionSink#elements* collects all values and the test assumes that all these values would be in the checkpoint and when we restore we don't see any of these values again. But it is not true since the test doesn't provide any guarantees that all values *CollectionSink#elements*  are checkpointed. So if the flink was canceled during the last checkpoint, we take the previous one for recovery which contains in-flight data for unaligned checkpoint and as result, several last records will be repeated.

The last one, in general, I am concerned about the correctness of this test. I don't really understand what we try to check there since the job doesn't use any state from recovery. So for me, the test looks like that: 
* wait until all data processed
* the checkpoint store nothing(because all data were processed)
* restore from empty checkpoint to different parallelism
* check that we can process new(totally independent of the first case) data

Does it really make sense?

Since I don't fully understand the purpose of this test. I would like to ask [~yunta] or [~Yanfei Lei] to think about how to fix it or give me more details about the purpose of this test. I recently created this test *RestoreUpgradedJobITCase* which also checks the correctness of different states after the restoring from different snapshots. So maybe we somehow can adapt my test to different parallelism.(if the idea of checking is same);;;","30/Mar/22 06:01;yunta;First of all, thanks very much for [~akalashnikov]'s great work. It's my bad to not make the {{RescaleCheckpointManuallyITCase}} stable.

I agree that the root cause of the unstable case is due to the current test case doesn't provide any guarantees that all content of state are checkpointed. 
However, I still need to give my two cents here to reveal the truth. If you can take a look at the comments of the {{RescaleCheckpointManuallyITCase}}, you can find that the test refers to {{RescalingITCase}}, and the main logic is almost the same as {{RescalingITCase#testSavepointRescalingKeyedState}}. 
Actually, current {{RescaleCheckpointManuallyITCase}} could verify the keyed state had been correctly restored with rescaling, and you can refer to the class {{#SubtaskIndexFlatMapper}} to see that there exit two value-states named {{counter}} and {{sum}}. These two value-states are the main targets to verify. Once restored, previous {{counter}} and {{sum}} would be picked up again, and that's why we think the expected elements in the 2nd rescale would be [numberElements + numberElements2|https://github.com/apache/flink/blob/4c8995917885e301ca11023fb5e4eb3d0b7a0c7e/flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescaleCheckpointManuallyITCase.java#L152]

In other words, the 2nd run does not restore from an empty checkpoint. And we indeed leverage states within class {{#SubtaskIndexFlatMapper}} to verify checkpoint restored as expected.

So the next question is how to make guarantees that all states within {{#SubtaskIndexFlatMapper}} could be included in the next checkpoints. If the 2nd job could restore from a checkpoint which triggers after we call {{CollectionSink#getElementsSet()}}, then we can say that checkpoint could be a safe one. And thanks for FLINK-24280, we can now trigger manual checkpoint in a mini cluster.

The last question is whther we can adopt *RestoreUpgradedJobITCase* to check the correctness of different snapshots. Unfortunatly, it cannot satisify our request as we want to verify the correctness of RocksDB keyed state rescale while {{RestoreUpgradedJobITCase}} only includes operator state. The reason why we introduce {{RescaleCheckpointManuallyITCase}} is that we improved the performance of RocksDB rescale via leveraging its {{deleteRange}} API (in FLINK-21321), which could help much on reactive mode during rescaling, and current Flink lacks of an IT case to verify checkpoint rescale.

I noticed that Anto had created a [PR|https://github.com/apache/flink/pull/19271] to ignore this test. I feel very sorry for this unstable test and could [~akalashnikov] also just spend some time to take a look at [my fix solution|https://github.com/apache/flink/pull/19276]? It's very easy to understand and could also be verified in local environment.

Finally, I just want to thank for [~akalashnikov]'s great work to figure out the unstable reason once again.


;;;","30/Mar/22 07:57;akalashnikov;My bad. You are right about states in *SubtaskIndexFlatMapper* I saw it but I didn't realize that we take into account these states during verification. I think your fix is correct. It is actually was the difference between *RescalingITCase* and the new *RescaleCheckpointManuallyITCase* because *RescalingITCase* triggers savepoint before the cancel.

Since this ticket is a blocker I propose to merge your fix ASAP but I actually want to discuss somewhere(here or maybe in another ticket) a couple more questions:
* Why do we have two test classes *RescalingITCase* and *RescaleCheckpointManuallyITCase*? Won't it be better to add one more parameter *INCREMENTAL_CHECKPOINTS* to *RescalingITCase* and keep only one class(since test logic in both classes is the same)?
* Right now, we have several tests(*RescalingITCase*, *RescaleCheckpointManuallyITCase*, *RestoreUpgradedJobITCase*, I think even more than that) that contains the same logic : fill different type of states - do checkpoint/savepoint - restore with different configuration - validate correcntes. In my opinion, it makes sense to think about the unification of all of these tests(of course, in a separate ticket). Does it make sense to think about it or do I understand something wrong and do these tests have more differences than I think?;;;","30/Mar/22 09:32;yunta;Since my local CI https://dev.azure.com/myasuka/flink/_build/results?buildId=415&view=results had turned green. I have already merged the fix.

* The reason why we create another *RescaleCheckpointManuallyITCase* is due to *RescalingITCase* only targets for savepoint rescale. This has some historical reason for the mechanism of Flink rescale. If you take a look at current Flink official document about [checkpoints difference to savepoints|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/ops/state/checkpoints/#difference-to-savepoints] : {{""Checkpoints have a few differences from savepoints. They do not support Flink specific features like rescaling.""}} 
From my point of view, this means Flink community wants to hold some space to explain and does not promise checkpoint could support rescale at any time. That's why {{RescalingITCase}} only focus on savepoint rescale. On the other hand, RocksDB incremental checkpoint is the factual choice for many companies in production environment, and they heavily depend on the incremenatl checkpoint rescaling feature. In FLINK-21321, we tried to improve the RocksDB rescale performance. Since we did not want to touch the existing scope of {{RescalingITCase}}, not to mentition that case has some other unrelated test parameters, we then introduce another IT case targets for RocksDB rescale due to we changed the RocksDB rescale implementation. 

* For current RestoreUpgradedJobITCase, I feel a bit weird about that case as it  just used operator state, which means there is no difference with native savepoint and canonical savepoint.
[~akalashnikov] I noticed that your PR of FLINK-26134 had dropped the limit of checkpoint rescaling, and I think it's okay to unify them together in one IT case if we follow the direction of FLIP-203. However, that test must include verifing on keyed state and all kinds of state-backends.

;;;","30/Mar/22 09:33;yunta;Merged in master: 317ba3318a8398087defb1e335e3a89f9fd8d920;;;","30/Mar/22 10:09;akalashnikov;Thanks for the feedback, as a result, I think I will create a new ticket where we investigate the existing tests with similar ideas and then we can decide what to do.;;;","30/Mar/22 14:19;nsemmler;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33858&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","31/Mar/22 03:05;yunta;[~nsemmler], the broken instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33858&view=results does not include the fix of https://github.com/apache/flink/commit/317ba3318a8398087defb1e335e3a89f9fd8d920 ;;;","31/Mar/22 07:40;nsemmler;[~yunta] ok fair;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop with savepoint should pick up the targetDirectory from Flink configuration,FLINK-26881,13436105,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,28/Mar/22 07:41,29/Mar/22 06:25,13/Jul/23 08:08,28/Mar/22 11:30,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Upgrading stateless FlinkDeployment to savepoint still could not work. Because {{FlinkService#cancelJob}} does not pick up the target directory in the flink configuration.

 
{code:java}
case SAVEPOINT:
    String savepoint =
            clusterClient
                    .stopWithSavepoint(jobID, false, null)
                    .get(1, TimeUnit.MINUTES); {code}
We should use configured savepoint directory instead of {{null}} in the above implementation.",,gyfora,Thesharing,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 06:25:46 UTC 2022,,,,,,,,,,"0|z10vlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 08:01;gyfora;thanks [~wangyang0918] are you planning to work on this? would be great to include this in the first RC today

 ;;;","28/Mar/22 08:27;wangyang0918;I will attach a PR soon.;;;","28/Mar/22 11:30;gyfora;fixed via 97c675ef4f3b57953d5306322136c34111c8cb2f;;;","29/Mar/22 02:36;wangyang0918;An unrelated question about the fix versions. Should we set both kubernetes-operator-0.1.0 and kubernetes-operator-1.0.0 here? IIUC, kubernetes-operator-1.0.0 should include all the tasks in the kubernetes-operator-0.1.0.;;;","29/Mar/22 06:25;gyfora;[~wangyang0918] I have seen this somewhere but I think technically you are right;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get previous window values in udf,FLINK-26878,13436083,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,karthik_jagadeeswaran,karthik_jagadeeswaran,28/Mar/22 05:44,01/Apr/22 06:20,13/Jul/23 08:08,01/Apr/22 06:20,1.13.6,,,,,,1.13.6,,,,Table SQL / API,,,,,0,table-api,UDF,window_function,"hi everyone,

I am facing a problem when I want previous window values.

I want them for some aggregation purpose.

Is there any way to get them?","OS: Linux, ubuntu 20

Flink Version: 1.13.6",karthik_jagadeeswaran,ZhangChaoming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,JAVA,sql,Fri Apr 01 06:19:49 UTC 2022,,,,,,,,,,"0|z10vgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 03:14;ZhangChaoming;[~karthik_jagadeeswaran] You may try to use another window to collect the result of the first one, then apply your udf ?;;;","01/Apr/22 06:19;karthik_jagadeeswaran;yeah that worked i tried that one;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the potential failure of loading library in Thread Mode,FLINK-26865,13435766,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,25/Mar/22 09:54,28/Mar/22 06:09,13/Jul/23 08:08,28/Mar/22 06:09,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,API / Python,,,,,0,pull-request-available,,,The failure occurs in session mode.,,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 28 06:09:57 UTC 2022,,,,,,,,,,"0|z10tio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 06:09;hxbks2ks;Merged into master via 159b0f54a3e531bd745fa6caf24bd211a5b1fe84
Merged into release-1.15 via b46ece18d8a8b3a4972c35d78ed0affa593d0313;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on 25.03.2022,FLINK-26864,13435765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,smattheis,pnowojski,pnowojski,25/Mar/22 09:50,11/Apr/22 08:54,13/Jul/23 08:08,11/Apr/22 08:54,1.16.0,,,,,,1.16.0,,,,Benchmarks,,,,,0,pull-request-available,,,"http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=arrayKeyBy&extr=on&quarts=on&equid=off&env=2&revs=200
http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=remoteFilePartition&extr=on&quarts=on&equid=off&env=2&revs=200
http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=remoteSortPartition&extr=on&quarts=on&equid=off&env=2&revs=200
http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=tupleKeyBy&extr=on&quarts=on&equid=off&env=2&revs=200",,kevin.cyj,pnowojski,roman,smattheis,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/22 20:01;smattheis;checkpointSingleInput_UNALIGNED_1.png;https://issues.apache.org/jira/secure/attachment/13042132/checkpointSingleInput_UNALIGNED_1.png","07/Apr/22 19:49;smattheis;jmh-result-summary.ods;https://issues.apache.org/jira/secure/attachment/13042131/jmh-result-summary.ods","07/Apr/22 19:48;smattheis;jmh-result-summary.png;https://issues.apache.org/jira/secure/attachment/13042130/jmh-result-summary.png","07/Apr/22 20:02;smattheis;serializerHeavyString.png;https://issues.apache.org/jira/secure/attachment/13042133/serializerHeavyString.png",,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 08:54:45 UTC 2022,,,,,,,,,,"0|z10tig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/22 09:57;pnowojski;Suspected range: 010f622555..ff3336266e

After first glance from this range the most likely candidate is FLINK-26279 and _maybe_ FLINK-26592

CC [~smattheis], [~roman];;;","26/Mar/22 12:01;ym;Is changelog enabled in the benchmark?

If not, the code path to add ""on success/failure"" action into the mailbox should not take effect. Specifically, I mean this FLINK-26592;;;","28/Mar/22 15:05;roman;I double [~ym] concern.

Changelog is disabled by default and is only enabled via randomization, which shouldn't affect benchmarks.

Looking at the changes in the commit range, the following seem suspicious to me: FLINK-26279, FLINK-25904, FLINK-26334;;;","28/Mar/22 17:46;smattheis;I have run some benchmarks tests locally and I'm sure performance regression comes from https://issues.apache.org/jira/browse/FLINK-26279 (my changes -> sorry). I'm currently running some more tests for possible hotfixes other than a commit revert.;;;","05/Apr/22 08:51;ym;Should we make this a blocker for Release 1.15?

(I see the change is not included 1.15, never mind);;;","05/Apr/22 14:17;smattheis;[~ym] , I talked to [~pnowojski]  if we should do a quick-fix like a revert while I'm working on it but we agreed that this is not too urgent for now as it is not included in 1.15. The fix is, as said, WIP and I will finish it this week where I'm expecting to get back to performance as before.
The performance regression is similar to what is observed in FLINK-23560:
 * Root cause: In the specific benchmarks, a lock elision takes effect because there are no mail actions generated/executed. This lock elision cannot be applied anymore and is normal if, e.g., checkpointing is executed but also with the changes which perform latency measurements for mailbox processing and, hence, generates/executes mail actions. If lock elision cannot be applied anymore, performance drops for these specific benchmarks as observed/described in this issue.

The implications are:
 # There is no performance regression if the application performs checkpointing anyways, i.e., in most streaming applications.
 # For batch processing applications, there might be the observed performance regression. To avoid the regression, the fix is to start latency measurements only if there are mails genergated/executed. This fix is WIP.;;;","07/Apr/22 20:06;smattheis;I have created a PR with a fix to recover performance values as before in [https://github.com/apache/flink/pull/19398]

*Performance evaluation:*
 # The performance before regression was assessed in *build #148* ([http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/148/]) based on apache/flink master commit 010f622555. ({*}This is the baseline{*}.)
 # The performanc regression was observed in *build #149* ([http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/149/]) based on apache/flink master commit ff3336266e.
 # The fix is based on current apache/master in commit 39433bafd0  and has been evaluated in *build #74* [http://codespeed.dak8s.net:8080/job/flink-benchmark-request/74/]. (This is the change of PR [https://github.com/apache/flink/pull/19398])
 # The fix has also been cherry-picked on top of commit ff3336266e (regression) to verify the fix which has been evaluted in *build #73* ([http://codespeed.dak8s.net:8080/job/flink-benchmark-request/73/]).

The following table shows the performance implications of the various changes where for the delta (in percent) the basis is build # 148. ([^jmh-result-summary.ods])

!jmh-result-summary.png!

*Conclusion:*

Build #73 shows that the fix, if cherry-picked on the commit that first showed a performance regression in build #149, recovers the performance values from before as of build #148. Build #74, which is the fix based on the current master, shows that there is no other significant performance except the following:
 * org.apache.flink.benchmark.CheckpointingTimeBenchmark.checkpointSingleInput -> -18.74%
 * org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString -> -14%

However, both deviations seem to be within a normal range of varying performance values and did not drop with the changes after March 25:
 * org.apache.flink.benchmark.CheckpointingTimeBenchmark.checkpointSingleInput.UNALIGNED_1
!checkpointSingleInput_UNALIGNED_1.png!
 * org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString
!serializerHeavyString.png!;;;","11/Apr/22 08:54;pnowojski;merged commit fecd0fb into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ImportError: cannot import name 'environmentfilter' from 'jinja2',FLINK-26855,13435697,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Juntao Hu,dianfu,dianfu,25/Mar/22 02:26,25/Mar/22 04:14,13/Jul/23 08:08,25/Mar/22 04:14,1.15.0,1.16.0,,,,,1.13.7,1.14.5,1.15.0,,API / Python,,,,,0,pull-request-available,,,"{code}
ar 24 17:38:39 ===========mypy checks... [SUCCESS]===========
Mar 24 17:38:39 rm -rf _build/*
Mar 24 17:38:39 /__w/2/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees  -a -W . _build/html
Mar 24 17:38:40 Traceback (most recent call last):
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/bin/sphinx-build"", line 6, in <module>
Mar 24 17:38:40     from sphinx.cmd.build import main
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/cmd/build.py"", line 23, in <module>
Mar 24 17:38:40     from sphinx.application import Sphinx
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/application.py"", line 42, in <module>
Mar 24 17:38:40     from sphinx.highlighting import lexer_classes, lexers
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/highlighting.py"", line 30, in <module>
Mar 24 17:38:40     from sphinx.ext import doctest
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/ext/doctest.py"", line 28, in <module>
Mar 24 17:38:40     from sphinx.builders import Builder
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/builders/__init__.py"", line 24, in <module>
Mar 24 17:38:40     from sphinx.io import read_doc
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/io.py"", line 42, in <module>
Mar 24 17:38:40     from sphinx.util.rst import append_epilog, docinfo_re, prepend_prolog
Mar 24 17:38:40   File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/util/rst.py"", line 22, in <module>
Mar 24 17:38:40     from jinja2 import environmentfilter
Mar 24 17:38:40 ImportError: cannot import name 'environmentfilter' from 'jinja2' (/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/jinja2/__init__.py)
Mar 24 17:38:40 Makefile:76: recipe for target 'html' failed
Mar 24 17:38:40 make: *** [html] Error 1
Mar 24 17:38:40 ==========sphinx checks... [FAILED]===========
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33717&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23450",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 25 04:14:17 UTC 2022,,,,,,,,,,"0|z10t3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/22 04:14;dianfu;Fixed in:
- master via dc00dadb8ad210e2665b0e9e6e923529201f4e2a
- release-1.15 via cbf6eecb0244eb87c769646bd73c76811574a030
- release-1.14 via 6103c8e2f37ca8c6e2f80f6e55ea2669272409b0
- release-1.13 via 88ccdf12539dcdd263c7170f6c29d065a78eb1e8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HeapStateBackend ignores metadata updates in certain cases,FLINK-26853,13435630,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,masteryhx,roman,roman,24/Mar/22 18:09,11/Oct/22 08:06,13/Jul/23 08:08,26/Jul/22 15:44,1.14.4,1.15.0,1.16.0,,,,1.16.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"On recovery, HeapRestoreOperation reads state handles one by one;
 * each handle contains metadata at the beginning;
 * the metadata is always read, but not actually used if a state with the corresponding name was already registered

In a rare case of downscaling + multiple checkpoints with different metadata; this might lead to data being deserialized incorrectly (always using the initial metadata).

It also prevents incremental checkpoints with schema evolution.

On first access, however, the backend itself will update (merge) metadata; so that it doesn't affect new state updates.",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28553,,,,,,,,,FLINK-23143,,,FLINK-21648,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 15:44:21 UTC 2022,,,,,,,,,,"0|z10soo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 15:44;roman;Merged into master as 8df50536ef913b63620d896423c39cdd01941c55.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBMapState#clear not forwarding exceptions,FLINK-26852,13435593,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,nkruber,nkruber,24/Mar/22 14:37,08/Apr/22 14:03,13/Jul/23 08:08,08/Apr/22 14:03,1.14.4,1.15.0,,,,,1.16.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"I accidentally found an inconsistent behaviour in the RocksDB state backend implementation:

If there's an exception in {{AbstractRocksDBState#clear()}} it will forward that inside a {{FlinkRuntimeException}}.

However, if there's an exception in {{RocksDBMapState#clear}} it will merely print the exception stacktrace and continue as is.

I assume, forwarding the exception as {{FlinkRuntimeException}} should be the desired behaviour for both use cases...",,nkruber,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 08 14:03:01 UTC 2022,,,,,,,,,,"0|z10sgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 07:04;yunta;I think this is a really bug, asked [~masteryhx] to take this ticket.;;;","08/Apr/22 14:03;yunta;merged in master: 241e819d82bf84535b355e3822894d6b685db0a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command line option '-py' doesn't work in YARN application mode,FLINK-26847,13435566,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,24/Mar/22 12:37,25/Mar/22 09:33,13/Jul/23 08:08,25/Mar/22 09:33,,,,,,,1.15.0,,,,API / Python,,,,,0,pull-request-available,,,,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 25 09:33:58 UTC 2022,,,,,,,,,,"0|z10sao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/22 09:33;dianfu;Fixed in:
- master via 330aae0c6e0811f50888d17830f10f7a29efe7d7
- release-1.15 via 608f5e1761b4fc011aec6cc958ef375d6992814c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gauge metrics doesn't work in PyFlink,FLINK-26846,13435564,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,24/Mar/22 12:27,25/Mar/22 01:27,13/Jul/23 08:08,25/Mar/22 01:27,1.12.0,,,,,,1.13.7,1.14.5,1.15.0,,API / Python,,,,,0,pull-request-available,,,See https://lists.apache.org/thread/w7jkwgpon6qy4p6k1nhhw5k4m81r8c8p for more details.,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 25 01:27:47 UTC 2022,,,,,,,,,,"0|z10sa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/22 01:27;dianfu;Fixed in:
- master via e299a1240a43ec493e56492c9d39cb88bd2bcb5e
- release-1.15 via 94c679ef99357a66fc7609ff6f2f0a91caa4476a
- release-1.14 via f63adf03649ac29aeb74e1811315362acf927c51
- release-1.13 via aa8057ebdb62eed114c4a275dcb478598d30e1ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WordCountSubclassPOJOITCase failed on azure,FLINK-26835,13435497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ZhangChaoming,gaoyunhaii,gaoyunhaii,24/Mar/22 07:11,07/Apr/22 07:10,13/Jul/23 08:08,07/Apr/22 07:10,1.15.0,,,,,,1.15.0,1.16.0,,,API / Type Serialization System,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-03-23T10:17:21.4299023Z Job execution failed.
2022-03-23T10:17:21.4302519Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-03-23T10:17:21.4303977Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-03-23T10:17:21.4305375Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-03-23T10:17:21.4306669Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-03-23T10:17:21.4307818Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-03-23T10:17:21.4309084Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-03-23T10:17:21.4310326Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-03-23T10:17:21.4311773Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
2022-03-23T10:17:21.4313817Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-03-23T10:17:21.4314754Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-03-23T10:17:21.4315516Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-03-23T10:17:21.4316374Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-03-23T10:17:21.4317031Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-03-23T10:17:21.4318008Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-03-23T10:17:21.4318771Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-03-23T10:17:21.4320119Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-03-23T10:17:21.4321262Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-03-23T10:17:21.4322370Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-03-23T10:17:21.4323463Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-03-23T10:17:21.4324523Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-03-23T10:17:21.4325464Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-03-23T10:17:21.4326700Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-03-23T10:17:21.4327381Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-03-23T10:17:21.4328048Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-03-23T10:17:21.4328928Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-03-23T10:17:21.4329822Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-03-23T10:17:21.4330681Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-03-23T10:17:21.4331401Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-03-23T10:17:21.4332336Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-03-23T10:17:21.4333325Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-03-23T10:17:21.4334292Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-03-23T10:17:21.4335314Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-03-23T10:17:21.4336167Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-03-23T10:17:21.4337330Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-03-23T10:17:21.4338240Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-03-23T10:17:21.4339230Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-03-23T10:17:21.4339978Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-03-23T10:17:21.4340703Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-03-23T10:17:21.4341316Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-03-23T10:17:21.4342302Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-03-23T10:17:21.4343139Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-03-23T10:17:21.4343998Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-03-23T10:17:21.4345018Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-03-23T10:17:21.4345840Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-03-23T10:17:21.4346949Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-03-23T10:17:21.4347692Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-03-23T10:17:21.4348597Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-03-23T10:17:21.4349653Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-03-23T10:17:21.4350488Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-03-23T10:17:21.4351454Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-03-23T10:17:21.4352486Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-03-23T10:17:21.4353796Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-03-23T10:17:21.4355359Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
2022-03-23T10:17:21.4356479Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
2022-03-23T10:17:21.4357489Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
2022-03-23T10:17:21.4358572Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
2022-03-23T10:17:21.4359753Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-03-23T10:17:21.4360757Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-03-23T10:17:21.4361354Z 	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
2022-03-23T10:17:21.4362251Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-23T10:17:21.4362999Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-23T10:17:21.4363953Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-03-23T10:17:21.4365256Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-03-23T10:17:21.4366345Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-03-23T10:17:21.4367331Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-03-23T10:17:21.4368273Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-03-23T10:17:21.4369396Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-03-23T10:17:21.4370207Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-03-23T10:17:21.4370873Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-03-23T10:17:21.4371452Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-03-23T10:17:21.4372272Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-03-23T10:17:21.4372935Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-03-23T10:17:21.4373529Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-03-23T10:17:21.4374098Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-03-23T10:17:21.4374684Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-03-23T10:17:21.4375216Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-03-23T10:17:21.4375803Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-03-23T10:17:21.4376315Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-03-23T10:17:21.4376869Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-03-23T10:17:21.4377383Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-03-23T10:17:21.4377886Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-03-23T10:17:21.4378386Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-03-23T10:17:21.4378858Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-03-23T10:17:21.4379213Z 	... 4 more
2022-03-23T10:17:21.4380979Z Caused by: java.lang.Exception: The data preparation for task 'CHAIN Reduce (Reduce at testProgram(WordCountSubclassPOJOITCase.java:59)) -> Map (Map at testProgram(WordCountSubclassPOJOITCase.java:70))' , caused an error: null
2022-03-23T10:17:21.4381932Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:487)
2022-03-23T10:17:21.4382552Z 	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:357)
2022-03-23T10:17:21.4383197Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
2022-03-23T10:17:21.4383982Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
2022-03-23T10:17:21.4384567Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
2022-03-23T10:17:21.4385327Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
2022-03-23T10:17:21.4386187Z 	at java.lang.Thread.run(Thread.java:748)
2022-03-23T10:17:21.4386894Z Caused by: java.util.ConcurrentModificationException
2022-03-23T10:17:21.4387530Z 	at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1704)
2022-03-23T10:17:21.4388430Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-03-23T10:17:21.4389409Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-03-23T10:17:21.4390201Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-03-23T10:17:21.4391029Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-03-23T10:17:21.4391817Z 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
2022-03-23T10:17:21.4392635Z 	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:178)
2022-03-23T10:17:21.4393658Z 	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:57)
2022-03-23T10:17:21.4394876Z 	at org.apache.flink.api.java.typeutils.runtime.RuntimeSerializerFactory.getSerializer(RuntimeSerializerFactory.java:94)
2022-03-23T10:17:21.4395908Z 	at org.apache.flink.runtime.operators.ReduceDriver.prepare(ReduceDriver.java:96)
2022-03-23T10:17:21.4396939Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:479)
2022-03-23T10:17:21.4397452Z 	... 6 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33649&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5423",,gaoyunhaii,lzljs3620320,pnowojski,ym,ZhangChaoming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26992,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 07 07:10:53 UTC 2022,,,,,,,,,,"0|z10rvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/22 07:50;ZhangChaoming;[~lzljs3620320] , could you please help to check this issue ~ I am glad to solve it.;;;","28/Mar/22 02:34;lzljs3620320;[~ZhangChaoming] Thanks for the contribution, we can find the developers of the relevant modules. [~gaoyunhaii] ;;;","28/Mar/22 02:36;ZhangChaoming;[~twalthr] Could you please assgin this issue to me ?;;;","30/Mar/22 08:12;gaoyunhaii;Thanks [~ZhangChaoming] for the contribution! I assigned the issue to you;;;","31/Mar/22 14:10;pnowojski;[~yunta]/[~ym]. While investigating this issue, we have found that probably state backends are also using non-thread safe serialisers from different threads.

For example: {{RocksFullSnapshotStrategy#syncPrepareResources}} is passing {{keySerializer}} from the task thread, to the async thread in order to serialize the serializer itself.  {{RocksIncrementalSnapshotStrategy.RocksDBIncrementalSnapshotOperation#materializeMetaData}} seems to be doing the same thing. If {{PojoSerializer}} is used as {{keySerializer}} I think this will lead to the same problems as above. Iterating through the {{PojoSerializer#subclassSerializerCache}} from the the async checkpoint thread, while the map can be changed from the task thread. It looks like in all of those places the serializer should have been duplicated ({{#duplicate}}) before being passed to another thread. Maybe this should happen in {{RocksDBSnapshotStrategyBase}}. I don't know about other state backends.

WDYT?;;;","01/Apr/22 12:39;ym;Thanks for pinging [~pnowojski] . I second that each thread obtains ownership of the Serializer passed in itself.

Although I need some time to dig into whether the current way of passing Serializer really causing problems in the state backend (concurrent modification possible).

I create a separate ticket FLINK-26992  to track this.;;;","07/Apr/22 07:10;pnowojski;Merged to master as 3ec386806a0
Merged to release-1.15 as 1d7b2361392;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AppendOnlyFirstNFunction produce wrong result when with offset and without rank number,FLINK-26819,13435296,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,23/Mar/22 08:05,23/Mar/22 14:21,13/Jul/23 08:08,23/Mar/22 14:21,1.14.4,1.15.0,,,,,1.16.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,"FLINK-19896 introduced an optimized AppendOnlyFirstNFunction, but produces wrong result when a constant rank with offset and without rank number.

In test case `AppendOnlyFirstNFunctionTest.testConstantRankRangeWithOffset` produces 4 output while 2 output is expected because only want rank range be [2,2]
{code}
        AbstractTopNFunction func =
                createFunction(RankType.ROW_NUMBER, new ConstantRankRange(2, 2), true, false);
        OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
        testHarness.open();
        testHarness.processElement(insertRecord(""book"", 2L, 12));
        testHarness.processElement(insertRecord(""book"", 2L, 19));
        testHarness.processElement(insertRecord(""book"", 2L, 11));
        testHarness.processElement(insertRecord(""fruit"", 1L, 33));
        testHarness.processElement(insertRecord(""fruit"", 1L, 44));
        testHarness.processElement(insertRecord(""fruit"", 1L, 22));
        testHarness.close();

        List<Object> expectedOutput = new ArrayList<>();
        expectedOutput.add(insertRecord(""book"", 2L, 12));
        expectedOutput.add(insertRecord(""book"", 2L, 19));
        expectedOutput.add(insertRecord(""fruit"", 1L, 33));
        expectedOutput.add(insertRecord(""fruit"", 1L, 44));
       ...
{code}

we should fix this.",,jark,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 14:21:45 UTC 2022,,,,,,,,,,"0|z10qmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 14:21;jark;Fixed in 
 - master: 0a24c955b56257cc716b14db7d476868cd3617ea;;;","23/Mar/22 14:21;jark;[~lincoln.86xy] could you open pull requests to backport the fix to release-1.15 and release-1.14 branches?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests in FlinkConfigBuilderTest are skipped because of NullPointerException,FLINK-26815,13435275,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,wangyang0918,wangyang0918,23/Mar/22 06:21,24/Nov/22 01:03,13/Jul/23 08:08,23/Mar/22 09:26,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"{code:java}
Test ignored.java.lang.NullPointerException
    at org.apache.flink.kubernetes.operator.utils.FlinkConfigBuilderTest.prepareFlinkDeployment(FlinkConfigBuilderTest.java:76)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
... ...{code}
It is not an expected behavior that the tests are skipped instead of failure in the [CI|https://github.com/apache/flink-kubernetes-operator/runs/5654182179?check_suite_focus=true].",,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26706,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 09:26:36 UTC 2022,,,,,,,,,,"0|z10qi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 06:22;wangyang0918;cc [~matyas] ;;;","23/Mar/22 07:48;morhidi;Will look at CI behaviour too;;;","23/Mar/22 09:26;gyfora;merged to main: 0562e642890ae3eca486198f918f154c236b9b91;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"It reported ""should only have one jar"" when submitting PyFlink jobs in YARN application mode",FLINK-26814,13435233,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,cun8cun8,cun8cun8,cun8cun8,23/Mar/22 02:47,29/Mar/22 01:51,13/Jul/23 08:08,29/Mar/22 01:51,1.14.4,,,,,,1.15.0,,,,API / Python,Deployment / YARN,,,,0,pull-request-available,,,"Solve the problem that ""should only have one jar"" is reported as an error when the application submits an operation in the yarn mode",,cun8cun8,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 01:51:01 UTC 2022,,,,,,,,,,"0|z10q8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 01:51;dianfu;Fixed in:
- master via d834b271366ed508aba327aa94a14bb2b2e47a4c
- release-1.15 via 52cc77df9f11b8ba21de80c445b1261959cf315a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The local time zone does not take effect when the dynamic index uses a field of type timestamp_ltz,FLINK-26810,13435165,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alexanderpreuss,hackergin,hackergin,22/Mar/22 17:00,13/Sep/22 08:59,13/Jul/23 08:08,07/Apr/22 11:00,,,,,,,1.15.0,elasticsearch-3.0.0,,,Connectors / ElasticSearch,Table SQL / Planner,Table SQL / Runtime,,,0,pull-request-available,,,"When using  TIMESTAMP_WITH_LOCAL_TIMEZONE field to generate a dynamic index,  it will alway use UTC timezone.   

 

 ",,fpaul,hackergin,martijnvisser,twalthr,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/22 16:59;hackergin;截屏2022-03-23 上午12.48.02.png;https://issues.apache.org/jira/secure/attachment/13041428/%E6%88%AA%E5%B1%8F2022-03-23+%E4%B8%8A%E5%8D%8812.48.02.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 13 08:59:32 UTC 2022,,,,,,,,,,"0|z10pu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 17:12;hackergin;[~fpaul]   Can you help to confirm this issue?;;;","23/Mar/22 08:39;martijnvisser;[~hackergin] I don't think this is a connector problem, but this would be Table/SQL related I guess. Which version of Flink are you using? ;;;","23/Mar/22 14:39;hackergin;[~martijnvisser]   I think this is a problem with the elasticSearch connector, because the conversion of timestampData to String is implemented inside the connector, not at the Table Runtime layer.
as shown in the code below 
{code:java}
//代码占位符
            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
                return (value, dateTimeFormatter) -> {
                    TimestampData indexField = (TimestampData) value;
                    return indexField.toInstant().atZone(ZoneOffset.UTC).format(dateTimeFormatter);
                };
{code}

When formatting TimestampData as String, session local timezone is not used;;;","23/Mar/22 14:45;martijnvisser;[~alexanderpreuss] Can you take a look?;;;","23/Mar/22 14:48;twalthr;IIRC [~matriv] was also involved here.;;;","07/Apr/22 10:59;fpaul;Merged in master: 0d0c7a5e10f38b394ccd956511d92f0d11569d26;;;","07/Apr/22 11:00;fpaul;Merged in release-1.15: 051825bd00275cbea2a684ad8085b9563aa6fb47;;;","13/Sep/22 08:59;chesnay;elasticsearch-main: 6e61d797872f1e49d1840c3339464bb55c3de2f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogStorageMetricsTest.testAttemptsPerUpload failed,FLINK-26809,13435159,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,22/Mar/22 16:37,25/Mar/22 05:55,13/Jul/23 08:08,25/Mar/22 05:55,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / Metrics,Runtime / State Backends,,,,0,pull-request-available,test-stability,,"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=901&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=24226] failed due a failure in {{ChangelogStorageMetricsTest.testAttemptsPerUpload}}:

{code}
Mar 22 12:23:09 [ERROR] Failures: 
Mar 22 12:23:09 [ERROR]   ChangelogStorageMetricsTest.testAttemptsPerUpload:208 
Mar 22 12:23:09 expected: 3L
Mar 22 12:23:09  but was: 0L
{code}",,gaoyunhaii,mapohl,roman,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 25 05:55:15 UTC 2022,,,,,,,,,,"0|z10pso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 16:43;mapohl;And another one: https://dev.azure.com/mapohl/flink/_build/results?buildId=902&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=24139

{code}
Mar 22 13:01:32 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.043 s <<< FAILURE! - in org.apache.flink.changelog.fs.ChangelogStorageMetricsTest
Mar 22 13:01:32 [ERROR] org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.testAttemptsPerUpload  Time elapsed: 0.052 s  <<< FAILURE!
Mar 22 13:01:32 org.opentest4j.AssertionFailedError: 
Mar 22 13:01:32 
Mar 22 13:01:32 expected: 3L
Mar 22 13:01:32  but was: 0L
Mar 22 13:01:32 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Mar 22 13:01:32 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Mar 22 13:01:32 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Mar 22 13:01:32 	at org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.testAttemptsPerUpload(ChangelogStorageMetricsTest.java:208)
Mar 22 13:01:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code};;;","24/Mar/22 07:05;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33671&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24143
;;;","24/Mar/22 07:41;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33627&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24239;;;","24/Mar/22 08:00;ym;[~roman] , would you please take a look;;;","24/Mar/22 08:02;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33672&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24251
;;;","24/Mar/22 09:44;roman;There is a race condition in test:
 * the assertion is made as soon as upload (future) is completed
 * the histogram is updated after completing the upload (on success)

Moving assertion out of try/close block solves the problem. I'll open a PR.;;;","25/Mar/22 05:55;roman;Merged into 1.15 as 3c88f887de26f58cff27c87931d7a2f6928c81da,

into master as 7e909ff2ee9e025796d357b13cf8877b01f6ca44.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed table breaks legacy connector without 'connector.type',FLINK-26805,13435125,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Mar/22 13:28,24/Mar/22 03:54,13/Jul/23 08:08,24/Mar/22 03:54,,,,,,,1.15.0,,,,Table SQL / API,,,,,0,pull-request-available,,,"{code:java}
CREATE TABLE T (a INT) WITH ('type'='legacy');
INSERT INTO T VALUES (1); {code}
This case can be misinterpreted as a managed table, which the user might expect to be resolved by the legacy table factory.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 04:56:31 UTC 2022,,,,,,,,,,"0|z10pl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 04:56;lzljs3620320;master: 7d7a111eba368043f8624e114daa29400a74c096

release-1.15: 6e63e6c2ab074f070389a0eae181269cfbc82772;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator e2e tests sporadically fail: DEPLOYED_NOT_READY,FLINK-26804,13435105,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,22/Mar/22 10:59,27/Mar/22 13:16,13/Jul/23 08:08,22/Mar/22 16:57,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"I managed to introduce a sporadic failure scenario for the e2e tests via my solution of FLINK-26715. Since the operator only checks on the job every couple second the job might still be observed as being in DEPLOYED_NOT_READY state even after successfully completing checkpoints.

{code:bash}
Run ls e2e-tests/test_*.sh | while read script_test;do \
Running e2e-tests/test_kubernetes_application_ha.sh
persistentvolumeclaim/flink-example-statemachine created
Error from server (InternalError): error when creating ""e2e-tests/data/cr.yaml"": Internal error occurred: failed calling webhook ""vflinkdeployments.flink.apache.org"": failed to call webhook: Post ""https://flink-operator-webhook-service.default.svc:443/validate?timeout=10s"": dial tcp 10.106.63.26:443: connect: connection refused
Command: kubectl apply -f e2e-tests/data/cr.yaml failed. Retrying...
flinkdeployment.flink.apache.org/flink-example-statemachine created
persistentvolumeclaim/flink-example-statemachine unchanged
Error from server (NotFound): deployments.apps ""flink-example-statemachine"" not found
Command: kubectl get deploy/flink-example-statemachine failed. Retrying...
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
flink-example-statemachine   0/1     1            0           1s
deployment.apps/flink-example-statemachine condition met
Waiting for jobmanager pod flink-example-statemachine-7fcf55c88b-h5r7r ready.
pod/flink-example-statemachine-7fcf55c88b-h5r7r condition met
Waiting for log ""Rest endpoint listening at""...
Log ""Rest endpoint listening at"" shows up.
Waiting for log ""Completed checkpoint [0-[9](https://github.com/apache/flink-kubernetes-operator/runs/5640468148?check_suite_focus=true#step:9:9)]+ for job""...
Log ""Completed checkpoint [0-9]+ for job"" shows up.
Successfully verified that flinkdep/flink-example-statemachine.status.jobManagerDeploymentStatus is in READY state.
Successfully verified that flinkdep/flink-example-statemachine.status.jobStatus.state is in RUNNING state.
Kill the flink-example-statemachine-7fcf55c88b-h5r7r
Defaulted container ""flink-main-container"" out of: flink-main-container, artifacts-fetcher (init)
Waiting for log ""Restoring job 00000000000000000000000000000000 from Checkpoint""...
Log ""Restoring job 00000000000000000000000000000000 from Checkpoint"" shows up.
Waiting for log ""Completed checkpoint [0-9]+ for job""...
Log ""Completed checkpoint [0-9]+ for job"" shows up.
Status verification for flinkdep/flink-example-statemachine.status.jobManagerDeploymentStatus failed. It is DEPLOYED_NOT_READY instead of READY.
Debugging failed e2e test:
{code}
",,gyfora,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 22 16:57:45 UTC 2022,,,,,,,,,,"0|z10pgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 16:57;gyfora;merged to main: 9fe5d25e4d780d22fba1d1853b422ed8d31e6e01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support duplicate checkpoint aborted messages,FLINK-26801,13435078,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,zhangzp,zhangzp,22/Mar/22 08:26,29/Jul/22 07:11,13/Jul/23 08:08,29/Jul/22 07:11,ml-2.0.0,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,test-stability,,"The flink-ml run fails at the following case [1]:

```
Error: Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 71.967 s <<< FAILURE! - in org.apache.flink.ml.classification.LogisticRegressionTest 
[27997|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:27997]Error: testGetModelData Time elapsed: 3.221 s <<< ERROR! 
[27998|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:27998]java.lang.RuntimeException: Failed to fetch next result 
[27999|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:27999] at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109) 
[28000|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28000] at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.next(CollectResultIterator.java:88) 
[28001|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28001] at org.apache.flink.ml.classification.LogisticRegressionTest.testGetModelData(LogisticRegressionTest.java:251) 
[28002|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28002] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
[28003|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28003] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
[28004|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28004] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
[28005|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28005] at java.lang.reflect.Method.invoke(Method.java:498) 
[28006|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28006] at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 
[28007|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28007] at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 
[28008|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28008] at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 
[28009|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28009] at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 
[28010|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28010] at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 
[28011|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28011] at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) 
[28012|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28012] at org.junit.rules.RunRules.evaluate(RunRules.java:20) 
[28013|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28013] at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 
[28014|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28014] at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 
[28015|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28015] at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 
[28016|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28016] at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 
[28017|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28017] at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 
[28018|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28018] at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 
[28019|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28019] at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 
[28020|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28020] at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 
[28021|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28021] at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 
[28022|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28022] at org.junit.runner.JUnitCore.run(JUnitCore.java:137) 
[28023|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28023] at org.junit.runner.JUnitCore.run(JUnitCore.java:115) 
[28024|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28024] at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) 
[28025|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28025] at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) 
[28026|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28026] at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) 
[28027|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28027] at java.util.Iterator.forEachRemaining(Iterator.java:116) 
[28028|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28028] at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) 
[28029|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28029] at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) 
[28030|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28030] at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) 
[28031|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28031] at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) 
[28032|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28032] at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) 
[28033|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28033] at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 
[28034|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28034] at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) 
[28035|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28035] at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) 
[28036|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28036] at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) 
[28037|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28037] at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220) 
[28038|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28038] at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188) 
[28039|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28039] at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202) 
[28040|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28040] at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181) 
[28041|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28041] at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128) 
[28042|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28042] at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142) 
[28043|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28043] at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:109) 
[28044|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28044] at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) 
[28045|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28045] at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) 
[28046|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28046] at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) 
[28047|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28047] at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) 
[28048|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28048]Caused by: java.io.IOException: Failed to fetch job execution result 
[28049|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28049] at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177) 
[28050|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28050] at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120) 
[28051|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28051] at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) 
[28052|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28052] ... 48 more 
[28053|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28053]Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. 
[28054|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28054] at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) 
[28055|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28055] at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) 
[28056|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28056] at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) 
[28057|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28057] ... 50 more 
[28058|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28058]Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. 
[28059|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28059] at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) 
[28060|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28060] at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137) 
[28061|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28061] at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) 
[28062|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28062] at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628) 
[28063|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28063] at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996) 
[28064|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28064] at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134) 
[28065|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28065] at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:174) 
[28066|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28066] ... 50 more 
[28067|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28067]Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy 
[28068|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28068] at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) 
[28069|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28069] at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) 
[28070|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28070] at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228) 
[28071|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28071] at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218) 
[28072|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28072] at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209) 
[28073|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28073] at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:679) 
[28074|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28074] at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) 
[28075|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28075] at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) 
[28076|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28076] at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source) 
[28077|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28077] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
[28078|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28078] at java.lang.reflect.Method.invoke(Method.java:498) 
[28079|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28079] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) 
[28080|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28080] at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) 
[28081|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28081] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) 
[28082|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28082] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) 
[28083|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28083] at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) 
[28084|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28084] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) 
[28085|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28085] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) 
[28086|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28086] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) 
[28087|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28087] at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) 
[28088|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28088] at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) 
[28089|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28089] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) 
[28090|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28090] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 
[28091|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28091] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
[28092|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28092] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
[28093|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28093] at akka.actor.Actor.aroundReceive(Actor.scala:537) 
[28094|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28094] at akka.actor.Actor.aroundReceive$(Actor.scala:535) 
[28095|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28095] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) 
[28096|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28096] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) 
[28097|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28097] at akka.actor.ActorCell.invoke(ActorCell.scala:548) 
[28098|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28098] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) 
[28099|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28099] at akka.dispatch.Mailbox.run(Mailbox.scala:231) 
[28100|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28100] at akka.dispatch.Mailbox.exec(Mailbox.scala:243) 
[28101|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28101] at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) 
[28102|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28102] at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) 
[28103|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28103] at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) 
[28104|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28104] at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) 
[28105|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28105]Caused by: java.lang.IllegalStateException: Should be blocked by checkpoint. 
[28106|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28106] at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
[28107|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28107] at org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.resumeConsumption(PipelinedSubpartition.java:381) 
[28108|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28108] at org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.resumeConsumption(PipelinedSubpartitionView.java:79) 
[28109|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28109] at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.resumeConsumption(LocalInputChannel.java:283) 
[28110|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28110] at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.resumeConsumption(SingleInputGate.java:867) 
[28111|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28111] at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.resumeConsumption(InputGateWithMetrics.java:67) 
[28112|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28112] at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:223) 
[28113|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28113] at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerUnfinishedChannelsCheckpoint(StreamTask.java:1209) 
[28114|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28114] at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$12(StreamTask.java:1126) 
[28115|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28115] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) 
[28116|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28116] at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) 
[28117|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28117] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.yield(MailboxExecutorImpl.java:86) 
[28118|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28118] at org.apache.flink.iteration.operator.HeadOperator.endInput(HeadOperator.java:408) 
[28119|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28119] at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:91) 
[28120|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28120] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:100) 
[28121|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28121] at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68) 
[28122|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28122] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) 
[28123|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28123] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) 
[28124|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28124] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) 
[28125|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28125] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) 
[28126|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28126] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) 
[28127|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28127] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) 
[28128|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28128] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) 
[28129|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28129] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) 
[28130|https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true#step:4:28130] at java.lang.Thread.run(Thread.java:750)
```

 

This bug cannot be repeated after 100 local runs.

 

 [1]https://github.com/apache/flink-ml/runs/5638224415?check_suite_focus=true",,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 07:11:04 UTC 2022,,,,,,,,,,"0|z10pao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 10:09;zhangzp;another similar one: https://github.com/apache/flink-ml/runs/5641917141?check_suite_focus=true;;;","06/May/22 03:05;zhangzp;another one: https://github.com/apache/flink-ml/runs/6316721763?check_suite_focus=true;;;","29/Jul/22 07:11;zhangzp;This is already resolved. The new bug is different from this one.

 

I have created a new Jira for the new bug: https://issues.apache.org/jira/browse/FLINK-28739;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateChangeFormat#read not seek to offset correctly,FLINK-26799,13435072,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,22/Mar/22 08:08,28/Mar/22 09:21,13/Jul/23 08:08,28/Mar/22 09:18,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,"StateChangeFormat#read must seek to offset before read, current implement as follows :

 
{code:java}
FSDataInputStream stream = handle.openInputStream();
DataInputViewStreamWrapper input = wrap(stream);
if (stream.getPos() != offset) {
    LOG.debug(""seek from {} to {}"", stream.getPos(), offset);
    input.skipBytesToRead((int) offset);
}{code}
But the if condition is incorrect, stream.getPos() return the position of underlying stream which is different from position of input.

By the way, because of wrapped by BufferedInputStream, position of underlying stream always at n*bufferSize or the end of file. 

Actually, input is aways at position 0 at beginning, so I think we can seek to the offset directly.

 ",,Feifan Wang,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/22 10:12;Feifan Wang;image-2022-03-24-18-12-09-742.png;https://issues.apache.org/jira/secure/attachment/13041525/image-2022-03-24-18-12-09-742.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 28 09:18:21 UTC 2022,,,,,,,,,,"0|z10p9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 02:48;Feifan Wang;Hi [~roman] , can you help confirm that ?;;;","24/Mar/22 01:04;roman;Hi [~Feifan Wang] , thanks for looking into it,

> But the if condition is incorrect, stream.getPos() return the position of underlying stream which is different from position of input.
Exactly, this is the underlying stream that needs to be positioned. Then ""offset"" bytes are skipped from the buffer. So I think this is correct.

> Actually, input is aways at position 0 at beginning, so I think we can seek to the offset directly.

Are you proposing to seek on the underlying stream {*}before wrapping{*}?
I think that won't work, because the compression flag is written at the beginning.

Alternatively, seeking on the underlying stream *after wrapping* seem dangerous to me: there is no contract that no bytes are buffered by constructor AFAIK.;;;","24/Mar/22 02:20;Feifan Wang;Hi [~roman] , I'm not mean seek on the underlying stream before wrapping. We should always seek on the wrapper stream as long as the offset not equal to 0. I think the code should be like below :

 
{code:java}
if (offset != 0) {
    LOG.debug(""seek from {} to {}"", stream.getPos(), offset);
    input.skipBytesToRead((int) offset);
} {code}
 ;;;","24/Mar/22 02:31;Feifan Wang;[~roman] , here the problem is *`underlyingStream.getPos() == offset` not mean the wrapper stream is on correct position.* ;;;","24/Mar/22 08:22;roman;Do you have a scenario where this issue arises? I've played a bit with StateChangelogStorageTest.testWriteAndRead altering buffer size and adding offsets and it works as expected.

OTH, the change you're proposing also seems correct so I wouldn't mind it.;;;","24/Mar/22 10:23;Feifan Wang;Thanks very much for reply [~roman] .

StateChangelogStorageTest.testWriteAndRead is not enough to find the problem. But with a little modification, the problem can be found :
 # change the keyLen from 10 to {color:#de350b}405{color}
 # {color:#172b4d}call writer.nextSequenceNumber(){color} after dealt with every entry of appendsByKeyGroup

 

Complete StateChangelogStorageTest.testWriteAndRead with above modification is :
{code:java}
@MethodSource(""parameters"")
@ParameterizedTest(name = ""compression = {0}"")
public void testWriteAndRead(boolean compression) throws Exception {
    KeyGroupRange kgRange = KeyGroupRange.of(0, 5);
    Map<Integer, List<byte[]>> appendsByKeyGroup = generateAppends(kgRange, 405, 20);

    try (StateChangelogStorage<T> client = getFactory(compression, temporaryFolder);
            StateChangelogWriter<T> writer =
                    client.createWriter(
                            new OperatorID().toString(), kgRange, new SyncMailboxExecutor())) {
        SequenceNumber prev = writer.initialSequenceNumber();
        for (Map.Entry<Integer, List<byte[]>> entry : appendsByKeyGroup.entrySet()) {
            Integer group = entry.getKey();
            List<byte[]> appends = entry.getValue();
            for (byte[] bytes : appends) {
                writer.append(group, bytes);
            }
            writer.nextSequenceNumber();
        }

        T handle = writer.persist(prev).get();
        StateChangelogHandleReader<T> reader = client.createReader();

        assertByteMapsEqual(appendsByKeyGroup, extract(handle, reader));
    }
}{code}
You can run FsStateChangelogStorageTest after above modification for reproducing this problem.;;;","24/Mar/22 10:52;roman;Thanks for the investigation [~Feifan Wang] .

Should the test fail with these changes? On my machine, it doesn't (both StateChangelogStorageTest and FsStateChangelogStorageTest).

If it does fail in your environment, do you mind opening a PR with the test change and the fix?;;;","24/Mar/22 10:58;Feifan Wang;FsStateChangelogStorageTest does fail with above changes on StateChangelogStorageTest.

I'am glad to prepare a pr to fix it and enhance the test.;;;","24/Mar/22 11:14;roman;Thanks a lot!
I've assigned the ticket to you.;;;","24/Mar/22 12:05;Feifan Wang;It's my pleasure, [~roman] .;;;","28/Mar/22 09:18;roman;Merged into master as d5e472af4f817d343fae9073aad162ee13f08d6a

into 1.15 as 3fed74d757b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMaster.testJobFailureWhenTaskExecutorHeartbeatTimeout failed due to missing Execution,FLINK-26798,13435068,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Mar/22 07:45,01/Apr/22 07:16,13/Jul/23 08:08,01/Apr/22 07:16,1.14.4,1.15.0,1.16.0,,,,1.14.5,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=897&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=8399] failed due to an {{ExecutionGraphException}} indicating that an expected {{Execution}} wasn't around:
{code}
[...]
Caused by: org.apache.flink.util.FlinkException: Execution 48dbc880c8225256b8bc112ea36e9082 is unexpectedly no longer running on task executor bbad15fcb93d4b2b4f80fe2c35e03e6d.
        at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:250) ~[classes/:?]
        ... 35 more
{code}",,guoyangze,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26741,,,"22/Mar/22 07:49;mapohl;logs-ci_build-test_ci_build_finegrained_resource_management-1647897104.zip;https://issues.apache.org/jira/secure/attachment/13041410/logs-ci_build-test_ci_build_finegrained_resource_management-1647897104.zip","22/Mar/22 07:49;mapohl;test-failure.log;https://issues.apache.org/jira/secure/attachment/13041408/test-failure.log","22/Mar/22 07:49;mapohl;test-success.log;https://issues.apache.org/jira/secure/attachment/13041409/test-success.log",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 01 07:16:28 UTC 2022,,,,,,,,,,"0|z10p8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 07:48;mapohl;This issue was identified while running CI for FLINK-26741 which addresses changes in the shutdown of the {{CheckpointIDCounter}} which runs in when stopping the {{JobMaster}} and, therefore, is quite unlikely to have caused this issue.;;;","22/Mar/22 07:49;mapohl;I attached the build artifacts and the extracted logs of the failed test run (+ the logs of a successful run for comparison).;;;","22/Mar/22 07:59;mapohl;I ran the test locally on commit {{7b41fe3f}} 15000 times without any failure;;;","22/Mar/22 08:16;mapohl;Ok, based on the stacktrace it looks like a heartbeat was actually triggered. Reducing the heartbeatInterval of this method produces the same error;;;","22/Mar/22 08:48;mapohl;Looks like it's caused by [DefaultExecutionDeploymentReconciler:53|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/DefaultExecutionDeploymentReconciler.java#L53]: An {{Execution}} is expected to be deployed to the {{TaskManager}} from the {{JobMaster}}'s side but wasn't reported in the {{TaskManager}}'s heartbeat payload. ...which makes sense considering that the {{TestingHeartbeatService}}'s payload contains an empty list of executions (see [JobMasterTest:1774|https://github.com/apache/flink/blob/1e7d45d53b7ea7b9cfadf2e293ba790f3a9e90c3/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java#L1774]).;;;","01/Apr/22 07:16;mapohl;master: 614e3b281d0c1f6198349aa1457b95ef26c2cd1b
1.15: 5a0f674f85cbbd909e22dd873e9beef79a658faa
1.14: ca3fb6e460bb61bdcbf1f191dd2bf46b74857402;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZKCheckpointIDCounterMultiServersTest.testRecoveredAfterConnectionLoss failed on azure,FLINK-26797,13435066,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,gaoyunhaii,gaoyunhaii,22/Mar/22 07:39,01/Apr/22 16:33,13/Jul/23 08:08,01/Apr/22 16:33,1.16.0,,,,,,1.14.5,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"
{code:java}
Mar 21 07:39:44 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 23.662 s <<< FAILURE! - in org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest
Mar 21 07:39:44 [ERROR] org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.testRecoveredAfterConnectionLoss  Time elapsed: 23.639 s  <<< FAILURE!
Mar 21 07:39:44 java.lang.AssertionError: 
Mar 21 07:39:44 ZooKeeperCheckpointIDCounter doesn't properly work after reconnected.
Mar 21 07:39:44 Expected: is <2L>
Mar 21 07:39:44      but: was <3L>
Mar 21 07:39:44 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Mar 21 07:39:44 	at org.junit.Assert.assertThat(Assert.java:964)
Mar 21 07:39:44 	at org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.testRecoveredAfterConnectionLoss(ZKCheckpointIDCounterMultiServersTest.java:86)
Mar 21 07:39:44 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 21 07:39:44 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 21 07:39:44 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 21 07:39:44 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 21 07:39:44 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 21 07:39:44 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 21 07:39:44 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 21 07:39:44 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 21 07:39:44 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 21 07:39:44 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 21 07:39:44 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 21 07:39:44 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 21 07:39:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 21 07:39:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 21 07:39:44 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Mar 21 07:39:44 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Mar 21 07:39:44 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Mar 21 07:39:44 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Mar 21 07:39:44 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Mar 21 07:39:44 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Mar 21 07:39:44 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33448&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9173",,gaoyunhaii,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26120,,,,,,,,,,"22/Mar/22 11:23;mapohl;FLINK-26797.tar.gz;https://issues.apache.org/jira/secure/attachment/13041416/FLINK-26797.tar.gz",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 01 16:33:49 UTC 2022,,,,,,,,,,"0|z10p80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 11:09;mapohl;It looks like a similar issue to FLINK-26120;;;","01/Apr/22 16:33;mapohl;master: e477686bcb8933e8eb16af83a2c74f61ddfecd6e
1.15: 4f39b034697e811c30835c28fda2aef6ad66ae2a
1.14: 7adbdddff5d3eee00a150d672fe4c6f36e4ad559;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the CI not fail fast after build failed ,FLINK-26795,13435061,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,22/Mar/22 07:15,27/Mar/22 13:16,13/Jul/23 08:08,22/Mar/22 09:47,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"The piping command ignore the truly exit code, so the maven build failed is skipped. see [here|https://stackoverflow.com/questions/6871859/piping-command-output-to-tee-but-also-save-exit-code-of-command]

 ",,aitozi,mbalassi,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/22 07:18;aitozi;image-2022-03-22-15-18-22-234.png;https://issues.apache.org/jira/secure/attachment/13041407/image-2022-03-22-15-18-22-234.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 22 09:47:44 UTC 2022,,,,,,,,,,"0|z10p6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 07:18;aitozi;[https://github.com/Aitozi/flink-kubernetes-operator/runs/5639503008?check_suite_focus=true]

!image-2022-03-22-15-18-22-234.png|width=501,height=169!;;;","22/Mar/22 07:34;wangyang0918;Nice catch.;;;","22/Mar/22 09:47;mbalassi;Fixed via [{{c2a3ba9}}|https://github.com/apache/flink-kubernetes-operator/commit/c2a3ba9b62972633cbc4ba693f0a4c1bcab52477];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRescalingITCase.test failed on azure due to java.nio.file.NoSuchFileException,FLINK-26794,13435060,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,gaoyunhaii,gaoyunhaii,22/Mar/22 07:14,29/Mar/22 16:28,13/Jul/23 08:08,29/Mar/22 16:28,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,test-stability,,"
{code:java}
Mar 21 17:33:56 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.589 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogRescalingITCase
Mar 21 17:33:56 [ERROR] ChangelogRescalingITCase.test  Time elapsed: 8.392 s  <<< ERROR!
Mar 21 17:33:56 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit4908969673123504454/junit6297505939941694356/d832f597d0b0414695fa746ffc400bb2/chk-43
Mar 21 17:33:56 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
Mar 21 17:33:56 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
Mar 21 17:33:56 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Mar 21 17:33:56 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Mar 21 17:33:56 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 21 17:33:56 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Mar 21 17:33:56 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
Mar 21 17:33:56 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Mar 21 17:33:56 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
Mar 21 17:33:56 	at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:582)
Mar 21 17:33:56 	at org.apache.flink.test.util.TestUtils.getMostRecentCompletedCheckpointMaybe(TestUtils.java:114)
Mar 21 17:33:56 	at org.apache.flink.test.state.ChangelogRescalingITCase.checkpointAndCancel(ChangelogRescalingITCase.java:333)
Mar 21 17:33:56 	at org.apache.flink.test.state.ChangelogRescalingITCase.test(ChangelogRescalingITCase.java:156)
Mar 21 17:33:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 21 17:33:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 21 17:33:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 21 17:33:56 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 21 17:33:56 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 21 17:33:56 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 21 17:33:56 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 21 17:33:56 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 21 17:33:56 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Mar 21 17:33:56 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Mar 21 17:33:56 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 21 17:33:56 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 21 17:33:56 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 21 17:33:56 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 21 17:33:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 21 17:33:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)

{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33515&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5643",,gaoyunhaii,mapohl,roman,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 16:28:00 UTC 2022,,,,,,,,,,"0|z10p6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 16:44;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=903&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=13277;;;","23/Mar/22 07:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33590&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=6384;;;","24/Mar/22 08:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33583&view=logs&j=38d6b56a-d502-56fb-7b73-c09f8fe7becd&t=6e6509fa-8a5d-5a6c-e17e-64f5ecc17842&l=12451;;;","24/Mar/22 09:44;ym;[~roman] would you please take a look;;;","24/Mar/22 10:55;roman;Thanks for pulling me in [~ym] , I'll take a look.;;;","29/Mar/22 07:20;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33802&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12450;;;","29/Mar/22 07:34;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33854&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5639;;;","29/Mar/22 16:28;roman;Merged into master as 69665e40b83f6f4a6f7b82cd0b8b236624338ac0,

into  release-1.15 as 4e87586dfb0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState failed,FLINK-26789,13434968,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,mapohl,mapohl,21/Mar/22 17:46,28/Mar/22 09:41,13/Jul/23 08:08,28/Mar/22 08:54,1.16.0,,,,,,1.16.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=894&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=5687] failed due to {{RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState}}:
{code}
Mar 21 17:05:32 [ERROR] org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState  Time elapsed: 23.966 s  <<< FAILURE!
Mar 21 17:05:32 java.lang.AssertionError: expected:<[(0,24000), (1,22500), (0,34500), (1,33000), (0,21000), (0,45000), (2,31500), (2,42000), (1,6000), (0,28500), (0,52500), (2,15000), (1,3000), (1,51000), (0,1500), (0,49500), (2,12000), (2,60000), (0,36000), (1,10500), (1,58500), (0,46500), (0,9000), (0,57000), (2,19500), (2,43500), (1,7500), (1,55500), (2,30000), (1,18000), (0,54000), (2,40500), (1,4500), (0,16500), (2,27000), (1,39000), (2,13500), (1,25500), (0,37500), (0,61500), (2,0), (2,48000)]> but was:<[(1,22500), (1,33000), (0,21000), (2,18000), (1,6000), (0,20500), (0,52500), (0,15000), (0,31000), (2,12000), (2,60000), (0,36000), (1,58500), (1,10500), (0,46500), (0,25000), (0,41000), (0,9000), (0,57000), (2,43500), (0,30000), (1,4500), (2,27000), (1,15000), (0,35000), (0,19000), (0,3000), (1,25500), (0,61500), (2,48000), (2,0), (0,24000), (0,34500), (0,45000), (2,31500), (1,19500), (2,10000), (2,42000), (0,12500), (0,28500), (2,15000), (1,3000), (1,51000), (0,23000), (0,49500), (0,1500), (0,33000), (0,1000), (2,19500), (1,7500), (1,55500), (2,30000), (1,18000), (0,6000), (0,38000), (0,54000), (2,40500), (0,500), (0,16500), (1,39000), (1,7000), (0,11000), (2,13500), (0,37500)]>
Mar 21 17:05:32 	at org.junit.Assert.fail(Assert.java:89)
Mar 21 17:05:32 	at org.junit.Assert.failNotEquals(Assert.java:835)
Mar 21 17:05:32 	at org.junit.Assert.assertEquals(Assert.java:120)
Mar 21 17:05:32 	at org.junit.Assert.assertEquals(Assert.java:146)
Mar 21 17:05:32 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.restoreAndAssert(RescaleCheckpointManuallyITCase.java:218)
Mar 21 17:05:32 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingKeyedState(RescaleCheckpointManuallyITCase.java:122)
Mar 21 17:05:32 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState(RescaleCheckpointManuallyITCase.java:88)
{code}",,gaoyunhaii,guoyangze,mapohl,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21321,,,,,,,FLINK-26882,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 28 09:41:31 UTC 2022,,,,,,,,,,"0|z10omg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 07:22;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33487&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5685;;;","22/Mar/22 07:22;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33487&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12409;;;","22/Mar/22 07:52;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=895&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=5687;;;","22/Mar/22 13:16;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=900&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=12471;;;","22/Mar/22 16:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33582&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5681;;;","22/Mar/22 16:39;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=901&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=13079;;;","22/Mar/22 16:42;mapohl;[~yunta] This test seems to fail more often. It looks like it was added by FLINK-21321 which was recently merged.;;;","23/Mar/22 07:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33590&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=6382;;;","23/Mar/22 08:35;yunta;[~mapohl] Thanks for your reminder, I will take a look.;;;","23/Mar/22 10:05;yunta;I tried to reproduce this problem with changelog, unaligned checkpoint and buffer deblaoting enabled. However, running the test alone would not come across such problem. After discussion with [~Yanfei Lei], we think this might be caused by {{RescaleCheckpointManuallyITCase}} and {{RescalingITCase}} sharing same static {{elements}} for verify.

We can fix this test soon.;;;","24/Mar/22 07:05;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33671&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5683
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33671&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12502
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33671&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12490
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33671&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=12931;;;","24/Mar/22 07:07;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33660&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5685;;;","24/Mar/22 07:16;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33654&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5685
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33654&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12502
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33637&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5685
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33637&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12414
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33636&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12406

And so on;;;","24/Mar/22 13:40;yunta;merged in master: c1bd957be3fe45df80602eab78f2980361df22cf;;;","26/Mar/22 23:16;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33776&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5623]

 

The build above used 84723eea4e7ddae846092ca8bb0905a7b9d6dc6a, i.e. included c1bd957be3fe45df80602eab78f2980361df22cf.

 

Reopening;;;","28/Mar/22 03:33;yunta;[~roman], I checked the logs in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33776&view=artifacts&pathAsName=false&type=publishedArtifacts and figured out that the broken test is caused by {{execution.checkpointing.unaligned: true}} and {{execution.checkpointing.alignment-timeout: PT0S}}. In other words, I think the new broken instance has no direct relationship with RocksDB incremental checkpoint rescale itself but with the unaligned checkpoint. You can reproduce this easily in local setup environment.;;;","28/Mar/22 06:40;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33787&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5626
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33787&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12409;;;","28/Mar/22 06:49;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5629
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12409
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=5733
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=12575
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=5838
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=12931
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33779&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5682;;;","28/Mar/22 06:56;roman;Thanks [~yunta] for looking into it.

[~pnowojski] could you please analyze it further?;;;","28/Mar/22 08:42;guoyangze;Can we ignore this test atm as it is quite unstable? ;;;","28/Mar/22 08:54;yunta;I have created another ticket https://issues.apache.org/jira/browse/FLINK-26789 to focus this new problem. Let's discuss there.
cc [~guoyangze], [~roman], [~gaoyunhaii], [~pnowojski];;;","28/Mar/22 09:22;roman;I guess the correct link is https://issues.apache.org/jira/browse/FLINK-26882;;;","28/Mar/22 09:41;yunta;[~roman] Yes, it's my fault to re-paste this link again :(;;;",,,,,,,,,,,,,,,,,,,,,,
Restore from a stop-with-savepoint if failed during committing,FLINK-26783,13434915,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,21/Mar/22 13:33,12/Apr/22 11:13,13/Jul/23 08:08,28/Mar/22 15:13,1.15.0,,,,,,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,We decided stop-with-savepoint should commit side-effects and thus we should fail over to those savepoints if a failure happens when committing side effects.,,dwysakowicz,Jiangang,klion26,pnowojski,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26683,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 28 15:13:12 UTC 2022,,,,,,,,,,"0|z10oao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/22 08:38;dwysakowicz;After an offline discussion we said that simply adding the savepoint to the {{CompletedCheckpointStore}} poses a problem for the savepoint ownership, as after a restart the savepoint will remain in the `CompletedCheckpointStore` and Flink will depend on its existence.

Therefore we propose a different approach to solve the issue that if we fallback to a checkpoint we might end up with duplicated records. We suggest to already not trigger a global failover in case the savepoint completed successfully, but the job failed during committing side effects. In that case we will finish the completable future with an exception that explains that the savepoint is consistent, but it might have uncommitted side effects and ask users to manually restart a job from that savepoint if they want to commit side effects.;;;","28/Mar/22 15:13;dwysakowicz;Fixed in
* master
** f21b551f4fc14900e473c88af662db25292e3b8c
* 1.15.0
** 0be2a28d1d1eae03a8cf9d64ab3e7d68f5d87b64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StopWithSavepoint fails when called from standby JM,FLINK-26779,13434908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,21/Mar/22 13:21,22/Mar/22 08:21,13/Jul/23 08:08,22/Mar/22 08:21,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,OperationKey and sub-classes are not serializable.,,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26780,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 22 08:21:35 UTC 2022,,,,,,,,,,"0|z10o94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 08:21;chesnay;master: 07150c47ffb4b9bcfc65ab5fa6ee894d401fdfb9
1.15: cbdf95ed11f95d07c09eddb2648038e0503a5286 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink WindowOperator#process_element register wrong cleanup timer,FLINK-26775,13434883,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,21/Mar/22 11:18,22/Mar/22 07:19,13/Jul/23 08:08,22/Mar/22 07:19,1.15.0,,,,,,1.13.7,1.14.5,1.15.0,,API / Python,,,,,0,pull-request-available,,,"In window_operator.py line 378, when dealing with merging window assigner:
{code:python}
self.register_cleanup_timer(window)
{code}
This should be registering a cleanup timer for `actual_window`, but won't causing window emitting bugs when session window trigger is implemented correctly.",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 22 07:19:43 UTC 2022,,,,,,,,,,"0|z10o3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 07:19;dianfu;Fixed in:
- master via 14d32f628e004f5012189de0626302fe6cb32f18
- release-1.15 via 065ddea6320fc25406d3e78c62844c09b578ca82
- release-1.14 via a0dd7aac8128a6a84fb712c820c1233c66c7a6c1
- release-1.13 via 0a89a59065107e0fb8bb7301e8fe82cc5292396f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nullable ArrayData should not be Object[],FLINK-26770,13434854,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,wenlong.lwl,wenlong.lwl,21/Mar/22 09:11,23/Mar/22 13:04,13/Jul/23 08:08,23/Mar/22 13:04,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,"sql:
         
""INSERT INTO %s ""
                    + "" (a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r) values (""
                    + ""1,'dim',cast(20.2007 as double),false,652482,cast('2020-07-08' as date),'source_test',cast('2020-07-10 16:28:07.737' as timestamp),""
                    + ""cast(8.58965 as float),cast(ARRAY [464,98661,32489] as array<int>),cast(ARRAY [8589934592,8589934593,8589934594] as array<bigint>),""
                    + ""ARRAY[cast(8.58967 as float),cast(96.4667 as float),cast(9345.16 as float)], ARRAY [cast(587897.4646746 as double),cast(792343.646446 as double),cast(76.46464 as double)],""
                    + ""cast(ARRAY [true,true,false,true] as array<boolean>),cast(ARRAY ['monday','saturday','sunday'] as array<STRING>),true,cast(8119.21 as numeric(6,2)), cast('2020-07-10 16:28:07.737' as timestamp)""
                    + "")"";

error:

Caused by: java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Ljava.lang.Integer;
	at org.apache.flink.table.data.GenericArrayData.toIntArray(GenericArrayData.java:297) ~[flink-table-common]
	

related codegen result:

          objArray$81 = new Object[result$76.size()];
          for ( i$82 = 0; i$82 < result$76.size(); i$82++) {
          if (!result$76.isNullAt(i$82)) {
          objArray$81[i$82] = result$76.getBoolean(i$82);

cause:
          ArrayToArrayCastRule#arrayElementType use Object when a column is nullable, but GenericArrayData only accepts array with specific
 types, like Integer[], I think we should follow CodeGenUtils#boxedTypeTermForType 

[~slinkydeveloper]
",,twalthr,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 13:04:08 UTC 2022,,,,,,,,,,"0|z10nx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 13:04;twalthr;Fixed in master: 7f3fa549638cb2e7b9ddf53366bf744f1f5c6568
Fixed in 1.15: d1445f115473efd389a2eadd8ec756cbcdfbde47;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reconciliationStatus.error could not be cleared properly,FLINK-26768,13434847,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,21/Mar/22 08:45,27/Mar/22 13:16,13/Jul/23 08:08,23/Mar/22 13:07,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"When testing the flink-kubernetes-operator, I find the {{reconciliationStatus.error}} could not be cleared properly. Even though, the job is running normally. It seems that {{reconciliationStatus.error}} could not be updated without spec change.
{code:java}
status:
  jobManagerDeploymentStatus: READY
  jobStatus:
    jobId: ""00000000000000000000000000000000""
    jobName: State machine job
    savepointInfo: {}
    startTime: ""1647851557367""
    state: RUNNING
    updateTime: ""1647851870081""
  reconciliationStatus:
    error: back-off 5m0s restarting failed container=flink-main-container pod=flink-example-statemachine-6f76c78cd4-dvwnr_default(5ae2e615-8348-4536-8f37-05adf551bd53)
    lastReconciledSpec:
      flinkConfiguration:
        high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
        high-availability.storageDir: file:///opt/flink/volume/flink-ha
        state.checkpoints.dir: file:///opt/flink/volume/flink-cp
        state.savepoints.dir: file:///opt/flink/volume/flink-sp
        taskmanager.numberOfTaskSlots: ""2""
      flinkVersion: v1_14
      image: flink:1.14.3 {code}",,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 13:07:13 UTC 2022,,,,,,,,,,"0|z10nvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 15:16;gyfora;I think we need to make sure that successful observation clears the errors. I think what happened here is that the operator successfully submitted the job then there was an error during observe stages later.

But since no further reconciliation occured we never cleared the errors.;;;","22/Mar/22 06:54;wangyang0918;Yes. We need to clear the errors after successful observation.;;;","22/Mar/22 07:29;wangyang0918;I will work on this ticket soon.;;;","23/Mar/22 13:07;gyfora;merged to main: 0b44a9a5695c0658224c972611c42e14d1e89d6a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ChangelogStateHandleStreamImpl#getIntersection,FLINK-26766,13434842,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,21/Mar/22 08:23,23/Mar/22 07:01,13/Jul/23 08:08,23/Mar/22 07:01,1.15.0,1.16.0,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,," 

Maybe mistake in ChangelogStateHandleStreamImpl :
{code:java}
public KeyedStateHandle getIntersection(KeyGroupRange keyGroupRange) {
    KeyGroupRange offsets = keyGroupRange.getIntersection(keyGroupRange);
    // ......
} {code}
I guess should be :

KeyGroupRange offsets = {color:#de350b}this{color}.keyGroupRange.getIntersection(keyGroupRange);

 

Hi [~roman] , can you help confirm that ?

 ",,Feifan Wang,roman,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 06:43:29 UTC 2022,,,,,,,,,,"0|z10nug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 08:48;roman;Hi [~Feifan Wang], you are right, thanks for noticing this.
I think the impact is limited to inefficient rescaling (handles are sent unnecessarily); because of the [check|https://github.com/apache/flink/blob/b643feedc88e660df7c333870454763f011d3f01/flink-state-backends/flink-statebackend-changelog/src/main/java/org/apache/flink/state/changelog/restore/ChangelogBackendLogApplier.java#L95] on TM, it shouldn't affect correctness.;;;","21/Mar/22 09:15;ym;[~Feifan Wang] I think you are right.

 

Would you mind preparing a fix/pr for this?;;;","21/Mar/22 09:30;Feifan Wang;Thinks for reply [~roman] , since not knowing only affects rescaling efficiency, I set a higher priority. Major is OK.

 

Hi, [~ym] , I glad to do that.;;;","21/Mar/22 10:27;Feifan Wang;Hi [~ym] , I open a [pull request|https://github.com/apache/flink/pull/19183], but I'm not sure is worth or not for adding a unit test , what do you think about ?;;;","21/Mar/22 11:03;ym;It is fine without UT in this case.;;;","23/Mar/22 06:43;ym;merged commit [{{2b8b660}}|https://github.com/apache/flink/commit/2b8b66002ff17be8a02a9f0612b1a310ea8bf3e7] into apache:master

merged commit [{{778a15c}}|https://github.com/apache/flink/commit/778a15c8e1cda342ea39ad373334bbe9ffca0a5e] into apache:release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to deserialize for match recognize,FLINK-26756,13434789,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,21/Mar/22 03:32,13/Apr/22 13:17,13/Jul/23 08:08,13/Apr/22 13:17,1.15.0,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"Currently, the json deserialization logic is not tested, there de is a bug in {{JsonPlanTestBase}}#{{compileSqlAndExecutePlan}} method. The correct logic is the {{CompiledPlan}} should be converted to json string, and then the json string be deserialized to  {{CompiledPlan}} object. 

After correcting the logic, {{MatchRecognizeJsonPlanITCase}} will get the following exception:


{code:java}
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.loadPlan(TableEnvironmentImpl.java:714)
	at org.apache.flink.table.planner.utils.JsonPlanTestBase.compileSqlAndExecutePlan(JsonPlanTestBase.java:77)
	at org.apache.flink.table.planner.runtime.stream.jsonplan.MatchRecognizeJsonPlanITCase.testSimpleMatch(MatchRecognizeJsonPlanITCase.java:66)
{code}

 ",,godfreyhe,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 13 13:17:17 UTC 2022,,,,,,,,,,"0|z10nio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 03:35;godfreyhe;cc [~twalthr];;;","13/Apr/22 13:17;godfreyhe;Fixed in
master: f8d2bdb19eb954ef384a78b5e21991a4327c23db
1.15.0: 1556adf753d11a9e3548cb7e6b73160a0721bdcb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointIDCounter.shutdown should expose errors asynchronously,FLINK-26741,13434692,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,19/Mar/22 15:24,24/Mar/22 09:08,13/Jul/23 08:08,24/Mar/22 09:08,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"{{CheckpointIDCounter.shutdown}} should return a {{CompletableFuture}} exposing errors if anything happens to enable retrying of the cleanup.

All implementations should also work in an idempotent fashion, i.e. nothing should happen if the artifact was already deleted.

This bug is not considered a 1.15 blocker because unit tests rerunning the shutdown worked properly already, i.e. in the worst case (of an error) we would just miss to clean up right now.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26798,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 24 09:08:22 UTC 2022,,,,,,,,,,"0|z10mx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/22 09:08;mapohl;master: 8e7266be34f2373442a1bd4824911d6d8bb67f04
1.15: 34133b9d040d22f76710b8da987ad848f63063d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default value of StateDescriptor is valid when enable state ttl config,FLINK-26738,13434675,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lam167,lam167,lam167,19/Mar/22 11:12,02/Apr/22 08:22,13/Jul/23 08:08,02/Apr/22 08:22,1.15.0,,,,,,1.14.5,1.15.0,1.16.0,,API / Core,,,,,0,pull-request-available,,,"Suppose we declare a ValueState like following:

{code:java}
ValueStateDescriptor<Tuple2<Long, Long>> descriptor =
                new ValueStateDescriptor<>(
                        ""average"", // the state name
                        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}),  
                        Tuple2.of(0L, 0L)); 
{code}

and then we add state ttl config to the state:

{code:java}
descriptor.enableTimeToLive(StateTtlConfigUtil.createTtlConfig(60000));
{code}

the default value Tuple2.of(0L, 0L) will be invalid and may cause NPE.

I don't know if this is a bug cause I see @Deprecated in the comment of the ValueStateDescriptor constructor with argument defaultValue:

{code:java}
Use {@link #ValueStateDescriptor(String, TypeSerializer)} instead and manually
     *     manage the default value by checking whether the contents of the state is {@code null}.
{code}

and if we decide not to use the defaultValue field in the class StateDescriptor, should we add @Deprecated annotation to the field defaultValue?


",,lam167,mayuehappy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 02 08:22:52 UTC 2022,,,,,,,,,,"0|z10mtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 03:44;yunta;[~lam167], what kind of NPE will you meet?

Once the state descriptor enables TTL, it will always create another internl state descriptor and make the default value as null (you can refer to [code|https://github.com/apache/flink/blob/1bf45b25791cc3fad8b7d0d863caa9b0eef9a87b/flink-runtime/src/main/java/org/apache/flink/runtime/state/ttl/TtlStateFactory.java#L140-L148]) . I'm not sure what kind of error will you meet.;;;","22/Mar/22 07:08;lam167;""it will always create another internl state descriptor and make the default value as null "", that's the point, the default value becomes null.
For example, when I try to execute the following code :

{code:java}
ValueStateDescriptor<Tuple2<Long, Long>> descriptor =
                new ValueStateDescriptor<>(
                        ""average"", // the state name
                        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}),  
                        Tuple2.of(0L, 0L)); 
descriptor.enableTimeToLive(StateTtlConfigUtil.createTtlConfig(60000));
sum = getRuntimeContext().getState(descriptor);
{code}

the sum will be null but not Tuple2.of(0L, 0L), and when I try to invoke sum.f0, it will cause NPE.
I tried to copy the default value to the code you referred to, but I saw that the constructor with default value was marked as deprecated. I do not know if flink would deprecate default value field in StateDescriptor or just deprecated the constructor in the subclass of StateDescriptor, like ValueStateDescriptor etc.
;;;","23/Mar/22 08:31;yunta;[~lam167], first of all, the returned {{sum}} is a value state not some internal value from your code. I don't know why it will cause NPE.

For TTL state, the semantics would be weird if we have a default value. We treat the data expired and return null and then let the user to decide whether to replace as default value.;;;","23/Mar/22 16:37;lam167;sorry, I forget to add the ValueState#value() method, 

{code:java}
ValueStateDescriptor<Tuple2<Long, Long>> descriptor =
                new ValueStateDescriptor<>(
                        ""average"", // the state name
                        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}),  
                        Tuple2.of(0L, 0L)); 
descriptor.enableTimeToLive(StateTtlConfigUtil.createTtlConfig(60000));
sum = getRuntimeContext().getState(descriptor).value();
{code}

now maybe you can see it would cause NPE.
;;;","23/Mar/22 16:48;lam167;And I think this is an issue worth discussing, from my personal point of view, if the default value is valid for TTL state, there's no difference between having the user set a default value to the state descriptor and replacing the expired state with a default value in the code manually. Even if a user sets the default value, they can also replace it with a new value, it's no conflict and would make the concept of default value clearer.
And one more question, why we add @Deprecated annotation to the ValueDescriptor constructor, would we remove the default value in class StateDescriptor in the future?;;;","24/Mar/22 06:18;yunta;[~lam167], the reason why Flink community mark the default value in state descriptor as deprecated is that user cannot judge whether the returned default value is for the real answer or just null.

This is also true for TTL state with default value, if we get the default value from TTL state, we cannot judge whether the key is expired, not existed or just the value is default value itself, which make the semantics unclear.
From my point of view, disabling default value for TTL state is reasonable and maybe we need to make the truth more clear in docs and javadocs. WDYT?;;;","24/Mar/22 06:51;lam167;[~yunta], thanks for your explanation, I think it's a good idea to make it clearer in docs and javadocs, and should we mark org.apache.flink.api.common.state.StateDescriptor#default(https://github.com/apache/flink/blob/7d7a111eba368043f8624e114daa29400a74c096/flink-core/src/main/java/org/apache/flink/api/common/state/StateDescriptor.java#L107) as deprecated too, not only mark org.apache.flink.api.common.state.ValueStateDescriptor constructor(https://github.com/apache/flink/blob/7d7a111eba368043f8624e114daa29400a74c096/flink-core/src/main/java/org/apache/flink/api/common/state/ValueStateDescriptor.java#L70);;;","24/Mar/22 08:34;yunta;[~lam167] I agree, would you like to take this ticket to make this semantics more clear in the code and docs? I can assign this ticket to you if possible.;;;","24/Mar/22 08:55;lam167;[~yunta], thanks, I'd like to try it~;;;","24/Mar/22 09:57;yunta;[~lam167], already assigned to you and downgrade the priority to major.;;;","24/Mar/22 13:43;lam167;[~yunta], this is the pr: https://github.com/apache/flink/pull/19229, thank you for taking the time to review.;;;","28/Mar/22 09:04;lam167;[~yunta], resolved conflict, please check it agian.;;;","02/Apr/22 08:22;yunta;merged in master: 5045f1f68687785b472536bfa041b09e9e896816
release-1.15: b3428b345a230446d7a502ef5e6827a2ab826d4e
release-1.14: d706b8169faa77dd126d923db326709dc5631a37
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the implementation of sub-interpreter in Thread Mode,FLINK-26727,13434528,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Mar/22 08:56,23/Mar/22 01:22,13/Jul/23 08:08,21/Mar/22 08:46,1.15.0,,,,,,1.15.0,,,,API / Python,,,,,0,pull-request-available,,,,,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 08:46:42 UTC 2022,,,,,,,,,,"0|z10lww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 08:46;hxbks2ks;Merged into master via b643feedc88e660df7c333870454763f011d3f01
Merged into release-1.15 via 1f60be8a39b4130de7e0e588d856623858d7b57e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the unregistered  task from readersAwaitingSplit,FLINK-26726,13434527,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zoucao,zoucao,zoucao,18/Mar/22 08:48,14/Oct/22 02:28,13/Jul/23 08:08,14/Oct/22 02:28,,,,,,,1.14.7,1.15.3,1.16.0,1.17.0,Connectors / Hive,Table SQL / Ecosystem,,,,0,pull-request-available,stale-assigned,,"Recently, we faced a problem caused by the unregistered task when using the hive table as a source to do streaming reading. 
I think the problem is that we do not remove the unregistered  task from `readersAwaitingSplit` in `ContinuousHiveSplitEnumerator` and `ContinuousFileSplitEnumerator`.

Assuming that we have two tasks 0 and 1, they all exist in `readersAwaitingSplit`,  if there does not exist any new file in the path for a long time. Then, a new split is generated, and it is assigned to task-1. Unfortunately, task-1 can not consume the split successfully, and the exception will be thrown and cause all tasks to restart. The failover will not affect the `readersAwaitingSplit`, but it will clear the `SourceCoordinatorContext#registeredReaders`.
After restarting, task-0 exists in `readersAwaitingSplit` but not in `registeredReaders`. if task-1 register first and send the request to get split, the SplitEnumerator will assign splits for both task-1 and task-0, but task-0 has not been registered.


The stack exists in the attachment.




",,hackergin,jark,lzljs3620320,Thesharing,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/22 08:48;zoucao;stack.txt;https://issues.apache.org/jira/secure/attachment/13041308/stack.txt",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 12 12:20:49 UTC 2022,,,,,,,,,,"0|z10lwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 02:40;zoucao;Gentle ping [~lzljs3620320], could you help me to confirm it? The exception message exists in the attachment, and plz correct me in time if i missed something.;;;","21/Mar/22 05:45;lzljs3620320;I think your analysis is correct. And the {{{}ContinuousFileSplitEnumerator is the correct one to deal with this behavior{}}}:
{code:java}
// if the reader that requested another split has failed in the meantime, remove
// it from the list of waiting readers
if (!context.registeredReaders().containsKey(nextAwaiting.getKey())) {
    awaitingReader.remove();
    continue;
} {code};;;","21/Mar/22 05:47;lzljs3620320;FLINK-20261 is the Jira to fix this bug in ContinuousFileSplitEnumerator;;;","21/Mar/22 06:22;zoucao;Hi, [~lzljs3620320] , It is really fixed in  `ContinuousFileSplitEnumerator`, the repair maybe lost in `ContinuousHiveSplitEnumerator` ? ;;;","21/Mar/22 13:15;lzljs3620320;[~zoucao] Yes, indeed.;;;","21/Mar/22 13:16;lzljs3620320;[~zoucao] I have assigned to you, If you don't have time to fix this, please let me know;;;","21/Mar/22 14:35;zoucao;Hi, [~lzljs3620320] , I will fix it as soon as possiable.;;;","06/Jul/22 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","12/Oct/22 12:20;jark;Fixed in 
 - master: ab9e5844703848e79b2a62abae757bc6bd2268d9
 - release-1.16: 9935b0c56b646b301dbd4d11a0838aacfeb5430f
 - release-1.15: a826fe8d501ed8bdd9cbdc5febf7aac4cfa0b947
 - release-1.14: 81d2c1b340f3a4d063e97db2519d6911028d807d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimestampsAndWatermarksOperator should not propagate WatermarkStatus,FLINK-26708,13434368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,17/Mar/22 13:17,29/Mar/22 11:30,13/Jul/23 08:08,28/Mar/22 08:21,1.14.4,1.15.0,,,,,1.14.5,1.15.0,,,API / DataStream,,,,,0,pull-request-available,,,The lifecycle/scope of WatermarkStatus is tightly coupled with watermarks. Upstream watermarks are cut off in the TimestampsAndWatermarksOperator and therefore watermark statuses should be cut off as well.,,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 11:29:51 UTC 2022,,,,,,,,,,"0|z10kxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/22 08:21;dwysakowicz;Fixed in:
* master
** d87808fe8a2fe6538b902056490395ae8597a48b
* 1.15.0
** f858421d67a27c155accdd7da9117c29541f8939
* 1.14.5
** df0363de3e5bbd284046600bf713df17741a39f9
;;;","29/Mar/22 11:04;chesnay;I reverted the changes on the 1.13 branch as the WatermarkStatus doesn't exist on that branch and it was breaking the build.;;;","29/Mar/22 11:29;dwysakowicz;Sorry, about that. I shouldn't have backported it to 1.13, as required changes where added only in 1.14: [FLINK-18934];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sporadic failures in JobObserverTest,FLINK-26702,13434319,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mbalassi,mbalassi,17/Mar/22 09:46,27/Mar/22 13:16,13/Jul/23 08:08,18/Mar/22 10:14,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,,,,"I have occasionally observed the following failure during the regular build:

 
{code:java}
mvn clean install

...
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.244 s - in org.apache.flink.kubernetes.operator.service.FlinkServiceTest
[INFO] Running org.apache.flink.kubernetes.operator.validation.DeploymentValidatorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 s - in org.apache.flink.kubernetes.operator.validation.DeploymentValidatorTest
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   JobObserverTest.observeApplicationCluster:99 expected: <0> but was: <1>
[INFO] 
[ERROR] Tests run: 34, Failures: 1, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Flink Kubernetes: .................................. SUCCESS [  4.223 s]
[INFO] Flink Kubernetes Shaded ............................ SUCCESS [  5.097 s]
[INFO] Flink Kubernetes Operator .......................... FAILURE [ 34.596 s]
[INFO] Flink Kubernetes Webhook ........................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 44.065 s
[INFO] Finished at: 2022-03-17T09:43:22+01:00
[INFO] Final Memory: 160M/554M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M4:test (default-test) on project flink-kubernetes-operator: There are test failures.
[ERROR] 
[ERROR] Please refer to /Users/mbalassi/git/apple/apache-flink-kubernetes-operator/flink-kubernetes-operator/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]{code}
I do not have a reliable way of reproducing this, however I have also seen CI failures recently that could be do to this (unfortunately the log was truncated):
https://github.com/apache/flink-kubernetes-operator/runs/5582743074?check_suite_focus=true",,bgeng777,mbalassi,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 18 10:13:41 UTC 2022,,,,,,,,,,"0|z10kmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 10:01;bgeng777;Hi [~mbalassi] you are not alone. I have also seen this failure in my openning PR's CI in my own github in the morning. I am not so sure why my CI is triggered. But I happened to set the build config of in my own CI to show more info for other issues and I got the full error messgae of this failure:
{quote}[INFO] Running org.apache.flink.kubernetes.operator.observer.JobObserverTest
2022-03-17 03:23:31,091 o.a.f.k.o.o.JobObserver [INFO ] [.] Getting job statuses for test-cluster
2022-03-17 03:23:31,091 o.a.f.k.o.o.JobObserver [INFO ] [.] Job statuses updated for test-cluster
2022-03-17 03:23:31,091 o.a.f.k.o.o.JobObserver [INFO ] [.] Getting job statuses for test-cluster
2022-03-17 03:23:31,091 o.a.f.k.o.o.JobObserver [INFO ] [.] Job statuses updated for test-cluster
2022-03-17 03:23:31,091 o.a.f.k.o.o.JobObserver [INFO ] [.] Getting job statuses for test-cluster
2022-03-17 03:23:31,091 o.a.f.k.o.o.JobObserver [INFO ] [.] Job statuses updated for test-cluster
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] JobManager deployment test-cluster in namespace flink-operator-test exists but not ready, status DeploymentStatus(availableReplicas=1, collisionCount=null, conditions=[], observedGeneration=null, readyReplicas=null, replicas=1, unavailableReplicas=null, updatedReplicas=null, additionalProperties={})
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] JobManager deployment test-cluster in namespace flink-operator-test exists but not ready, status DeploymentStatus(availableReplicas=1, collisionCount=null, conditions=[], observedGeneration=null, readyReplicas=null, replicas=1, unavailableReplicas=null, updatedReplicas=null, additionalProperties={})
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] JobManager deployment test-cluster in namespace flink-operator-test port ready, waiting for the REST API...
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] Getting job statuses for test-cluster
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] Job statuses updated for test-cluster
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] Getting job statuses for test-cluster
2022-03-17 03:23:31,093 o.a.f.k.o.o.JobObserver [INFO ] [.] Job statuses updated for test-cluster
Error: Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.039 s <<< FAILURE! - in org.apache.flink.kubernetes.operator.observer.JobObserverTest
Error: org.apache.flink.kubernetes.operator.observer.JobObserverTest.observeApplicationCluster Time elapsed: 0.013 s <<< FAILURE!
org.opentest4j.AssertionFailedError: expected: <0> but was: <1>
at org.apache.flink.kubernetes.operator.observer.JobObserverTest.observeApplicationCluster(JobObserverTest.java:99)
{quote}
 ;;;","18/Mar/22 10:13;nicholasjiang;[~mbalassi], this problem has been solved in the PR of FLINK-26714. Please close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Relocation of connector-base might break user jars due to changed imports,FLINK-26701,13434303,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,afedulov,fpaul,fpaul,17/Mar/22 08:34,23/Mar/22 09:16,13/Jul/23 08:08,23/Mar/22 09:16,1.15.0,,,,,,1.15.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"With the introduction of FLINK-25927, every connector now relocates connector-base to better support connectors compatibility with different Flink versions. Unfortunately, not all classes in connector-base are only used by connector but some are supposed to be used inside the user jar directly i.e. DeliveryGuarantee, HybridSource...

Since the connector now relocates the module the existing imports are broken.",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 09:16:04 UTC 2022,,,,,,,,,,"0|z10kiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/22 09:16;arvid;Merged into 1.15 as 1b04ba84222dd44cce9b3241e7a939275557478a, into master as 62defcae6bfe325265c7386542cf44ee179f8385.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystemJobResultStore#constructDirtyPath might lost the scheme,FLINK-26698,13434297,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,wangyang0918,wangyang0918,17/Mar/22 07:50,18/Mar/22 14:53,13/Jul/23 08:08,18/Mar/22 06:27,1.15.0,1.16.0,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,," 
{code:java}
/**
     * Given a job ID, construct the path for a dirty entry corresponding to it in the job result
     * store.
     *
     * @param jobId The job ID to construct a dirty entry path from.
     * @return A path for a dirty entry for the given the Job ID.
     */
    private Path constructDirtyPath(JobID jobId) {
        return new Path(this.basePath.getPath(), jobId.toString() + DIRTY_FILE_EXTENSION);
    } {code}
 

Just like above piece of code, we are using {{{}this.basePath.getPath(){}}}, not directly use {{this.basePath}} when create a new Path. I am afraid this will cause scheme lost and cause issue when some filesystem implementation tries to stat the path.",,mapohl,wangyang0918,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 18 06:27:42 UTC 2022,,,,,,,,,,"0|z10khk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 07:51;wangyang0918;cc [~mapohl] ;;;","17/Mar/22 08:09;yunta;+1, for path like ""dfs://schema/path/to"", #getPath() would return ""/path/to"" which will drop the schema.;;;","18/Mar/22 06:27;mapohl;master: 25cb71e1528a2ecf8adcff6e651be1045a41a874
1.15: 265788bf7a6f12f5c6b417326cb60437a5f313d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fabric8FlinkKubeClient.deleteConfigMap and deleteConfigMapsByLabels don't necessarily fail if the deletion didn't succeed,FLINK-26695,13434269,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,17/Mar/22 05:48,24/Mar/22 08:44,13/Jul/23 08:08,24/Mar/22 08:44,1.16.0,,,,,,1.16.0,,,,Deployment / Kubernetes,Runtime / Coordination,,,,0,pull-request-available,,,"{{Fabric8FlinkKubeClient.deleteConfigMap}} and {{deleteConfigMapsByLabels}} do not evaluate the return value and, therefore, might miss an error during deletion.",,mapohl,nicholasjiang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 24 08:44:10 UTC 2022,,,,,,,,,,"0|z10kbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 06:59;nicholasjiang;[~mapohl], does the Fabric8FlinkKubeClient.deleteConfigMapsByLabels have the same problem?;;;","17/Mar/22 07:05;wangyang0918;If the ConfigMap deletion failed, I think the k8s client will throw an exception. So why could we miss an error?;;;","17/Mar/22 07:55;mapohl;[~wangyang0918] you might be right. The fabric8 JavaDoc stays kind of vague on the return value, though. See [Deletable.java:25|https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/dsl/Deletable.java#L25]. We might want to harden our implementation in that regard to be sure that the ConfigMap is actually non-existent anymore. WDYT?;;;","17/Mar/22 07:58;mapohl;{quote}Matthias Pohl, does the Fabric8FlinkKubeClient.deleteConfigMapsByLabels have the same problem?{quote}

Yeah, the same applies to the deleteConfigMapsByLabels. Thanks for the pointer. I'm gonna update the issue description;;;","17/Mar/22 07:59;wangyang0918;Yes. We could harden the current implementation by checking the existence of deleted ConfigMaps. And maybe we also need to retry the deletion.;;;","17/Mar/22 08:03;mapohl;The exception will be propagated to the calling methods, which would be the Job-related cleanup and the cluster cleanup in HaServices. The former one will be covered by the retry logic implemented in the Dispatcher. The latter one will cause a fatal error in the shutdown of the cluster which is still desired, I guess.;;;","17/Mar/22 08:10;nicholasjiang;[~wangyang0918], if the retries of the deletion ConfigMaps have still exception, what's the behavior for shutdown of the cluster?;;;","17/Mar/22 08:35;wangyang0918;Just like [~mapohl] said, we already have the retry logic in the Dispatcher for job related cleanup. As for the cluster cleanup, maybe FLIP-194[1] has also covered. 

 

[1]. https://cwiki.apache.org/confluence/display/FLINK/FLIP-194%3A+Introduce+the+JobResultStore;;;","17/Mar/22 09:00;mapohl;No, FLIP-194 does not cover the retry during the cluster cleanup. But we want to fail in that case actually in the same way we do if the k8s cleanup failed due to an exception to make the user aware of it.;;;","17/Mar/22 09:24;wangyang0918;We will have a DIRTY file job result store if the cleanup failed. Users could re-submit the application to finish the cleanup. Right?;;;","17/Mar/22 12:12;mapohl;{quote}We will have a DIRTY file job result store if the cleanup failed. Users could re-submit the application to finish the cleanup. Right?{quote}

Correct. ...in case of the job-related cleanup;;;","24/Mar/22 08:44;mapohl;master: e6dbeffc6f9797a7c14e3751a721698d421460e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultJobGraphStore.globalCleanup does not trigger the cleanup when retrying the cleanup after a failover,FLINK-26690,13434170,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,16/Mar/22 16:47,19/Mar/22 14:06,13/Jul/23 08:08,19/Mar/22 14:06,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"The {{DefaultJobGraphStore}} does not get the JobGraph of a dirty globally-terminated job after failover anymore because the job is not going to be recovered. The globalCleanup checks the addedJobs still, which is not necessary but prevents the cleanup in that case.",,mapohl,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26652,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Mar 19 14:06:25 UTC 2022,,,,,,,,,,"0|z10jpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/22 11:35;mapohl;FYI: PR was approved. Backport PR is created and waits for CI;;;","19/Mar/22 14:06;mapohl;master: 89bfd3994c18e770cae4d4c98dffb8aefd808a21
1.15: 3966744a18d9f0e408474969c1f61ea43c8b41d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support sql statement end with "";"" for Hive dialect",FLINK-26681,13434087,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,luoyuxia,luoyuxia,luoyuxia,16/Mar/22 11:03,12/Apr/22 11:56,13/Jul/23 08:08,12/Apr/22 11:56,1.15.0,,,,,,1.15.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,"In FLINK-25600, the sql client won't remove ';' at the end of command, so the sql statement will keep the semicolon. When using Hive dialect, it'll be passed to HiveParser and then throw the ParseException like 
{code:java}
org.apache.flink.table.planner.delegation.hive.copy.HiveASTParseException: line 1:28 cannot recognize input near  ';' '<EOF>' ..{code}
So we need to support thesql statement end with "";"" for Hive dialect. ",,jingzhang,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 12 10:00:02 UTC 2022,,,,,,,,,,"0|z10j6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/22 09:44;jingzhang;Fixed in master: 36660cce32e7dbacf9b2408c6d65d8fc9c7c2c93
Fixed in release-1.15: 85e70e2222976375a5e33987da67166c0ffb4c92;;;","12/Apr/22 10:00;jingzhang;[~luoyuxia] [~jark] The bug is new introduced in release-1.15, it's better to be fixed in 1.15.0.

I'm sorry to merge it late.

Maybe I should add a comment in [VOTE release 1.15.0 maillist|https://lists.apache.org/thread/1fsxq4b9t0c5mb25j4zyov1bm27fc8d5]？

WDYT? [~jark];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionDispatcherLeaderProcess#onAddJobGraph crashes if the job is marked as deleted,FLINK-26680,13434085,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,16/Mar/22 11:00,17/Mar/22 10:23,13/Jul/23 08:08,17/Mar/22 10:23,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"Under certain situations it can happen that a dispatcher is informed about an added jobgraph while that JobGraph is already marked for deletion. This case isn't properly handled, causing an NPE that crashes the cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26652,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 17 10:22:45 UTC 2022,,,,,,,,,,"0|z10j6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 10:22;chesnay;master: 25ba49558bfd5ea3c35ac4a0f7723fbc585ec79d
1.15: 63817b5ffdf7ba24a168aeec95464d13e4d78e13 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make flink-connector-base dependency consistent,FLINK-26677,13434068,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyubin117,liyubin117,liyubin117,16/Mar/22 10:00,11/Apr/22 09:20,13/Jul/23 08:08,25/Mar/22 05:50,,,,,,,table-store-0.1.0,,,,Table Store,,,,,0,pull-request-available,,,"as https://issues.apache.org/jira/browse/FLINK-25927 has been merged into flink master branch and 1.15 branch, the flink-connector-base modules has been shaded into many connectors modules, the main change in `pom.xml`  as follow:
{code:java}
<relocations>
   <relocation>
      <pattern>org.apache.flink.connector.base</pattern>
      <shadedPattern>org.apache.flink.connector.files.shaded.org.apache.flink.connector.base</shadedPattern>
   </relocation>
</relocations>{code}
now the building of table-store project has been failed caused by above, it should to be updated right now.",,liyubin117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-03-16 10:00:10.0,,,,,,,,,,"0|z10j2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogPeriodicMaterializationITCase.testNonMaterialization failed on master with IllegalStateException,FLINK-26673,13434042,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,masteryhx,mapohl,mapohl,16/Mar/22 07:50,29/Mar/22 16:44,13/Jul/23 08:08,29/Mar/22 16:44,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / State Backends,Tests,,,,0,pull-request-available,test-stability,,"There was a [build failure|https://dev.azure.com/mapohl/flink/_build/results?buildId=873&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=6297] caused by {{ChangelogPeriodicMaterializationITCase}} on {{{}master{}}}. I also added 1.15 as affected because we're in the middle of the release:
{code:java}
Mar 15 22:21:58 [ERROR] Errors: 
Mar 15 22:21:58 [ERROR] ChangelogPeriodicMaterializationITCase.testNonMaterialization
Mar 15 22:21:58 [ERROR]   Run 1: null
Mar 15 22:21:58 [INFO]   Run 2: PASS
Mar 15 22:21:58 [INFO]   Run 3: PASS {code}
Actual error:
{code:java}
Mar 15 21:43:32 java.lang.IllegalStateException
Mar 15 21:43:32 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177)
Mar 15 21:43:32 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationITCase.testNonMaterialization(ChangelogPeriodicMaterializationITCase.java:74)
Mar 15 21:43:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 15 21:43:32 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[...] {code}",,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26684,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 16:44:53 UTC 2022,,,,,,,,,,"0|z10iww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/22 07:11;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=886&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=12176;;;","22/Mar/22 07:51;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=896&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=5407;;;","29/Mar/22 16:44;roman;Merged into master as fc98c70bd54504fa06f6cb9ab8f75917272490be,

into 1.15 as f59404fe646590abcfbb4a512ea6762f43108646.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing to cleanup a job should not fail the Flink Cluster in Session Mode,FLINK-26652,13433862,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,15/Mar/22 12:03,17/Mar/22 14:42,13/Jul/23 08:08,16/Mar/22 18:26,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"We introduced the option to disable the retryable cleanup in FLINK-26331. This should make Flink fall back to the 1.14- functionality with just printing a warning in session mode.

Instead, a {{RetryException}} is thrown which causes Flink to fail fatally. For Job and Application Mode failing fatally is ok because it doesn't affect other builds. But for session mode, we want to print a warning, instead.",,mapohl,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26331,,,,,,,,,,,,FLINK-26680,FLINK-26690,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 16 18:26:16 UTC 2022,,,,,,,,,,"0|z10ht4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/22 18:26;mapohl;master: dfdc36c9223a922834c3bae403fa983a218b0ad0
1.15: 0d8412fdac72a2a03fed482f05be21cf885580cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator helm template is broken,FLINK-26646,13433807,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,wangyang0918,wangyang0918,15/Mar/22 07:24,27/Mar/22 13:16,13/Jul/23 08:08,15/Mar/22 09:36,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"{code:java}
wangyang-pc:flink-kubernetes-operator danrtsey.wy$ helm install flink-operator helm/flink-operator --set image.repository=wangyang09180523/flink-java-operator --set image.tag=latest --set metrics.port=9999

Error: template: flink-operator/templates/flink-operator.yaml:143:12: executing ""flink-operator/templates/flink-operator.yaml"" at <eq (.Values.operatorConfiguration).append false>: error calling eq: incompatible types for comparison {code}",,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26574,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 15 09:36:47 UTC 2022,,,,,,,,,,"0|z10hh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/22 07:27;wangyang0918;cc [~gyfora] 

I am not sure whether it is related with my helm version(v3.6.3).;;;","15/Mar/22 07:35;gyfora;when do you get this error?;;;","15/Mar/22 07:38;wangyang0918;If you run the same command, I think you could reproduce. It is strange the CI passed.

 
{code:java}
helm install flink-operator helm/flink-operator --set image.repository=wangyang09180523/flink-java-operator --set image.tag=latest --set metrics.port=9999 {code};;;","15/Mar/22 07:44;gyfora;for me this works correctly, could be a helm version issue indeed. For me it's ""v3.8.0"";;;","15/Mar/22 08:11;wangyang0918;Hmm... It works with v3.8.0.;;;","15/Mar/22 08:14;gyfora;Maybe there is another way to do this check correctly. I wanted to have append as optional with default true and this was the only thing that seemed to have worked corectly.

The error you are getting is probably due to comparing a nil with False. ;;;","15/Mar/22 08:18;wangyang0918;Yes. Maybe we could explicitly set the {{.Values.operatorConfiguration.append}} to false.;;;","15/Mar/22 08:21;gyfora;yes, let's do that and then we can avoid using eq;;;","15/Mar/22 09:36;gyfora;merged: 81593eca3643bd7f17db5fe371d4f355967f70df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Source subscribe to a single topic partition will consume all partitions from that topic ,FLINK-26645,13433806,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,affe,affe,affe,15/Mar/22 07:19,10/May/22 11:28,13/Jul/23 08:08,10/May/22 11:28,1.14.4,1.15.0,,,,,1.14.5,1.15.1,1.16.0,,Connectors / Pulsar,,,,,0,pull-request-available,,,"Say users subscribe to 4 partitions of a topic with 16 partitions, current Pulsar source

will actually consume from all 16 partitions. Expect to consume from 4 partitions only.",,affe,huldar,martijnvisser,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26713,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 10 11:26:21 UTC 2022,,,,,,,,,,"0|z10hgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/22 08:29;arvid;Merged into master as 71a9f317594d8f02cfc0242050ed634bafae873a.;;;","25/Apr/22 21:56;syhily;[~arvid] The backport PR for 1.14 and 1.15 is ready, can you review and merge them.;;;","28/Apr/22 06:29;arvid;Merged into 1.15 as 5de8016a5dcee8a38d21f54e685633809f800c48.;;;","10/May/22 11:26;martijnvisser;Merged into 1.14 as 10e6b3ef591a0c42155f9fd799bf5d53a5f7df4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar sink fails with non-partitioned topic,FLINK-26642,13433785,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,goldenyang,goldenyang,goldenyang,15/Mar/22 04:06,21/Mar/22 08:56,13/Jul/23 08:08,21/Mar/22 08:56,1.15.0,,,,,,1.15.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,"Flink support pulsar sink now in [FLINK-20732|https://issues.apache.org/jira/browse/FLINK-20732]. I encountered a problem when using pulsar sink in master branch, when I use non-partitioned topic.

The current test found that both partitioned topics and non-partitioned topics ending with -partition-i can be supported, but ordinary non-partitioned topics without -partition-i will have problems, such as 'test_topic'. 

Reproducing the problem requires writing to a non-partitioned topic. Below is the stack information when the exception is encountered. I briefly communicated with [~Jianyun Zhao] , this may be a bug. 

 
{code:java}
2022-03-08 21:39:13,622 - INFO - [flink-akka.actor.default-dispatcher-13:Execution@1419] - Source: Pulsar Source -> (Sink: Writer -> Sink: Committer, Sink: Print to Std. Out) (1/6) (44af5e8a2b9d553952c7ed3e5d40e672) switched from RUNNING to FAILED on 54284e57-42a9-4e2e-9c41-54b0ad559832 @ 127.0.0.1 (dataPort=-1).java.lang.IllegalArgumentException: You should provide topics for routing topic by message key hash.at org.apache.flink.shaded.guava30.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144)at org.apache.flink.connector.pulsar.sink.writer.router.RoundRobinTopicRouter.route(RoundRobinTopicRouter.java:54)at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.write(PulsarWriter.java:138)at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:124)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:77)at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:205)at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)at org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.emitRecord(PulsarRecordEmitter.java:41)at org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.emitRecord(PulsarRecordEmitter.java:33)at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:143)at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:106)at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:382)at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)at java.lang.Thread.run(Thread.java:748) {code}
 
 ",,affe,fpaul,goldenyang,martijnvisser,syhily,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,20732,https://issues.apache.org/jira/browse/FLINK-20732,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 08:56:25 UTC 2022,,,,,,,,,,"0|z10hc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/22 06:01;affe;Roger. I'll try to reproduce it. ;;;","15/Mar/22 08:26;goldenyang;[~affe] Thanks. Please take a look, we tried to use this Pulsar Sink, and we fixed a version after encountering this problem. Please see if this modification is acceptable.;;;","17/Mar/22 02:48;syhily;[~goldenyang] Tks for your PR. It's awesome and fixes the non-partitioned topic support issue.;;;","17/Mar/22 10:31;goldenyang;[~Jianyun Zhao] [~arvid]  [~martijnvisser]   
Hello, how are you doing?
I'm ready to fix this, could you assign this issue to me. Thank you very much~;;;","17/Mar/22 10:33;martijnvisser;[~goldenyang] I've assigned it to you;;;","21/Mar/22 08:56;fpaul;Merged in master: 0e6f33b1ad9607faefd2f8fd7fdf4d62f612f6df

Merged in release-1.15: 3c963cc69368530f651b12b91c16cc157f76e1c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector does not report recordsSend metric,FLINK-26633,13433635,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,fpaul,fpaul,14/Mar/22 10:53,24/Mar/22 16:46,13/Jul/23 08:08,24/Mar/22 16:46,1.15.0,,,,,,1.15.0,,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,"As part of a unified sink, it is recommended to report a recordSend metric to let users track the number of outgoing records from Flink.",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 24 16:46:07 UTC 2022,,,,,,,,,,"0|z10gfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 08:14;fpaul;Merged in master: 3c02c4a10bb24ac7eb84ced0bc52a2fb76f2440d;;;","24/Mar/22 16:46;fpaul;Merged in release-1.15: 328c0f360834901191a03c59076cd0c41b1745eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation is unavailable,FLINK-26631,13433618,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,soberChina0@gamil.com,soberChina0@gamil.com,14/Mar/22 09:39,14/Mar/22 09:47,13/Jul/23 08:08,14/Mar/22 09:47,,,,,,,,,,,Documentation,,,,,0,,,,!image-2022-03-14-17-39-49-898.png!,,soberChina0@gamil.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/22 09:39;soberChina0@gamil.com;image-2022-03-14-17-39-49-898.png;https://issues.apache.org/jira/secure/attachment/13041048/image-2022-03-14-17-39-49-898.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-03-14 09:39:55.0,,,,,,,,,,"0|z10gbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in code comment for SubtaskStateMapper.RANGE,FLINK-26629,13433607,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanshubin,ruanshubin,ruanshubin,14/Mar/22 08:39,29/Mar/22 10:10,13/Jul/23 08:08,29/Mar/22 10:10,1.15.0,,,,,,1.14.5,1.15.0,,,Runtime / Network,,,,,0,pull-request-available,,,"The code comments for SubtaskStateMapper.RANGE are as follows:
{code:java}
 * <p>Example:<br>
 * old assignment: 0 -> [0;43); 1 -> [43;87); 2 -> [87;128)<br>
 * new assignment: 0 -> [0;64]; 1 -> [64;128)<br>
 * subtask 0 recovers data from old subtask 0 + 1 and subtask 1 recovers data from old subtask 0
 * + 2{code}
The correct code comment should be：

 
{code:java}
 * <p>Example:<br>
 * old assignment: 0 -> [0;43); 1 -> [43;87); 2 -> [87;128)<br>
 * new assignment: 0 -> [0;64]; 1 -> [64;128)<br>
 * subtask 0 recovers data from old subtask 0 + 1 and subtask 1 recovers data from old subtask 1
 * + 2{code}
*subtask 1 should recovers data from old subtask 1 + 2，not from old subtask 0 + 2*

 ",,dwysakowicz,ruanshubin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 10:10:12 UTC 2022,,,,,,,,,,"0|z10g9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/22 10:10;dwysakowicz;Fixed in:
* master
** 65bafa043bbf79e8cc0ff3f015e5fa34681b39dc
* 1.15.0
** c4ed56a870f3fc58ac0d2af66dcd35eb5bf5b7d3
* 1.14.5
** 0d8178d11a934217747f47b342999aeba73a26e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove jar statement not aligned with pipleline.jars,FLINK-26618,13433495,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Paul Lin,Paul Lin,Paul Lin,13/Mar/22 04:23,20/Mar/22 04:50,13/Jul/23 08:08,20/Mar/22 04:50,1.14.3,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,"Currently, `remove jar` statement doesn't remove the corresponding jars in pipeline.jars.",,leonard,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Mar 19 03:56:46 UTC 2022,,,,,,,,,,"0|z10fko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/22 02:15;Paul Lin;cc [~jark][~twalthr];;;","19/Mar/22 03:56;leonard;Fixed:
  master(1.16): 1b041dbd9a781ef7a3927df45149be4f09e9ea7b
  release-1.15: 8dbef3f91fa763fc12310875b350317e1763e642
  release-1.14: 4fc5b96976843501580f7f335cf39c5c3d8e04d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI failed with a timeout,FLINK-26616,13433369,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,mapohl,mapohl,11/Mar/22 15:21,28/Sep/22 09:24,13/Jul/23 08:08,13/Apr/22 11:10,1.15.0,,,,,,1.15.1,1.16.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,"{{AdaptiveSchedulerITCase.}} failed in [this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=855&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=5778] due to a timeout.
{code}
Mar 11 14:41:36 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 76.177 s <<< FAILURE! - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Mar 11 14:41:36 [ERROR] org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI  Time elapsed: 60.146 s  <<< ERROR!
Mar 11 14:41:36 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Mar 11 14:41:36 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:167)
Mar 11 14:41:36 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 11 14:41:36 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 11 14:41:36 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI(AdaptiveSchedulerITCase.java:268)
{code}",,gaoyunhaii,mapohl,roman,smattheis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21564,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 07:39:01 UTC 2022,,,,,,,,,,"0|z10eso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 18:19;smattheis;Same here:

[https://dev.azure.com/smattheis/Flink/_build/results?buildId=39&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=6426]

{code:java}
Mar 11 15:35:07 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 80.865 s <<< FAILURE! - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Mar 11 15:35:07 [ERROR] org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI  Time elapsed: 60.122 s  <<< ERROR!
Mar 11 15:35:07 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
...
Mar 11 15:35:07 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI(AdaptiveSchedulerITCase.java:268)
...
{code};;;","14/Mar/22 10:59;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32981&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12537;;;","14/Mar/22 11:17;chesnay;I couldn't reproduce it so far. The logs show that the tasks are deployed just fine (with a number of failed checkpoint triggers because not everything was running it).

But then strangely once the first checkpoint is triggered:
{code}
15:34:07,406 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1647012847406 for job 339650c79b74dd74a1f27117a41a6dbf.
{code}
nothing happens for an entire minute, and then the test fails with the timeout.

It is a bit too consistent for it to be one of those ""VM just stopped doing stuff for a bit"" issues, as it fails exactly when the timeout triggers.
;;;","16/Mar/22 13:00;chesnay;I ran the ITCase around 40k times locally over an entire day and couldn't reproduce the issue. I'm inclined to just remove the timeout and see what happens.;;;","29/Mar/22 07:45;chesnay;master: 7d85b273ccdbd5a2242e05e5d645ea82280f5eea
1.15: 489827520b1a53db04a94346c98327d0d42301c5;;;","03/Apr/22 09:19;mapohl;I'm wondering whether it would be worth it to backport the change of this issue as well. This test is also unstable in 1.15 (see [failed build|https://dev.azure.com/mapohl/flink/_build/results?buildId=926&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=13067]).;;;","11/Apr/22 07:39;gaoyunhaii;1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34494&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12137;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchingStateChangeUploadSchedulerTest.testRetryOnTimeout fails on azure,FLINK-26615,13433368,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,11/Mar/22 15:05,27/Mar/22 10:58,13/Jul/23 08:08,27/Mar/22 10:58,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / State Backends,Tests,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32896&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24724]

{code}
[ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s <<< FAILURE! - in org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest
[ERROR] org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.testRetryOnTimeout  Time elapsed: 0.042 s  <<< FAILURE!
java.lang.AssertionError: expected:<[0]> but was:<[]>
   at org.junit.Assert.fail(Assert.java:89)
   at org.junit.Assert.failNotEquals(Assert.java:835)
   at org.junit.Assert.assertEquals(Assert.java:120)
   at org.junit.Assert.assertEquals(Assert.java:146)
   at org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.testRetryOnTimeout(BatchingStateChangeUploadSchedulerTest.java:240)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
   at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
   at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
   at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
   at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
   at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
   at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
   at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
   at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
   at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
   at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
   at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
   at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
   at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
   at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrato r.java:107)
   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrato r.java:88)
{code}",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26838,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Mar 27 10:58:08 UTC 2022,,,,,,,,,,"0|z10esg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/22 10:58;roman;Merged into master as 9da51e1ee72a7e8137923554bba5f67f09756104,

into 1.15 as 0f475a42dfb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful unified Sink V2 upgrades only work when operator uids are given,FLINK-26613,13433358,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,fpaul,fpaul,11/Mar/22 14:17,17/Mar/22 08:04,13/Jul/23 08:08,17/Mar/22 08:04,1.15.0,,,,,,1.15.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"As part of documentation [1]we guarantee that in case a stateful migration fails and no uids are used that it is possible to bind the recovered state via the uidHash. 

This ticket should add a uidHash setter for the operators that are affected by the migration (writer, committer, global committer).

 

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/#preconditions",,fpaul,Jiangang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 17 08:04:08 UTC 2022,,,,,,,,,,"0|z10eq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/22 14:11;fpaul;Merged in master: 672829753015d24813cb4a949cbb0e36efb965ef;;;","17/Mar/22 08:04;fpaul;Merged in release-1.15: 31cd7376e64cce16808f01eedc70b1e678170781;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink can not upgrade from 1.13 if the uid of the origin sink is not set.,FLINK-26610,13433341,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,pltbkd,pltbkd,11/Mar/22 12:01,14/Mar/22 02:22,13/Jul/23 08:08,14/Mar/22 02:22,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,,gaoyunhaii,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 02:22:22 UTC 2022,,,,,,,,,,"0|z10emg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/22 02:22;gaoyunhaii;Merged on master via b8e7fc387dd1a7e34e28b0b5fa16d6788be62fbb^..9ef8873c0739688a37fcdc365dcd83f321919704;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There are multiple MAX_LONG_VALUE value errors in pyflink code,FLINK-26607,13433333,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,cun8cun8,cun8cun8,11/Mar/22 10:52,17/Mar/22 05:46,13/Jul/23 08:08,17/Mar/22 05:46,1.14.0,,,,,,1.14.5,1.15.0,,,API / Python,,,,,0,pull-request-available,,,"There are multiple MAX_LONG_VALUE values sys. In pyflink code maxsize

MAX_LONG_VALUE = sys.maxsize

*maxsize* attribute of the *sys* module fetches the largest value a variable of data type *Py_ssize_t* ** can store. It is the Python platform’s pointer that dictates the maximum size of lists and strings in Python. The size value returned by maxsize depends on the platform architecture:
 * *32-bit:* the value will be 2^31 – 1, i.e. 2147483647
 * *64-bit:* the value will be 2^63 – 1, i.e. 9223372036854775807",,cun8cun8,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 17 05:46:06 UTC 2022,,,,,,,,,,"0|z10eko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 05:46;dianfu;Fixed in:
- master via d2d0065f136115ae7bbbdea6c41abbee685d6967
- release-1.15 via 891ec9dfb05fa05c91cc4e9fd0108b56d8d7a3db
- release-1.14 via dc62cc4220315172b0345acf2202b00a8506ca02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification failed on azure,FLINK-26596,13433283,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,gaoyunhaii,gaoyunhaii,11/Mar/22 07:04,16/Mar/22 07:08,13/Jul/23 08:08,16/Mar/22 07:08,1.14.4,1.15.0,,,,,1.14.5,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"
{code:java}
Mar 10 09:16:30 [ERROR] org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification  Time elapsed: 20.752 s  <<< ERROR!
Mar 10 09:16:30 java.lang.NullPointerException
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.lambda$null$9(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:292)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:161)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.lambda$testNewLeaderAfterReconnectTriggersListenerNotification$10(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:288)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testWithQueueLeaderElectionListener(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:313)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:250)
Mar 10 09:16:30 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 10 09:16:30 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 10 09:16:30 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 10 09:16:30 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 10 09:16:30 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32815&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7544",,gaoyunhaii,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26121,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 13:50:21 UTC 2022,,,,,,,,,,"0|z10e9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 17:10;mapohl;I'm reopening the issue again. It's quite related to FLINK-26121. But the {{NullPointerException}} is now caused because the leadership loss is not handled properly.;;;","11/Mar/22 18:05;mapohl;The connection loss can be observed in the zookeeper-client-1.log between 09:16:07,840 and 09:16:09,579:

{code}
09:16:07,840 [main-SendThread(127.0.0.1:44061)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Unable to read additional data from server sessionid 0x101773132360000, likely server has closed socket, closing socket connection and attempting reconnect
09:16:09,579 [main-SendThread(127.0.0.1:44061)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Opening socket connection to server localhost/127.0.0.1:44061. Will not attempt to authenticate using SASL (unknown error)
09:16:09,579 [main-SendThread(127.0.0.1:44061)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Socket connection established, initiating session, client: /127.0.0.1:34516, server: localhost/127.0.0.1:44061
{code};;;","14/Mar/22 13:50;mapohl;master: 61d1f67c11f245d96f82e78c25f78ed2ca603919
1.14: 143f8e0b16f3127f5f70c7a827a7642b463e9493;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Deadlock in FsStateChangelogWriter,FLINK-26592,13433222,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,10/Mar/22 22:15,21/Mar/22 11:42,13/Jul/23 08:08,21/Mar/22 11:41,1.15.0,,,,,,1.15.0,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,"The issue occurs when sizes of buffers are set to minimum (e.g. 1 byte).
Task thread tries to update state -> schedules to upload changes -> waits for capacity.
Upload threads do release capacity on upload completion; however, they are unable to send back the results because the Writer lock is taken; therefore, they're unable to proceed with the next uploads.",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25144,,,"10/Mar/22 22:16;roman;deadlock;https://issues.apache.org/jira/secure/attachment/13040946/deadlock",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 11:41:59 UTC 2022,,,,,,,,,,"0|z10dw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 11:41;roman;Merged into master as db4d05a9c1bf63a56218d7bbfcc2b8e9978387e0,

into 1.15 as 543e8854b8f6b1e0b6997383de0691d2e9fffba7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compilation fails due to RawToBinaryCastRule,FLINK-26591,13433179,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,roman,roman,10/Mar/22 17:05,11/Mar/22 08:18,13/Jul/23 08:08,10/Mar/22 18:05,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32857&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=6288

{code}
2022-03-10T15:15:36.8803533Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on proj*ect flink-table-planner_2.12: Compilation failure
2022-03-10T15:15:36.8805068Z [ERROR] /__w/3/s/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/casting/RawTo BinaryCastRule.java:[46,5] method does not override or implement a method from a supertype
{code}

cc: [~slinkydeveloper]",,mapohl,martijnvisser,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 11 08:18:15 UTC 2022,,,,,,,,,,"0|z10dmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 17:10;roman;I've created a PR to revert cbfca3afae5d33f9137ae97eb4f4c27ea0d82919 to have a quick solution:
https://github.com/apache/flink/pull/19048;;;","10/Mar/22 18:00;mapohl;FYI: I did the same but already merged [my PR #19049|https://github.com/apache/flink/pull/19049] into {{master}} to have this fixed fast. The specific method wasn't used anywhere.;;;","10/Mar/22 18:06;roman;Thanks!;;;","11/Mar/22 08:18;martijnvisser;Thank you both :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The user is not informed in any way when a job is resubmitted but already globally-terminated,FLINK-26583,13433091,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,10/Mar/22 12:00,14/Mar/22 09:50,13/Jul/23 08:08,11/Mar/22 17:02,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"-We experience some unwanted behavior if a clean JobResult is listed in the JobResultStore and a job with the same Job ID is submitted in Application Mode.
We would expect that the second submission fails with a {{DuplicateJobSubmissionException}}. Instead, the submission succeeds with the job not running anymore.-

Update:
We reiterated over the problem and decided that the Exception is not the desired failure because it would cause a failover in k8s setups, for instance. We rather want to inform the user through a warning because Flink still behaves as expected.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25430,,,,,,,,,,,FLINK-26391,,,"10/Mar/22 12:04;mapohl;log.jm;https://issues.apache.org/jira/secure/attachment/13040918/log.jm",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 11 17:02:29 UTC 2022,,,,,,,,,,"0|z10d34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 12:04;mapohl;I attached the logs of the Application Mode log run from FLINK-26391 to this issue.;;;","11/Mar/22 11:45;chesnay;Is this specific to YARN?;;;","11/Mar/22 17:02;mapohl;No, in the end it wasn't specific to YARN or Application Mode. I'm going to update the ticket description.

master: 106a45e5deb34dafff09583c5ba607d0d409c0bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink Compactor is not properly processing in-progress files.,FLINK-26580,13433082,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,pltbkd,pltbkd,10/Mar/22 10:39,14/Mar/22 02:21,13/Jul/23 08:08,14/Mar/22 02:21,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,,gaoyunhaii,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 02:21:33 UTC 2022,,,,,,,,,,"0|z10d14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/22 02:21;gaoyunhaii;Merged on master via 106a45e5deb34dafff09583c5ba607d0d409c0bb^..b8e7fc387dd1a7e34e28b0b5fa16d6788be62fbb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogPeriodicMaterializationRescaleITCase.testRescaleIn failed on azure,FLINK-26573,13433035,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,masteryhx,gaoyunhaii,gaoyunhaii,10/Mar/22 07:39,18/Mar/22 09:00,13/Jul/23 08:08,16/Mar/22 14:18,1.15.0,,,,,,1.15.0,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,test-stability,,"{code:java}
2022-03-09T18:27:58.5345435Z Mar 09 18:27:58 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.81 s <<< FAILURE! - in org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationRescaleITCase
2022-03-09T18:27:58.5348017Z Mar 09 18:27:58 [ERROR] ChangelogPeriodicMaterializationRescaleITCase.testRescaleIn  Time elapsed: 5.292 s  <<< ERROR!
2022-03-09T18:27:58.5349888Z Mar 09 18:27:58 org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=0, backoffTimeMS=0)
2022-03-09T18:27:58.5351920Z Mar 09 18:27:58 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-03-09T18:27:58.5354136Z Mar 09 18:27:58 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-03-09T18:27:58.5355966Z Mar 09 18:27:58 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
2022-03-09T18:27:58.5357860Z Mar 09 18:27:58 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
2022-03-09T18:27:58.5359532Z Mar 09 18:27:58 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
2022-03-09T18:27:58.5361149Z Mar 09 18:27:58 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
2022-03-09T18:27:58.5362868Z Mar 09 18:27:58 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-03-09T18:27:58.5365410Z Mar 09 18:27:58 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-03-09T18:27:58.5366584Z Mar 09 18:27:58 	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
2022-03-09T18:27:58.5367849Z Mar 09 18:27:58 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-09T18:27:58.5371509Z Mar 09 18:27:58 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-09T18:27:58.5372555Z Mar 09 18:27:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-03-09T18:27:58.5373584Z Mar 09 18:27:58 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-03-09T18:27:58.5374532Z Mar 09 18:27:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-03-09T18:27:58.5375556Z Mar 09 18:27:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-03-09T18:27:58.5376329Z Mar 09 18:27:58 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-03-09T18:27:58.5377045Z Mar 09 18:27:58 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-03-09T18:27:58.5377845Z Mar 09 18:27:58 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-03-09T18:27:58.5378453Z Mar 09 18:27:58 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-03-09T18:27:58.5379053Z Mar 09 18:27:58 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-03-09T18:27:58.5379654Z Mar 09 18:27:58 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-03-09T18:27:58.5380478Z Mar 09 18:27:58 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-03-09T18:27:58.5381228Z Mar 09 18:27:58 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-03-09T18:27:58.5381918Z Mar 09 18:27:58 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-03-09T18:27:58.5382540Z Mar 09 18:27:58 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-03-09T18:27:58.5383115Z Mar 09 18:27:58 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-03-09T18:27:58.5383652Z Mar 09 18:27:58 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-03-09T18:27:58.5384420Z Mar 09 18:27:58 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-03-09T18:27:58.5385037Z Mar 09 18:27:58 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-03-09T18:27:58.5385594Z Mar 09 18:27:58 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-03-09T18:27:58.5386182Z Mar 09 18:27:58 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-03-09T18:27:58.5386751Z Mar 09 18:27:58 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-03-09T18:27:58.5387291Z Mar 09 18:27:58 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-03-09T18:27:58.5387971Z Mar 09 18:27:58 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-03-09T18:27:58.5388625Z Mar 09 18:27:58 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-03-09T18:27:58.5389266Z Mar 09 18:27:58 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-03-09T18:27:58.5390218Z Mar 09 18:27:58 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-03-09T18:27:58.5392425Z Mar 09 18:27:58 Caused by: java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit7413765326024147067/junit5421015997665040773/d62aece95b78eb1198d959898d0e8fa1/chk-3/._metadata.inprogress.528d97cf-1897-455a-8d80-17a8de1b7e82
2022-03-09T18:27:58.5393492Z Mar 09 18:27:58 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
2022-03-09T18:27:58.5394155Z Mar 09 18:27:58 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
2022-03-09T18:27:58.5394834Z Mar 09 18:27:58 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811)
2022-03-09T18:27:58.5396033Z Mar 09 18:27:58 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
2022-03-09T18:27:58.5396753Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:499)
2022-03-09T18:27:58.5397394Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:486)
2022-03-09T18:27:58.5398208Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-03-09T18:27:58.5398912Z Mar 09 18:27:58 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
2022-03-09T18:27:58.5399545Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-03-09T18:27:58.5400379Z Mar 09 18:27:58 	at java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:536)
2022-03-09T18:27:58.5401046Z Mar 09 18:27:58 	at org.apache.flink.test.util.TestUtils.hasMetadata(TestUtils.java:139)
2022-03-09T18:27:58.5401808Z Mar 09 18:27:58 	at org.apache.flink.test.util.TestUtils.isCompletedCheckpoint(TestUtils.java:129)
2022-03-09T18:27:58.5402441Z Mar 09 18:27:58 	at java.nio.file.Files.lambda$find$2(Files.java:3691)
2022-03-09T18:27:58.5403046Z Mar 09 18:27:58 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
2022-03-09T18:27:58.5403678Z Mar 09 18:27:58 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-03-09T18:27:58.5404330Z Mar 09 18:27:58 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2022-03-09T18:27:58.5405021Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-03-09T18:27:58.5405875Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-03-09T18:27:58.5406661Z Mar 09 18:27:58 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-03-09T18:27:58.5407308Z Mar 09 18:27:58 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-03-09T18:27:58.5408061Z Mar 09 18:27:58 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
2022-03-09T18:27:58.5408708Z Mar 09 18:27:58 	at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:582)
2022-03-09T18:27:58.5409414Z Mar 09 18:27:58 	at org.apache.flink.test.util.TestUtils.getMostRecentCompletedCheckpointMaybe(TestUtils.java:122)
2022-03-09T18:27:58.5410320Z Mar 09 18:27:58 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationTestBase.getAllStateHandleId(ChangelogPeriodicMaterializationTestBase.java:213)
2022-03-09T18:27:58.5411517Z Mar 09 18:27:58 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationRescaleITCase$2.lambda$beforeElement$78302915$1(ChangelogPeriodicMaterializationRescaleITCase.java:118)
2022-03-09T18:27:58.5412847Z Mar 09 18:27:58 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationTestBase$ControlledSource.waitWhile(ChangelogPeriodicMaterializationTestBase.java:337)
2022-03-09T18:27:58.5413901Z Mar 09 18:27:58 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationRescaleITCase$2.beforeElement(ChangelogPeriodicMaterializationRescaleITCase.java:115)
2022-03-09T18:27:58.5414942Z Mar 09 18:27:58 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationTestBase$ControlledSource.run(ChangelogPeriodicMaterializationTestBase.java:320)
2022-03-09T18:27:58.5415785Z Mar 09 18:27:58 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-03-09T18:27:58.5416491Z Mar 09 18:27:58 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-03-09T18:27:58.5417284Z Mar 09 18:27:58 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
2022-03-09T18:27:58.5418796Z Mar 09 18:27:58 Caused by: java.nio.file.NoSuchFileException: /tmp/junit7413765326024147067/junit5421015997665040773/d62aece95b78eb1198d959898d0e8fa1/chk-3/._metadata.inprogress.528d97cf-1897-455a-8d80-17a8de1b7e82
2022-03-09T18:27:58.5419677Z Mar 09 18:27:58 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
2022-03-09T18:27:58.5420341Z Mar 09 18:27:58 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
2022-03-09T18:27:58.5421001Z Mar 09 18:27:58 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
2022-03-09T18:27:58.5421768Z Mar 09 18:27:58 	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
2022-03-09T18:27:58.5422490Z Mar 09 18:27:58 	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
2022-03-09T18:27:58.5423195Z Mar 09 18:27:58 	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
2022-03-09T18:27:58.5423846Z Mar 09 18:27:58 	at java.nio.file.Files.readAttributes(Files.java:1737)
2022-03-09T18:27:58.5424452Z Mar 09 18:27:58 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
2022-03-09T18:27:58.5425076Z Mar 09 18:27:58 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
2022-03-09T18:27:58.5425688Z Mar 09 18:27:58 	at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
2022-03-09T18:27:58.5426331Z Mar 09 18:27:58 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:84)
2022-03-09T18:27:58.5426833Z Mar 09 18:27:58 	... 30 more
2022-03-09T18:27:58.5427157Z Mar 09 18:27:58 

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32774&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5593",,gaoyunhaii,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26684,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 16 14:18:52 UTC 2022,,,,,,,,,,"0|z10cqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 07:39;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32793&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5674;;;","10/Mar/22 07:43;gaoyunhaii;Perhaps cc [~roman] [~ym] 

Is it the same issue with https://issues.apache.org/jira/browse/FLINK-26318 ? ;;;","10/Mar/22 07:51;roman;[~gaoyunhaii], yes, I think FLINK-26318 is similar.
[~masteryhx], could you please take a look?;;;","11/Mar/22 15:02;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32902&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12458];;;","13/Mar/22 17:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32975&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13095;;;","14/Mar/22 05:26;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32961&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12494;;;","16/Mar/22 14:18;roman;Merged into master as b8bafc843eb79d9a642b8914c3823a792ee7b372,
into 1.15 as cc7a6407cd006ea20843ae6771125e4d54e0e72f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactCoordinatorStateHandler doesn't properly handle the cleanup-in-progress requests.,FLINK-26564,13432986,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,pltbkd,pltbkd,10/Mar/22 03:22,11/Mar/22 03:06,13/Jul/23 08:08,11/Mar/22 03:06,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,It is found in FLINK-26322 that the CompactCoordinatorStateHandler doesn't properly handle the cleanup-in-progress requests but submit them as compacting requests. The issue happens when a job with compaction enabled is stop-with-savepoint and restarted with compaction disabled.,,gaoyunhaii,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 11 03:06:29 UTC 2022,,,,,,,,,,"0|z10cfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 03:06;gaoyunhaii;Merged on master via 155f17d62952fff5f5b583256f959f5fb63d1a8e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deployment Status cannot be updated,FLINK-26561,13432982,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,leinenglian,leinenglian,10/Mar/22 03:01,27/Mar/22 13:16,13/Jul/23 08:08,11/Mar/22 11:23,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,,,,"I'd like to know when deployment  status will change to ready, beacuse i see  deployment  status  is MISSING

flink operator log is always printing this info:

2022-03-10 02:35:46,774 o.a.f.k.o.s.FlinkService       [DEBUG] Creating RestClusterClient(http://xx:8081)

2022-03-10 02:35:46,774 o.a.f.r.r.RestClient           [DEBUG] Rest client endpoint started.

2022-03-10 02:35:46,774 o.a.f.r.r.RestClient           [DEBUG] Shutting down rest endpoint.

2022-03-10 02:35:46,774 o.a.f.r.r.RestClient           [DEBUG] Rest endpoint shutdown complete.

2022-03-10 02:35:46,775 o.a.f.k.o.o.Observer           [INFO ] JobManager deployment xx in namespace flink-operator port ready, waiting for the REST API...

2022-03-10 02:35:46,775 i.j.o.p.e.ReconciliationDispatcher [DEBUG] Updating resource: 6b6d6f59-b2db-4490-abe5-9d5843268653 with version: 52817831

2022-03-10 02:35:46,775 i.j.o.p.e.ReconciliationDispatcher [DEBUG] Trying to replace resource xx, version: 52817831

2022-03-10 02:35:46,781 i.f.k.c.i.c.Reflector          [DEBUG] Event received MODIFIED FlinkDeployment resourceVersion 52817832

2022-03-10 02:35:46,781 i.j.o.p.e.s.c.ControllerResourceEventSource [DEBUG] Event received for resource:xx

2022-03-10 02:35:46,781 i.j.o.p.e.EventProcessor       [DEBUG] Received event: ResourceEvent\{action=UPDATED, associated resource id=CustomResourceID{name='xx, namespace='flink-operator'}}

2022-03-10 02:35:46,781 i.j.o.p.e.EventProcessor       [DEBUG] Skipping executing controller for resource id: CustomResourceID\{name='xx', namespace='flink-operator'}. Controller in execution: true. Latest Resource present: true","k8s version: 1.20

flink version: 1.14.3",leinenglian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-03-10 03:01:11.0,,,,,,,,,,"0|z10cew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing close in FileSystemJobResultStore,FLINK-26555,13432901,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,09/Mar/22 16:08,31/Mar/22 12:14,13/Jul/23 08:08,10/Mar/22 08:35,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,{{FileSystemJobResultStore.createDirtyResultInternal}} does not close the opened {{OutputStream}},,mapohl,tanyuxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25431,,FLINK-26957,,,,,,,,,,,,"31/Mar/22 08:43;tanyuxin;image-2022-03-31-16-43-56-322.png;https://issues.apache.org/jira/secure/attachment/13041828/image-2022-03-31-16-43-56-322.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 31 12:14:04 UTC 2022,,,,,,,,,,"0|z10bx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 08:35;mapohl;master: 281b8e744dd2f8677d41d9e97c9b8ceeeddff450;;;","31/Mar/22 09:16;tanyuxin;[~mapohl] , hello, about the issue, we encountered an exception as follows. Could you please help take a look?

When a job is finished in a session cluster, the job result may flush failed.
!image-2022-03-31-16-43-56-322.png|width=819,height=423!
{code:java}
mapper.writeValue(os, new JsonJobResultEntry(jobResultEntry)); {code}
About this line in the patch, I checked the source code and found it called the method 
{code:java}
this._writeValueAndClose(this.createGenerator(out, JsonEncoding.UTF8), value); {code}
 and an _UTF8JsonGenerator_ is inited and used. 
{code:java}
protected final void _writeValueAndClose(JsonGenerator g, Object value) throws IOException {
    SerializationConfig cfg = this.getSerializationConfig();
    if (cfg.isEnabled(SerializationFeature.CLOSE_CLOSEABLE) && value instanceof Closeable) {
        this._writeCloseable(g, value, cfg);
    } else {
        try {
            this._serializerProvider(cfg).serializeValue(g, value);
        } catch (Exception var5) {
            ClassUtil.closeOnFailAndThrowAsIOE(g, var5);
            return;
        }

        g.close();
    }
} {code}
The _UTF8JsonGenerator#close_ will be called finally and I found the OutputStream may be closed in the method when some features of Json generator is enabled.
{code:java}
public void close() throws IOException {
    ...
    if (this._outputStream != null) {
        if (!this._ioContext.isResourceManaged() && !this.isEnabled(Feature.AUTO_CLOSE_TARGET)) {
            if (this.isEnabled(Feature.FLUSH_PASSED_TO_STREAM)) {
                this._outputStream.flush();
            }
        } else {
            this._outputStream.close();
        }
    }
    ...
} {code}
If the output stream is closed after {_}writeValue{_}, the above _ClosedChannelException_ may be thrown when calling the _flush_ method added. 
{code:java}
os.flush(); {code}
I found this patch has changed  the initialization of the output stream to the try-with-resource mode. Generally, the data will be flushed before the file system is closed. Could we delete this line of code  _os.flush();_ to avoid the exception?

[~mapohl] , WDYT about the exception and could you help take a look when having free time? If I missed something, please correct it at any time. Thanks very much.

 ;;;","31/Mar/22 10:14;mapohl;Thanks for sharing, [~tanyuxin]. How come the flag {{CLOSE_CLOSEABLE}} is enabled? It's not enabled within the Flink code. We haven't experienced something like that during our tests.;;;","31/Mar/22 10:26;mapohl;Ok, that seems to be the default behavior: https://fasterxml.github.io/jackson-databind/javadoc/2.7/com/fasterxml/jackson/databind/ObjectMapper.html#writeValue(java.io.OutputStream,%20java.lang.Object);;;","31/Mar/22 10:30;mapohl;I created FLINK-26957 as a follow-up to cover this issue.;;;","31/Mar/22 12:14;tanyuxin;[~mapohl] Thanks for your quick reply. We are looking forward to the follow-up issue being resolved. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the information of checkpoint failure ,FLINK-26550,13432849,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,09/Mar/22 11:39,10/Mar/22 14:07,13/Jul/23 08:08,10/Mar/22 14:07,,,,,,,1.14.5,1.15.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"After FLINK-26049, all failed checkpoint would print message with {{ Failed to trigger checkpoint }}:


{code:java}
5812 [pool-5-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1646825286424 for job d2fd07b3b33af453a4e115f3197f81bb.
5913 [pool-5-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Checkpoint 1 of job d2fd07b3b33af453a4e115f3197f81bb expired before completing.
451518 [pool-5-thread-1] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint 1 for job d2fd07b3b33af453a4e115f3197f81bb. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2172) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) [?:1.8.0_292]
	at java.util.concurrent.FutureTask.run(FutureTask.java) [?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}

This is extremely strange as the failure does not happen during the trigger phase.",,akalashnikov,fanrui,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26049,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 14:07:31 UTC 2022,,,,,,,,,,"0|z10blk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 14:07;yunta;resolved in master: 254c918b3e681f3e7014b558413f5c8db9c8e67f
in release-1.14: 00fed2be8643d62de47c7af1f7f1818377fd0615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accepting slots without a matching requirement leads to unfulfillable requirements,FLINK-26547,13432827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,09/Mar/22 10:31,11/Mar/22 10:35,13/Jul/23 08:08,11/Mar/22 10:35,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"To allow recovered TMs to eagerly re-offer their slots we allowed the registration of slots without a matching requirement if the job is currently restarting.

All slots that the pool accepts are mapped to a certain requirement, in order to determine whether sufficient slots were received yet. If a slot is reserved for a requirement that does not coincide with the mapping the pool come up with, then the mapping and requirements are changed accordingly to ensure we still request sufficient slots.

This leads to issues with slots that were accepted without a matching requirement. Those were mapped to the actual resource profile of the slot (to fit into the book-keeping). With the above logic in place this could lead to a specific resource requirement being added, which the remaining JM components are not aware of (and thus will never get rid of).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25855,,,,,,,FLINK-26274,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 11 10:35:12 UTC 2022,,,,,,,,,,"0|z10bgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 10:35;chesnay;master: 3f1a66b9da3e2c9b169ab1a7075468c5f2b559c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ingress rules should be created in the same namespace with FlinkDeployment CR,FLINK-26545,13432797,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,wangyang0918,wangyang0918,09/Mar/22 07:36,24/Nov/22 01:03,13/Jul/23 08:08,17/Mar/22 12:58,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Currently, the ingress rules are always created in the operator namespace(e.g. default). It could not work when the FlinkDeloyment CR is submitted in a different namespace(e.g. flink-test).

Refer to here[1] for why it could not work.

 

[1]. https://stackoverflow.com/questions/59844622/ingress-configuration-for-k8s-in-different-namespaces

 ",,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 17 12:58:34 UTC 2022,,,,,,,,,,"0|z10ba0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/22 11:33;gyfora;cc [~matyas] ;;;","09/Mar/22 11:51;morhidi;[~wangyang0918] if not urgent I can work on it later this week. In this case feel free to assign it to me.;;;","09/Mar/22 12:20;wangyang0918;[~matyas] Great. You have been assigned.;;;","10/Mar/22 03:50;wangyang0918;It seems that the current behavior is not correct. We add the ingress rules only when flink-operator ingress exists. I could not find when the ingress is created.;;;","10/Mar/22 07:58;morhidi;[~wangyang0918] you can enable it from helm.;;;","11/Mar/22 02:19;wangyang0918;You are right. We might need to document this feature and validate the CR in webhook whether the ingressDomain is configured without having ingress.;;;","17/Mar/22 12:58;gyfora;merged to main: 4e15ac7563e8ebc4cd328d9e89341fc2fca9f1bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the issue that exceptions generated in startup are missed in Python loopback mode,FLINK-26543,13432789,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,09/Mar/22 06:52,09/Mar/22 08:40,13/Jul/23 08:08,09/Mar/22 08:40,1.14.3,1.15.0,,,,,1.14.5,1.15.0,,,API / Python,,,,,0,pull-request-available,,,,,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 09 08:40:38 UTC 2022,,,,,,,,,,"0|z10b88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/22 08:40;hxbks2ks;Merged into master via 23f5fe2b13aaaebb581adab8ec606999ededdb0a
Merged into release-1.14 via a5fcaec68bce683e6b9e56a5c6347fee296e41b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink RemoteKeyedStateBackend#merge_namespaces bug,FLINK-26536,13432597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,08/Mar/22 09:06,22/Mar/22 14:12,13/Jul/23 08:08,22/Mar/22 12:09,1.13.0,,,,,,1.13.7,1.14.5,1.15.0,,API / Python,,,,,0,pull-request-available,,,"There's two bugs in RemoteKeyedStateBackend#merge_namespaces:
 * data in source namespaces are not commited before merging
 * target namespace is added at the head of sources_bytes, which causes java side SimpleStateRequestHandler to leave one source namespace unmerged",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 22 12:09:16 UTC 2022,,,,,,,,,,"0|z10a1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/22 12:09;dianfu;Fixed in:
 - master via a54c11040dd0103ffce5146a09a2ab6103134906
 - release-1.15 via 41ee1d23af17644eb312c704eba9c6dcf4809745
 - release-1.14 via 28f04b1871600e04047f1cd393826993178784d2
 - release-1.13 via 9abd874ee1c1d9ef9294a23b67d5ab7b4e956008;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shuffle by sink's primary key should cover the case that input changelog stream has a different parallelism,FLINK-26534,13432579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,08/Mar/22 07:05,14/Mar/22 04:12,13/Jul/23 08:08,14/Mar/22 04:12,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"FLINK-20370 fix the wrong result when sink primary key is not the same with query and introduced a new auto-keyby sink's primary key strategy for append stream if the sink's parallelism differs from input stream's.

But still exists one case to be solved:
for a changelog stream, its changelog upsert key same as sink's primary key, but sink's parallelism changed by user (via those sinks which implement the `ParallelismProvider` interface, e.g., KafkaDynamicSink), we should fix it.

And a minor change: keyby canbe omitted when sink has single parallism (because none partitioner will cause worse disorder)

",,jark,libenchao,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 04:12:45 UTC 2022,,,,,,,,,,"0|z109xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/22 04:12;jark;Fixed in master: 15d2239376244ecae6b52d0a365510ff1883a8a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkMetricsITCase.testMetrics failed on azure,FLINK-26532,13432573,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jingge,gaoyunhaii,gaoyunhaii,08/Mar/22 06:44,14/Mar/22 10:37,13/Jul/23 08:08,14/Mar/22 10:37,1.15.0,,,,,,1.15.0,,,,API / Core,,,,,0,pull-request-available,test-stability,,"{code:java}
Mar 08 05:38:35 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.512 s <<< FAILURE! - in org.apache.flink.test.streaming.runtime.SinkMetricsITCase
Mar 08 05:38:35 [ERROR] org.apache.flink.test.streaming.runtime.SinkMetricsITCase.testMetrics  Time elapsed: 1.607 s  <<< FAILURE!
Mar 08 05:38:35 java.lang.AssertionError: 
Mar 08 05:38:35 
Mar 08 05:38:35 Expected: Counter with <4L>
Mar 08 05:38:35      but: Counter with was <0L>
Mar 08 05:38:35 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Mar 08 05:38:35 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Mar 08 05:38:35 	at org.apache.flink.test.streaming.runtime.SinkMetricsITCase.assertSinkMetrics(SinkMetricsITCase.java:139)
Mar 08 05:38:35 	at org.apache.flink.test.streaming.runtime.SinkMetricsITCase.testMetrics(SinkMetricsITCase.java:113)
Mar 08 05:38:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 08 05:38:35 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 08 05:38:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 08 05:38:35 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 08 05:38:35 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 08 05:38:35 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 08 05:38:35 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 08 05:38:35 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 08 05:38:35 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 08 05:38:35 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 08 05:38:35 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 08 05:38:35 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 08 05:38:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 08 05:38:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Mar 08 05:38:35 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 08 05:38:35 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 08 05:38:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Mar 08 05:38:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Mar 08 05:38:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32655&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=6032",,fpaul,gaoyunhaii,jingge,mapohl,roman,smattheis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 10:37:30 UTC 2022,,,,,,,,,,"0|z109wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/22 08:08;fpaul;This might be caused by FLINK-26126

[~jinge] ;;;","09/Mar/22 08:25;gaoyunhaii;I first change the fixVersion to 1.15 for tracking. We could change it to the next version if we decide to~;;;","09/Mar/22 19:26;jingge;It seems like a flaky test. I could not reproduce it locally. More failed cases are expected.;;;","10/Mar/22 13:12;roman;[https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1463&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=12799];;;","11/Mar/22 15:23;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=855&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=13065;;;","11/Mar/22 18:12;smattheis;Same issue for me:

[https://dev.azure.com/smattheis/Flink/_build/results?buildId=34&view=logs&j=281fe72b-9353-5bae-d857-998bea2baeb7&t=7e9b251b-f1e8-5b74-3868-6358f6faedda&l=6292]

[https://dev.azure.com/smattheis/Flink/_build/results?buildId=35&view=logs&j=281fe72b-9353-5bae-d857-998bea2baeb7&t=7e9b251b-f1e8-5b74-3868-6358f6faedda&l=6292]

Also couldn't reproduce it reliably locally and it seems (not sure though) that it occured only in interference with other tests.;;;","14/Mar/22 02:09;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32945&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=6034;;;","14/Mar/22 05:47;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32945&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=6030;;;","14/Mar/22 09:31;jingge;Found solution to fix the bug, PR is under review.;;;","14/Mar/22 10:37;jingge;merged to the master: 1a899767aa1932495ee0056e384dba3ece2f0e9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaWriterITCase.testMetadataPublisher  failed on azure,FLINK-26531,13432569,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,gaoyunhaii,gaoyunhaii,08/Mar/22 06:33,09/Mar/22 03:45,13/Jul/23 08:08,09/Mar/22 03:45,1.15.0,,,,,,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,"{code:java}
022-03-07T13:43:34.3882626Z Mar 07 13:43:34 [ERROR] org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testMetadataPublisher  Time elapsed: 0.205 s  <<< FAILURE!
2022-03-07T13:43:34.3883743Z Mar 07 13:43:34 java.lang.AssertionError: 
2022-03-07T13:43:34.3884867Z Mar 07 13:43:34 
2022-03-07T13:43:34.3885412Z Mar 07 13:43:34 Expecting actual:
2022-03-07T13:43:34.3886464Z Mar 07 13:43:34   [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.3887361Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.3888222Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.3888833Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.3892032Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.3893140Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.3893849Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.3895077Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.3895779Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.3896423Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.3897164Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.3897792Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.3949208Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.3950956Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.3952287Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.3954341Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.3955834Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.3957048Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.3958287Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.3959519Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.3960798Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.3961973Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.3963302Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.3964563Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.3966941Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.3968246Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.3969452Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.3970656Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.3971853Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.3974163Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.3975441Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.3976380Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.3977278Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.3978197Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.3979120Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.3980051Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.3981017Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.3981952Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.3982975Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.3983882Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.3984940Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.3985838Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.3986702Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.3987661Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.3988564Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.3989444Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.3990347Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.3991206Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.3992100Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.3993091Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.3994383Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.3995399Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.3996287Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.3997270Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.3998095Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.3998915Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.3999752Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4000838Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4001693Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4002979Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4003852Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4004693Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4005626Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4006473Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4007269Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4008109Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4008948Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4009771Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4010608Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4011436Z Mar 07 13:43:34     ""testMetadataPublisher-0@69""]
2022-03-07T13:43:34.4011993Z Mar 07 13:43:34 to be equal to:
2022-03-07T13:43:34.4012951Z Mar 07 13:43:34   [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4013825Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4014670Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4015693Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4016554Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4017408Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4018257Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4019101Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4019961Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4020826Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4021702Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4022572Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4023663Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4024553Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4025531Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4026421Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4027296Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4028132Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4029000Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4029870Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4030733Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4031578Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4032456Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4033487Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4034372Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4035335Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4036217Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4037098Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4037963Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4038783Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4039621Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4040479Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4041774Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4042684Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4043595Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4044311Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4045166Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4045886Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4046625Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4047419Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4048409Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4049167Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4049936Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4050682Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4051494Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4052280Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4053217Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4053988Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4054891Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4055671Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4056416Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4057149Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4058037Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4059007Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4059996Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4060761Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4061524Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4062243Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4063142Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4063867Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4064594Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4065485Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4066207Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4066926Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4067666Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4068423Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4069208Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4069987Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4070747Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4071503Z Mar 07 13:43:34     ""testMetadataPublisher-0@69"",
2022-03-07T13:43:34.4072260Z Mar 07 13:43:34     ""testMetadataPublisher-0@70"",
2022-03-07T13:43:34.4073205Z Mar 07 13:43:34     ""testMetadataPublisher-0@71"",
2022-03-07T13:43:34.4073954Z Mar 07 13:43:34     ""testMetadataPublisher-0@72"",
2022-03-07T13:43:34.4074687Z Mar 07 13:43:34     ""testMetadataPublisher-0@73"",
2022-03-07T13:43:34.4075576Z Mar 07 13:43:34     ""testMetadataPublisher-0@74"",
2022-03-07T13:43:34.4076438Z Mar 07 13:43:34     ""testMetadataPublisher-0@75"",
2022-03-07T13:43:34.4077280Z Mar 07 13:43:34     ""testMetadataPublisher-0@76"",
2022-03-07T13:43:34.4078103Z Mar 07 13:43:34     ""testMetadataPublisher-0@77"",
2022-03-07T13:43:34.4078906Z Mar 07 13:43:34     ""testMetadataPublisher-0@78"",
2022-03-07T13:43:34.4079740Z Mar 07 13:43:34     ""testMetadataPublisher-0@79"",
2022-03-07T13:43:34.4080568Z Mar 07 13:43:34     ""testMetadataPublisher-0@80"",
2022-03-07T13:43:34.4081382Z Mar 07 13:43:34     ""testMetadataPublisher-0@81"",
2022-03-07T13:43:34.4082183Z Mar 07 13:43:34     ""testMetadataPublisher-0@82"",
2022-03-07T13:43:34.4083434Z Mar 07 13:43:34     ""testMetadataPublisher-0@83"",
2022-03-07T13:43:34.4084219Z Mar 07 13:43:34     ""testMetadataPublisher-0@84"",
2022-03-07T13:43:34.4085164Z Mar 07 13:43:34     ""testMetadataPublisher-0@85"",
2022-03-07T13:43:34.4085981Z Mar 07 13:43:34     ""testMetadataPublisher-0@86"",
2022-03-07T13:43:34.4086783Z Mar 07 13:43:34     ""testMetadataPublisher-0@87"",
2022-03-07T13:43:34.4087594Z Mar 07 13:43:34     ""testMetadataPublisher-0@88"",
2022-03-07T13:43:34.4088399Z Mar 07 13:43:34     ""testMetadataPublisher-0@89"",
2022-03-07T13:43:34.4089197Z Mar 07 13:43:34     ""testMetadataPublisher-0@90"",
2022-03-07T13:43:34.4090016Z Mar 07 13:43:34     ""testMetadataPublisher-0@91"",
2022-03-07T13:43:34.4091073Z Mar 07 13:43:34     ""testMetadataPublisher-0@92"",
2022-03-07T13:43:34.4091883Z Mar 07 13:43:34     ""testMetadataPublisher-0@93"",
2022-03-07T13:43:34.4092685Z Mar 07 13:43:34     ""testMetadataPublisher-0@94"",
2022-03-07T13:43:34.4093638Z Mar 07 13:43:34     ""testMetadataPublisher-0@95"",
2022-03-07T13:43:34.4094429Z Mar 07 13:43:34     ""testMetadataPublisher-0@96"",
2022-03-07T13:43:34.4095340Z Mar 07 13:43:34     ""testMetadataPublisher-0@97"",
2022-03-07T13:43:34.4096112Z Mar 07 13:43:34     ""testMetadataPublisher-0@98"",
2022-03-07T13:43:34.4096909Z Mar 07 13:43:34     ""testMetadataPublisher-0@99""]
2022-03-07T13:43:34.4097650Z Mar 07 13:43:34 when recursively comparing field by field, but found the following difference:
2022-03-07T13:43:34.4098330Z Mar 07 13:43:34 
2022-03-07T13:43:34.4098903Z Mar 07 13:43:34 Top level actual and expected objects differ:
2022-03-07T13:43:34.4099837Z Mar 07 13:43:34 - actual value  : [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4100698Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4101474Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4102387Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4103366Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4104138Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4105064Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4105984Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4106818Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4107589Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4108447Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4109302Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4110170Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4111034Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4111918Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4113454Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4114503Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4115478Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4116351Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4117223Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4118093Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4118957Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4119823Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4120664Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4121523Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4122384Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4123353Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4124229Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4125165Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4126006Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4126874Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4127731Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4129264Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4130106Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4130969Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4131840Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4132709Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4133674Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4134532Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4135442Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4136523Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4137388Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4138251Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4139110Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4139970Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4140817Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4141681Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4142544Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4143507Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4144371Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4145318Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4146161Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4147023Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4147874Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4148716Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4149571Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4150426Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4151259Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4152128Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4153090Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4153968Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4154903Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4155764Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4156608Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4157477Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4158334Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4159195Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4160059Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4160917Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4161745Z Mar 07 13:43:34     ""testMetadataPublisher-0@69""]
2022-03-07T13:43:34.4162674Z Mar 07 13:43:34 - expected value: [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4163702Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4164568Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4165546Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4166403Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4167243Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4168103Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4168961Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4169823Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4170689Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4171559Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4172403Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4173359Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4174233Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4175394Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4176266Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4177121Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4177964Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4178833Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4179694Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4180556Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4181413Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4182411Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4183347Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4184211Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4185151Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4186023Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4186901Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4187764Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4188609Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4189479Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4190338Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4191198Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4192060Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4193009Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4193872Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4194809Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4195680Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4196549Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4197410Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4198275Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4199110Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4199970Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4200826Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4201685Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4202544Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4203506Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4204356Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4205303Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4206169Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4207030Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4207895Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4208754Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4209606Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4210468Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4211327Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4212182Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4213139Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4214002Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4214920Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4215784Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4216653Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4217523Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4218387Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4219251Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4220089Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4222481Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4223512Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4224379Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4225326Z Mar 07 13:43:34     ""testMetadataPublisher-0@69"",
2022-03-07T13:43:34.4226187Z Mar 07 13:43:34     ""testMetadataPublisher-0@70"",
2022-03-07T13:43:34.4227046Z Mar 07 13:43:34     ""testMetadataPublisher-0@71"",
2022-03-07T13:43:34.4227888Z Mar 07 13:43:34     ""testMetadataPublisher-0@72"",
2022-03-07T13:43:34.4228748Z Mar 07 13:43:34     ""testMetadataPublisher-0@73"",
2022-03-07T13:43:34.4229755Z Mar 07 13:43:34     ""testMetadataPublisher-0@74"",
2022-03-07T13:43:34.4230618Z Mar 07 13:43:34     ""testMetadataPublisher-0@75"",
2022-03-07T13:43:34.4231483Z Mar 07 13:43:34     ""testMetadataPublisher-0@76"",
2022-03-07T13:43:34.4232344Z Mar 07 13:43:34     ""testMetadataPublisher-0@77"",
2022-03-07T13:43:34.4233292Z Mar 07 13:43:34     ""testMetadataPublisher-0@78"",
2022-03-07T13:43:34.4234161Z Mar 07 13:43:34     ""testMetadataPublisher-0@79"",
2022-03-07T13:43:34.4235139Z Mar 07 13:43:34     ""testMetadataPublisher-0@80"",
2022-03-07T13:43:34.4236004Z Mar 07 13:43:34     ""testMetadataPublisher-0@81"",
2022-03-07T13:43:34.4236856Z Mar 07 13:43:34     ""testMetadataPublisher-0@82"",
2022-03-07T13:43:34.4237713Z Mar 07 13:43:34     ""testMetadataPublisher-0@83"",
2022-03-07T13:43:34.4238554Z Mar 07 13:43:34     ""testMetadataPublisher-0@84"",
2022-03-07T13:43:34.4239420Z Mar 07 13:43:34     ""testMetadataPublisher-0@85"",
2022-03-07T13:43:34.4240278Z Mar 07 13:43:34     ""testMetadataPublisher-0@86"",
2022-03-07T13:43:34.4241143Z Mar 07 13:43:34     ""testMetadataPublisher-0@87"",
2022-03-07T13:43:34.4242004Z Mar 07 13:43:34     ""testMetadataPublisher-0@88"",
2022-03-07T13:43:34.4242931Z Mar 07 13:43:34     ""testMetadataPublisher-0@89"",
2022-03-07T13:43:34.4243787Z Mar 07 13:43:34     ""testMetadataPublisher-0@90"",
2022-03-07T13:43:34.4244657Z Mar 07 13:43:34     ""testMetadataPublisher-0@91"",
2022-03-07T13:43:34.4245634Z Mar 07 13:43:34     ""testMetadataPublisher-0@92"",
2022-03-07T13:43:34.4246508Z Mar 07 13:43:34     ""testMetadataPublisher-0@93"",
2022-03-07T13:43:34.4247365Z Mar 07 13:43:34     ""testMetadataPublisher-0@94"",
2022-03-07T13:43:34.4248219Z Mar 07 13:43:34     ""testMetadataPublisher-0@95"",
2022-03-07T13:43:34.4249064Z Mar 07 13:43:34     ""testMetadataPublisher-0@96"",
2022-03-07T13:43:34.4249919Z Mar 07 13:43:34     ""testMetadataPublisher-0@97"",
2022-03-07T13:43:34.4250774Z Mar 07 13:43:34     ""testMetadataPublisher-0@98"",
2022-03-07T13:43:34.4251627Z Mar 07 13:43:34     ""testMetadataPublisher-0@99""]
2022-03-07T13:43:34.4252463Z Mar 07 13:43:34 actual and expected values are collections of different size, actual size=70 when expected size=100
2022-03-07T13:43:34.4253310Z Mar 07 13:43:34 
2022-03-07T13:43:34.4254001Z Mar 07 13:43:34 The recursive comparison was performed with this configuration:
2022-03-07T13:43:34.4255270Z Mar 07 13:43:34 - no overridden equals methods were used in the comparison (except for java types)
2022-03-07T13:43:34.4256419Z Mar 07 13:43:34 - these types were compared with the following comparators:
2022-03-07T13:43:34.4257510Z Mar 07 13:43:34   - java.lang.Double -> DoubleComparator[precision=1.0E-15]
2022-03-07T13:43:34.4258541Z Mar 07 13:43:34   - java.lang.Float -> FloatComparator[precision=1.0E-6]
2022-03-07T13:43:34.4259606Z Mar 07 13:43:34   - java.nio.file.Path -> lexicographic comparator (Path natural order)
2022-03-07T13:43:34.4261366Z Mar 07 13:43:34 - actual and expected objects and their fields were compared field by field recursively even if they were not of the same type, this allows for example to compare a Person to a PersonDto (call strictTypeChecking(true) to change that behavior).
2022-03-07T13:43:34.4262493Z Mar 07 13:43:34 
2022-03-07T13:43:34.4263436Z Mar 07 13:43:34 	at org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testMetadataPublisher(KafkaWriterITCase.java:236)
2022-03-07T13:43:34.4264640Z Mar 07 13:43:34 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-07T13:43:34.4265699Z Mar 07 13:43:34 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-07T13:43:34.4266770Z Mar 07 13:43:34 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-07T13:43:34.4267730Z Mar 07 13:43:34 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-07T13:43:34.4268684Z Mar 07 13:43:34 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-03-07T13:43:34.4269757Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-03-07T13:43:34.4271099Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-03-07T13:43:34.4272304Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-03-07T13:43:34.4273543Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-03-07T13:43:34.4274806Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-03-07T13:43:34.4276066Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-03-07T13:43:34.4277323Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-03-07T13:43:34.4278583Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-03-07T13:43:34.4279873Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-03-07T13:43:34.4281114Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-03-07T13:43:34.4282354Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-03-07T13:43:34.4283570Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-03-07T13:43:34.4284671Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-03-07T13:43:34.4285944Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-03-07T13:43:34.4287198Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4288409Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-03-07T13:43:34.4289624Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-03-07T13:43:34.4290798Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-03-07T13:43:34.4291999Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-03-07T13:43:34.4293287Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4294482Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4295692Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4296804Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4298169Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4299357Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4300494Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4301851Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4303669Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-03-07T13:43:34.4305318Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-03-07T13:43:34.4306715Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-03-07T13:43:34.4307899Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4309109Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4310224Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4311342Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4312547Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4313807Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4315052Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4316417Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4317981Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-03-07T13:43:34.4319355Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-03-07T13:43:34.4320552Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4321766Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4322967Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4324075Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4325327Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4326373Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4327462Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4328690Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4330020Z Mar 07 13:43:34 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-03-07T13:43:34.4330850Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-03-07T13:43:34.4331809Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-03-07T13:43:34.4332901Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-03-07T13:43:34.4333858Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32628&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36036",,gaoyunhaii,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 09 03:45:26 UTC 2022,,,,,,,,,,"0|z109vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/22 06:34;gaoyunhaii;Hi [~lzljs3620320]  could you have a look at this test~?;;;","08/Mar/22 06:36;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32628&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36037;;;","08/Mar/22 06:55;lzljs3620320;Thanks for reporting. I will take a look~;;;","09/Mar/22 03:45;gaoyunhaii;Fix on master via 92a6850250df658a2310e6d45bd30f3e76dc0b96.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Normalize the decided parallelism to power of 2 when using adaptive batch scheduler,FLINK-26517,13432387,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wanglijie,wanglijie,wanglijie,07/Mar/22 09:32,10/Mar/22 02:22,13/Jul/23 08:08,10/Mar/22 02:22,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"As describe in FLINK-26330, in order to make the number of subpartitoins evenly consumed by downstream tasks, we need to normalize the decided parallelism to 2^N.",,wanglijie,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25011,,,,,,,,,,,,,FLINK-26330,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 02:22:59 UTC 2022,,,,,,,,,,"0|z108rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 02:22;zhuzh;master/release-1.15:
4d42c642cc3c45acf25d41172f6a2132d4c6c60b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink V2 is not state compatible with Sink V1,FLINK-26516,13432384,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,fpaul,fpaul,07/Mar/22 09:16,09/Mar/22 10:03,13/Jul/23 08:08,08/Mar/22 13:52,1.15.0,,,,,,1.15.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"While working https://issues.apache.org/jira/browse/FLINK-26173 we decided to split off the state compatibility issue to lower the priority of the behavioral issue since it does not affect many users.

 

This ticket is solely responsible to fix the state incompatibility between Sink V1 and Sink V2.",,fpaul,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,FLINK-26416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 08 13:52:41 UTC 2022,,,,,,,,,,"0|z108qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/22 13:52;gaoyunhaii;Fix on master via 13b203fef748bdbe9b1d14ba01f23ca6c6b24b7e^..955e5ff34082ff8a4a46bb74889612235458eb76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the incorrect type error in unbounded Python UDAF,FLINK-26504,13432314,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,07/Mar/22 03:13,07/Mar/22 12:38,13/Jul/23 08:08,07/Mar/22 12:38,1.13.2,1.14.0,1.15.0,,,,1.13.7,1.14.5,1.15.0,,API / Python,,,,,0,pull-request-available,,,"The stack trace is 
{code:java}
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
    response = task()
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
    element.data)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 71, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 84, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/operations.py"", line 115, in process_element
    return self.func(value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/operations.py"", line 384, in process_element_or_timer
    self.group_agg_function.on_timer(input_data[3])
TypeError: Argument 'key' has incorrect type (expected pyflink.fn_execution.coder_impl_fast.InternalRow, got Row)

	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:60)
	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:381)
{code}
The condition for this bug to occur is that state clean is triggered. The workaround is to disable table.exec.state.ttl

",,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 12:38:32 UTC 2022,,,,,,,,,,"0|z108bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 12:38;hxbks2ks;Merged into master via 8746017cc5c3f2f69a0c5cf09d01462ded470a14
Merged into release-1.14 via 6d3aa1839b5e4a9a099d4c59384430965e80bc00
Merged into release-1.13 via 812e674ee00cbca78ad8db26d5e2ccd739a36975;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple component leader election has different close/stop behavior ,FLINK-26502,13432297,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,nsemmler,nsemmler,06/Mar/22 19:38,14/Mar/22 09:53,13/Jul/23 08:08,07/Mar/22 14:00,1.15.0,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"The new multi component leader election driver behaves different when the service is closed.

The [ZooKeeperLeaderElectionDriver#close|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperLeaderElectionDriver.java#L120] method closes the Zookeeper connection. In contrast, the [MultipleComponentLeaderElectionDriverAdapter#close|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/MultipleComponentLeaderElectionDriverAdapter.java#L55] (also a LeaderElectionDriver object) only unregisters the event handler. It relies on the [ZooKeeperMultipleComponentLeaderElectionDriver#close|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriver.java#L106] method to close the Zookeeper connection. Currently this method is only called when the HighAvailabilityServices are stopped. This difference in behavior means that previously enabled tests are not working anymore FLINK-25235.

Together with [~dmvk], I propose that we align the behavior of the MultipleComponentLeaderElectionDriverAdapter to the existing behavior. We can do so by closing the Zookeeper connection once all components that are part of the multiple component leader election have stopped.

PS: I use ZooKeeper here as an example, the same can be said about Kubernetes as well.",,nsemmler,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26630,,,,,FLINK-25235,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 14:00:53 UTC 2022,,,,,,,,,,"0|z1087k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 14:00;nsemmler;We found a better solution. Instead, we will ensure that the TestingMiniCluster stops HaServices of additional JobManagers.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quickstarts Scala nightly end-to-end test failed on azure due to checkponts failed and logs contains exceptions,FLINK-26501,13432281,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,gaoyunhaii,gaoyunhaii,06/Mar/22 15:57,14/Mar/22 12:32,13/Jul/23 08:08,14/Mar/22 12:32,1.14.3,1.15.0,,,,,1.14.5,1.15.0,,,API / Scala,Runtime / Checkpointing,,,,0,pull-request-available,test-stability,,"{code:java}
2022-03-05T02:35:36.4040037Z Mar 05 02:35:36 2022-03-05 02:35:34,334 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1646447734295 for job b236087395260dc34648b84c2b86d6e8.
2022-03-05T02:35:36.4041701Z Mar 05 02:35:36 2022-03-05 02:35:34,387 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Decline checkpoint 1 by task e8a324cae6bf452d32db6797bbbafad0 of job b236087395260dc34648b84c2b86d6e8 at 127.0.0.1:45911-0a50f5 @ localhost (dataPort=44047).
2022-03-05T02:35:36.4043279Z Mar 05 02:35:36 org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -> Map -> Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)
2022-03-05T02:35:36.4044531Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4045729Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4047172Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4049092Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4050158Z Mar 05 02:35:36 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]
2022-03-05T02:35:36.4050929Z Mar 05 02:35:36 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]
2022-03-05T02:35:36.4051776Z Mar 05 02:35:36 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]
2022-03-05T02:35:36.4052559Z Mar 05 02:35:36 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]
2022-03-05T02:35:36.4053373Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
2022-03-05T02:35:36.4054849Z Mar 05 02:35:36 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
2022-03-05T02:35:36.4055685Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
2022-03-05T02:35:36.4056461Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
2022-03-05T02:35:36.4057219Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
2022-03-05T02:35:36.4057899Z Mar 05 02:35:36 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
2022-03-05T02:35:36.4059666Z Mar 05 02:35:36 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
2022-03-05T02:35:36.4061005Z Mar 05 02:35:36 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4062324Z Mar 05 02:35:36 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4063941Z Mar 05 02:35:36 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
2022-03-05T02:35:36.4065009Z Mar 05 02:35:36 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4066205Z Mar 05 02:35:36 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4067514Z Mar 05 02:35:36 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4068255Z Mar 05 02:35:36 	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
2022-03-05T02:35:36.4069019Z Mar 05 02:35:36 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
2022-03-05T02:35:36.4069638Z Mar 05 02:35:36 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
2022-03-05T02:35:36.4070271Z Mar 05 02:35:36 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
2022-03-05T02:35:36.4070862Z Mar 05 02:35:36 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
2022-03-05T02:35:36.4071453Z Mar 05 02:35:36 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
2022-03-05T02:35:36.4072430Z Mar 05 02:35:36 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
2022-03-05T02:35:36.4073023Z Mar 05 02:35:36 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
2022-03-05T02:35:36.4073687Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]
2022-03-05T02:35:36.4074596Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]
2022-03-05T02:35:36.4075712Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]
2022-03-05T02:35:36.4076437Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]
2022-03-05T02:35:36.4077754Z Mar 05 02:35:36 2022-03-05 02:35:34,410 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint 1 for job b236087395260dc34648b84c2b86d6e8. (0 consecutive failed attempts so far)
2022-03-05T02:35:36.4078865Z Mar 05 02:35:36 org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was declined (task is closing)
2022-03-05T02:35:36.4080161Z Mar 05 02:35:36 	at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4081619Z Mar 05 02:35:36 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:988) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4083063Z Mar 05 02:35:36 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4085407Z Mar 05 02:35:36 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4086635Z Mar 05 02:35:36 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]
2022-03-05T02:35:36.4087419Z Mar 05 02:35:36 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]
2022-03-05T02:35:36.4088438Z Mar 05 02:35:36 	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]
2022-03-05T02:35:36.4089614Z Mar 05 02:35:36 Caused by: org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -> Map -> Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)
2022-03-05T02:35:36.4090937Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4092177Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4093430Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4094740Z Mar 05 02:35:36 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4095836Z Mar 05 02:35:36 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]
2022-03-05T02:35:36.4096579Z Mar 05 02:35:36 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]
2022-03-05T02:35:36.4097766Z Mar 05 02:35:36 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]
2022-03-05T02:35:36.4098684Z Mar 05 02:35:36 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]
2022-03-05T02:35:36.4101381Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
2022-03-05T02:35:36.4102353Z Mar 05 02:35:36 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
2022-03-05T02:35:36.4103218Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
2022-03-05T02:35:36.4104019Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
2022-03-05T02:35:36.4104801Z Mar 05 02:35:36 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
2022-03-05T02:35:36.4105719Z Mar 05 02:35:36 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
2022-03-05T02:35:36.4108356Z Mar 05 02:35:36 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
2022-03-05T02:35:36.4110333Z Mar 05 02:35:36 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4112523Z Mar 05 02:35:36 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4113601Z Mar 05 02:35:36 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
2022-03-05T02:35:36.4114790Z Mar 05 02:35:36 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4116110Z Mar 05 02:35:36 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4117636Z Mar 05 02:35:36 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-03-05T02:35:36.4118641Z Mar 05 02:35:36 	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
2022-03-05T02:35:36.4119307Z Mar 05 02:35:36 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
2022-03-05T02:35:36.4120161Z Mar 05 02:35:36 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
2022-03-05T02:35:36.4120842Z Mar 05 02:35:36 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
2022-03-05T02:35:36.4121482Z Mar 05 02:35:36 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
2022-03-05T02:35:36.4122113Z Mar 05 02:35:36 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
2022-03-05T02:35:36.4122736Z Mar 05 02:35:36 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
2022-03-05T02:35:36.4123332Z Mar 05 02:35:36 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
2022-03-05T02:35:36.4123984Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]
2022-03-05T02:35:36.4124749Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]
2022-03-05T02:35:36.4125750Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]
2022-03-05T02:35:36.4126591Z Mar 05 02:35:36 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]
2022-03-05T02:35:36.4128133Z Mar 05 02:35:36 2022-03-05 02:35:34,430 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Sequence Source (Deprecated) -> Map -> Sink: Unnamed (1/1) (e8a324cae6bf452d32db6797bbbafad0) switched from RUNNING to FINISHED. {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32553&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=18735]",,dwysakowicz,gaoyunhaii,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 12:32:00 UTC 2022,,,,,,,,,,"0|z10840:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/22 16:09;gaoyunhaii;Same issue for Quickstarts Java nightly end-to-end test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32569&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=89ed5489-a970-5ff2-67f7-d7391de0165f&l=19137;;;","07/Mar/22 08:52;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32569&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=89ed5489-a970-5ff2-67f7-d7391de0165f&l=19179;;;","08/Mar/22 06:48;gaoyunhaii;1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32594&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=17640;;;","10/Mar/22 03:13;gaoyunhaii;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32717&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=16804;;;","11/Mar/22 08:21;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32881&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=3290;;;","14/Mar/22 12:32;dwysakowicz;Fixed in:
* master
** e981675f5dfe6d561eead5e151a750ea042eaf2c
* 1.14.5
** 416a0c1f663c3c81e52e937fc5687b2f67fbe251;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerClusterITCase.testAutomaticScaleUp failed on azure,FLINK-26500,13432280,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,gaoyunhaii,gaoyunhaii,06/Mar/22 15:46,16/Mar/22 12:09,13/Jul/23 08:08,15/Mar/22 07:10,1.14.4,1.15.0,1.16.0,,,,1.14.5,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"{code:java}
Mar 03 13:38:24 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.854 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase
Mar 03 13:38:24 [ERROR] org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp  Time elapsed: 16.035 s  <<< ERROR!
Mar 03 13:38:24 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Mar 03 13:38:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:167)
Mar 03 13:38:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 03 13:38:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 03 13:38:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:267)
Mar 03 13:38:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:147)
Mar 03 13:38:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 03 13:38:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 03 13:38:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 03 13:38:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 03 13:38:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 03 13:38:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 03 13:38:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 03 13:38:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 03 13:38:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Mar 03 13:38:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 03 13:38:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 03 13:38:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 03 13:38:24 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 03 13:38:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 03 13:38:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Mar 03 13:38:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Mar 03 13:38:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Mar 03 13:38:24 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32488&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9618",,gaoyunhaii,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23524,,,,,,,,,,"14/Mar/22 11:17;mapohl;test-failure.log;https://issues.apache.org/jira/secure/attachment/13041051/test-failure.log","14/Mar/22 11:17;mapohl;test-success.log;https://issues.apache.org/jira/secure/attachment/13041052/test-success.log",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 15 07:10:23 UTC 2022,,,,,,,,,,"0|z1083s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/22 11:20;mapohl;Looks like the 10s deadline to wait for job to reach to expected parallelism is a bit too low for Azure runs. The logs (see {{test-failure.log}} in comparison to a local successful run {{test-success.log}}) indicate that the job was still in initialization phase when the timeout was reached and the job got cancelled.;;;","15/Mar/22 07:10;mapohl;master: 1d2f1e73e250fc5a4a11e82fa6a6e77fb2637e37
1.15: 6d4d50da7b46155d4ffdb58d4b61eb1742a98932
1.14: cf1f70ee9d61b8d6eced9c77ddd6c9e3f5e492f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing logs during retry,FLINK-26494,13432061,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,04/Mar/22 17:09,10/Mar/22 14:17,13/Jul/23 08:08,10/Mar/22 14:17,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,The {{FutureRetry.retry}} functionality doesn't log the errors but just trigger a retry. This makes it harder for the user to figure out what's wrong.,,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26388,FLINK-26568,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 14:17:33 UTC 2022,,,,,,,,,,"0|z106rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 14:17;mapohl;master: c5352fc55972420ed5bf1afdfd97834540b1407a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The operator restart ingress is lost,FLINK-26486,13431884,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,leinenglian,leinenglian,04/Mar/22 07:37,09/Apr/22 22:52,13/Jul/23 08:08,09/Apr/22 22:52,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,,gyfora,leinenglian,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 31 09:29:57 UTC 2022,,,,,,,,,,"0|z105o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/22 13:20;gyfora;Could you please add a little more detail to this ticket? ;;;","31/Mar/22 09:29;mbalassi;[~leinenglian] please provide more information, we can not reproduce this issue from this much description. Also there have been much improvement on the ingress from in the past weeks, it might make sense to retest with the latest main branch or the 0.1.0 release candidate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] State not discarded after multiple retries,FLINK-26485,13431883,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,04/Mar/22 07:33,09/Mar/22 17:24,13/Jul/23 08:08,09/Mar/22 17:24,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 09 17:24:37 UTC 2022,,,,,,,,,,"0|z105o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/22 17:24;roman;Merged into master as a3080eec4d74eda12b563e7088f7d5bef417d38a..3fb170398d4d1d25c544825b07077b0942eb0106.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystem.delete is not implemented consistently,FLINK-26484,13431872,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,04/Mar/22 06:45,10/Mar/22 09:30,13/Jul/23 08:08,10/Mar/22 09:29,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,Runtime / Coordination,,,,0,pull-request-available,,,"The BlobServer cleanup does not work for the Presto S3 filesystem in case of failure due to some bug in the recursive delete implementation. The {{false}} return value is not processed which leads to an error case being ""swallowed"", i.e. recursive cleanups do not work in this case (see [PrestoS3FileSystem:496|https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L496])",,dmvk,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26450,,,FLINK-26488,,,,FLINK-26388,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 09:29:01 UTC 2022,,,,,,,,,,"0|z105lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/22 08:12;dmvk;This seems to be deep down in the Presto code, with limited options for any overrides. Can you please outline how do you want to tackle this?;;;","04/Mar/22 08:16;mapohl;We should implement the recursive deletion in {{FileSystem}} and only rely on {{FileSystem.deleteFile(path)}} (which is a replacement for {{FileSystem.delete(path, false)}}), {{FileSystem.exists(path)}}, and {{FileSystem.listStatuses(path)}}. We can remove redundant code in {{LocalFileSystem}} as well due to that.

{{LocalFileSystem}} and {{HadoopFileSystem}} are the only implementations that don't delegate to another {{FileSystem}} implementation internally.;;;","10/Mar/22 09:29;mapohl;Further discussion happened in the PR. No need to backport this change because we don't do retries in older versions of Flink.

master: 4907f5fb7033b1f918c4e721a1ccf95878f980a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce new metrics to represent full checkpoint size,FLINK-26464,13431686,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,03/Mar/22 11:14,07/Mar/22 02:50,13/Jul/23 08:08,07/Mar/22 02:50,,,,,,,1.15.0,,,,Runtime / Checkpointing,Runtime / Metrics,,,,0,pull-request-available,,,"After FLINK-25557, the meaning of previous metric {{lastCheckpointSize}} has been changed to last completed full checkpoint size. We could introduce another metric {{lastCheckpointFullSize}} to represent the meaning of last completed full checkpoint size and let {{lastCheckpointSize}} stay as it was.",,nkruber,qinjunjerry,Thesharing,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25557,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 02:50:24 UTC 2022,,,,,,,,,,"0|z104g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 02:50;yunta;Merged in master: b2f4c0ea259027ed3f6a04595202cf2589e2495a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentITCase should use MiniCluster,FLINK-26463,13431676,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,03/Mar/22 10:14,07/Mar/22 12:35,13/Jul/23 08:08,07/Mar/22 12:35,,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 12:35:08 UTC 2022,,,,,,,,,,"0|z104e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 12:35;twalthr;Fixed in master: 898f2d350391f96ca281baf538a5fe7dc376a9cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Unsupported type when convertTypeToSpec: MAP,FLINK-26460,13431653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,03/Mar/22 08:31,28/Apr/22 10:05,13/Jul/23 08:08,01/Apr/22 08:38,1.13.1,1.15.0,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"{code:java}
CREATE TABLE zm_test (
  `a` BIGINT,
  `m` MAP<STRING,BIGINT>
);
{code}
if we insert into zm_test use
{code:java}
INSERT INTO zm_test(`a`) SELECT `a` FROM MyTable;
{code}
then will throw Exception
{code:java}
Unsupported type when convertTypeToSpec: MAP
{code}
we must use
{code:java}
INSERT INTO zm_test SELECT `a`, cast(null AS MAP<STRING,BIGINT>) FROM MyTable;
{code}",,godfreyhe,jark,libenchao,tartarus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27438,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 01 08:38:41 UTC 2022,,,,,,,,,,"0|z1048w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/22 08:32;tartarus;[~jark]   please assign it to me, thanks;;;","01/Apr/22 08:38;godfreyhe;Fixed in
1.16.0: 33e7c84fb0f6aadf9d228c41c0ba6808634a7e36
1.15.0: 99bd36c986d4c6e34359696245b83de2b472e6a9
1.14.5: b161cb0261df6299f5670d9271588f235088194c
1.13.7: f3a8af5831b1b39025bc61ff7dc446d9435c197f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Materialization interleaved with task cancellation can fail the job,FLINK-26455,13431603,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,02/Mar/22 23:47,07/Mar/22 12:17,13/Jul/23 08:08,07/Mar/22 12:17,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"When the task is cancelled, AsyncCheckpointRunnables are cancelled as well.
Cancellation exception can reach PeriodicMaterializationManager.
When the failure limit is reached, it invokes asyncExceptionHandler.handleAsyncException, which fails the job.
",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 12:17:34 UTC 2022,,,,,,,,,,"0|z103xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 12:17;roman;Merged as b68da579ed0b993d4c6e709205bebc0df4c05e1a into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
execution.allow-client-job-configurations not checked for executeAsync,FLINK-26453,13431537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,matriv,matriv,02/Mar/22 16:53,07/Mar/22 07:49,13/Jul/23 08:08,07/Mar/22 07:49,1.15.0,,,,,,1.15.0,,,,API / DataStream,,,,,0,pull-request-available,,,"* *checkNotAllowedConfigurations()* should be called by  *StreamContextEnvironment#executeAsync()*
 * Description of the *DeploymentOption* should be more clear, and it's not only checked by application mode.
 * When using a config option which is the same as the one in the environment *(flink-conf.yaml + CLI options)*  we still throw an exception, and we also throwing the exception even if the option is not in the environment, but it's the default value of the option anyway. Should we check for those cases, or should we at least document them and say explicitly that no config option is allowed to be set, if the *execution.allow-client-job-configurations* is set to {*}false{*}?

 

 

 ",,fpaul,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25206,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 07:49:00 UTC 2022,,,,,,,,,,"0|z103jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 07:49;fpaul;Merged in master: 47c599cc95b305c5ae6db5a50f15cfef0a9755d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Flink deploy on k8s https SSLPeerUnverifiedException,FLINK-26452,13431523,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hjw,hjw,02/Mar/22 15:42,04/Mar/22 16:54,13/Jul/23 08:08,04/Mar/22 16:54,1.13.6,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,"~/.kube/config

apiVersion:v1
kind:config
cluster:
-name: ""yf-dev-cluster1""
  cluster:
    server: ""https://in-acpmanager.test.yfzx.cn/k8s/clusters/c-t5h2t""
    certificate-authority-data : “……""


{code:java}
2022-03-02 18:59:30 | OkHttp https://in-acpmanager.test.yfzx.cn/...io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener  
Exec Failure javax.net.ssl.SSLPeerUnverifiedException Hostname in-acpmanager.test.yfzx.cn not verified:
    certificate: sha256/cw2T2s+Swhl7z+H35/3C1dTLxL26OOMO5VoEN9kAZCA=
    DN: CN=in-acpmanager.test.yfzx.cn
    subjectAltNames: []
io.fabric8.kubernetes.client.KubernetesClientException: Failed to start websocket
        at io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.onFailure(WatcherWebSocketListener.java:77)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:570)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$1.onFailure(RealWebSocket.java:216)
        at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:180)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.lang.Throwable: waiting here
                at io.fabric8.kubernetes.client.utils.Utils.waitUntilReady(Utils.java:164)
                at io.fabric8.kubernetes.client.utils.Utils.waitUntilReadyOrFail(Utils.java:175)
                at io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.waitUntilReady(WatcherWebSocketListener.java:120)
                at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager.waitUntilReady(WatchConnectionManager.java:82)
                at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:705)
                at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:678)
                at io.fabric8.kubernetes.client.dsl.base.BaseOperation.watch(BaseOperation.java:
{code}



{code:java}
Caused by: javax.net.ssl.SSLPeerUnverifiedException: Hostname in-acpmanager.test.yfzx.cn not verified:
    certificate: sha256/cw2T2s+Swhl7z+H35/3C1dTLxL26OOMO5VoEN9kAZCA=
    DN: CN=in-acpmanager.test.yfzx.cn
    subjectAltNames: []
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnection.connectTls(RealConnection.java:350)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnection.establishProtocol(RealConnection.java:300)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnection.connect(RealConnection.java:185)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.java:224)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.java:108)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.java:88)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.Transmitter.newExchange(Transmitter.java:169)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:41)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:133)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at io.fabric8.kubernetes.client.utils.TokenRefreshInterceptor.intercept(TokenRefreshInterceptor.java:42)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createApplicableInterceptors$6(HttpClientUtils.java:290)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
        at org.apache.flink.kubernetes.shaded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
        at org.apache.flink.kubernetes.shaded.okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229)
        at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:172)
        ... 4 more
{code}


By the way . ""kubectl get pod -n namespace"" command is success in this node.  The node is configured with DNS.
",,hjw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-03-02 15:42:28.0,,,,,,,,,,"0|z103gg:",9223372036854775807,".kube/config add a line

insecure-skip-tls-verify : true",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileStateHandle.discardState does not process return value,FLINK-26450,13431504,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,02/Mar/22 13:15,10/Mar/22 12:43,13/Jul/23 08:08,10/Mar/22 12:07,1.13.6,1.14.3,1.15.0,,,,1.15.0,,,,Connectors / FileSystem,Runtime / Coordination,,,,0,pull-request-available,,,"The retryable cleanup does not work properly if there's an error appearing during the {{FileSystem.delete}} call which is used within [FileStateHandle.discardState|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FileStateHandle.java#L85]. Some {{FileSystem}} implementations (e.g. S3 presto; see [PrestoS3FileSystem:512|https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L512] through [PrestoS3FileSystem.delete(Path, boolean)|https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L480]) return {{false}} in case of an error which will be swallowed in {{FileStateHandle.discardState}}.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25484,,,,,,,FLINK-4910,,,,,,,FLINK-26484,FLINK-26488,,,FLINK-26388,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 12:07:16 UTC 2022,,,,,,,,,,"0|z103c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/22 17:39;mapohl;Tests become flaky due to this change, e.g. [this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=808&view=results]
{code}
2022-03-03 14:30:11,282 WARN  org.apache.flink.runtime.checkpoint.OperatorSubtaskState     [] - Error while discarding operator states.
java.io.IOException: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-47072687872/savepoint-e2e-test-chckpt-dir/b570100734a17ad72d8d2ccc712f681d/chk-11/73833c1e-bc28-4d68-8752-496d0ea65e8b could not be deleted for unknown reasons.
        at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:86) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.KeyGroupsStateHandle.discardState(KeyGroupsStateHandle.java:125) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.util.LambdaUtil.applyToAllWhileSuppressingExceptions(LambdaUtil.java:55) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.StateUtil.bestEffortDiscardAllStateObjects(StateUtil.java:62) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.discardState(OperatorSubtaskState.java:211) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.util.LambdaUtil.applyToAllWhileSuppressingExceptions(LambdaUtil.java:55) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.StateUtil.bestEffortDiscardAllStateObjects(StateUtil.java:62) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.TaskStateSnapshot.discardState(TaskStateSnapshot.java:156) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$1.run(CheckpointCoordinator.java:2007) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]
{code}

The error is logged in [CheckpointCoordinator:2009|https://github.com/apache/flink/blob/d91cb003221d65e07e135d510ff897f7520add6f/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L2009];;;","04/Mar/22 13:31;mapohl;This issue will be fixed as part of FLINK-25484;;;","07/Mar/22 12:30;mapohl;We have to reopen this issue again after deciding that the deletion logic shouldn't be fixed in {{FileSystem}} since it's annotated as {{@Public}} interface.;;;","10/Mar/22 12:07;mapohl;master: 2c4b296a1bb88c2fd33877a8c1ad3362af9d93ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactorOperatorStateHandler can not work with unaligned checkpoint,FLINK-26440,13431404,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,pltbkd,pltbkd,02/Mar/22 03:19,08/Mar/22 06:10,13/Jul/23 08:08,08/Mar/22 06:09,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,"As mentioned in FLINK-26314, CompactorOperatorStateHandler can not work with unaligned checkpoint currently. Though FLINK-26314 is actually caused by another issue in the writer, we should still fix this issue.",,gaoyunhaii,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26314,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 08 06:09:15 UTC 2022,,,,,,,,,,"0|z102q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/22 06:09;gaoyunhaii;Fix on master via 13b203fef748bdbe9b1d14ba01f23ca6c6b24b7e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot discover a connector using option: 'connector'='jdbc',FLINK-26437,13431346,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,arindbha,arindbha,01/Mar/22 18:43,11/Mar/22 06:50,13/Jul/23 08:08,11/Mar/22 06:50,1.13.6,,,,,,,,,,Table SQL / API,,,,,0,sql-api,table-api,,"Hi Team,

When I was running SQL in Flink SQL-API, was getting the below error - 

*Caused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='jdbc'*
        at org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:467)
        at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:441)
        at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:167)
        ... 32 more
Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'jdbc' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.

Available factory identifiers are:

blackhole
datagen
filesystem
kafka
print
upsert-kafka
        at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:319)
        at org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:463)
        ... 34 more

------------------------

 

SQL I was using - 

_CREATE TABLE pvuv_sink (_
 _dt varchar PRIMARY KEY,_
 _pv BIGINT,_
 _uv BIGINT_
_) WITH (_
 _'connector' = 'jdbc',_
 _'url' = 'jdbc:mysql://localhost:3306/flinksql_test',_
 _'table-name' = 'pvuv_sink',_
 _'username' = 'root',_
 _'password' = 'xxxxxx',_
 _'sink.buffer-flush.max-rows' = '1'_
_);_",,arindbha,jark,straw,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/22 02:03;straw;image-2022-03-03-10-03-50-763.png;https://issues.apache.org/jira/secure/attachment/13040665/image-2022-03-03-10-03-50-763.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 09 08:46:52 UTC 2022,,,,,,,,,,"0|z102d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/22 02:03;straw;Do you have {{[flink-connector-jdbc.jar|https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/jdbc/#dependencies]}} and mysql-connector-java.jar in the flink lib directory?;;;","02/Mar/22 09:52;arindbha;Thanks [~straw] . But now I am getting  error at Sink end - 

 

SQL:CREATE TABLE user_details_fs (
  user_id varchar,
  item_id varchar,
  category_id varchar,
  behavior varchar,
ts TIMESTAMP(3)
) WITH (
'connector' = 'filesystem',
  'path' = 'file:///Users/arindam.b/Documents/SparkCheckPointDirectory/user_details/',
  'format' = 'parquet'
  )
java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration
        at org.apache.flink.formats.parquet.ParquetFileFormatFactory.getParquetConfiguration(ParquetFileFormatFactory.java:115)
        at org.apache.flink.formats.parquet.ParquetFileFormatFactory.access$000(ParquetFileFormatFactory.java:51)
        at org.apache.flink.formats.parquet.ParquetFileFormatFactory$2.createRuntimeEncoder(ParquetFileFormatFactory.java:103)
        at org.apache.flink.formats.parquet.ParquetFileFormatFactory$2.createRuntimeEncoder(ParquetFileFormatFactory.java:97)
        at org.apache.flink.table.filesystem.FileSystemTableSink.createWriter(FileSystemTableSink.java:385)
        at org.apache.flink.table.filesystem.FileSystemTableSink.createStreamingSink(FileSystemTableSink.java:192)
        at org.apache.flink.table.filesystem.FileSystemTableSink.consume(FileSystemTableSink.java:153)
        at org.apache.flink.table.filesystem.FileSystemTableSink.lambda$getSinkRuntimeProvider$0(FileSystemTableSink.java:139)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.applySinkProvider(CommonExecSink.java:294)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.createSinkTransformation(CommonExecSink.java:145)
        at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:140)
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
        at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:70)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
        at scala.collection.Iterator.foreach(Iterator.scala:937)
        at scala.collection.Iterator.foreach$(Iterator.scala:937)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
        at scala.collection.IterableLike.foreach(IterableLike.scala:70)
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike.map(TraversableLike.scala:233)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:69)
        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1510)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1460)
        at huangxu.chase.flinksql.demo.SqlSubmit.run(SqlSubmit.java:49)
        at huangxu.chase.flinksql.demo.SqlSubmit.main(SqlSubmit.java:24)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration
        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        ... 43 more

 

I am running it on local machine, but getting the error for Hadoop ClassPath. 

Can you please help me here?

 

Thanks,

Arindam;;;","02/Mar/22 12:28;straw;You can try adding [hadoop jar|https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/flink/flink-shaded-hadoop-3-uber/3.1.1.7.2.1.0-327-9.0/flink-shaded-hadoop-3-uber-3.1.1.7.2.1.0-327-9.0.jar] to your flink lib directory.;;;","02/Mar/22 13:19;arindbha;[~straw] I have added the jar, but I am getting the below exception now - 

java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.UserGroupInformation
    at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:67)
    at org.apache.flink.runtime.security.SecurityUtils.installModules(SecurityUtils.java:76)
    at org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:57)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1131)

 

Also having added the below jars in flink library already - 

 

commons-compiler-3.1.1.jar            hadoop-client-3.3.0.jar
flink-connector-jdbc_2.12-1.13.6.jar        hadoop-client-runtime-3.3.0.jar
flink-csv-1.13.6.jar                hadoop-common-3.3.0.jar
flink-dist_2.12-1.13.6.jar            hadoop-hdfs-3.3.0.jar
flink-json-1.13.6.jar                hadoop-hdfs-client-3.3.0.jar
flink-orc_2.12-1.13.6.jar            kafka-clients-3.1.0.jar
flink-parquet_2.12-1.13.6.jar            log4j-1.2-api-2.17.1.jar
flink-shaded-zookeeper-3.4.14.jar        log4j-api-2.17.1.jar
flink-sql-connector-kafka_2.12-1.13.6.jar    log4j-core-2.17.1.jar
flink-streaming-scala_2.12-1.13.6.jar        log4j-slf4j-impl-2.17.1.jar
flink-table-api-scala-bridge_2.12-1.13.6.jar    mysql-connector-java-8.0.28.jar
flink-table-blink_2.12-1.13.6.jar        parquet-column-1.12.2.jar
flink-table-common-1.13.6.jar            parquet-common-1.12.2.jar
flink-table-planner-blink_2.12-1.13.6.jar    parquet-format-2.9.0.jar
flink-table-planner_2.12-1.13.6.jar        parquet-hadoop-1.12.2.jar
flink-table_2.12-1.13.6.jar            stax2-api-4.0.0.jar
guava-30.1-jre.jar                woodstox-core-6.2.8.jar

 

Please help me. Thanks in advance.;;;","02/Mar/22 13:27;arindbha;[~straw]  After putting this shaded jar , getting the below error now - 

 

java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;
    at org.apache.flink.client.cli.DynamicPropertiesUtil.<clinit>(DynamicPropertiesUtil.java:39)
    at org.apache.flink.client.cli.GenericCLI.addGeneralOptions(GenericCLI.java:108)
    at org.apache.flink.client.cli.CliFrontend.<init>(CliFrontend.java:136)
    at org.apache.flink.client.cli.CliFrontend.<init>(CliFrontend.java:119)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1129)

 

Seems to be dependency conflict. Can you please help?;;;","02/Mar/22 14:52;straw;How do you submit your sql? Are you using sql client locally?;;;","02/Mar/22 16:13;arindbha;[~straw] I have run with mode - sql client and jar as well through table api. Also if you have any sample code where we are getting data from kafka and writing the same to filesystem , which can help me to refer.;;;","03/Mar/22 02:07;straw;In my local env(wsl-ubuntu-20.04), I can run ""sql-client.sh embedded"" with jars, which are listed below, in flink lib.

!image-2022-03-03-10-03-50-763.png!

And the sql is almost like yours:
{code:java}
CREATE TABLE source (
  user_id varchar,
  item_id varchar,
  category_id varchar,
  behavior varchar,
ts TIMESTAMP(3)
) WITH (
'connector' = 'kafka',
    'topic' = 'test_json',
    'scan.startup.mode' = 'earliest-offset',
    'properties.bootstrap.servers' = 'localhost:9392',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true',
    'properties.group.id' = 'test');

CREATE TABLE user_details_fs  WITH (
'connector' = 'filesystem',
  'path' = 'file:///mnt/c/Users/zhuyuan/Desktop/',
  'format' = 'parquet'
  )  LIKE source (EXCLUDING ALL); 

insert into user_details_fs select * from source;{code};;;","09/Mar/22 06:43;jark;[~straw] please just put the flink-hadoop-uber jar[1] instead of separate various hadoop jars.  And please remove all the unnecessary jars.


[1]: https://flink.apache.org/downloads.html#additional-components;;;","09/Mar/22 06:51;straw;[~jark] Thanks for your advice. [~arindbha], you can try as [~jark] said.;;;","09/Mar/22 07:32;arindbha;Thanks [~straw]  and [~jark] . Can you please let me know what are the dependencies required for orc file sink? Also is it necessary to add external dependencies to flink library or we can bundle them together with the application JAR, will it work during runtime?

 ;;;","09/Mar/22 08:46;jark;[~arindbha], please download the orc format jar in this page[1]. 

[1]: https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/formats/orc/#dependencies


If you are submitting a DataStream jar job, it's suggested to bundle dependecies in your application jar. 
If you are submitting a SQL job via SQL Client, it's suggested to add jars via {{bin/sql-client.sh --jar <jar_path>}} option. But putting the dependencies under lib is also fine. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide the container name for CI debug log ,FLINK-26435,13431313,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,01/Mar/22 16:11,27/Mar/22 13:16,13/Jul/23 08:08,03/Mar/22 02:41,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Current kubectl log for the CI have not specified the container name as below

 
{code:java}
Flink logs:
Current logs for flink-operator-6c66c5dddd-9ptqw: 
error: a container name must be specified for pod flink-operator-6c66c5dddd-9ptqw, choose one of: [flink-operator flink-webhook] {code}",,aitozi,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 03 02:41:06 UTC 2022,,,,,,,,,,"0|z1025s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/22 08:22;wangyang0918;Nice catch. I think this is a bug and maybe we could move this ticket out of the umbrella.;;;","02/Mar/22 11:00;aitozi;Hmm.. It's come from the {{flink-kubernetes-operator}} project's ci, see [here|https://github.com/apache/flink-kubernetes-operator/runs/5378023577?check_suite_focus=true];;;","02/Mar/22 11:48;wangyang0918;What I means is make this ticket to a BUG, not a subtask of FLINK-25963.;;;","02/Mar/22 12:00;aitozi;Get it, Done. ;;;","03/Mar/22 02:41;wangyang0918;Fixed via:

main: 47f02cb7e0318dee16d321c91867b9a94337b9da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Chinese documentation with the new TablePipeline docs,FLINK-26422,13431195,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zoucao,slinkydeveloper,slinkydeveloper,01/Mar/22 10:02,22/Mar/22 02:23,13/Jul/23 08:08,22/Mar/22 02:23,1.15.0,,,,,,1.15.0,,,,chinese-translation,Documentation,,,,0,chinese-translation,pull-request-available,,Chinese docs needs to be updated with the content of this commit: https://github.com/apache/flink/commit/4f65c7950f2c3ef849f2094deab0e199ffedf57b,,leonard,slinkydeveloper,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 06:56:26 UTC 2022,,,,,,,,,,"0|z101fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/22 06:07;zoucao;Hi [~slinkydeveloper], I'm willing to fix it, could you assign the ticket to me?;;;","21/Mar/22 06:56;leonard;Fixed in 
master(1.16) : 6ae234546bd33f14ee3fdb50f59fd7085c735441
release-1.15: 50bfbdbbf1fe817d33aca3a675904ce682203a12  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkWriter should emit all the pending committables on endOfInput,FLINK-26403,13431052,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,gaoyunhaii,gaoyunhaii,28/Feb/22 17:34,04/Mar/22 09:13,13/Jul/23 08:08,03/Mar/22 03:43,1.15.0,,,,,,1.15.0,,,,API / Core,,,,,0,pull-request-available,,,"Currently the SinkWriterOperator not drained all the pending committables on endOfInput() and left them till final checkpoint, which would be in fact deserted. This might cause data loss or the CommitterOperator hanged on endOfInput() due to not received expected number of committables.",,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26314,FLINK-26321,,,,,,,FLINK-26321,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 03 03:42:43 UTC 2022,,,,,,,,,,"0|z100k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/22 03:42;gaoyunhaii;Fix on master via 2c9d64e34cfd3025c87c1d3bbd2d1b596df11691.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Upload is not failed even if all attempts timeout,FLINK-26396,13431000,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,28/Feb/22 14:02,07/Mar/22 13:18,13/Jul/23 08:08,07/Mar/22 13:18,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,,roman,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 13:18:14 UTC 2022,,,,,,,,,,"0|z1008o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 13:18;roman;Merged into master as 2292c19d5ac83e60d136b6fa8354b85d834c3c23..276a40a87698b8948153c6181180c930b33fe305;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The description of RAND_INTEGER is wrong in SQL function documents,FLINK-26395,13430981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,huwh,huwh,huwh,28/Feb/22 11:57,29/Mar/22 14:16,13/Jul/23 08:08,29/Mar/22 14:16,1.13.5,1.14.3,1.15.0,,,,1.14.5,1.15.0,1.16.0,,Documentation,,,,,0,pull-request-available,,,"RAND_INTEGER will returns a integer value, but document of SQL function shows it will return a double value.

!image-2022-02-28-19-57-18-390.png!",,huwh,libenchao,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/22 11:57;huwh;image-2022-02-28-19-57-18-390.png;https://issues.apache.org/jira/secure/attachment/13040528/image-2022-02-28-19-57-18-390.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 14:16:49 UTC 2022,,,,,,,,,,"0|z1004g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/22 09:22;libenchao;[~huwh] thanks for reporting this, assigned to you~;;;","16/Mar/22 02:05;huwh;[~libenchao] hi, I have submitted this PR, please help review when you have time;;;","28/Mar/22 06:57;martijnvisser;Fixed in master: adb6152b2361d58bf0de3108fb219e44c560a2de
Waiting for the backports to release-1.15 and release-1.14. ;;;","29/Mar/22 14:16;martijnvisser;Fixed in release-1.15: 34afc9b7434fec1c9895a2a3ed52cce3830b2676
Fixed in release-1.14: c75f6f3db99d829c3c097fdcb7a6f8e930577ca1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceLegacyITCase.testBrokerFailure hang on azure,FLINK-26387,13430930,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,gaoyunhaii,gaoyunhaii,28/Feb/22 07:40,07/Mar/22 09:44,13/Jul/23 08:08,04/Mar/22 02:48,1.15.0,,,,,,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,"{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007f489c00b000 nid=0x170e waiting on condition [0x00007f48a64d2000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081f14838> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:58)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBrokerFailureTest(KafkaConsumerTestBase.java:1509)
	at org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase.testBrokerFailure(KafkaSourceLegacyITCase.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.apache.flink.testutils.junit.RetryRule$RetryOnFailureStatement.evaluate(RetryRule.java:135)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32272&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39718",,gaoyunhaii,jingge,leonard,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26393,,,,,,,,,FLINK-26393,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 04 02:48:28 UTC 2022,,,,,,,,,,"0|z0zzt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/22 15:58;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32295&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39800];;;","01/Mar/22 02:50;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32259&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=37726;;;","01/Mar/22 03:27;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32263&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=39879;;;","01/Mar/22 03:28;gaoyunhaii;Also perhaps cc [~fpaul]  and [~renqs] ;;;","01/Mar/22 03:30;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32272&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39717;;;","01/Mar/22 03:49;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32308&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=40223;;;","01/Mar/22 03:50;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32310&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=40128;;;","01/Mar/22 08:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32285&view=logs&j=32a18cd8-d404-5807-996d-abcee436b891&t=cbe195d9-c9fd-51fd-0368-5c19b2aab503;;;","01/Mar/22 09:24;guoyangze;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32333&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c5f0071e-1851-543e-9a45-9ac140befc32;;;","02/Mar/22 07:43;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32326&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=37280]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32326&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=37709]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32326&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=40223;;;","02/Mar/22 07:59;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32355&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39811;;;","02/Mar/22 08:02;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32376&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=37237;;;","02/Mar/22 13:03;jingge;https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/32365/logs/210;;;","02/Mar/22 13:04;jingge;This issue might be cause by the Kafka test env with 1 broker. [~renqs] is working on it. In the meantime, we will disable the tests temporarily with https://github.com/apache/flink/pull/18960;;;","02/Mar/22 13:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32387&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39792;;;","02/Mar/22 14:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32377&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39710;;;","02/Mar/22 15:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32369&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=39307;;;","04/Mar/22 02:48;leonard;Fixed in master(1.15) : 506476581e66f3252f02b9e8af000478f0b7facd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong document order of Chinese version,FLINK-26381,13430815,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,tonnydan,tonnydan,26/Feb/22 11:36,21/Mar/22 07:49,13/Jul/23 08:08,21/Mar/22 07:49,1.14.3,,,,,,1.14.5,1.15.0,1.16.0,,chinese-translation,,,,,0,pull-request-available,,,"The chapter named ""流式分析""(streaming analytics) and ""数据管道 & ETL""(Data Pipelines & ETL) under the ""实践练习""(Learn Flink) are transposed compared to the English version. 

It causes important concepts such as ""keyed state"", ""map"" missed to solve the exercise in the chapter of streaming analytics.",,martijnvisser,paul8263,tonnydan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 07:49:11 UTC 2022,,,,,,,,,,"0|z0zz3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/22 02:08;paul8263;The page weight values of Chinese version are different from those of English version.

I'd like to do this. Could anyone assign this to me?;;;","18/Mar/22 22:38;martijnvisser;Fixed in master: 48cf55c36fe7c35c827f063e4016669328aa774e ;;;","21/Mar/22 07:49;martijnvisser;Fixed in release-1.15: 94d03f1e3a95f21585353dc74407c6f060ffa879
Fixed in release-1.14: 1636e928acf958784f41f2050a562986e71548c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Statefun Golang SDK to return nil from Context.Caller when there is no caller,FLINK-26375,13430714,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,galenwarren,galenwarren,galenwarren,25/Feb/22 14:53,25/Feb/22 19:45,13/Jul/23 08:08,25/Feb/22 19:39,statefun-3.0.0,statefun-3.1.0,statefun-3.1.1,statefun-3.2.0,,,statefun-3.2.0,,,,Stateful Functions,,,,,0,pull-request-available,,,When a stateful function is invoked from an ingress – i.e. when there is no upstream function caller -- Context.Caller() should return nil. Currently it returns a pointer to a zero-value Address. This issue would fix that.,,galenwarren,igal,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 25 19:39:31 UTC 2022,,,,,,,,,,"0|z0zyh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/22 16:10;galenwarren;PR created: [[FLINK-26375][statefun-golang-sdk] Fix Statefun Golang SDK to return nil from Context.Caller when there is no caller by galenwarren · Pull Request #304 · apache/flink-statefun (github.com)|https://github.com/apache/flink-statefun/pull/304];;;","25/Feb/22 19:39;igal;Fixed at 788417d2198b9eb382bbaeabb96dd81e4c04aa21.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON_OBJECT may throw NullPointerException on nullable column ,FLINK-26374,13430709,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,sjwiesman,sjwiesman,25/Feb/22 14:36,01/Mar/22 16:36,13/Jul/23 08:08,28/Feb/22 16:34,1.15.0,,,,,,1.15.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,"From ML: 

 

Using the latest SNAPSHOT BUILD.
 
If I have a column definition as
 
 .column(
                ""events"",
                DataTypes.ARRAY(
                    DataTypes.ROW(
                        DataTypes.FIELD(""status"", DataTypes.STRING().notNull()),
                        DataTypes.FIELD(""timestamp"", DataTypes.STRING().notNull()),
                        DataTypes.FIELD(""increment_identifier"", DataTypes.STRING().nullable()))))
 
And a query as
 
JSON_OBJECT('events' VALUE events) event_json
 
Will generate JSON correctly ONLY if increment_identifier is NOT NULL but will throw a NullPointerException on the first record that has that column as null.
 
Exception is not helpful.
 
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
at akka.dispatch.OnComplete.internal(Future.scala:300)
at akka.dispatch.OnComplete.internal(Future.scala:297)
at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
at akka.actor.Actor.aroundReceive(Actor.scala:537)
at akka.actor.Actor.aroundReceive$(Actor.scala:535)
at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
at akka.actor.ActorCell.invoke(ActorCell.scala:548)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
at akka.dispatch.Mailbox.run(Mailbox.scala:231)
at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
... 5 more
Caused by: java.lang.NullPointerException
at StreamExecCalc$422.convertRow$317$(Unknown Source)
at StreamExecCalc$422.convertArray$316$(Unknown Source)
at StreamExecCalc$422.processElement_split71(Unknown Source)
at StreamExecCalc$422.processElement(Unknown Source)
at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:64)
at org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)
at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:302)
at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:79)
at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:381)
at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:362)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
at java.base/java.lang.Thread.run(Thread.java:829)",,jonathan.weaver,martijnvisser,sjwiesman,twalthr,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 01 16:36:41 UTC 2022,,,,,,,,,,"0|z0zyg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/22 17:05;jonathan.weaver;I'm the OP.

I did a little more experimentation to narrow it down and have a minimal broken example.

 
{code:java}
SELECT JSON_OBJECT('notbroke' VALUE ARRAY[ ('foo', CAST(null AS STRING)), ('bar', CAST(null AS STRING))]){code}
  – WORKS

 

I created 3 Scalar Functions with the following query:

 
{code:java}
SELECT JSON_OBJECT('broke' VALUE broken_json()){code}
 

 

 

 
{code:java}
public class NotBrokenJsonExample extends ScalarFunction {
  @DataTypeHint(""ARRAY<ROW<f0 STRING, f1 STRING>>"")
  public Row[] eval() { return new Row[]{ Row.of(""foo"", (String)null)}; }
}
{code}
WORKS

 

 
{code:java}
public class BrokenJsonExample extends ScalarFunction {
  @DataTypeHint(""ARRAY<ROW<f0 STRING, f1 STRING>>"")
  public RowData[] eval()
{     return new RowData[] { 
GenericRowData.of(StringData.fromString(""foo""),(StringData)null)};
  }
}
{code}
FAILS

 
{code:java}
public class BrokenJsonExample extends ScalarFunction {
  @DataTypeHint(""ARRAY<ROW<f0 STRING, f1 STRING>>"")
  public ArrayData eval() {
    return new GenericArrayData( new RowData[]
{ GenericRowData.of(StringData.fromString(""foo""), (StringData)null) }
);
  }
}
{code}
FAILS

 

Give's the NullPointer example. It seems limited to internal types.

The table in question is a custom source that returns internal types.

 
{code:java}
public class NotBrokenJsonExample extends ScalarFunction {
  @DataTypeHint(""ARRAY<ROW<f0 STRING, f1 STRING>>"")
  public ArrayData eval() {
    return new GenericArrayData( new RowData[] { GenericRowData.of(StringData.fromString(""foo""), StringData.fromString(""bar"")) } );
  }
} {code}
Works just fine. So it's definitely the nulls in internal types.;;;","28/Feb/22 16:34;twalthr;Fixed in master: 6fc2933f2c81ae5dbf338cbdf859c8b6148eacc3;;;","28/Feb/22 16:34;twalthr;[~jonathan.weaver] please let us know if this fix solved all your issues as well.;;;","01/Mar/22 16:36;jonathan.weaver;I tested on the latest snapshot this morning and it worked! Thank you!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""-restoreMode"" should be ""--restoreMode"" and should have a shorthand",FLINK-26354,13430444,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dwysakowicz,knaufk,knaufk,24/Feb/22 13:55,25/Feb/22 13:48,13/Jul/23 08:08,25/Feb/22 13:48,1.15.0,,,,,,1.15.0,,,,Command Line Client,,,,,0,pull-request-available,,,"
{code:java}
-restoreMode <arg>                         Defines how should we restore
                                                from the given savepoint.
                                                Supported options: [claim -
                                                claim ownership of the savepoint
                                                and delete once it is subsumed,
                                                no_claim (default) - do not
                                                claim ownership, the first
                                                checkpoint will not reuse any
                                                files from the restored one,
                                                legacy - the old behaviour, do
                                                not assume ownership of the
                                                savepoint files, but can reuse
                                                some shared files.
{code}
",,dwysakowicz,knaufk,pnowojski,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26273,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 25 13:48:09 UTC 2022,,,,,,,,,,"0|z0zwtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 16:00;dwysakowicz;The shorthand was explicitly voted against: https://github.com/apache/flink/pull/18024#discussion_r763933325;;;","24/Feb/22 16:06;dwysakowicz;Having the above in mind. I am afraid we can not have \-restoreMode as \-\-restoreMode. Currently, it is actually treated as the ""short"" version which is not optional in commons-cli. Only the long option is optional. The long option is prepended with ""--"". Any suggestions on how to proceed [~knaufk]?;;;","24/Feb/22 16:14;knaufk;In the discussion linked I don't understand why the short option was dropped in the end. If ""-r"" is not an option, we could also go for ""-rm"" or so ans the short option. ;;;","24/Feb/22 16:23;dwysakowicz;Ah, forgot that has not been the only place. There is some continuation here as well: https://github.com/apache/flink/pull/18024#discussion_r764760913.

Can we agree on a solution? [~pnowojski][~yunta][~roman];;;","24/Feb/22 16:52;pnowojski;I don't have very strong preferences so feel free to out vote me, but I stay by my previous comment, that such abbreviations in parameters that are used once a year are doing more harm then good. 

How difficult would it be to make the short versions optional?;;;","24/Feb/22 16:55;dwysakowicz;> How difficult would it be to make the short versions optional?

Close to impossible, as far as I am concerned (if we do not want to change the library and I don't think the problem justifies such a move). The library we use does not support it.;;;","25/Feb/22 03:16;yunta;Since the latest version of commons-cli is 1.5.0, which is what we already used in Flink. And it must have a short option key while the long option key could be optional. And FLINK-22701 actually suffers from this problem due to short option key conflicts. Maybe we could change the dependent library in Flink-1.16. For current solution, I prefer [~knaufk]'s idea, maybe we could introduce the short option key such 'rm' for the long option key 'restoreMode'.;;;","25/Feb/22 07:42;knaufk;From my perspective consistency with existing options trumps the other arguments here. So far, we (almost) always have a short option and a long option. So that's what we should do here and in https://issues.apache.org/jira/browse/FLINK-26353 as well.;;;","25/Feb/22 08:42;roman;My major concerns were about the name of the long option (restoreMode) and  unclear meaning of ""legacy"" without specifying the option (either short or long). I don't have any objections against having a short form.;;;","25/Feb/22 13:48;dwysakowicz;Fixed in a34b448c8e8f0dc7705548c028dd463a2fb0ddc9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""flink stop --help"" does not list ""--type"" option ",FLINK-26353,13430442,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,knaufk,knaufk,24/Feb/22 13:37,20/Apr/22 09:52,13/Jul/23 08:08,02/Mar/22 15:32,1.15.0,,,,,,1.15.0,,,,Command Line Client,,,,,0,pull-request-available,,,"{code:bash}
./bin/flink stop --help 

Action ""stop"" stops a running program with a savepoint (streaming jobs only).

  Syntax: stop [OPTIONS] <Job ID>
  ""stop"" action options:
     -d,--drain                           Send MAX_WATERMARK before taking the
                                          savepoint and stopping the pipelne.
     -p,--savepointPath <savepointPath>   Path to the savepoint (for example
                                          hdfs:///flink/savepoint-1537). If no
                                          directory is specified, the configured
                                          default will be used
                                          (""state.savepoints.dir"").
  Options for Generic CLI mode:
     -D <property=value>   Allows specifying multiple generic configuration
                           options. The available options can be found at
                           https://nightlies.apache.org/flink/flink-docs-stable/
                           ops/config.html
     -e,--executor <arg>   DEPRECATED: Please use the -t option instead which is
                           also available with the ""Application Mode"".
                           The name of the executor to be used for executing the
                           given job, which is equivalent to the
                           ""execution.target"" config option. The currently
                           available executors are: ""remote"", ""local"",
                           ""kubernetes-session"", ""yarn-per-job"" (deprecated),
                           ""yarn-session"".
     -t,--target <arg>     The deployment target for the given application,
                           which is equivalent to the ""execution.target"" config
                           option. For the ""run"" action the currently available
                           targets are: ""remote"", ""local"", ""kubernetes-session"",
                           ""yarn-per-job"" (deprecated), ""yarn-session"". For the
                           ""run-application"" action the currently available
                           targets are: ""kubernetes-application"".

  Options for yarn-cluster mode:
     -m,--jobmanager <arg>            Set to yarn-cluster to use YARN execution
                                      mode.
     -yid,--yarnapplicationId <arg>   Attach to running YARN session
     -z,--zookeeperNamespace <arg>    Namespace to create the Zookeeper
                                      sub-paths for high availability mode

  Options for default mode:
     -D <property=value>             Allows specifying multiple generic
                                     configuration options. The available
                                     options can be found at
                                     https://nightlies.apache.org/flink/flink-do
                                     cs-stable/ops/config.html
     -m,--jobmanager <arg>           Address of the JobManager to which to
                                     connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration. Attention: This
                                     option is respected only if the
                                     high-availability configuration is NONE.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode
{code}
",,dwysakowicz,knaufk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26273,,FLINK-27319,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 02 15:32:52 UTC 2022,,,,,,,,,,"0|z0zwt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 16:46;dwysakowicz;This will have the same problem [FLINK-26354]. Can we agree on the options here as well? [~pnowojski] [~yunta] [~roman]  [~knaufk];;;","02/Mar/22 15:32;dwysakowicz;Fixed in 595d5467136b0d00faf83c192f937517fe4b58c7..6d4189ef0928e6d8ddb3da64b2947c5a5dfcf901;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroParquetReaders does not work with ReflectData,FLINK-26349,13430433,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jingge,dwysakowicz,dwysakowicz,24/Feb/22 12:46,14/Mar/22 14:07,13/Jul/23 08:08,11/Mar/22 08:01,1.15.0,,,,,,1.15.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"I tried to change the {{AvroParquetFileReadITCase}} to read the data as {{ReflectData}} and I stumbled on a problem. The scenario is that I use exact same code for writing parquet files, but changed the reading part to:

{code}
    public static final class User {
        private final String name;
        private final Integer favoriteNumber;
        private final String favoriteColor;

        public User(String name, Integer favoriteNumber, String favoriteColor) {
            this.name = name;
            this.favoriteNumber = favoriteNumber;
            this.favoriteColor = favoriteColor;
        }
    }

        final FileSource<User> source =
                FileSource.forRecordStreamFormat(
                                AvroParquetReaders.forReflectRecord(User.class),
                                Path.fromLocalFile(TEMPORARY_FOLDER.getRoot()))
                        .monitorContinuously(Duration.ofMillis(5))
                        .build();
{code}

I get an error:
{code}
819020 [flink-akka.actor.default-dispatcher-9] DEBUG org.apache.flink.runtime.jobmaster.JobMaster [] - Archive local failure causing attempt cc9f5e814ea9a3a5b397018dbffcb6a9 to fail: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException
Serialization trace:
reserved (org.apache.avro.Schema$Field)
fieldMap (org.apache.avro.Schema$RecordSchema)
schema (org.apache.avro.generic.GenericData$Record)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:143)
	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:402)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:191)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46)
	at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:53)
	at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readInto(NonSpanningWrapper.java:337)
	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNonSpanningRecord(SpillingAdaptiveSpanningRecordDeserializer.java:128)
	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:103)
	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:93)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:95)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.UnsupportedOperationException
	at java.util.Collections$UnmodifiableCollection.add(Collections.java:1057)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	... 30 more
{code}",,dwysakowicz,jingge,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26301,,,FLINK-26604,FLINK-25416,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 11 08:01:34 UTC 2022,,,,,,,,,,"0|z0zwr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 19:11;jingge;I think there is a conflict between write model and read model. It seems you are reading the file with reflect model, which created with generic model.;;;","25/Feb/22 08:59;dwysakowicz;Yes, and why is that not supported? In the end, the binary format on disk is parquet. So what is the problem? 

Even pure avro, does not care if data has been written with GenericData or Reflect or Specific as long as the schema is the same.;;;","25/Feb/22 11:41;jingge;hmm, I think it is a question of how the standard AvroParquet works beyond Flink format. The issue you found shows that it only works when the Avro data model is compatible between write and read. Specifically, it is not possible to read parquet file with ReflectData that is created with GenericData, because ReflectData is a subclass of GenericData.;;;","25/Feb/22 11:45;dwysakowicz;That is not true. The problem I pointed above is purely Flink issue. It is already past any parquet-mr classes and because of the PojoTypeInfo we create.;;;","25/Feb/22 12:21;jingge;Thanks for the input, I will do further research on it.;;;","09/Mar/22 14:04;jingge;The user schema in AvroParquetRecordFormatTest is defined only for Avro GenericRecord. In order to make it support ReflectData read, a namespace is required, so that the program could find the class to do reflection.

I have updated the user schema and add test cases to cover this case. Thanks for pointing it out.

------------------------ FYI ----------------------------
parquet file created by the user schema has the following meta:
{code:java}
creator:        parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)
extra:          parquet.avro.schema = {""type"":""record"",""name"":""User"",""fields"":[{""name"":""name"",""type"":""string""},{""name"":""favoriteNumber"",""type"":[""int"",""null""]},{""name"":""favoriteColor"",""type"":[""string"",""null""]}]}
extra:          writer.model.name = avro

file schema:  User
--------------------------------------------------------------------------------
name:           REQUIRED BINARY L:STRING R:0 D:0
favoriteNumber: OPTIONAL INT32 R:0 D:1
favoriteColor:  OPTIONAL BINARY L:STRING R:0 D:1

row group 1:    RC:3 TS:143 OFFSET:4
--------------------------------------------------------------------------------
name:            BINARY UNCOMPRESSED DO:0 FPO:4 SZ:47/47/1.00 VC:3 ENC:PLAIN,BIT_PACKED ST:[min: Jack, max: Tom, num_nulls: 0]
favoriteNumber:  INT32 UNCOMPRESSED DO:0 FPO:51 SZ:41/41/1.00 VC:3 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 1, max: 3, num_nulls: 0]
favoriteColor:   BINARY UNCOMPRESSED DO:0 FPO:92 SZ:55/55/1.00 VC:3 ENC:RLE,PLAIN,BIT_PACKED ST:[min: green, max: yellow, num_nulls: 0]
{code}
parquet file created by Datum POJO class has the following meta:
{code:java}
creator:     parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)
extra:       parquet.avro.schema = {""type"":""record"",""name"":""Datum"",""namespace"":""org.apache.flink.formats.parquet.avro"",""fields"":[{""name"":""a"",""type"":""string""},{""name"":""b"",""type"":""int""}]}
extra:       writer.model.name = avro

file schema: org.apache.flink.formats.parquet.avro.Datum
--------------------------------------------------------------------------------
a:           REQUIRED BINARY L:STRING R:0 D:0
b:           REQUIRED INT32 R:0 D:0

row group 1: RC:3 TS:73 OFFSET:4
--------------------------------------------------------------------------------
a:            BINARY UNCOMPRESSED DO:0 FPO:4 SZ:38/38/1.00 VC:3 ENC:PLAIN,BIT_PACKED ST:[min: a, max: c, num_nulls: 0]
b:            INT32 UNCOMPRESSED DO:0 FPO:42 SZ:35/35/1.00 VC:3 ENC:PLAIN,BIT_PACKED ST:[min: 1, max: 3, num_nulls: 0]
{code};;;","11/Mar/22 08:01;jingge;Merged in master: 128e19156208a306669f1736514f61d85f9ee065;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should use Flink system Classloader (AppClassloader) when deserializing RPC message,FLINK-26347,13430389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,kevin.cyj,kevin.cyj,24/Feb/22 09:40,02/Mar/22 09:04,13/Jul/23 08:08,02/Mar/22 09:04,1.15.0,,,,,,1.15.0,,,,,,,,,0,pull-request-available,,,"FLINK-25742 removed the redundant serialization of RPC invocation at Flink side. However, by accident, it changes the class loading behavior. Before FLINK-25742, Flink system Classloader is used to load RPC message class, but after FLINK-25742, the RpcSystem Classloader (its parent Classloader is not Flink system Classloader) is used which can cause ClassNotFoundException. I encountered this exception when trying to run flink-remote-shuffle on the latest Flink 1.15-SNAPSHOT, the remote shuffle class (shuffle descriptor class) can not be found even when the corresponding jar file is in Flink lib/ directory.",,guoyangze,kevin.cyj,Thesharing,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 02 09:04:21 UTC 2022,,,,,,,,,,"0|z0zwhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 09:41;kevin.cyj;cc [~guoyangze] .;;;","02/Mar/22 09:04;guoyangze;master: e1e84e96526429d6e6735e0f969a20f2fb7ae0e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
waitForClusterShutdown does not work correctly in Flink Kubernetes operator ,FLINK-26344,13430332,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,wangyang0918,wangyang0918,24/Feb/22 03:57,27/Mar/22 13:16,13/Jul/23 08:08,01/Mar/22 08:22,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,,,," 
{code:java}
Fabric8FlinkKubeClient flinkKubeClient =
        new Fabric8FlinkKubeClient(
                conf, kubernetesClient, Executors.newSingleThreadExecutor()); {code}
 

{{Fabric8FlinkKubeClient}} expects an namespaced Kubernetes client. If we create FlinkDeployment is a different namespace with K8s operator, {{FlinkService#waitForClusterShutdown()}} will not work.

 

We could directly use the Kubernetes client to do the existence check for deployment.
{code:java}
kubernetesClient.apps().deployments().inNamespace(namespace).withName(clusterId).get() == null{code}",,gyfora,nicholasjiang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 01 08:22:04 UTC 2022,,,,,,,,,,"0|z0zw4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 04:55;nicholasjiang;[~wangyang0918], I would like to fix this bug above mentioned. Could you please assign this ticket to me?;;;","24/Feb/22 05:54;wangyang0918;[~nicholasjiang]  You have got assigned. Happy coding.;;;","01/Mar/22 08:22;gyfora;As a side effect this was also fixed by: 38c366de0d8fa11751051eadbddcf19cb1f31e8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When timestamp - offset + windowSize < 0, elements cannot be assigned to the correct window",FLINK-26334,13430214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,realdengziqi,realdengziqi,realdengziqi,23/Feb/22 12:49,07/Apr/22 05:36,13/Jul/23 08:08,21/Mar/22 08:52,1.14.4,1.15.0,,,,,1.14.5,1.15.0,,,API / DataStream,,,,,1,pull-request-available,,,"h2. issue

        Hello!

        When we were studying the flink source code, we found that there was a problem with its algorithm for calculating the window start time. When _timestamp - offset + windowSize < 0_ , the element will be incorrectly allocated to a window with a WindowSize larger than its own timestamp.

        The problem is in _org.apache.flink.streaming.api.windowing.windows.TimeWindow_
{code:java}
public static long getWindowStartWithOffset(long timestamp, long offset, long windowSize) {
    return timestamp - (timestamp - offset + windowSize) % windowSize;
} {code}
_!image-2022-03-04-11-28-26-616.png|width=710,height=251!_

        We believe that this violates the constraints between time and window. *That is, an element should fall within a window whose start time is less than its own timestamp and whose end time is greater than its own timestamp.* However, the current situation is when {_}timestamp - offset + windowSize < 0{_}, *the element falls into a future time window.*

       *You can reproduce the bug with the code at the end of the post.*
h2. Solution       

        In fact, the original algorithm is no problem in python, the key to this problem is the processing of the remainder operation by the programming language.

        We finally think that it should be modified to the following algorithm.
{code:java}
public static long getWindowStartWithOffset(long timestamp, long offset, long windowSize) {
    return timestamp
            - (timestamp - offset) % windowSize
            - (windowSize & (timestamp - offset) >> 63);
} {code}
        _windowSize & (timestamp - offset) >> 63_ The function of this formula is to subtract windowSize from the overall operation result when {_}timestamp - offset<0{_}, otherwise do nothing. This way we can handle both positive and negative timestamps.

        Finally, the element can be assigned to the correct window.

!image-2022-03-04-11-37-10-035.png|width=712,height=284!

        This code can pass current unit tests.
h2. getWindowStartWithOffset methods in other packages

        I think that there should be many places in {_}getWindowStartWithOffset{_}. We searched for this method in the project and found that the problem of negative timestamps is handled in _flink.table._

        Below is their source code.

        _{{org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping}}_
{code:java}
private long getWindowStartWithOffset(long timestamp, long offset, long windowSize) {
    long remainder = (timestamp - offset) % windowSize;
    // handle both positive and negative cases
    if (remainder < 0) {
        return timestamp - (remainder + windowSize);
    } else {
        return timestamp - remainder;
    }
} {code}
h2.  further

When we wrote the test case, we found that the algorithm we wrote would violate the convention that the window is closed on the left and open on the right. In addition, considering the readability of the code, we decided to use the same code as in _{{{}org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping{}}}._

 
{code:java}
private long getWindowStartWithOffset(long timestamp, long offset, long windowSize) {
    long remainder = (timestamp - offset) % windowSize;
    // handle both positive and negative cases
    if (remainder < 0){
         return timestamp - (remainder + windowSize);
     }else {
         return timestamp - remainder;
     }
} {code}
​
In addition, in the process of modification, we found that the algorithm of _{{getWindowStartWithOffset}}_ in _{{org.apache.flink.table.runtime.operators.window.TimeWindow}}_ is the same as that in {_}{{org.apache.flink.streaming.api.windowing.windows.TimeWindow}}{_}. So it should cause the same problem. I think it should also be modified to support negative timestamps

[[FLINK-26334][datastream] Fix getWindowStartWithOffset in TimeWindow.java by realdengziqi · Pull Request #18982 · apache/flink (github.com)|https://github.com/apache/flink/pull/18982]

 
h2. Can we make a pull request?

        If the community deems it necessary to revise it, hopefully this task can be handed over to us. Our members are all students who have just graduated from school, and it is a great encouragement for us to contribute code to flink.

        Thank you so much!

        From Deng Ziqi & Lin Wanni & Guo Yuanfang

 
----
----
h2. reproduce
{code:java}
/* output
WindowStart: -15000    ExactSize:1    (a,-17000)
WindowStart: -10000    ExactSize:1    (b,-12000)
WindowStart: -5000 ExactSize:2    (c,-7000)
WindowStart: -5000 ExactSize:2    (d,-2000)
WindowStart: 0 ExactSize:1    (e,3000)
WindowStart: 5000  ExactSize:1    (f,8000)
WindowStart: 10000 ExactSize:1    (g,13000)
WindowStart: 15000 ExactSize:1    (h,18000)
 */
public class Example {
    public static void main(String[] args) throws Exception {

        final TimeZone timeZone = TimeZone.getTimeZone(""GTM+0"");
        TimeZone.setDefault(timeZone);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env
                .setParallelism(1)
                .fromElements(
                        Tuple2.of(""a"",-17*1000L),
                        Tuple2.of(""b"",-12*1000L),
                        Tuple2.of(""c"",-7*1000L),
                        Tuple2.of(""d"",-2*1000L),
                        Tuple2.of(""e"",3*1000L),
                        Tuple2.of(""f"",8*1000L),
                        Tuple2.of(""g"",13*1000L),
                        Tuple2.of(""h"",18*1000L)
                )
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy.<Tuple2<String,Long>>forMonotonousTimestamps()
                                .withTimestampAssigner(
                                        new SerializableTimestampAssigner<Tuple2<String, Long>>() {
                                            @Override
                                            public long extractTimestamp(Tuple2<String, Long> element, long l) {
                                                return element.f1;
                                            }
                                        }
                                )
                )
                .keyBy(r->1)
                .window(TumblingEventTimeWindows.of(Time.seconds(5)))
                .process(
                        new ProcessWindowFunction<Tuple2<String, Long>, String, Integer, TimeWindow>() {
                            @Override
                            public void process(Integer integer, ProcessWindowFunction<Tuple2<String, Long>, String, Integer, TimeWindow>.Context context, Iterable<Tuple2<String, Long>> elements, Collector<String> out) throws Exception {
                                for (Tuple2<String, Long> element : elements) {
                                    out.collect(""WindowStart: ""+context.window().getStart()
                                            + ""\tExactSize:"" + elements.spliterator().getExactSizeIfKnown()+""\t""
                                            + element
                                    );
                                }
                            }
                        }
                )
                .print();
        env.execute();
    }
} {code}",flink version 1.14.3,danderson,fpaul,guoyf,hackergin,maguowei,martijnvisser,realdengziqi,Terry1897,twalthr,zoucao,,,,,,10800,10800,,0%,10800,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/22 03:28;realdengziqi;image-2022-03-04-11-28-26-616.png;https://issues.apache.org/jira/secure/attachment/13040700/image-2022-03-04-11-28-26-616.png","04/Mar/22 03:37;realdengziqi;image-2022-03-04-11-37-10-035.png;https://issues.apache.org/jira/secure/attachment/13040701/image-2022-03-04-11-37-10-035.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,JAVA,,Mon Mar 21 08:52:09 UTC 2022,,,,,,,,,,"0|z0zveg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/22 00:57;realdengziqi;I want to work on this:D;;;","03/Mar/22 09:45;martijnvisser;[~realdengziqi] Thanks for reporting this. I have reduced the priority given that I don't think that this should block a Flink release. I'm hoping that some of the maintainers can give more insights on this, but since we're getting close to the new Flink release, most of them are busy with testing so it might take a bit longer. ;;;","04/Mar/22 03:54;realdengziqi;[~martijnvisser] Thanks for your attention, I've updated the problem description to make it clearer. And I came up with my solution in the description. Hope the community can assign this issue to me.
thanks;);;;","04/Mar/22 09:11;martijnvisser;[~realdengziqi] I've assigned it to you. Looking forward to your PR!;;;","04/Mar/22 10:52;Terry1897;Good insight about the description of problem and cause analysis.(y);;;","04/Mar/22 19:11;realdengziqi;[~martijnvisser] Thanks a lot, we just launched the PR :D;;;","04/Mar/22 19:13;realdengziqi;[~Terry1897] Thank you, we will continue to work hard.;;;","21/Mar/22 08:52;fpaul;Merged in master: c1bbbe6f0d148d109f903c5d3b207e9923a74e23

Merged in release-1.15: cad7cfc4b700c64422bb65599dac1d11116513e2

Merged in release-1.14: fc86673dc0aaed6152af3e584df59637fcdf6e88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CastFunctionITCase.testFunction failed on azure,FLINK-26319,13430117,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,gaoyunhaii,gaoyunhaii,23/Feb/22 07:23,23/Feb/22 12:49,13/Jul/23 08:08,23/Feb/22 12:49,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,test-stability,,,"{code:java}
Feb 22 18:39:45 [ERROR] Tests run: 34, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 42.844 s <<< FAILURE! - in org.apache.flink.table.planner.functions.CastFunctionITCase
Feb 22 18:39:45 [ERROR] CastFunctionITCase.testFunction  Time elapsed: 0.214 s  <<< FAILURE!
Feb 22 18:39:45 java.lang.AssertionError: Failing test item: [SQL] CAST(f1 AS TIMESTAMP_LTZ(9))
Feb 22 18:39:45 	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.testFunction(BuiltInFunctionTestBase.java:116)
Feb 22 18:39:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 18:39:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 18:39:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 18:39:45 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 18:39:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 18:39:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 18:39:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 18:39:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 18:39:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Feb 22 18:39:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Feb 22 18:39:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 22 18:39:45 	at org.junit.runners.Suite.runChild(Suite.java:128)
Feb 22 18:39:45 	at org.junit.runners.Suite.runChild(Suite.java:27)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 22 18:39:45 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Feb 22 18:39:45 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 22 18:39:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Feb 22 18:39:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Feb 22 18:39:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32060&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10315",,gaoyunhaii,mapohl,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 23 12:49:31 UTC 2022,,,,,,,,,,"0|z0zusw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/22 07:24;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32064&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10714;;;","23/Feb/22 07:25;gaoyunhaii;Perhaps cc [~twalthr] [~slinkydeveloper] ;;;","23/Feb/22 07:25;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32066&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10714;;;","23/Feb/22 07:28;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32078&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10315]

 

This seems to be deterministic.;;;","23/Feb/22 07:37;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=769&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&l=11019;;;","23/Feb/22 07:59;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32077&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10315
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32077&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10342]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32077&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10795]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32077&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=10493]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32077&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=10714;;;","23/Feb/22 08:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32068&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11023;;;","23/Feb/22 11:18;twalthr;We will investigate.;;;","23/Feb/22 12:49;twalthr;Fixed in master: e432723a2651f864e6d9c84ef07e0645b9f1319f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogCompatibilityITCase.testRestore failed on azure,FLINK-26318,13430115,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,gaoyunhaii,gaoyunhaii,23/Feb/22 07:18,24/Feb/22 03:22,13/Jul/23 08:08,24/Feb/22 03:22,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,test-stability,,"{code:java}
Feb 22 19:09:43 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.815 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogCompatibilityITCase
Feb 22 19:09:43 [ERROR] ChangelogCompatibilityITCase.testRestore  Time elapsed: 1.309 s  <<< ERROR!
Feb 22 19:09:43 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit2665494390926857042/junit2441494118455041375/accbd512d1546402f50f07832672cf2a/chk-1/._metadata.inprogress.7ebadfd7-88be-41ef-9889-f9cb5fa59113
Feb 22 19:09:43 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
Feb 22 19:09:43 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
Feb 22 19:09:43 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:499)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:486)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Feb 22 19:09:43 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:536)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.hasMetadata(TestUtils.java:122)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.isCompletedCheckpoint(TestUtils.java:112)
Feb 22 19:09:43 	at java.nio.file.Files.lambda$find$2(Files.java:3691)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
Feb 22 19:09:43 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Feb 22 19:09:43 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Feb 22 19:09:43 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:582)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.getMostRecentCompletedCheckpointMaybe(TestUtils.java:105)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.waitForCheckpoint(ChangelogCompatibilityITCase.java:264)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.tryCheckpointAndStop(ChangelogCompatibilityITCase.java:145)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.runAndStoreIfAllowed(ChangelogCompatibilityITCase.java:109)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.testRestore(ChangelogCompatibilityITCase.java:103)
Feb 22 19:09:43 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 19:09:43 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 19:09:43 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 19:09:43 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 19:09:43 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 19:09:43 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 19:09:43 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 19:09:43 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 19:09:43 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32057&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12444",,gaoyunhaii,roman,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 03:22:20 UTC 2022,,,,,,,,,,"0|z0zusg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/22 07:19;gaoyunhaii;Perhaps cc [~roman] [~ym] ;;;","23/Feb/22 10:56;roman;Thanks for pulling me in [~gaoyunhaii] 

Could you please take a look at the PR: [https://github.com/apache/flink/pull/18898] ?;;;","23/Feb/22 14:18;twalthr;https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/32115/logs/130;;;","24/Feb/22 03:22;gaoyunhaii;Fix on master via f8ac2bf8f9a8f95f8c82554c994a98ee984967d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-yarn-tests Exit code 137 returned from process,FLINK-26317,13430113,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,gaoyunhaii,gaoyunhaii,23/Feb/22 07:10,23/Aug/22 07:10,13/Jul/23 08:08,23/Aug/22 07:10,1.15.0,1.16.0,,,,,,,,,Deployment / YARN,,,,,0,auto-deprioritized-major,test-stability,,"{code:java}
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1004  -w /home/agent05_azpcontainer 834a3573f528c47747bd741a5fdbfa3fa9b0da8c968bddd7ca1a5207ce54fcb7 /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
Finishing: Test - misc {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32030&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=33701",,gaoyunhaii,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 10:40:10 UTC 2022,,,,,,,,,,"0|z0zus0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/22 07:55;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33095&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=32964;;;","06/Jul/22 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jul/22 10:40;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingCompactingFileSinkITCase.testFileSink failed on azure,FLINK-26314,13430068,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,gaoyunhaii,gaoyunhaii,23/Feb/22 03:23,08/Mar/22 06:11,13/Jul/23 08:08,08/Mar/22 06:11,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,test-stability,,"{code:java}
Feb 22 13:34:32 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.735 s <<< FAILURE! - in org.apache.flink.connector.file.sink.StreamingCompactingFileSinkITCase
Feb 22 13:34:32 [ERROR] StreamingCompactingFileSinkITCase.testFileSink  Time elapsed: 3.311 s  <<< FAILURE!
Feb 22 13:34:32 java.lang.AssertionError: The record 6788 should occur 4 times,  but only occurs 3time expected:<4> but was:<3>
Feb 22 13:34:32 	at org.junit.Assert.fail(Assert.java:89)
Feb 22 13:34:32 	at org.junit.Assert.failNotEquals(Assert.java:835)
Feb 22 13:34:32 	at org.junit.Assert.assertEquals(Assert.java:647)
Feb 22 13:34:32 	at org.apache.flink.connector.file.sink.utils.IntegerFileSinkTestDataUtils.checkIntegerSequenceSinkOutput(IntegerFileSinkTestDataUtils.java:155)
Feb 22 13:34:32 	at org.apache.flink.connector.file.sink.FileSinkITBase.testFileSink(FileSinkITBase.java:84)
Feb 22 13:34:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 13:34:32 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 13:34:32 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 13:34:32 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 13:34:32 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 13:34:32 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 13:34:32 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 13:34:32 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 13:34:32 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Feb 22 13:34:32 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Feb 22 13:34:32 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Feb 22 13:34:32 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Feb 22 13:34:32 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 13:34:32 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Feb 22 13:34:32 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Feb 22 13:34:32 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 22 13:34:32 	at org.junit.runners.Suite.runChild(Suite.java:128)
Feb 22 13:34:32 	at org.junit.runners.Suite.runChild(Suite.java:27)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 13:34:32 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32023&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11103",,chalixar,gaoyunhaii,mapohl,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26403,FLINK-26440,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 08 06:10:52 UTC 2022,,,,,,,,,,"0|z0zui0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/22 03:23;gaoyunhaii;[~pltbkd] Could you have a look at this issue?;;;","23/Feb/22 07:13;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32057&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11501;;;","23/Feb/22 07:40;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=769&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=e204f081-e6cd-5c04-4f4c-919639b63be9&l=14003

This build has a slightly different failure where it leads to a timeout and the thread dump being printed.;;;","23/Feb/22 08:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32068&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11569;;;","24/Feb/22 07:21;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32094&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11104;;;","24/Feb/22 07:22;pltbkd;I'll take a look soon.;;;","25/Feb/22 03:38;pltbkd;I found that the PseudoRandomValueSelector is selecting PT0S for execution.checkpointing.alignment-timeout and true for execution.checkpointing.unaligned for this test. I can reproduce these issues with these options. It seems that the compactor for FileSink can not work with unaligned checkpoint at present. 

I will first create a PR with a hotfix for the test to disable unaligned checkpoint explicitly, then work on a bugfix to make the compactor supporting unaligned.;;;","25/Feb/22 18:58;chalixar;Same issue here.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32250&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d;;;","28/Feb/22 17:31;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32142&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=11993;;;","28/Feb/22 17:36;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32147&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11500;;;","28/Feb/22 17:38;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32149&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11103;;;","01/Mar/22 03:46;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32313&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35618;;;","02/Mar/22 07:53;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32344&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11476;;;","08/Mar/22 06:10;gaoyunhaii;I'll first close this issue since the two root-cause issues are all fixed. We could reopen the issue if it reproduced. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Thundering herd problem with materialization,FLINK-26306,13429969,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,22/Feb/22 16:05,10/Mar/22 15:09,13/Jul/23 08:08,10/Mar/22 15:09,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"Quick note: CheckpointCleaner is not involved here.

When a checkpoint is subsumed, SharedStateRegistry schedules its unused shared state for async deletion. It uses common IO pool for this and adds a Runnable per state handle. ( see SharedStateRegistryImpl.scheduleAsyncDelete)

When a checkpoint is started, CheckpointCoordinator uses the same thread pool to initialize the location for it. (see CheckpointCoordinator.initializeCheckpoint)

The thread pool is of fixed size [jobmanager.io-pool.size|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-io-pool-size]; by default it's the number of CPU cores) and uses FIFO queue for tasks.

When there is a spike in state deletion, the next checkpoint is delayed waiting for an available IO thread.

Back-pressure seems reasonable here (similar to CheckpointCleaner); however, this shared state deletion could be spread across multiple subsequent checkpoints, not neccesarily the next one.

---- 

I believe the issue is an pre-existing one; but it particularly affects changelog state backend, because 1) such spikes are likely there; 2) workloads are latency sensitive.

In the tests, checkpoint duration grows from seconds to minutes immediately after the materialization.",,klion26,pnowojski,roman,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26590,,,,,,,,,,,,,FLINK-13698,,,,,,,FLINK-25144,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 15:09:21 UTC 2022,,,,,,,,,,"0|z0ztx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 16:23;roman;Theoretically, I think there are the following ways to solve this problem:
 
1. Batch deletions and leave one thread idle (e.g. group 1K handles into 10 big batches handled by 11 IO threads)
2. Spread changelog materialization across multiple checkpoints; i.e. materialize different tasks at different times
3. Use fork-join pool and rely on its task ordering (IIUC the implementation)
4. Use separate thread pools
5. Use unbounded thread pool
 
(1) seems the way to go to me.
(2) is not guaranteed to improve and is limited to changelog
(3) relies on fork-join pool implementation
(4) and (5) can be quickly ruled out because they remove back-pressure completly
 
(1) leaves one thread free for new checkpoints  initially. If the rest don't keep up with deletion, then the next spike will consume the remaining capacity, and checkpointing will be back-pressured.
The size/number of batches should be determined by the pool itself, so we'd need to wrap the original IO executor.
 
WDYT [~pnowojski], [~trohrmann@apache.org], [~ym], [~yunta] ?;;;","23/Feb/22 06:45;yunta;[~roman] After materialization, I think the new checkpoint would be delayed to trigger instead of increasing the checkpoint duration, right?

Why batch the deletion would help this? I think the idle thread actually make the effect not back-pressure, right?;;;","23/Feb/22 08:04;roman;[~yunta] after materialization, triggering does start, but then it's stuck in location initialization. 
In the UI, it's shown as long checkpoint duration (because trigger timestamp is generated in the very beginning). In the logs, ""triggering"" is printed with a delay. Most importantly, checkpointing is delayed and end-to-en latency increases.

Batching deletions helps by occupying less threads and leaving more for the new checkpoint.;;;","23/Feb/22 08:30;pnowojski;I've changed type of this issue to an improvement.

> 2. Spread changelog materialization across multiple checkpoints; i.e. materialize different tasks at different times

Can you [~roman] elaborate why would that help? Is it because materialised parts of the changelog checkpoints are causing those deletion spikes? If so, why is that the case? Why is this only because of the ""materialised parts""?

Maybe we should think about some more fair thread pool for async jobs? For example every async IO job could get assigned an id/key, and each id/key would have it's own queue of tasks to perform. Based on that we could implement all kinds of fancy priority schemes, but we could start with something as simple as just going in a round robing fashion through all individual per id/key queues when polling for a new task to execute. This could be generic and flexible enough to be re-used in other use cases (I was thinking about something like that for the TMs IO executor in the past).

Re batching. Isn't this more of an independent potential optimisation that we could consider independently of the main issue? Depending how long is single IO operation. If it's more then a couple of ms, I would prefer to leave them separate.;;;","23/Feb/22 10:10;roman;> > 2. Spread changelog materialization across multiple checkpoints; i.e. materialize different tasks at different times
> Can you Roman Khachatryan elaborate why would that help? Is it because materialised parts of the changelog checkpoints are causing those deletion spikes? If so, why is that the case? Why is this only because of the ""materialised parts""?

Let me calrify what happens currently:
1. JM triggers a checkpoint
2. TMs send non-materialized changes
3. The above is repeated until the materialization happens (with 1s checkpoint and 10m materialization interval - that's 600 checkpoints times nr. of tasks))
4. Materialization finishes more or less simultaneously on all tasks
5. Checkpoint N is triggered - TMs don't send ""old"" non-materialized state (only mat. state + changelog after it)
6. Checkpoint N is completed, checkpoint (N - 1) is subsumed; all ""old"" non-materialized state is scheduled for async deletion
7. Checkpoint (N + 1) is triggered; but it is waiting for an IO thread to initialize the location

It *is* desirable to preserve this back-pressure from deletion to new checkpoints. But if possible, deletions should be spread more evenly. 
So I was thinking that distributing different task materializations evenly should reduce the wait time (although it does not eliminate it completely).
The other way is to adjust threads workings (which I think is a better way).

> Maybe we should think about some more fair thread pool for async jobs? For example every async IO job could get assigned an id/key, and each id/key would have it's own queue of tasks to perform. Based on that we could implement all kinds of fancy priority schemes, but we could start with something as simple as just going in a round robing fashion through all individual per id/key queues when polling for a new task to execute. This could be generic and flexible enough to be re-used in other use cases (I was thinking about something like that for the TMs IO executor in the past).

I think only priorities won't work here because we'd need to assign different priorities depending on the ""queue length"" to preserve back-pressure.
If we always prioritize new checkpoints over deletions, we'll likely end up with OOMs (the case before the CheckpointCleaner was introduced).
Having different queues would work I think - but with a check of the length of the deletion queue.

> Re batching. Isn't this more of an independent potential optimisation that we could consider independently of the main issue? Depending how long is single IO operation. If it's more then a couple of ms, I would prefer to leave them separate.

I think this can be viewed as an optimization if the problem solved by other means; or as an actual way to solve this problem.
The benefits of the batching solution are simplicity and lesser invasiveness.;;;","23/Feb/22 12:53;pnowojski;Thanks for the explanation, I get it now.

> 1. Batch deletions and leave one thread idle (e.g. group 1K handles into 10 big batches handled by 11 IO threads)

Is this the right level to provide back pressure functionality? Would it even work if you hardcoded in the {{CheckpointCoordinator}} assumptions about pool size and the number of used threads? We don't know how else this thread pool is being used.

Apart of that. Don't we already have a backpressure mechanism on a higher level? {{CheckpointRequestDecider#numberOfCleaningCheckpointsSupplier}} from FLINK-17073? It looks like simple fair io thread pool as I described above, without any priorities + addjusting/relaxing {{numberOfCleaningCheckpointsSupplier.getAsInt() > maxConcurrentCheckpointAttempts}} check to something like {{numberOfCleaningCheckpointsSupplier.getAsInt() > maxConcurrentCheckpointAttempts + CONSTANT}} would do the trick, wouldn't it?;;;","23/Feb/22 14:10;roman;> > 1. Batch deletions and leave one thread idle (e.g. group 1K handles into 10 big batches handled by 11 IO threads)
> Would it even work if you hardcoded in the CheckpointCoordinator assumptions about pool size and the number of used threads? We don't know how else this thread pool is being used.
 
We can avoid hardcoding these numbers and put the logic into some wrapper around the pool (with a new method that accepts a list of Runnables or Handles).
 
> Is this the right level to provide back pressure functionality?
> Don't we already have a backpressure mechanism on a higher level? CheckpointRequestDecider#numberOfCleaningCheckpointsSupplier from FLINK-17073?
 
Currently, deletions from SharedStateRegistry don't go through the CheckpointCleaner. SharedStateRegistry differs in the following:
1. A handle is associated with multiple checkpoints (or none at the time of deletion) - so a separate counter would be needed
2. It's not clear to me what number of pending handle deletions should be allowed
Conceptually, it should be a condition instead of a number:
 - when scheduling shared state deletion upon checkpoint subsumption
 - if there are any deletions from a previous subsumption then don't allow new checkpoints
 - start allowing new checkpoints when there newPending <= oldPending

This would be achieved by batchind deletions naturally; but implemented in CheckpointRequestDecider it could add extra complexity.
 
The advantage of doing it in CheckpointRequestDecider is more accurate reporting of checkpoint duration.
 
Maybe batching can be a short-term solution; which can be evolved gradually (by replacing executor in wrapper by multiple queues; and then checking queue size in CheckpointRequestDecider). WDYT?
 
> It looks like simple fair io thread pool as I described above, without any priorities + addjusting/relaxing numberOfCleaningCheckpointsSupplier.getAsInt() > maxConcurrentCheckpointAttempts check to something like numberOfCleaningCheckpointsSupplier.getAsInt() > maxConcurrentCheckpointAttempts + CONSTANT would do the trick, wouldn't it?
IIUC, it wouldn't exert back-pressure, because the number of handles is not directly related to the number of checkpoints to clean (I'm assuming seperate thread pools for discarding shared state and initializing new checkpoints).
Taken to the extreme, checkpoints might consist only of shared state, so checkpoint deletion will be fast. But then discarding shared state might take arbitrarily long.;;;","23/Feb/22 15:05;pnowojski;{quote}
We can avoid hardcoding these numbers and put the logic into some wrapper around the pool (with a new method that accepts a list of Runnables or Handles).

Maybe batching can be a short-term solution; which can be evolved gradually (by replacing executor in wrapper by multiple queues; and then checking queue size in CheckpointRequestDecider). WDYT?
{quote}

I still do not think this is a good solution. You would need to assume what other users of the ioExecutor pool are doing and how are they using it, so tweaking the number of batches to ""size of the pool - 1"" sounds like a bad idea. At the same time I don't see a reason to rush this?

I agree that expressing the right condition for which {{CheckpointRequestDecider}} should be back pressured is quite tricky.;;;","23/Feb/22 15:30;roman;Other users (such as handling RPC) wouldn't have to use the new method; the only impact on them will be the same speedup as for new checkpoints.

Having this fix would be allow to enable changelog without increasing the jobmanager.io-pool.size (on object storages where deletes are slower) or experiencing these checkpoint pauses.

If there is a simple working fix and everyone agrees on the approach then it could be implemented quickly. But it dshouldn't be rushed, I agree.

 

edit:

Queues approach seems similar to the one proposed earlier: [https://docs.google.com/document/d/1p0m4FAmpWxShFaicgHXKqKCHYjnsDnKzvyU7BuA9CT8/edit?usp=sharing]

Maybe it's time to consider it again, but it can be a bigger effort.;;;","24/Feb/22 14:08;ym;# Why using a separate pool for deletion is not a good idea?
 # If the answer to 1 is due to ""backpressure"". When mentioning ""backpressure"", do you mean triggering/starting new checkpoints faster than we can subsume/delete the old ones' states? 
 # If yes, then using separate pools, we can still pause triggering new checkpoint if state deletion speed not catching up
 # I agree that batching deletion and randomizing triggering materialization can mitigate the problem, but can not prevent the problem completely, because neither can not guarantee that ""checkpoint triggering speed < state deletion speed"".
 # When talking about `backpressure`, isn't it usually related to data processing? I do not think checkpointing should affect normal data processing if that's the case.;;;","25/Feb/22 07:12;ym;The severity of this problem also depends on the test setup:
 * how often checkpoint is triggered
 * how big the state is
 * materialization interval (maybe)

If as the number/example shared above, triggering cp each 1s, I do agree that severity is an improvement/major.;;;","10/Mar/22 15:09;roman;I've extracted non-backend related issue into FLINK-26590.

Backend-related fix (randomized materialization) merged into master as ed5e6144441bfbc020f525f9c10fd29cb3d83cbf.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlobalCommitter can receive failed committables,FLINK-26304,13429966,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,fpaul,fpaul,22/Feb/22 15:50,23/Feb/22 18:14,13/Jul/23 08:08,23/Feb/22 18:14,1.14.3,,,,,,1.14.4,,,,API / DataStream,Connectors / Common,,,,0,pull-request-available,,,"After the introduction of retrying failed committables in the committer, it is important to only forward committables that have been committed to the GlobalCommitter. Currently, this is ignored and the Committer will forward all committable nevertheless if they succeeded. 

 

[1] https://github.com/apache/flink/blob/0a76d632f33d9a69df87457a63043bd7f609ed40/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/AbstractStreamingCommitterHandler.java#L144",,fpaul,knaufk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 23 18:14:35 UTC 2022,,,,,,,,,,"0|z0ztwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/22 18:14;fpaul;Merged in release-1.14: 8a168e147ef889116a3db9872ded73fa8ce7ea7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisFirehoseSinkITCase failed on azure IOException,FLINK-26300,13429928,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,CrynetLogistics,CrynetLogistics,CrynetLogistics,22/Feb/22 12:45,28/Feb/22 14:56,13/Jul/23 08:08,22/Feb/22 17:53,1.15.0,,,,,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,test-stability,,"{code:java}
2022-02-21T18:54:53.9969213Z Feb 21 18:54:53 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-02-21T18:54:55.3288815Z Feb 21 18:54:55 [INFO] Running org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase
2022-02-21T18:57:06.3259955Z Feb 21 18:57:06 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 130.939 s <<< FAILURE! - in org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase
2022-02-21T18:57:06.3261426Z Feb 21 18:57:06 [ERROR] org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase  Time elapsed: 130.939 s  <<< ERROR!
2022-02-21T18:57:06.3262397Z Feb 21 18:57:06 org.testcontainers.containers.ContainerLaunchException: Container startup failed
2022-02-21T18:57:06.3263302Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:336)
2022-02-21T18:57:06.3264433Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
2022-02-21T18:57:06.3265382Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1066)
2022-02-21T18:57:06.3266542Z Feb 21 18:57:06   at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
2022-02-21T18:57:06.3267493Z Feb 21 18:57:06   at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-02-21T18:57:06.3268226Z Feb 21 18:57:06   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-21T18:57:06.3268807Z Feb 21 18:57:06   at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-02-21T18:57:06.3269328Z Feb 21 18:57:06   at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-02-21T18:57:06.3269832Z Feb 21 18:57:06   at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-02-21T18:57:06.3270391Z Feb 21 18:57:06   at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-02-21T18:57:06.3271025Z Feb 21 18:57:06   at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-02-21T18:57:06.3271782Z Feb 21 18:57:06   at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-02-21T18:57:06.3272455Z Feb 21 18:57:06   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-21T18:57:06.3273175Z Feb 21 18:57:06   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-21T18:57:06.3273887Z Feb 21 18:57:06   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-21T18:57:06.3274680Z Feb 21 18:57:06   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-21T18:57:06.3275387Z Feb 21 18:57:06   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-21T18:57:06.3276124Z Feb 21 18:57:06   at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-21T18:57:06.3276745Z Feb 21 18:57:06   at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-21T18:57:06.3277730Z Feb 21 18:57:06   at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-21T18:57:06.3278780Z Feb 21 18:57:06   at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-21T18:57:06.3279551Z Feb 21 18:57:06   at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-21T18:57:06.3280412Z Feb 21 18:57:06   at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-21T18:57:06.3281184Z Feb 21 18:57:06   at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-21T18:57:06.3281984Z Feb 21 18:57:06   at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-21T18:57:06.3282614Z Feb 21 18:57:06   at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-21T18:57:06.3283208Z Feb 21 18:57:06   at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-21T18:57:06.3283802Z Feb 21 18:57:06   at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-02-21T18:57:06.3284637Z Feb 21 18:57:06 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
2022-02-21T18:57:06.3285288Z Feb 21 18:57:06   at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
2022-02-21T18:57:06.3286000Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
2022-02-21T18:57:06.3286478Z Feb 21 18:57:06   ... 27 more
2022-02-21T18:57:06.3287048Z Feb 21 18:57:06 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
2022-02-21T18:57:06.3287937Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:525)
2022-02-21T18:57:06.3288745Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
2022-02-21T18:57:06.3289398Z Feb 21 18:57:06   at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
2022-02-21T18:57:06.3289881Z Feb 21 18:57:06   ... 28 more
2022-02-21T18:57:06.3290355Z Feb 21 18:57:06 Caused by: org.rnorth.ducttape.TimeoutException: Timeout waiting for result with exception
2022-02-21T18:57:06.3290966Z Feb 21 18:57:06   at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:54)
2022-02-21T18:57:06.3291670Z Feb 21 18:57:06   at org.apache.flink.connector.aws.testutils.LocalstackContainer$ListBucketObjectsWaitStrategy.waitUntilReady(LocalstackContainer.java:70)
2022-02-21T18:57:06.3292450Z Feb 21 18:57:06   at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)
2022-02-21T18:57:06.3293148Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:929)
2022-02-21T18:57:06.3293794Z Feb 21 18:57:06   at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:468)
2022-02-21T18:57:06.3294349Z Feb 21 18:57:06   ... 30 more
2022-02-21T18:57:06.3295145Z Feb 21 18:57:06 Caused by: java.util.concurrent.ExecutionException: software.amazon.awssdk.core.exception.SdkClientException: An exception was thrown and did not match any waiter acceptors
2022-02-21T18:57:06.3296202Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-02-21T18:57:06.3296805Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-02-21T18:57:06.3297464Z Feb 21 18:57:06   at org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.createBucket(AWSServicesTestUtils.java:117)
2022-02-21T18:57:06.3298222Z Feb 21 18:57:06   at org.apache.flink.connector.aws.testutils.LocalstackContainer$ListBucketObjectsWaitStrategy.list(LocalstackContainer.java:82)
2022-02-21T18:57:06.3299057Z Feb 21 18:57:06   at org.rnorth.ducttape.ratelimits.RateLimiter.getWhenReady(RateLimiter.java:51)
2022-02-21T18:57:06.3299917Z Feb 21 18:57:06   at org.apache.flink.connector.aws.testutils.LocalstackContainer$ListBucketObjectsWaitStrategy.lambda$waitUntilReady$0(LocalstackContainer.java:73)
2022-02-21T18:57:06.3300743Z Feb 21 18:57:06   at org.rnorth.ducttape.unreliables.Unreliables.lambda$retryUntilSuccess$0(Unreliables.java:43)
2022-02-21T18:57:06.3301342Z Feb 21 18:57:06   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-02-21T18:57:06.3301903Z Feb 21 18:57:06   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-02-21T18:57:06.3302493Z Feb 21 18:57:06   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-02-21T18:57:06.3303087Z Feb 21 18:57:06   at java.lang.Thread.run(Thread.java:748)
2022-02-21T18:57:06.3303998Z Feb 21 18:57:06 Caused by: software.amazon.awssdk.core.exception.SdkClientException: An exception was thrown and did not match any waiter acceptors
2022-02-21T18:57:06.3305037Z Feb 21 18:57:06   at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:98)
2022-02-21T18:57:06.3305883Z Feb 21 18:57:06   at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:43)
2022-02-21T18:57:06.3306599Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.waiters.WaiterExecutorHelper.lambda$noneMatchException$3(WaiterExecutorHelper.java:94)
2022-02-21T18:57:06.3307204Z Feb 21 18:57:06   at java.util.Optional.map(Optional.java:215)
2022-02-21T18:57:06.3307883Z Feb 21 18:57:06   at software.amazon.awssdk.utils.Either.lambda$map$0(Either.java:51)
2022-02-21T18:57:06.3308505Z Feb 21 18:57:06   at java.util.Optional.orElseGet(Optional.java:267)
2022-02-21T18:57:06.3309248Z Feb 21 18:57:06   at software.amazon.awssdk.utils.Either.map(Either.java:51)
2022-02-21T18:57:06.3310018Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.waiters.WaiterExecutorHelper.noneMatchException(WaiterExecutorHelper.java:92)
2022-02-21T18:57:06.3310799Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.waiters.AsyncWaiterExecutor.lambda$runAsyncPollingFunction$0(AsyncWaiterExecutor.java:107)
2022-02-21T18:57:06.3311509Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3312146Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3312765Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3313523Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3314746Z Feb 21 18:57:06   at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:76)
2022-02-21T18:57:06.3316008Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3316881Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3317531Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3318163Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3318934Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallMetricCollectionStage.lambda$execute$0(AsyncApiCallMetricCollectionStage.java:54)
2022-02-21T18:57:06.3319703Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3320331Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3320967Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3321597Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3322538Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallTimeoutTrackingStage.lambda$execute$2(AsyncApiCallTimeoutTrackingStage.java:67)
2022-02-21T18:57:06.3323295Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3324001Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3324633Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3325348Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3326252Z Feb 21 18:57:06   at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:76)
2022-02-21T18:57:06.3327077Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3327696Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3328318Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3328941Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3329693Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeAttemptExecute(AsyncRetryableStage.java:103)
2022-02-21T18:57:06.3330558Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:181)
2022-02-21T18:57:06.3331418Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.lambda$attemptExecute$1(AsyncRetryableStage.java:159)
2022-02-21T18:57:06.3332173Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3332807Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3333628Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3334373Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3335064Z Feb 21 18:57:06   at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:76)
2022-02-21T18:57:06.3335878Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3336533Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3337169Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3337798Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3338525Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$null$0(MakeAsyncHttpRequestStage.java:104)
2022-02-21T18:57:06.3339247Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-21T18:57:06.3339864Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-21T18:57:06.3340487Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-21T18:57:06.3341108Z Feb 21 18:57:06   at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-21T18:57:06.3342009Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage$WrappedErrorForwardingResponseHandler.onError(MakeAsyncHttpRequestStage.java:158)
2022-02-21T18:57:06.3342862Z Feb 21 18:57:06   at software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.handleFailure(NettyRequestExecutor.java:300)
2022-02-21T18:57:06.3343610Z Feb 21 18:57:06   at software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.makeRequestListener(NettyRequestExecutor.java:172)
2022-02-21T18:57:06.3344349Z Feb 21 18:57:06   at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
2022-02-21T18:57:06.3345029Z Feb 21 18:57:06   at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
2022-02-21T18:57:06.3345795Z Feb 21 18:57:06   at io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)
2022-02-21T18:57:06.3346397Z Feb 21 18:57:06   at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:502)
2022-02-21T18:57:06.3347023Z Feb 21 18:57:06   at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
2022-02-21T18:57:06.3347674Z Feb 21 18:57:06   at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
2022-02-21T18:57:06.3348296Z Feb 21 18:57:06   at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
2022-02-21T18:57:06.3348904Z Feb 21 18:57:06   at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
2022-02-21T18:57:06.3349766Z Feb 21 18:57:06   at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2022-02-21T18:57:06.3350518Z Feb 21 18:57:06   ... 1 more
2022-02-21T18:57:06.3351510Z Feb 21 18:57:06 Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: The channel was closed before the protocol could be determined.
2022-02-21T18:57:06.3352753Z Feb 21 18:57:06   at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:98)
2022-02-21T18:57:06.3353528Z Feb 21 18:57:06   at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:43)
2022-02-21T18:57:06.3354426Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:204)
2022-02-21T18:57:06.3355245Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:200)
2022-02-21T18:57:06.3356274Z Feb 21 18:57:06   at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:179)
2022-02-21T18:57:06.3356869Z Feb 21 18:57:06   ... 28 more
2022-02-21T18:57:06.3357348Z Feb 21 18:57:06 Caused by: java.io.IOException: The channel was closed before the protocol could be determined.
2022-02-21T18:57:06.3358066Z Feb 21 18:57:06   at software.amazon.awssdk.http.nio.netty.internal.http2.Http2SettingsFrameHandler.channelUnregistered(Http2SettingsFrameHandler.java:58)
2022-02-21T18:57:06.3358852Z Feb 21 18:57:06   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:198)
2022-02-21T18:57:06.3359594Z Feb 21 18:57:06   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:184)
2022-02-21T18:57:06.3360328Z Feb 21 18:57:06   at io.netty.channel.AbstractChannelHandlerContext.fireChannelUnregistered(AbstractChannelHandlerContext.java:177)
2022-02-21T18:57:06.3361042Z Feb 21 18:57:06   at io.netty.channel.DefaultChannelPipeline$HeadContext.channelUnregistered(DefaultChannelPipeline.java:1388)
2022-02-21T18:57:06.3361770Z Feb 21 18:57:06   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:198)
2022-02-21T18:57:06.3362505Z Feb 21 18:57:06   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:184)
2022-02-21T18:57:06.3363364Z Feb 21 18:57:06   at io.netty.channel.DefaultChannelPipeline.fireChannelUnregistered(DefaultChannelPipeline.java:821)
2022-02-21T18:57:06.3364090Z Feb 21 18:57:06   at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:839)
2022-02-21T18:57:06.3364702Z Feb 21 18:57:06   at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
2022-02-21T18:57:06.3365344Z Feb 21 18:57:06   at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
2022-02-21T18:57:06.3366139Z Feb 21 18:57:06   at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
2022-02-21T18:57:06.3366677Z Feb 21 18:57:06   ... 3 more
2022-02-21T18:57:06.3366982Z Feb 21 18:57:06
2022-02-21T18:57:06.7065756Z Feb 21 18:57:06 [INFO]
2022-02-21T18:57:06.7066309Z Feb 21 18:57:06 [INFO] Results:
2022-02-21T18:57:06.7066664Z Feb 21 18:57:06 [INFO]
2022-02-21T18:57:06.7067042Z Feb 21 18:57:06 [ERROR] Errors:
2022-02-21T18:57:06.7068246Z Feb 21 18:57:06 [ERROR]   KinesisFirehoseSinkITCase » ContainerLaunch Container startup failed
2022-02-21T18:57:06.7068701Z Feb 21 18:57:06 [INFO]
2022-02-21T18:57:06.7069136Z Feb 21 18:57:06 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0  {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31971&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d",,CrynetLogistics,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26069,,,,,,,,,FLINK-26291,,,FLINK-26309,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 22 17:53:32 UTC 2022,,,,,,,,,,"0|z0zto0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 17:53;dannycranmer;Merged into master https://github.com/apache/flink/commit/f1937c80beb10ecf4ecb2f769231850fd353ea49;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'HELP ;', 'QUIT ;' and other sql-client commands fail with CalciteException: Non-query expression encountered in illegal context",FLINK-26295,13429898,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Feb/22 10:22,14/Mar/22 12:17,13/Jul/23 08:08,14/Mar/22 12:17,1.15.0,,,,,,1.15.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,"It seems the reason is https://github.com/apache/flink/pull/18363/files
where added condition like 
{code:java}
super(Pattern.compile(""HELP;?"", DEFAULT_PATTERN_FLAGS))
{code}
that means {{HELP;}} will work however if there is any space between {{HELP}} and {{;}} then not
and it fails like below (before that it worked without failing)

Having a space between command and a semicolon is pretty common since autocompletion inserts it after tab
{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.calcite.runtime.CalciteException: Non-query expression encountered in illegal context
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.checkNonQueryExpression(FlinkSqlParserImpl.java:395)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression3(FlinkSqlParserImpl.java:21147)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2b(FlinkSqlParserImpl.java:20816)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2(FlinkSqlParserImpl.java:20857)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression(FlinkSqlParserImpl.java:20788)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQueryOrExpr(FlinkSqlParserImpl.java:20765)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.QueryOrExpr(FlinkSqlParserImpl.java:20213)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.OrderedQueryOrExpr(FlinkSqlParserImpl.java:588)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3986)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:2915)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:287)
	at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:193)
	at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:77)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$parseStatement$1(LocalExecutor.java:172)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172)
	at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45)
	at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71)
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2731)
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:585)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:296)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)

{noformat}",,jark,Sergey Nuyanzin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 12:17:54 UTC 2022,,,,,,,,,,"0|z0zthc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/22 12:17;twalthr;Fixed in 1.15: c83ec411768bbfc046bae5a305c232bc75f93269;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisFirehoseSinkITCase failed on azure,FLINK-26291,13429833,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,CrynetLogistics,gaoyunhaii,gaoyunhaii,22/Feb/22 06:41,23/Feb/22 08:42,13/Jul/23 08:08,23/Feb/22 08:41,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,test-stability,,"{code:java}
Feb 22 02:47:37 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 83.215 s <<< FAILURE! - in org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase
Feb 22 02:47:37 [ERROR] org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.test  Time elapsed: 50.712 s  <<< FAILURE!
Feb 22 02:47:37 org.opentest4j.AssertionFailedError: 
Feb 22 02:47:37 
Feb 22 02:47:37 expected: 92
Feb 22 02:47:37  but was: 93
Feb 22 02:47:37 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Feb 22 02:47:37 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Feb 22 02:47:37 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Feb 22 02:47:37 	at org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.test(KinesisFirehoseSinkITCase.java:133)
Feb 22 02:47:37 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 02:47:37 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 02:47:37 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 02:47:37 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 02:47:37 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 02:47:37 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 02:47:37 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 02:47:37 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 02:47:37 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Feb 22 02:47:37 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 02:47:37 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Feb 22 02:47:37 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Feb 22 02:47:37 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 22 02:47:37 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Feb 22 02:47:37 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 02:47:37 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 22 02:47:37 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Feb 22 02:47:37 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Feb 22 02:47:37 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Feb 22 02:47:37 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31983&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=44249",,CrynetLogistics,dannycranmer,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26305,,,FLINK-26300,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 23 08:41:52 UTC 2022,,,,,,,,,,"0|z0zt2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 13:31;CrynetLogistics;Hey [~gaoyunhaii] 

How do you feel about this PR #18876 that I raised.;;;","23/Feb/22 08:41;dannycranmer;Merged with https://github.com/apache/flink/commit/8d5a7f1fe88db002c460a5dbb1ea14b8a75c60c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler: Exception history can not be queried by the REST API,FLINK-26289,13429767,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,dmvk,dmvk,21/Feb/22 17:51,01/Mar/22 06:37,13/Jul/23 08:08,01/Mar/22 06:37,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"In FLINK-21439, we've started collecting a history of exceptions in the Adaptive Scheduler. We have a good coverage that this part works properly, but we've missed the part that exposes the history via REST API.

The problematic part is that execution graph attached with the `ExecutionGraphInfo` does no longer contain a failure info.",,dmvk,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21439,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 01 06:37:51 UTC 2022,,,,,,,,,,"0|z0zsoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/22 06:37;mapohl;{{master}}: [5c4d2632e991696e0cb7e016a34ce045a06faf83|https://github.com/apache/flink/commit/5c4d2632e991696e0cb7e016a34ce045a06faf83];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The KubernetesStateHandleStore cleans the metadata before cleaning the StateHandle,FLINK-26286,13429752,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,mapohl,mapohl,21/Feb/22 16:14,02/Mar/22 13:49,13/Jul/23 08:08,02/Mar/22 13:49,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"Cleanup of job state does not work properly in an HA setup. {{releaseAndTryRemove}} deletes the meta data stored in the store before cleaning up the {{StateHandle}}. If the {{StateHandle}} cleanup fails after the reference is already deleted in the {{StateHandleStore}}, a cleanup retry will constantly fail because it cannot deserialize the {{StateHandle}} anymore.",,aitozi,huwh,mapohl,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26284,,,,,,,,FLINK-25432,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 02 13:49:21 UTC 2022,,,,,,,,,,"0|z0zslc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 02:47;wangyang0918;So are you suggesting to do the ""clean up StateHandle"" first?;;;","22/Feb/22 17:25;mapohl;We're planning to do it analogously to FLINK-26284 introducing some kind of marker that indicates that the entry is going to be deleted.;;;","02/Mar/22 13:49;mapohl;{{master}}: [449d968c7edb7d9a5cc60b16ec9546cb75c9895f|https://github.com/apache/flink/commit/449d968c7edb7d9a5cc60b16ec9546cb75c9895f];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperStateHandleStore does not handle not existing nodes properly in getAllAndLock,FLINK-26285,13429750,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,21/Feb/22 15:45,11/Mar/22 09:33,13/Jul/23 08:08,28/Feb/22 07:44,1.13.6,1.14.3,1.15.0,,,,1.13.7,1.14.5,1.15.0,,Runtime / Coordination,,,,,0,pull-request-available,,,"[c3a6b514595ea3c1bf52126f6f1715b26c871ae9|https://github.com/apache/flink/commit/c3a6b514595ea3c1bf52126f6f1715b26c871ae9] introduces new exceptions that are not properly handled in [ZooKeeperStateHandleStore:378|https://github.com/apache/flink/blob/0cf7c3dedd3575cdfed57727e9712c28c013d7ca/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L378]",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26284,,,,,,,,,,,,,,,,,FLINK-19543,,,,,,,,,,,FLINK-26284,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 28 07:44:28 UTC 2022,,,,,,,,,,"0|z0zskw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/22 07:44;mapohl;master: [431f757aa3547833b3684f7e80dcc3ec8d2d8311|https://github.com/apache/flink/commit/431f757aa3547833b3684f7e80dcc3ec8d2d8311]
1.14: [77a7d4059e2ef666aefa308f0ac40dbf6c3a7e36|https://github.com/apache/flink/commit/77a7d4059e2ef666aefa308f0ac40dbf6c3a7e36]
1.13: [ef75b87c29fec65c8c4a8b5bd83f80bd72d5b67c|https://github.com/apache/flink/commit/ef75b87c29fec65c8c4a8b5bd83f80bd72d5b67c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The ZooKeeperStateHandleStore cleans the metadata before cleaning the StateHandle,FLINK-26284,13429737,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,21/Feb/22 14:27,01/Apr/22 12:59,13/Jul/23 08:08,28/Feb/22 10:35,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"Cleanup of job state does not work properly in an HA setup. {{releaseAndTryRemove}} deletes the meta data stored in the store before cleaning up the {{StateHandle}}. If the {{StateHandle}} cleanup fails after the reference is already deleted in the {{StateHandleStore}}, a cleanup retry will constantly fail because it cannot deserialize the {{StateHandle}} anymore.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,FLINK-26285,,,,,,,,,FLINK-26286,,,,,,,,,FLINK-25432,,FLINK-26987,,,,,,,,,,FLINK-26285,FLINK-26288,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 28 10:35:53 UTC 2022,,,,,,,,,,"0|z0zsi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 07:08;mapohl;Several options were considered:
* Have an additional flag for deletion as a child ZK node: This is problematic because of the transactional deletion of the already existing access child lock. We're not supposed to delete the ZK node if there's a lock on it due to some other client accessing it. Hence, we would have to check for the existince of the access child lock, and delete everything if it doesn't exist or delete only the deletion-flag child node if it does.
* Alternatively, we could model the deletion-flag node on the same level as the StateHandle node. This makes it possible to run the transactional delete on the StateHandle node and clear the deletion-flag node afterwards.
* Move the deletion marker into the {{RetrievableStateHandle}}. It has the benefit that it can be used by the {{KubernetesStateHandleStore}} (FLINK-26286) as well but has the flaw that it requires a deserialization which is a problem in cases where the data was corrupted. But in these cases, we want to delete the data anyway.;;;","22/Feb/22 16:54;mapohl;We still had trouble running the check for locks and marking-for-deletion in an atomic manner. Therefore, [~dmvk] proposed a different solution which worked in the end: A {{/locks}} subfolder was introduced that holds all the locks of open ZK connections. Deleting this subfolder indicates that the node is ready for deletion. This operation fails if there are still connections open.

This approach allows us to do the connection check and mark-for-deletion operation in an atomic manner.;;;","28/Feb/22 10:35;mapohl;{{master}}: [c3df4c3f1f868d40e1e70404bea41b7a007e8b08|https://github.com/apache/flink/commit/c3df4c3f1f868d40e1e70404bea41b7a007e8b08];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Expressions.col as a synonym for $,FLINK-26278,13429671,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,21/Feb/22 09:26,22/Feb/22 09:08,13/Jul/23 08:08,22/Feb/22 09:08,,,,,,,1.15.0,,,,Table SQL / API,,,,,0,pull-request-available,,,"`$` might not always be the right choice for referring to a column. Python API has already `col` for this. We should offer `col` as well for e.g. Kotlin users:

https://stackoverflow.com/questions/71145050/how-to-write-user-in-kotlin-with-flink",,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 22 09:08:21 UTC 2022,,,,,,,,,,"0|z0zs3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 09:08;twalthr;Fixed in master: ec468160b184c5a5cc047721d6373ddc10810a71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink SQL write data to kafka by CSV format , decimal type was converted to scientific notation",FLINK-26270,13429629,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jkf6160@163.com,jkf6160@163.com,jkf6160@163.com,21/Feb/22 06:14,01/Aug/22 10:17,13/Jul/23 08:08,01/Aug/22 10:02,1.12.4,,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"Source：Oracle

field type：decimal

!image-2022-02-21-14-12-17-845.png|width=362,height=137!

!image-2022-02-25-17-38-06-764.png!

 

Sink:：kafka

field type：decimal

format：CSV

!image-2022-02-21-14-13-28-605.png|width=259,height=184!

 

Cannot set not to convert to scientific notation

 ",,jkf6160@163.com,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/22 06:11;jkf6160@163.com;image-2022-02-21-14-11-49-617.png;https://issues.apache.org/jira/secure/attachment/13040286/image-2022-02-21-14-11-49-617.png","21/Feb/22 06:12;jkf6160@163.com;image-2022-02-21-14-12-17-845.png;https://issues.apache.org/jira/secure/attachment/13040285/image-2022-02-21-14-12-17-845.png","21/Feb/22 06:13;jkf6160@163.com;image-2022-02-21-14-13-28-605.png;https://issues.apache.org/jira/secure/attachment/13040284/image-2022-02-21-14-13-28-605.png","25/Feb/22 09:38;jkf6160@163.com;image-2022-02-25-17-38-06-764.png;https://issues.apache.org/jira/secure/attachment/13040472/image-2022-02-25-17-38-06-764.png","07/Mar/22 06:56;jkf6160@163.com;image-2022-03-07-14-56-00-328.png;https://issues.apache.org/jira/secure/attachment/13040767/image-2022-03-07-14-56-00-328.png",,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Mon Aug 01 10:17:04 UTC 2022,,,,,,,,,,"0|z0zru0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 12:16;jkf6160@163.com;Can you assign this problem to me.;;;","22/Jun/22 12:20;martijnvisser;[~jkf6160@163.com] I've assigned it to you. Please make sure that any PR is made for {master} first. ;;;","22/Jun/22 12:22;jkf6160@163.com;Copy that.;;;","24/Jul/22 03:30;jkf6160@163.com;hi [~martijnvisser] , Could you please pay attention to the progress of PR reform ? thanks;;;","01/Aug/22 10:02;martijnvisser;Fixed in master: 09747f999e54b7921e8c12c944b941b0777be48f;;;","01/Aug/22 10:17;jkf6160@163.com;Thanks  [~martijnvisser] [~afedulov] for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add check for data size in LogisticRegression,FLINK-26263,13429588,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,21/Feb/22 01:43,24/Jun/22 09:08,13/Jul/23 08:08,14/Mar/22 06:47,ml-2.0.0,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,,,"In Flink ML LogisticRegression, the algorithm would fail if the parallelism is larger than input data size. For example, in `LogisticRegressionTest.testFitAndPredict()` if we add the following code

```java

env.setParallelism(12);

```

Then the test case would fail with the following exception

```

Caused by: java.lang.IllegalArgumentException: bound must be positive
    at java.base/java.util.Random.nextInt(Random.java:388)
    at org.apache.flink.ml.classification.logisticregression.LogisticRegression$CacheDataAndDoTrain.getMiniBatchData(LogisticRegression.java:351)
    at org.apache.flink.ml.classification.logisticregression.LogisticRegression$CacheDataAndDoTrain.onEpochWatermarkIncremented(LogisticRegression.java:381)
    at org.apache.flink.iteration.operator.AbstractWrapperOperator.notifyEpochWatermarkIncrement(AbstractWrapperOperator.java:129)
    at org.apache.flink.iteration.operator.allround.AbstractAllRoundWrapperOperator.lambda$1(AbstractAllRoundWrapperOperator.java:105)
    at org.apache.flink.iteration.operator.OperatorUtils.processOperatorOrUdfIfSatisfy(OperatorUtils.java:79)
    at org.apache.flink.iteration.operator.allround.AbstractAllRoundWrapperOperator.onEpochWatermarkIncrement(AbstractAllRoundWrapperOperator.java:102)
    at org.apache.flink.iteration.progresstrack.OperatorEpochWatermarkTracker.tryUpdateLowerBound(OperatorEpochWatermarkTracker.java:79)
    at org.apache.flink.iteration.progresstrack.OperatorEpochWatermarkTracker.onEpochWatermark(OperatorEpochWatermarkTracker.java:63)
    at org.apache.flink.iteration.operator.AbstractWrapperOperator.onEpochWatermarkEvent(AbstractWrapperOperator.java:121)
    at org.apache.flink.iteration.operator.allround.TwoInputAllRoundWrapperOperator.processElement(TwoInputAllRoundWrapperOperator.java:77)
    at org.apache.flink.iteration.operator.allround.TwoInputAllRoundWrapperOperator.processElement2(TwoInputAllRoundWrapperOperator.java:59)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.processRecord2(StreamTwoInputProcessorFactory.java:225)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.lambda$create$1(StreamTwoInputProcessorFactory.java:194)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:266)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:86)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
    at java.base/java.lang.Thread.run(Thread.java:834)

```

The cause of this exception is that LogisticRegression has not considered the case when input data size is 0. This can be resolved by adding an additional check.",,ghandzhipeng,yunfengzhou,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 14 06:47:26 UTC 2022,,,,,,,,,,"0|z0zrkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/22 06:47;zhangzp;fixed via: bd6d67f78fe5341ec992d72b633b26e2def7783a, 2015dafc6bad65d3ce9b5e8ae6d1ae9b567e910c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink might violate order of sequence numbers and risk exactly-once processing,FLINK-26254,13429346,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,alexanderpreuss,fpaul,fpaul,18/Feb/22 14:20,23/Nov/22 08:24,13/Jul/23 08:08,23/Nov/22 08:24,1.14.3,1.15.0,,,,,,,,,Connectors / Kafka,,,,,0,,,,"When running the KafkaSink in exactly-once mode with a very low checkpoint interval users are seeing `OutOfOrderSequenceException`.

It could be caused by the fact that the connector has a pool of KafkaProducers and the sequence numbers are not shared/reset if a new KafkaProducer tries to write to a partition while the previous KafkaProducer is still occupied for committing.",,alexanderpreuss,flyaruu,fpaul,gaoyunhaii,knaufk,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 23 08:24:47 UTC 2022,,,,,,,,,,"0|z0zq3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 14:35;knaufk;This also happens with a higher checkpoint interval. It was easily reproducible with a 10s interval.;;;","21/Feb/22 15:36;alexanderpreuss;[~knaufk] how was your setup to reproduce this? I'm trying to extend the existing ITCase to create some more checkpoints but I do not see it happening there, neither with a high nor low checkpoint interval;;;","24/Feb/22 14:10;knaufk;Important: this only happened with RedPanda not with Kafka.;;;","24/Feb/22 14:57;alexanderpreuss;Added some testcases [here|https://github.com/alpreu/flink/commit/a4c1b4b5dd8049e4bec6b2cc6f0b8ddfd500d065] that could maybe provoke the exception to happen, yet still always pass. We believe the issue to be on RedPandas side;;;","23/Nov/22 08:24;martijnvisser;Verified that this issue no longer occurs with Flink 1.16.0 and the latest version of Redpanda. Most likely this was resolved because Flink has updated to a newer version of the Kafka Client. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor MiniClusterExtension,FLINK-26252,13429341,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,18/Feb/22 13:49,24/Feb/22 15:57,13/Jul/23 08:08,24/Feb/22 15:56,,,,,,,1.15.0,,,,Test Infrastructure,,,,,0,pull-request-available,,,,,slinkydeveloper,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 15:56:27 UTC 2022,,,,,,,,,,"0|z0zq2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 15:56;chesnay;master: 8cdd0b85c3ab2bfbe47aa5b680cdcbaf186c0731;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run BuiltInFunctionITCase tests in parallel,FLINK-26249,13429306,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,18/Feb/22 09:44,21/Mar/22 13:27,13/Jul/23 08:08,21/Mar/22 13:27,,,,,,,1.16.0,,,,Table SQL / Planner,Test Infrastructure,,,,0,pull-request-available,,,,,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,FLINK-26252,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 13:27:40 UTC 2022,,,,,,,,,,"0|z0zpuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/22 13:27;twalthr;Fixed in master: d07fd36af66ebdf7200650f387489018bd9df454;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactingFileWriter and PendingFileRecoverable should not be exposed to users.,FLINK-26235,13429252,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,pltbkd,pltbkd,18/Feb/22 03:53,21/Feb/22 09:26,13/Jul/23 08:08,21/Feb/22 09:26,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,"In FLINK-25583, we added a FileCompactingWriter to write the compacting file. The writer is exposed to users in the FileCompactor, while it has a `closeForCommit` function, and relies on the `PendingFileRecoverable`, both of which should not be exposed to users. We should fix this and annotate them as Internal.",,gaoyunhaii,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 21 09:26:36 UTC 2022,,,,,,,,,,"0|z0zpiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/22 09:26;gaoyunhaii;Fix on master via 3c99b005563debf35f43df5fb4ec92863e7775a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start minikube manually to replace external github actions,FLINK-26234,13429241,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,18/Feb/22 02:28,27/Mar/22 13:16,13/Jul/23 08:08,18/Feb/22 06:14,,,,,,,kubernetes-operator-0.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"According to the apache requirement, external github actions are not allowed to used in the workflow. So we need to start minikube manually instead of relying on the existing actions.

See [https://github.com/apache/flink-kubernetes-operator/actions/runs/1859307064].
{code:java}
Error: .github#L1medyagh/setup-minikube@master and medyagh/setup-minikube@master are not allowed to be used in apache/flink-kubernetes-operator. Actions in this workflow must be: within a repository owned by apache, created by GitHub, verified in the GitHub Marketplace or match the following: */*@[a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9][a-f0-9]+, AdoptOpenJDK/install-jdk@*, JamesIves/github-pages-deploy-action@5dc1d5a192aeb5ab5b7d5a77b7d36aea4a7f5c92, TobKed/label-when-approved-action@*, actions-cool/issues-helper@*, actions-rs/*, al-cheb/configure-pagefile-action@*, amannn/action-semantic-pull-request@*, apache/*, burrunan/gradle-cache-action@*, bytedeco/javacpp-presets/.github/actions/*, chromaui/action@*, codecov/codecov-action@*, conda-incubator/setup-miniconda@*, container-tools/kind-action@*, container-tools/microshift-action@*, dawidd6/action-download-artifact@*, delaguardo/setup-graalvm@*, docker://jekyll/jekyll:*, docker://pandoc/core:2.9, eps1lon/actions-label-merge-conflict@*, gau... {code}",,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26142,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 06:14:06 UTC 2022,,,,,,,,,,"0|z0zpgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 06:14;wangyang0918;Fixed via:

main: a6d216365af7ba26eacfdd601f609059d8f7c2e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSinkCompactionSwitchITCase.testSwitchingCompaction() fails in CI,FLINK-26233,13429209,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,afedulov,afedulov,17/Feb/22 21:16,21/Feb/22 02:46,13/Jul/23 08:08,21/Feb/22 02:46,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,test-stability,,"{code:java}
2022-02-17T20:13:20.2895110Z Feb 17 20:13:20 [INFO] Running org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase
2022-02-17T20:13:40.2160260Z Feb 17 20:13:40 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.905 s - in org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase
2022-02-17T20:13:58.8860609Z Feb 17 20:13:58 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 102.488 s <<< FAILURE! - in org.apache.flink.connector.file.sink.FileSinkCompactionSwitchITCase
2022-02-17T20:13:58.8864562Z Feb 17 20:13:58 [ERROR] FileSinkCompactionSwitchITCase.testSwitchingCompaction  Time elapsed: 37.28 s  <<< ERROR!
2022-02-17T20:13:58.8865526Z Feb 17 20:13:58 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-02-17T20:13:58.8866319Z Feb 17 20:13:58     at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-02-17T20:13:58.8867102Z Feb 17 20:13:58     at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:934)
2022-02-17T20:13:58.8867985Z Feb 17 20:13:58     at org.apache.flink.connector.file.sink.FileSinkCompactionSwitchITCase.testSwitchingCompaction(FileSinkCompactionSwitchITCase.java:175)
[...]
2022-02-17T20:13:58.8919634Z Feb 17 20:13:58 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
[...]
2022-02-17T20:13:58.8939468Z Feb 17 20:13:58 Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
2022-02-17T20:13:58.8940119Z Feb 17 20:13:58     at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:160)
2022-02-17T20:13:58.8940863Z Feb 17 20:13:58     at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:145)
2022-02-17T20:13:58.8941613Z Feb 17 20:13:58     at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:97)
2022-02-17T20:13:58.8942321Z Feb 17 20:13:58     at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2046)
2022-02-17T20:13:58.8943011Z Feb 17 20:13:58     at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1040)
2022-02-17T20:13:58.8943830Z Feb 17 20:13:58     at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
2022-02-17T20:13:58.8944567Z Feb 17 20:13:58     at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
2022-02-17T20:13:58.8945240Z Feb 17 20:13:58     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-02-17T20:13:58.8945794Z Feb 17 20:13:58     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-02-17T20:13:58.8946276Z Feb 17 20:13:58     at java.lang.Thread.run(Thread.java:748)  {code}
https://dev.azure.com/alexanderfedulov/Flink/_build/results?buildId=37&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=e204f081-e6cd-5c04-4f4c-919639b63be9&l=11112",,afedulov,gaoyunhaii,pltbkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 21 02:45:56 UTC 2022,,,,,,,,,,"0|z0zp9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 21:17;afedulov;cc [~gaoyunhaii] [~pltbkd] ;;;","18/Feb/22 04:17;gaoyunhaii;Thanks [~afedulov] for the report! We'll have a look as soon as possible~;;;","18/Feb/22 08:20;pltbkd;This is because when the compaction is switch from on to off, the pending files remained in the compactor state will be flushed to the committer at the first checkpoint after restarting. So the state of the committer can be larger than the stable status, which in this test case is more than 5MB.

I have created a pr to fix this by reducing the speed of the source and using the FileSystemCheckpointStorage.;;;","21/Feb/22 02:45;gaoyunhaii;Fix on master via 8af24957d2ffae10b65e1f34a5d23ae1a06db26d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Incorrect MaterializationID passed to ChangelogStateBackendHandleImpl,FLINK-26231,13429142,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,17/Feb/22 16:06,09/Mar/22 10:03,13/Jul/23 08:08,18/Feb/22 11:30,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"In ChangelogStateBackendHandleImpl constructor, materializationID and persistedSizeOfThisCheckpoint are mixed up.

 

cc: [~yunta]",,roman,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21352,,,,,,,,,,,,FLINK-26255,,,FLINK-26239,,FLINK-25557,,,,,,,,,,,FLINK-25144,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 11:30:58 UTC 2022,,,,,,,,,,"0|z0zoug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 11:30;roman;Merged into master as 8d4d109b0d561145494f912023f0a4298ac41e95.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StatementSet.compilePlan doesn't pass the archiunit tests,FLINK-26199,13428899,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,dmvk,dmvk,16/Feb/22 18:55,17/Feb/22 08:16,13/Jul/23 08:08,17/Feb/22 07:36,,,,,,,1.15.0,,,,Table SQL / API,,,,,0,pull-request-available,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31678&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=26852

{code}
Feb 16 18:10:30 Architecture Violation [Priority: MEDIUM] - Rule 'Return and argument types of methods annotated with @PublicEvolving must be annotated with @Public(Evolving).' was violated (1 times):
Feb 16 18:10:30 org.apache.flink.table.api.StatementSet.compilePlan(): Returned leaf type org.apache.flink.table.api.CompiledPlan does not satisfy: reside outside of package 'org.apache.flink..' or annotated with @Public or annotated with @PublicEvolving or annotated with @Deprecated
Feb 16 18:10:30 	at com.tngtech.archunit.lang.ArchRule$Assertions.assertNoViolation(ArchRule.java:94)
Feb 16 18:10:30 	at com.tngtech.archunit.lang.ArchRule$Assertions.check(ArchRule.java:82)
Feb 16 18:10:30 	at com.tngtech.archunit.library.freeze.FreezingArchRule.check(FreezingArchRule.java:96)
Feb 16 18:10:30 	at com.tngtech.archunit.junit.ArchUnitTestDescriptor$ArchUnitRuleDescriptor.execute(ArchUnitTestDescriptor.java:159)
Feb 16 18:10:30 	at com.tngtech.archunit.junit.ArchUnitTestDescriptor$ArchUnitRuleDescriptor.execute(ArchUnitTestDescriptor.java:142)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Feb 16 18:10:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Feb 16 18:10:30 	at java.util.ArrayList.forEach(ArrayList.java:1259)
{code}",,dmvk,gaoyunhaii,leonard,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26198,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 17 07:36:55 UTC 2022,,,,,,,,,,"0|z0zncw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 07:36;twalthr;Fixed in master: fc86f13659dec102bbb7cb4c27ffff41f3d4c530;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarOrderedSourceReaderTest fails with exit code 255,FLINK-26192,13428825,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,16/Feb/22 14:15,18/Feb/22 09:18,13/Jul/23 08:08,18/Feb/22 09:18,1.15.0,,,,,,1.15.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,"https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=1367&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=26787

{code}
Feb 16 13:49:46 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-pulsar: There are test failures.
Feb 16 13:49:46 [ERROR] 
Feb 16 13:49:46 [ERROR] Please refer to /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire-reports for the individual test results.
Feb 16 13:49:46 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Feb 16 13:49:46 [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 16 13:49:46 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-pulsar && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 						-XX:-UseGCOverheadLimit -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire/surefirebooter3139517882560779643.jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire 2022-02-16T13-48-34_435-jvmRun1 surefire3358354372075396323tmp surefire_08509996975514960300tmp
Feb 16 13:49:46 [ERROR] Error occurred in starting fork, check output in log
Feb 16 13:49:46 [ERROR] Process Exit Code: 255
Feb 16 13:49:46 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 16 13:49:46 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-pulsar && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 						-XX:-UseGCOverheadLimit -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire/surefirebooter3139517882560779643.jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire 2022-02-16T13-48-34_435-jvmRun1 surefire3358354372075396323tmp surefire_08509996975514960300tmp
Feb 16 13:49:46 [ERROR] Error occurred in starting fork, check output in log
Feb 16 13:49:46 [ERROR] Process Exit Code: 255
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:305)
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:265)
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Feb 16 13:49:46 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Feb 16 13:49:46 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Feb 16 13:49:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 16 13:49:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 16 13:49:46 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 16 13:49:46 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Feb 16 13:49:46 [ERROR] -> [Help 1]
Feb 16 13:49:46 [ERROR] 
Feb 16 13:49:46 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Feb 16 13:49:46 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Feb 16 13:49:46 [ERROR] 
Feb 16 13:49:46 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Feb 16 13:49:46 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
Feb 16 13:49:46 [ERROR] 
Feb 16 13:49:46 [ERROR] After correcting the problems, you can resume the build with the command
Feb 16 13:49:46 [ERROR]   mvn <goals> -rf :flink-connector-pulsar

{code}",,affe,dwysakowicz,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26242,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 09:18:04 UTC 2022,,,,,,,,,,"0|z0zmwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 08:49;affe;This looked to me the test VM crashed, let's keep observing it for a while, if it does not happen again for 1 month, can we close it ?;;;","17/Feb/22 08:53;chesnay;Given the amount of issues that pulsar is causing on CI, I'd say no to that.;;;","17/Feb/22 08:56;chesnay;The PulsarEmbeddedRuntime has a codepath for shutting down the JVM, see #startPulsarService.;;;","17/Feb/22 08:58;chesnay;Which, to emphasize, is a _really bad idea_ and shouldn't happen.;;;","17/Feb/22 09:00;chesnay;The crazy thing is that PulsarService even explicitly offers a constructor for testing purposes that doesn't do that.;;;","17/Feb/22 09:11;affe;Got it, I'll bring this issue up with my team tomorrow~ Thanks for your advice ~ We'll try our best to solve it soon.;;;","17/Feb/22 09:13;chesnay;I've already opened a PR.;;;","18/Feb/22 07:43;mapohl;Just for documentation purposes: [This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=751&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=27020] also failed with an exit code 255.;;;","18/Feb/22 08:26;mapohl;And [another one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31774&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=27147] where the failure appears in {{PulsarWriterTest}};;;","18/Feb/22 09:18;chesnay;master: b3cdfda518c023b92893bc522853adfc0275af9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chinese docs override english aliases,FLINK-26187,13428775,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Feb/22 10:53,17/Feb/22 08:00,13/Jul/23 08:08,17/Feb/22 08:00,,,,,,,1.14.4,1.15.0,,,Documentation,,,,,0,pull-request-available,,,Various chinese pages define an alias for an URL to an english page. This results in redirects being set up that point to the chinese version of the docs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 17 08:00:55 UTC 2022,,,,,,,,,,"0|z0zmlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 08:00;chesnay;master: de220ba263a3512673336a91e8f439696945a6f5
1.14: 03883ebb690b497d2ec421515d3f0b788323655c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2E Elasticsearch Tests should use the new Sink interface,FLINK-26185,13428767,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,16/Feb/22 10:22,17/Feb/22 17:45,13/Jul/23 08:08,17/Feb/22 17:45,1.15.0,,,,,,1.15.0,,,,Connectors / ElasticSearch,Tests,,,,0,pull-request-available,,,"Currently the E2E tests for Elasticsearch (test_streaming_elasticsearch.sh) is testing the old Sink interface implementation. As we are now moving to the new Sink interface, we should update the tests to test the new implementation",,alexanderpreuss,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 17 17:45:35 UTC 2022,,,,,,,,,,"0|z0zmjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 17:45;fpaul;Merged in master: 3b845ea95c5feaf093629d6c09b43c3260ea6176;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala savepoint migration tests don't run as configured (not testing RocksDB savepoints),FLINK-26176,13428731,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,smattheis,smattheis,16/Feb/22 07:59,22/Feb/22 08:29,13/Jul/23 08:08,22/Feb/22 08:29,1.10.3,1.11.6,1.12.7,1.13.5,1.14.3,1.9.3,1.15.0,,,,Tests,,,,,0,,,,"The following Scala integration test cases in flink-tests:
 * flink-tests/src/test/scala/org/apache/flink/api/scala/migration/StatefulJobSavepointMigrationITCase.scala
 * flink-tests/src/test/scala/org/apache/flink/api/scala/migration/StatefulJobWBroadcastStateMigrationITCase.scala

are supposed and configured to test migration of savepoints of the RocksDB, Memory, and HashMap state backend between minor version upgrades (1.x -> 1.y). However, the test methods (in both test cases), which are
 * testCreateSavepoint and testRestoreSavepoint
 * testCreateSavepointWithBroadcastState and testRestoreSavepointWithBroadcast

do set the state backend to use only MemoryStateBackend and overwrite any previous (correct according to test configuration) setting of the state backend.

The consequence is:
 * tests run only with MemoryStateBackend
 * previous test resources, i.e., savepoints for RocksDB/HashMap states of versions (1.3-1.14), are corrputed

Proposed solution;
 * fix the setting of state backend
 * delete corrupted savepoints
 * recreate savepoints for older versions OR omit test runs for older versions",,dwysakowicz,smattheis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 22 08:29:16 UTC 2022,,,,,,,,,,"0|z0zmc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 08:29;dwysakowicz;Fixed in 9dd6043f4e474658cd6830a011d74f0147fbcc07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisDataStreamsSink.restoreWriter failed due to Architecture Violation,FLINK-26174,13428709,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,gaoyunhaii,gaoyunhaii,16/Feb/22 06:38,16/Feb/22 10:48,13/Jul/23 08:08,16/Feb/22 09:18,1.15.0,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,test-stability,,"
{code:java}
Feb 15 19:28:44 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 38.14 s <<< FAILURE! - in org.apache.flink.architecture.rules.ApiAnnotationRules
Feb 15 19:28:44 [ERROR] ApiAnnotationRules.PUBLIC_EVOLVING_API_METHODS_USE_ONLY_PUBLIC_EVOLVING_API_TYPES  Time elapsed: 0.391 s  <<< FAILURE!
Feb 15 19:28:44 java.lang.AssertionError: 
Feb 15 19:28:44 Architecture Violation [Priority: MEDIUM] - Rule 'Return and argument types of methods annotated with @PublicEvolving must be annotated with @Public(Evolving).' was violated (2 times):
Feb 15 19:28:44 org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSink.restoreWriter(org.apache.flink.api.connector.sink2.Sink$InitContext, java.util.Collection): Argument leaf type org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.kinesis.model.PutRecordsRequestEntry does not satisfy: reside outside of package 'org.apache.flink..' or annotated with @Public or annotated with @PublicEvolving or annotated with @Deprecated
Feb 15 19:28:44 org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSink.restoreWriter(org.apache.flink.api.connector.sink2.Sink$InitContext, java.util.Collection): Returned leaf type org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.kinesis.model.PutRecordsRequestEntry does not satisfy: reside outside of package 'org.apache.flink..' or annotated with @Public or annotated with @PublicEvolving or annotated with @Deprecated
Feb 15 19:28:44 	at com.tngtech.archunit.lang.ArchRule$Assertions.assertNoViolation(ArchRule.java:94)
Feb 15 19:28:44 	at com.tngtech.archunit.lang.ArchRule$Assertions.check(ArchRule.java:82)
Feb 15 19:28:44 	at com.tngtech.archunit.library.freeze.FreezingArchRule.check(FreezingArchRule.java:96)
Feb 15 19:28:44 	at com.tngtech.archunit.junit.ArchUnitTestDescriptor$ArchUnitRuleDescriptor.execute(ArchUnitTestDescriptor.java:159)
Feb 15 19:28:44 	at com.tngtech.archunit.junit.ArchUnitTestDescriptor$ArchUnitRuleDescriptor.execute(ArchUnitTestDescriptor.java:142)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Feb 15 19:28:44 	at java.util.ArrayList.forEach(ArrayList.java:1259)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Feb 15 19:28:44 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Feb 15 19:28:44 	at java.util.ArrayList.forEach(ArrayList.java:1259)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31563&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=27092
",,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25574,,,,,,,FLINK-26186,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 16 08:22:49 UTC 2022,,,,,,,,,,"0|z0zm74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/22 06:39;gaoyunhaii;cc [~fpaul];;;","16/Feb/22 08:22;gaoyunhaii;Merged on master via 70ee694c62d6762de4a909988c6399bfede28c62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink GlobalCommitter's behavior is not compatible,FLINK-26173,13428692,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,lzljs3620320,lzljs3620320,16/Feb/22 03:51,09/Feb/23 09:46,13/Jul/23 08:08,09/Feb/23 09:45,1.15.0,,,,,,1.15.0,,,,API / DataStream,,,,,0,pull-request-available,stale-assigned,,"GlobalCommitter's behavior is not compatible.
 * filterRecoveredCommittables is never invoked, Previously it would be called on recovery
 * GlobalCommT is solidified after producing by GlobalCommitter.combine. Instead of creating every time",,gaoyunhaii,hackergin,lzljs3620320,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 09 09:46:38 UTC 2023,,,,,,,,,,"0|z0zm3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/22 03:53;lzljs3620320;CC [~gaoyunhaii] [~fpaul] [~maguowei] ;;;","08/Apr/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","09/Feb/23 09:46;gaoyunhaii;I'll first close this issue since it seems the PR has been merged. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is a typo in the annotation for the WatermarkAlignmentParams class,FLINK-26169,13428669,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Echo Lee,Echo Lee,Echo Lee,16/Feb/22 01:03,18/Feb/22 15:24,13/Jul/23 08:08,18/Feb/22 15:24,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"{code:java}
/** Configuration parameters for watermark alignemnt. */
public static class WatermarkAlignmentParams {{code}
*alignemnt* should be corrected to alignment",,Echo Lee,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 15:24:18 UTC 2022,,,,,,,,,,"0|z0zly8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 08:52;Echo Lee;CC [~pnowojski] ;;;","18/Feb/22 15:19;pnowojski;Thanks for spotting the typo!;;;","18/Feb/22 15:24;pnowojski;Fixed as part of already merged issue FLINK-25983;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explicitly set the partitioner for the sql operators whose shuffle and sort are removed,FLINK-26167,13428607,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,15/Feb/22 16:34,18/Feb/22 06:08,13/Jul/23 08:08,18/Feb/22 06:08,,,,,,,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"After FLINK-25995 is finished, we have add an exchange (which will be converted to ForwardForConsecutiveHashPartitioner) for the nodes which do not need explicitly hash shuffle (which input has already hashed)

e.g.
{code:sql}
WITH r AS (SELECT * FROM T1, T2 WHERE a1 = a2 AND c1 LIKE 'He%') SELECT sum(b1) FROM r group by a1
{code}

the plan after FLINK-25995 is finished:
{code:java}
Calc(select=[EXPR$0])
+- HashAggregate(isMerge=[false], groupBy=[a1], select=[a1, SUM(b1) AS EXPR$0])
   +- Exchange(distribution=[keep_input_as_is[hash[a1]])
      +- Calc(select=[a1, b1])
         +- HashJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2], build=[left])
            :- Exchange(distribution=[hash[a1]])
            :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
            :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
            +- Exchange(distribution=[hash[a2]])
               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
{code}

but data between {{Calc}} and {{HashJoin}} may be out of order once their parallelism is different, so an {{Exchange(distribution=[keep_input_as_is[hash[a1]])}} should be added between them.",,godfreyhe,libenchao,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 06:08:44 UTC 2022,,,,,,,,,,"0|z0zlkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 06:08;godfreyhe;Fixed in 1.15.0: 910149c49a6b5f662dfc5503326339f5e62cf4f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-runtime-web fails to compile if newline is cr lf,FLINK-26166,13428605,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,15/Feb/22 16:24,16/Mar/22 09:35,13/Jul/23 08:08,16/Mar/22 09:35,1.16.0,,,,,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,"Normally I'm developing on linux based system but sometimes reviewing on Windows based machines. Compile blows up in the following way:
{code:java}
[INFO] d:\projects\flink\flink-runtime-web\web-dashboard\src\@types\d3-flame-graph\index.d.ts
[INFO]    1:3   error  Delete `â??`  prettier/prettier
...
{code}
",,gaborgsomogyi,martijnvisser,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 16 09:35:39 UTC 2022,,,,,,,,,,"0|z0zlk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/22 16:33;chesnay;This usually works fine for me. How have you configured git on the windows side?

I have this in my git config:
{code:java}
core.eol=lf
core.autocrlf=input
{code}
Without which I had various other problems.
;;;","15/Feb/22 16:39;gaborgsomogyi;It would be good to work out of the box. I've found that .eslintrc.js assumes lf.
One can set there auto which would solve the problem and no additional magic would be needed.
WDYT, worth the one-liner in the code or not?

Actual:
{code:java}
rules: {
        'prettier/prettier': [
          'error',
          {
            parser: 'angular'
          }
        ]
      }
{code}

Modified:
{code:java}
rules: {
        'prettier/prettier': [
          'error',
          {
            parser: 'angular',
            endOfLine: 'auto'
          }
        ]
      }
{code}
;;;","15/Feb/22 16:39;gaborgsomogyi;I've double checked and your suggestion works.;;;","15/Feb/22 16:49;gaborgsomogyi;Copying the auto meaning here for reference:
{code:java}
""auto"" - Maintain existing line endings (mixed values within one file are normalised by looking at what’s used after the first line)
{code}
;;;","16/Feb/22 08:39;chesnay;I can see where you're coming from and if this were the only problem with using CRLF I'd agree, but as I said I had other issues in the past and thus would urge anyone to set up git accordingly in any case.

Now that I think about it though...this _is_ a hassle for users who want to build Flink on their own...;;;","17/Feb/22 08:00;mbalassi;[~chesnay] unless you see any harm in this I prefer having [~gaborgsomogyi]'s proposed change in to aid the out-of-box experience as you suggested.;;;","01/Mar/22 14:27;gaborgsomogyi;[~MartijnVisser] not sure what you've done but pretty sure made a mistake so reopening it.;;;","01/Mar/22 14:43;martijnvisser;[~gaborgsomogyi] Thanks, looks like I've accidentally closed the wrong ticket. Sorry for that. ;;;","16/Mar/22 09:35;mbalassi;Closed via 958894b68fa82c6146de98f97687f6c9dde234c2 (master).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointFormatITCase fails on azure,FLINK-26165,13428604,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,15/Feb/22 16:24,17/Feb/22 15:04,13/Jul/23 08:08,17/Feb/22 15:04,1.15.0,,,,,,1.15.0,,,,Tests,,,,,0,pull-request-available,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31474&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13116]

{code}
[ERROR] org.apache.flink.test.checkpointing.SavepointFormatITCase.testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath(SavepointFormatType, StateBackendConfig)[2]  Time elapsed: 14.209 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.io.IOException: Unknown implementation of StreamStateHa ndle: class org.apache.flink.runtime.state.PlaceholderStreamStateHandle
   at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
   at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
   at org.apache.flink.test.checkpointing.SavepointFormatITCase.submitJobAndTakeSavepoint(SavepointFormatITCase.java:328)
   at org.apache.flink.test.checkpointing.SavepointFormatITCase.testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath(SavepointFormatITCase.java:248)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
   at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(Invo cationInterceptorChain.java:131)
   at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
   at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.ja va:140)
   at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtensio n.java:92)
   at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMet hod$0(ExecutableInvoker.java:115)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105 )
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(Inv ocationInterceptorChain.java:106)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChai n.java:64)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationIntercep torChain.java:45)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain .java:37)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
   at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMeth odTestDescriptor.java:214)
   at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.ja
{code}",,gaoyunhaii,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26093,,,,,,,FLINK-26154,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 17 15:04:47 UTC 2022,,,,,,,,,,"0|z0zlk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/22 07:36;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31702&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5848;;;","17/Feb/22 08:08;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31702&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5706;;;","17/Feb/22 12:10;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31730&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12474;;;","17/Feb/22 15:04;roman;Merged into master as 10abe7cbf3b262527eaae3e1cc0ee3669ead1763..77811884f693d0899b0bf25fb35a6542744b693e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Connector: stopCursor description should be changed. Connector only stop when auto discovery is disabled.,FLINK-26160,13428569,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,affe,affe,affe,15/Feb/22 12:26,24/Feb/22 16:04,13/Jul/23 08:08,24/Feb/22 16:03,1.14.3,1.15.0,,,,,1.14.4,1.15.0,,,Connectors / Pulsar,Documentation,,,,0,pull-request-available,Pulsar,,"In Pulsar source connector, the stopCursor description can mislead user to believe that the source connector will exit if a BoundedStopCursor is set.  However this might not be true if auto partition discovery is enabled (the discovery loop will always run and expects new partitionSplits). We need to modify the description.",,affe,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 16:03:48 UTC 2022,,,,,,,,,,"0|z0zlc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 09:23;fpaul;Merged in master: 6080c7885d02c11b7aef8f24a26f1c794b6953d7;;;","24/Feb/22 16:03;fpaul;Merged in release-1.14: 3c830d02bbf763c9fe2fa3391cd17e078106ce1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Connector: should add description MAX_FETCH_RECORD in doc to explain slow consumption,FLINK-26159,13428568,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,affe,affe,affe,15/Feb/22 12:23,24/Feb/22 16:06,13/Jul/23 08:08,24/Feb/22 16:06,1.14.3,1.15.0,,,,,1.14.4,1.15.0,,,Connectors / Pulsar,,,,,0,pull-request-available,Pulsar,,"Pulsar source connector can consume slow when the data volume is low (like 1 record/s), this is because the MAX_FETCH_RECORD is set to 100 by default, meaning that the pulsar source emits records either after a timeout (1s) or buffered messages has reached 100. Users will observe each message has a 1s delay. This can be add to the documentation and inform users how to change MAX_FETCH_RECORD",,affe,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 16:05:49 UTC 2022,,,,,,,,,,"0|z0zlc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 09:07;fpaul;Merged in master: 52df489063ae90af73c663dc6abbe42e3794f1ac;;;","24/Feb/22 16:05;fpaul;Merged in release-1.14: a5dc36dd323ec064c8a3d2e2f64bca7f7e5fbd1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inprogressfileRecoverable not be clean up after restoring the bucket,FLINK-26151,13428538,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lovewin99,lovewin99,lovewin99,15/Feb/22 10:09,21/Mar/22 03:22,13/Jul/23 08:08,21/Mar/22 03:20,1.15.0,,,,,,1.15.0,1.16.0,,,FileSystems,,,,,0,pull-request-available,,,"In order to clear the previous inProgressFileRecoverable when the checkpoint is successful, inProgressFileRecoverable will be added to inProgressFileRecoverablesPerCheckpoint when the checkpoint is started, but when the bucket is recovered from bucketState, inProgressFileRecoverablesPerCheckpoint does not record inProgressFileRecoverable, resulting in inProgressFileRecoverable not be clean up.",,fpaul,gaoyunhaii,lovewin99,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26755,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 03:20:31 UTC 2022,,,,,,,,,,"0|z0zl5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/22 10:25;fpaul;[~gaoyunhaii] I suppose this is related to the FileSink?;;;","16/Feb/22 07:58;gaoyunhaii;Thanks [~lovewin99] for reporting the issue! I'll have a look very soon~;;;","21/Mar/22 03:20;gaoyunhaii;Merged on master via 1fa91ba6ab515aa0e7ab88cc165fe571b60ee37f
Merged on 1.15 via 34034a6c54f3b23825bc318806c683b3495f72d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"k8s docs jobmanager-pod-template  artifacts-fetcher:latest image is not exist, we can use busybox to replace it ",FLINK-26145,13428512,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,15/Feb/22 08:54,21/Feb/22 09:36,13/Jul/23 08:08,21/Feb/22 09:36,1.15.0,,,,,,1.15.0,,,,Documentation,,,,,0,pull-request-available,,,!image-2022-02-15-16-54-42-861.png!,,jackylau,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/22 08:55;jackylau;image-2022-02-15-16-55-43-593.png;https://issues.apache.org/jira/secure/attachment/13040026/image-2022-02-15-16-55-43-593.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 21 09:36:53 UTC 2022,,,,,,,,,,"0|z0zkzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/22 03:01;wangyang0918;Could you please attach a PR for this ticket?;;;","16/Feb/22 10:47;jackylau;hi [~wangyang0918] ， i commit pr just now. if the ci passed, i request you.;;;","21/Feb/22 09:35;wangyang0918;Fixed via:
master(release-1.15): 38b918591a62c8bb694bfb19e2d773bddd7e826c;;;","21/Feb/22 09:36;wangyang0918;[~jackylau] Thanks for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink V2 will cause error numRecordsOut metric,FLINK-26126,13428291,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,ruanhang1993,ruanhang1993,14/Feb/22 09:55,01/Oct/22 14:34,13/Jul/23 08:08,07/Mar/22 08:30,1.15.0,,,,,,1.15.0,,,,Connectors / Common,Connectors / Kafka,,,,0,pull-request-available,,,"We found that the new sink v2 interface will have a wrong numRecordsOut metric for the sink writers. We send a fixed number of records to the source, but the numRecordsOut of the sink continues to increase by the time.

The problem lies in the method `emitCommittables` in the class `SinkWriterOperator`.  The field `output` in its parent class `AbstractStreamOperator` uses the same counter object as the `KafkaWriter`. It will cause the numRecordsOut increasing when doing the checkpoint. 

I found this problem when we implement the metric test in the testframe, now I disable this metric test in the [PR|https://github.com/apache/flink/pull/18496] We could reopen this test case after the fix.",,fpaul,gaoyunhaii,hackergin,jingge,leonard,ruanhang1993,,,,,,,,,,,,,,,,,,,FLINK-26420,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26282,FLINK-26492,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 07 08:30:18 UTC 2022,,,,,,,,,,"0|z0zjmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/22 04:25;leonard;[~ruanhang1993] The blocker priority means we have to  fix it in the coming release.

CC: [~arvid] Could you help take a look this one?;;;","15/Feb/22 06:52;ruanhang1993;The Kafka sink is already using the sink v2 interface in the master branch. 

I think it should be fixed before the release. Am I right? Or do I need to change the priority?

CC: [~fpaul] ;;;","15/Feb/22 08:15;fpaul;I plan to work on this today or respectively tomorrow. It is high on my list.;;;","16/Feb/22 12:46;jingge;reference: the specific commit of the disabled test https://github.com/apache/flink/pull/18496/commits/cc23b8d007ad7df80d90db437789470502b78f53;;;","16/Feb/22 18:53;jingge;It seems fine that KafkaWriter uses the Counter metric from SinkWriterOperator/AbstractStreamOperator, because the count will be increased only when the write(...) of KafkaWriter has been called. Sink V1 has the same implementation of the count metric.;;;","07/Mar/22 08:30;fpaul;Merged in master: 9725933fc0a09274801d2acb52a6a5256afa10f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification failed on azure,FLINK-26121,13428282,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,gaoyunhaii,gaoyunhaii,14/Feb/22 09:36,15/Mar/22 07:38,13/Jul/23 08:08,15/Mar/22 07:38,1.14.4,1.15.0,,,,,1.14.5,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-02-11T21:43:35.4936452Z Feb 11 21:43:35 java.lang.AssertionError: The TestingFatalErrorHandler caught an exception.
2022-02-11T21:43:35.4940444Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.after(TestingFatalErrorHandlerResource.java:81)
2022-02-11T21:43:35.4941937Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.access$300(TestingFatalErrorHandlerResource.java:36)
2022-02-11T21:43:35.4943249Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:60)
2022-02-11T21:43:35.4944745Z Feb 11 21:43:35 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-02-11T21:43:35.4945682Z Feb 11 21:43:35 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-02-11T21:43:35.4946655Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-11T21:43:35.4947847Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-02-11T21:43:35.4948876Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-02-11T21:43:35.4949842Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-02-11T21:43:35.4951142Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-02-11T21:43:35.4952153Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-02-11T21:43:35.4953115Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-02-11T21:43:35.4954068Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-02-11T21:43:35.4955003Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-02-11T21:43:35.4955981Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-02-11T21:43:35.4956930Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-11T21:43:35.4958008Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-02-11T21:43:35.4958899Z Feb 11 21:43:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-02-11T21:43:35.4959774Z Feb 11 21:43:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-02-11T21:43:35.4960911Z Feb 11 21:43:35 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-02-11T21:43:35.4962095Z Feb 11 21:43:35 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-02-11T21:43:35.4963136Z Feb 11 21:43:35 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-02-11T21:43:35.4964275Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-11T21:43:35.4965527Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-11T21:43:35.4966787Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-11T21:43:35.4968228Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-11T21:43:35.4969485Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-11T21:43:35.4970753Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-11T21:43:35.4971842Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-11T21:43:35.4973291Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-11T21:43:35.4974538Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-11T21:43:35.4975737Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-11T21:43:35.4976936Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-11T21:43:35.4978291Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-11T21:43:35.4979588Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-11T21:43:35.4980728Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-11T21:43:35.4981753Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-11T21:43:35.4982863Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-02-11T21:43:35.4983958Z Feb 11 21:43:35 Caused by: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Could not handle node changed event.
2022-02-11T21:43:35.4985193Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper(ZooKeeperLeaderRetrievalDriver.java:143)
2022-02-11T21:43:35.4986451Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.onReconnectedConnectionState(ZooKeeperLeaderRetrievalDriver.java:181)
2022-02-11T21:43:35.4987902Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.handleStateChange(ZooKeeperLeaderRetrievalDriver.java:164)
2022-02-11T21:43:35.4989235Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.lambda$new$0(ZooKeeperLeaderRetrievalDriver.java:61)
2022-02-11T21:43:35.4990632Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.lambda$processEvents$0(ConnectionStateManager.java:279)
2022-02-11T21:43:35.4991926Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2022-02-11T21:43:35.4993137Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2022-02-11T21:43:35.4994438Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2022-02-11T21:43:35.4995807Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:279)
2022-02-11T21:43:35.4997130Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
2022-02-11T21:43:35.4999203Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:132)
2022-02-11T21:43:35.4999927Z Feb 11 21:43:35 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-02-11T21:43:35.5000679Z Feb 11 21:43:35 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-02-11T21:43:35.5001306Z Feb 11 21:43:35 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-02-11T21:43:35.5001866Z Feb 11 21:43:35 	at java.lang.Thread.run(Thread.java:748)
2022-02-11T21:43:35.5002381Z Feb 11 21:43:35 Caused by: java.lang.IllegalStateException: java.lang.InterruptedException
2022-02-11T21:43:35.5003374Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest$QueueLeaderElectionListener.notifyLeaderAddress(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:375)
2022-02-11T21:43:35.5004370Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper(ZooKeeperLeaderRetrievalDriver.java:136)
2022-02-11T21:43:35.5005005Z Feb 11 21:43:35 	... 14 more
2022-02-11T21:43:35.5005464Z Feb 11 21:43:35 Caused by: java.lang.InterruptedException
2022-02-11T21:43:35.5006079Z Feb 11 21:43:35 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
2022-02-11T21:43:35.5006973Z Feb 11 21:43:35 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
2022-02-11T21:43:35.5007877Z Feb 11 21:43:35 	at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:353)
2022-02-11T21:43:35.5008787Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest$QueueLeaderElectionListener.notifyLeaderAddress(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:373)
2022-02-11T21:43:35.5009547Z Feb 11 21:43:35 	... 15 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31294&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7581
",,gaoyunhaii,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26596,,,,,,,,,FLINK-26120,FLINK-26223,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 15 07:38:04 UTC 2022,,,,,,,,,,"0|z0zjkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/22 12:18;mapohl;I wasn't able to reproduce it locally. But it looks like a connection instability to the ZooKeeper server. The test behaves as expected until the we wait for the new leader to be picked up. The connection seems to be lost which triggers an additional connection state change (suspended -> reconnect) which adds another leader address change to the blocking queue of the test's {{QueueLeaderElectionListener}}. It looks like a race condition between the thread that is blocked in the queue's put operation and the close operation that interrupts all threads while shutting down the ExecutorServices.
{code}
[...]
21:43:16,346 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: SUSPENDED
21:43:16,346 [Curator-ConnectionStateManager-0] WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Connection to ZooKeeper suspended, waiting for reconnection.
21:43:18,194 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: RECONNECTED
21:43:18,222 [    main-EventThread] DEBUG org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Leader node has changed.
21:43:21,353 [Curator-ConnectionStateManager-0] INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.
21:43:21,353 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Leader node has changed.
21:43:21,453 [                main] INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Closing ZookeeperLeaderRetrievalDriver{connectionInformationPath='/testNewLeaderAfterReconnectTriggersListenerNotification/connection_info'}.
21:43:21,462 [ Curator-Framework-0] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - backgroundOperationsLoop exiting
21:43:21,463 [Curator-ConnectionStateManager-0] ERROR org.apache.flink.runtime.util.TestingFatalErrorHandler       [] - OnFatalError:
org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Could not handle node changed event.
{code};;;","15/Feb/22 12:35;chesnay;FYI, in FLINK-25120 we also have an issue because the ZK connection momentarily breaks down.;;;","16/Feb/22 09:19;mapohl;{quote}FYI, in FLINK-25120 we also have an issue because the ZK connection momentarily breaks down.{quote}
Update: It's actually FLINK-26120. I linked the issue;;;","16/Feb/22 10:33;mapohl;There seems to be a gap of 10s in processing time while waiting for the leadership information. It's hard to tell here whether that's a AzureCI worker issue where the instance was just not operating for some time or whether there was no output due to the busy waiting.;;;","25/Feb/22 08:11;mapohl;{{{}master{}}}: [8d86133d0619a4c3a94bc0657aa21d87292963ea|https://github.com/apache/flink/commit/8d86133d0619a4c3a94bc0657aa21d87292963ea];;;","11/Mar/22 14:19;mapohl;I'm reopening the issue because there was another build failure recently:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32815&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7544

This time, the root cause it a {{NullPointerException}} instead of a {{LeaderRetrievalException}} but the issue seems to be again an flaky ZK connection;;;","11/Mar/22 14:27;mapohl;The connection loss can be observed in the zookeeper-client-1.log between {{09:16:07,840}} and {{09:16:09,579}}:
{code}
09:16:07,840 [main-SendThread(127.0.0.1:44061)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Unable to read additional data from server sessionid 0x101773132360000, likely server has closed socket, closing socket connection and attempting reconnect
09:16:09,579 [main-SendThread(127.0.0.1:44061)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Opening socket connection to server localhost/127.0.0.1:44061. Will not attempt to authenticate using SASL (unknown error)
09:16:09,579 [main-SendThread(127.0.0.1:44061)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Socket connection established, initiating session, client: /127.0.0.1:34516, server: localhost/127.0.0.1:44061
{code};;;","11/Mar/22 17:11;mapohl;Closing the issue again. It's quite related to FLINK-26596. But the {{NullPointerException}} is now caused because the leadership loss is not handled properly.;;;","14/Mar/22 13:39;mapohl;Reopening the issue to backport it 1.14 as well.;;;","15/Mar/22 07:38;mapohl;1.14: 2ecc3614201526a1cbb8bd3e5cee44d3f1f5b42c

master/1.15 was already updated earlier (see comment above):
{quote}
master: 8d86133d0619a4c3a94bc0657aa21d87292963ea
{quote};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncSinkWriterStateSerializer needs to be PublicEvolving,FLINK-26119,13428278,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dannycranmer,fpaul,fpaul,14/Feb/22 09:27,16/Feb/22 11:55,13/Jul/23 08:08,16/Feb/22 11:55,,,,,,,1.15.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"Developers who want to create an AsyncSink are supposed to use the AsyncSinkWriterStateSerializer to create the writer serializer. In hindsight of the upcoming connector externalization, this class needs to be made PublicEvolving to guarantee some stability.",,dannycranmer,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25943,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 16 11:55:20 UTC 2022,,,,,,,,,,"0|z0zjjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/22 11:55;dannycranmer;Merged https://github.com/apache/flink/commit/85bc4e36fbad598dc78f30c892ee6dddf3884f14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncSinks do not support downscaling with state,FLINK-26118,13428276,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dannycranmer,fpaul,fpaul,14/Feb/22 09:24,01/Mar/22 08:38,13/Jul/23 08:08,16/Feb/22 20:00,1.15.0,,,,,,,,,,Connectors / Common,,,,,0,pull-request-available,,,"Currently, the AsyncSinkWriter assumes to always be restored exactly from only one previous writer state but in case of a downscaling after a snapshot a writer will receive multiple states.

 

https://github.com/apache/flink/blob/c60eb0c3b4bf7dc045dd7a1da2080c7befebb8dc/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/writer/AsyncSinkWriter.java#L438",,dannycranmer,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25943,,,,FLINK-26417,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 16 20:00:55 UTC 2022,,,,,,,,,,"0|z0zjj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/22 20:00;dannycranmer;Merged https://github.com/apache/flink/commit/979313a520f97ea16491650154d45518d4d4c3e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatorEventsExactlyOnceITCase.test failed on azure,FLINK-26107,13428250,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,gaoyunhaii,gaoyunhaii,14/Feb/22 07:25,16/Feb/22 08:51,13/Jul/23 08:08,16/Feb/22 08:51,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"{code:java}
Feb 14 02:23:11 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.135 s <<< FAILURE! - in org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase
Feb 14 02:23:11 [ERROR] org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test  Time elapsed: 0.72 s  <<< ERROR!
Feb 14 02:23:11 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Feb 14 02:23:11 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
Feb 14 02:23:11 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:933)
Feb 14 02:23:11 	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:192)
Feb 14 02:23:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 14 02:23:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 14 02:23:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 14 02:23:11 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 14 02:23:11 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 14 02:23:11 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 14 02:23:11 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 14 02:23:11 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 14 02:23:11 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Feb 14 02:23:11 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 14 02:23:11 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Feb 14 02:23:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Feb 14 02:23:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 14 02:23:11 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Feb 14 02:23:11 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 14 02:23:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 14 02:23:11 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Feb 14 02:23:11 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Feb 14 02:23:11 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Feb 14 02:23:11 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Feb 14 02:23:11 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Feb 14 02:23:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
Feb 14 02:23:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
Feb 14 02:23:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9512",,dmvk,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26108,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 16 08:51:23 UTC 2022,,,,,,,,,,"0|z0zjdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 07:30;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31323&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9424;;;","14/Feb/22 07:45;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31305&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9585;;;","14/Feb/22 07:47;gaoyunhaii;Hi [~dmvk] could you also have a look at this issue and https://issues.apache.org/jira/browse/FLINK-26108 ~? The errors seems to be related to adaptive scheduler~;;;","14/Feb/22 10:17;dmvk;Bisected to https://github.com/apache/flink/commit/5180e83ec01236ecce47a56951daf2a34eb4a1f6;;;","14/Feb/22 15:16;dmvk;Both this issue and FLINK-26108 should be fixed by the attached PR.;;;","16/Feb/22 08:51;chesnay;master: b64a838235f885af013f9214e82d304cd30cdf1c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedSourceITCase failed due to JVM exits with code 239,FLINK-26106,13428248,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,gaoyunhaii,gaoyunhaii,14/Feb/22 07:20,16/Mar/22 13:14,13/Jul/23 08:08,16/Mar/22 13:07,1.15.0,,,,,,1.15.0,1.16.0,,,API / Core,,,,,0,pull-request-available,stale-critical,test-stability,"{code:java}
Feb 14 03:52:55 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (integration-tests) on project flink-tests: There are test failures.
Feb 14 03:52:55 [ERROR] 
Feb 14 03:52:55 [ERROR] Please refer to /__w/1/s/flink-tests/target/surefire-reports for the individual test results.
Feb 14 03:52:55 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Feb 14 03:52:55 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 14 03:52:55 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-tests/target/surefire/surefirebooter4517679582273332440.jar /__w/1/s/flink-tests/target/surefire 2022-02-14T02-22-54_233-jvmRun2 surefire8233172397230542561tmp surefire_13666643105378974503190tmp
Feb 14 03:52:55 [ERROR] Error occurred in starting fork, check output in log
Feb 14 03:52:55 [ERROR] Process Exit Code: 239
Feb 14 03:52:55 [ERROR] Crashed tests:
Feb 14 03:52:55 [ERROR] BoundedSourceITCase
Feb 14 03:52:55 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 14 03:52:55 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-tests/target/surefire/surefirebooter4517679582273332440.jar /__w/1/s/flink-tests/target/surefire 2022-02-14T02-22-54_233-jvmRun2 surefire8233172397230542561tmp surefire_13666643105378974503190tmp
Feb 14 03:52:55 [ERROR] Error occurred in starting fork, check output in log
Feb 14 03:52:55 [ERROR] Process Exit Code: 239
Feb 14 03:52:55 [ERROR] Crashed tests:
Feb 14 03:52:55 [ERROR] BoundedSourceITCase
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Feb 14 03:52:55 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Feb 14 03:52:55 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Feb 14 03:52:55 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Feb 14 03:52:55 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Feb 14 03:52:55 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Feb 14 03:52:55 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12999",,akalashnikov,gaoyunhaii,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25903,FLINK-26037,,,,,,,,FLINK-26685,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 16 13:14:35 UTC 2022,,,,,,,,,,"0|z0zjcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/22 10:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Mar/22 06:34;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=837&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=6296;;;","10/Mar/22 03:31;gaoyunhaii;TaskManagerRunnerTest@1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9164;;;","10/Mar/22 03:33;gaoyunhaii;JobManagerHAProcessFailureRecoveryITCase@1.14 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32723&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5493;;;","10/Mar/22 16:19;akalashnikov;The cause:
{noformat}
03:50:15,237 [AsyncOperations-thread-3] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'AsyncOperations-thread-3' produced an uncaught exception. Stopping the process...
java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:3236) ~[?:1.8.0_292]
        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118) ~[?:1.8.0_292]
        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) ~[?:1.8.0_292]
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1842) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1534) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) ~[?:1.8.0_292]
        at java.util.ArrayList.writeObject(ArrayList.java:768) ~[?:1.8.0_292]
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) ~[?:1.8.0_292]
        at java.util.ArrayList.writeObject(ArrayList.java:768) ~[?:1.8.0_292]
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_292]
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) ~[?:1.8.0_292]
{noformat}
 ;;;","16/Mar/22 13:07;roman;Merged into master as 3c98598c5a715563602993121cb808f1b239c89c,
into release-1.15 as 537b871962b806fc1e40a5c987e046e482a509c5;;;","16/Mar/22 13:14;roman;I've created FLINK-26685 to investigate more general ways to improve tests stability.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Rolling log filenames cause end-to-end test to fail (example test failure ""Running HA (hashmap, async)"")",FLINK-26105,13428247,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,gaoyunhaii,gaoyunhaii,14/Feb/22 07:16,18/Feb/22 08:51,13/Jul/23 08:08,18/Feb/22 08:51,1.13.6,1.14.3,1.15.0,,,,1.13.7,1.14.4,1.15.0,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"{code:java}
Feb 14 01:31:29 Killed TM @ 255483
Feb 14 01:31:29 Starting new TM.
Feb 14 01:31:42 Killed TM @ 258722
Feb 14 01:31:42 Starting new TM.
Feb 14 01:32:00 Checking for non-empty .out files...
Feb 14 01:32:00 No non-empty .out files.
Feb 14 01:32:00 FAILURE: A JM did not take over.
Feb 14 01:32:00 One or more tests FAILED.
Feb 14 01:32:00 Stopping job timeout watchdog (with pid=250820)
Feb 14 01:32:00 Killing JM watchdog @ 252644
Feb 14 01:32:00 Killing TM watchdog @ 253262
Feb 14 01:32:00 [FAIL] Test script contains errors.
Feb 14 01:32:00 Checking of logs skipped.
Feb 14 01:32:00 
Feb 14 01:32:00 [FAIL] 'Running HA (hashmap, async) end-to-end test' failed after 2 minutes and 51 seconds! Test exited with exit code 1
Feb 14 01:32:00 
01:32:00 ##[group]Environment Information
Feb 14 01:32:01 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
dmesg: read kernel buffer failed: Operation not permitted
Feb 14 01:32:06 Stopping taskexecutor daemon (pid: 259377) on host fv-az313-602.
Feb 14 01:32:07 Stopping standalonesession daemon (pid: 256528) on host fv-az313-602.
Feb 14 01:32:08 Stopping zookeeper...
Feb 14 01:32:08 Stopping zookeeper daemon (pid: 251023) on host fv-az313-602.
Feb 14 01:32:09 Skipping taskexecutor daemon (pid: 251636), because it is not running anymore on fv-az313-602.
Feb 14 01:32:09 Skipping taskexecutor daemon (pid: 255483), because it is not running anymore on fv-az313-602.
Feb 14 01:32:09 Skipping taskexecutor daemon (pid: 258722), because it is not running anymore on fv-az313-602.
The STDIO streams did not close within 10 seconds of the exit event from process '/usr/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '1'.
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=8020",,gaoyunhaii,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17166,FLINK-8357,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 08:51:44 UTC 2022,,,,,,,,,,"0|z0zjco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 07:21;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=8020;;;","16/Feb/22 12:56;mapohl;{quote}https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=8020
{quote}
FYI: the link from the comment is the same as the one in the issue description, i.e. we only have one error;;;","16/Feb/22 14:31;mapohl;The failing assert checks the log files for the ""Recovered JobGraph"" (see [common_ha.sh:68|https://github.com/apache/flink/blob/badce69548a30e77b1964fb570110c241e7703d5/flink-end-to-end-tests/test-scripts/common_ha.sh#L68]) and counts the number of files that contain this substring. 2 log files are expected to contain this substring due to two JM failovers.

The JM logs reveal that there is a TM connection issue which makes the TM to failover. In the meantime, the JM logs are polluted with {{RecipientUnreachableException: Could not send message}} error messages which result in the log rollover strategy to kick in. The older logs (including the JM initialization) move into a {{*.log.1}} file which is not considered by the assert in {{common_ha.sh}} resulting in the failover.;;;","16/Feb/22 14:37;mapohl;The issue is caused by us enabling rolling log file names which  is not necessarily considered in the test code. I'm linking FLINK-17166 and FLINK-8357 as causes to reflect that.;;;","17/Feb/22 08:45;mapohl;I updated the title and added additional affected versions because this issue is also present in older versions of Flink.;;;","18/Feb/22 08:51;mapohl;* {{master}}:
[a8700ddd0674989d2bb918b56994b4f3405eea41|https://github.com/apache/flink/commit/a8700ddd0674989d2bb918b56994b4f3405eea41]
[aee12bc412559a50a419907fff51df5f91fc6b52|https://github.com/apache/flink/commit/aee12bc412559a50a419907fff51df5f91fc6b52]
 * {{release-1.14}}:
[075c65da9813e2302f5dda63e0e738583e3f3ce2|https://github.com/apache/flink/commit/075c65da9813e2302f5dda63e0e738583e3f3ce2]
[7cebc6cfe1b16a49e67841263b61039f094099a7|https://github.com/apache/flink/commit/7cebc6cfe1b16a49e67841263b61039f094099a7]
* {{release-1.13}}:
[f44513693d6a93af60584fa10f9fd6fb5b088869|https://github.com/apache/flink/commit/f44513693d6a93af60584fa10f9fd6fb5b088869]
[b3e520dee26c2bc82a6e80f5b22dc22361f4d65e|https://github.com/apache/flink/commit/b3e520dee26c2bc82a6e80f5b22dc22361f4d65e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid shared state registry to discard multi-registered identical changelog state,FLINK-26101,13428221,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,14/Feb/22 02:42,14/Feb/22 23:26,13/Jul/23 08:08,14/Feb/22 15:07,,,,,,,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"Under change-log state backend, we will register same materialized keyed state handle multi times, and {{SharedStateRegistryImpl}} will discard the duplicated state handle.

{code:java}
if (!Objects.equals(state, entry.stateHandle)) {
    if (entry.confirmed || isPlaceholder(state)) {
        scheduledStateDeletion = state;
    } else {
        // Old entry is not in a confirmed checkpoint yet, and the new one differs.
        // This might result from (omitted KG range here for simplicity):
        // 1. Flink recovers from a failure using a checkpoint 1
        // 2. State Backend is initialized to UID xyz and a set of SST: { 01.sst }
        // 3. JM triggers checkpoint 2
        // 4. TM sends handle: ""xyz-002.sst""; JM registers it under ""xyz-002.sst""
        // 5. TM crashes; everything is repeated from (2)
        // 6. TM recovers from CP 1 again: backend UID ""xyz"", SST { 01.sst }
        // 7. JM triggers checkpoint 3
        // 8. TM sends NEW state ""xyz-002.sst""
        // 9. JM discards it as duplicate
        // 10. checkpoint completes, but a wrong SST file is used
        // So we use a new entry and discard the old one:
        scheduledStateDeletion = entry.stateHandle;
        entry.stateHandle = state;
    }
{code}

Thus, we need to implement the {{#equals}} method for the registered state handles.",,Ming Li,yunta,,,,,,,,,,,,,,,,,,,,,,,FLINK-23559,,,,,,,,,,,,,,,,,FLINK-25478,,,,,,,FLINK-25478,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 14 15:07:54 UTC 2022,,,,,,,,,,"0|z0zj6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 15:07;yunta;Merged in master: 8b25ae64794727e49de66d0b61e0955da7f21961;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetricQueryService actor system binds to different address than main actor system,FLINK-26095,13428042,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Feb/22 14:08,16/Feb/22 16:19,13/Jul/23 08:08,16/Feb/22 16:19,1.13.0,,,,,,1.15.0,,,,Runtime / Coordination,Runtime / Metrics,,,,0,pull-request-available,,,"We pass RpcService#getAddress() to MetricUtils.startRemoteMetricsRpcService as the bind address, but this is the address under which the actor system is reachable from the outside.",,huwh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 16 16:19:47 UTC 2022,,,,,,,,,,"0|z0zi34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/22 16:19;chesnay;master: c7aa5e502f71e20e7433a1715f4eee05b9efe78e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointFormatITCase fails with ChangelogStateBackend,FLINK-26093,13428022,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,11/Feb/22 13:18,15/Feb/22 16:27,13/Jul/23 08:08,14/Feb/22 11:00,1.15.0,,,,,,1.15.0,,,,Tests,,,,,0,pull-request-available,,,"The test vaildates corrects types of state handles created by savepoint. For NATIVE savepoints, it expects IncrementalRemoteKeyedStateHandle and KeyGroupsStateHandle.

However, with changelog those will be wrapped into ChangelogStateBackendHandle and the test fails.

It can be refactored to account for changelog.",,roman,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23559,,,,,,,,FLINK-26144,,,,,,,,,,,FLINK-26154,FLINK-26165,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 15 08:50:39 UTC 2022,,,,,,,,,,"0|z0zhyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 11:00;roman;Merged into master as 746acb487be3e4fb8f94505aa9b79cb60b4ce851.;;;","15/Feb/22 08:50;roman;I've creatd FLINK-26144 to investigate the possible issue with randomization.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonAggregationFunctionsITCase fails with NPE when using RocksDB,FLINK-26092,13427985,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,matriv,roman,roman,11/Feb/22 10:46,29/Mar/22 22:27,13/Jul/23 08:08,29/Mar/22 22:26,1.15.0,1.16.0,,,,,1.15.0,1.16.0,,,Runtime / State Backends,Tests,,,,0,pull-request-available,,,"Whith RocksDB backend chosen manually (instead of Heap; e.g. by altering mini-cluster configuration in BuiltInAggregateFunctionTestBase);
the test [0: JSON_OBJECTAGG_NULL_ON_NULL (Basic Aggregation)] fails with NPE (below).
 
Not sure whether it's a RocksDB issue, a test issue, or not an issue at all.
The current Changelog backend behavior mimics RocksDB, and therefore enabling it with materialization fails the test too (Changelog +  Heap).
 
{code:java}
java.lang.RuntimeException: Could not collect results
    at org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.materializeResult(BuiltInAggregateFunctionTestBase.java:169)
    at org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.assertRows(BuiltInAggregateFunctionTestBase.java:133)
    at org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase$SuccessItem.execute(BuiltInAggregateFunctionTestBase.java:279)
    at org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.testFunction(BuiltInAggregateFunctionTestBase.java:93)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.RuntimeException: Failed to fetch next result
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
    at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:216)
    at java.util.Iterator.forEachRemaining(Iterator.java:115)
    at org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.materializeResult(BuiltInAggregateFunctionTestBase.java:150)
    ... 38 more
Caused by: java.io.IOException: Failed to fetch job execution result
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
    ... 42 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
    ... 44 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
    at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
    at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
    at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
    at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
    ... 44 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:816)
    at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
    at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: java.lang.NullPointerException
    at org.apache.flink.table.runtime.typeutils.StringDataSerializer.serialize(StringDataSerializer.java:74)
    at org.apache.flink.table.runtime.typeutils.StringDataSerializer.serialize(StringDataSerializer.java:34)
    at org.apache.flink.table.runtime.typeutils.ExternalSerializer.serialize(ExternalSerializer.java:158)
    at org.apache.flink.contrib.streaming.state.AbstractRocksDBState.serializeValueInternal(AbstractRocksDBState.java:158)
    at org.apache.flink.contrib.streaming.state.AbstractRocksDBState.serializeValueNullSensitive(AbstractRocksDBState.java:175)
    at org.apache.flink.contrib.streaming.state.RocksDBMapState.put(RocksDBMapState.java:136)
    at org.apache.flink.runtime.state.UserFacingMapState.put(UserFacingMapState.java:52)
    at org.apache.flink.table.runtime.dataview.StateMapView$StateMapViewWithKeysNotNull.put(StateMapView.java:89)
    at org.apache.flink.table.runtime.dataview.StateMapView$KeyedStateMapViewWithKeysNotNull.put(StateMapView.java:316)
    at org.apache.flink.table.runtime.functions.aggregate.JsonObjectAggFunction.accumulate(JsonObjectAggFunction.java:110)
    at GroupAggsHandler$27.accumulate(Unknown Source)
    at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:151)
    at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
    at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)
{code}
 ",,matriv,roman,twalthr,,,,,,,,,,,,,,,,,,,,,,FLINK-23559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 22:26:57 UTC 2022,,,,,,,,,,"0|z0zhqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/22 10:48;roman;[~twalthr], [~airblader] could you please take a look at this test?;;;","27/Mar/22 08:47;matriv;[~roman] I opened a PR to address this: [https://github.com/apache/flink/pull/19249]

but I wanted to ask, with StringDataSerializer, doesn't support nullability? Is that something we would like to address?;;;","29/Mar/22 15:54;roman;Thanks for creating PR [~matriv] ,

I think StringDataSerializer is a part of Table Runtime, therefore its behavior depends on the usage inside Table Runtime (it's also marked ""Internal""). I'm not familiar with the Table Runtime, but it looks like its other *DataSerializers don't allow nulls as well.

The problem seem to be specific to this serializer+caller because no other tests failed after enabling Changelog (which involves serialization).

So I'd leave the serializer as is and fix the call site (as you did in your PR).;;;","29/Mar/22 22:26;roman;Fix merged into master as 4c8995917885e301ca11023fb5e4eb3d0b7a0c7e,
into 1.15 as b12f8efad939c1654142b893d8f630646956387e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove pre FLIP-84 methods,FLINK-26090,13427966,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,11/Feb/22 09:18,07/Apr/22 10:05,13/Jul/23 08:08,15/Feb/22 08:22,,,,,,,1.15.0,,,,Table SQL / API,,,,,0,pull-request-available,,,See https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=134745878 and https://issues.apache.org/jira/browse/FLINK-16364.,,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,FLINK-26089,,,,,,,,,,,,,,,,,,,,,FLINK-16364,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 15 08:22:40 UTC 2022,,,,,,,,,,"0|z0zhm8:",9223372036854775807,"The previously deprecated methods TableEnvironment.execute, Table.insertInto, TableEnvironment.fromTableSource, TableEnvironment.sqlUpdate, and TableEnvironment.explain have been removed. Please use the provided alternatives introduced in FLIP-84.",,,,,,,,,,,,,,,,,,,"15/Feb/22 08:22;twalthr;Fixed in master: 17aed84aac8eeb3634e96987baa14ff2dc94f602;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CancelPartitionRequestTest.testDuplicateCancel failed on azure due to bind failed,FLINK-26082,13427940,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,akalashnikov,gaoyunhaii,gaoyunhaii,11/Feb/22 07:01,27/Jan/23 09:15,13/Jul/23 08:08,27/Jan/23 09:15,1.13.5,1.16.0,,,,,1.17.0,,,,Runtime / Network,,,,,0,auto-deprioritized-major,pull-request-available,test-stability,"
{code:java}
Feb 10 01:56:01 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.273 s <<< FAILURE! - in org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest
Feb 10 01:56:01 [ERROR] testDuplicateCancel(org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest)  Time elapsed: 1.877 s  <<< ERROR!
Feb 10 01:56:01 org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
Feb 10 01:56:01 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31070&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&l=6768
",,akalashnikov,gaoyunhaii,hxb,mapohl,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 27 09:15:35 UTC 2023,,,,,,,,,,"0|z0zhgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Apr/22 22:40;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","07/Sep/22 01:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40747&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","25/Nov/22 03:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43458&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7095;;;","27/Jan/23 09:15;pnowojski;Merged to master as 2b5fe306114..952c7831e74;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Disallow recovery from non-changelog checkpoints,FLINK-26079,13427881,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,10/Feb/22 21:14,21/Feb/22 11:29,13/Jul/23 08:08,16/Feb/22 15:50,,,,,,,1.15.0,,,,Runtime / Configuration,Runtime / State Backends,,,,0,pull-request-available,,,"Extracted from FLINK-25872. 

The issue is with the CLAIM mode:
> Because discarding an initial checkpoint will invalidate its ""private"" state which might be in use by future checkpoints.
> Normally, changelog backend wraps it and registers with tjhe SharedStateRegistry.
> But when recovering from non-changelog checkpoint, it is first added to the Checkpoint store, and wrapping in subsequent checkpoints doesn't help.

NO_CLAIM mode is not supported.
LEGACY could work.

But it's difficult to differentiate between the modes on TM, where backend type is reliably known (see the discussion below).

CANONICAL non-changelog savepoints must still be supported.",,dwysakowicz,pnowojski,roman,ym,,,,,,,,,,,,,,,,,,,,,FLINK-21352,,,,,,,,,,,,,,,,,FLINK-25872,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 16 15:50:26 UTC 2022,,,,,,,,,,"0|z0zh3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/22 21:56;roman;To implement the validation, we need cluster config (read by TM) to load state backend; and application settings to read restore mode (and overriding backend if any).

Reading restore mode should be prone to JM failover; in particular, when failure happens before creating any new checkpoints.

 

I don't see any reliable way to access full cluster config on JM ({*}am I wrong?{*}). So the validation should be done on TM.

 

I see the following ways to pass Pass RestoreMode to each task:
1. in StreamConfig - set from JobGraph.savepointRestoreSettings in StreamingJobGraphGenerator
2. in TaskStateSnapshot - set in StateAssignmentOperation
3. in CheckpointProperties - set in CheckpointCoordinator
4. (other options?)

The 1st option is resilient to failures because job graph is generated once. The main disadvantage is multple places where it has to be checked; DataSourceTask for example uses TaskConfig instead of StreamConfig IIUC.
2nd and 3rd options need something similar to completedCheckpoint.props.unclaimed to survive crashes. 
Additionally, 3rd has non-clear semantics (we're interested in the initial checkpoint, not the current one).

 

WDYT [~pnowojski], [~dwysakowicz], [~ym]?;;;","11/Feb/22 07:32;dwysakowicz;Why do you want to disallow the combination? What is the problem? Does it mean {{LEGACY}} would be the only supported mode?;;;","11/Feb/22 07:44;roman;> Why do you want to disallow the combination? What is the problem?
Because discarding an initial checkpoint will invalidate its ""private"" state which might be in use by future checkpoints.
Normally, changelog backend wraps it and registers with tjhe SharedStateRegistry.
But when recovering from non-changelog checkpoint, it is first added to the Checkpoint store, and wrapping in subsequent checkpoints doesn't help.
(I mistakenly omitted ""from non-changelog checkpoint"" from the ticket title, sorry). 
 
> Does it mean LEGACY would be the only supported mode?
 
Yes.;;;","11/Feb/22 08:06;dwysakowicz;Do I understand it correctly, that the use case that breaks is basically changing the state backend from a non-changelog to a changelog state backend? As food for thought. Is that something that we support/want to support? So far the only way to change the state backend was via a savepoint.;;;","11/Feb/22 08:08;pnowojski;{quote}
The 1st option is resilient to failures because job graph is generated once. The main disadvantage is multiple places where it has to be checked; DataSourceTask for example uses TaskConfig instead of StreamConfig IIUC.
{quote}
Why is that so? {{StreamTask#createStateBackend}} or {{StateBackendLoader#fromApplicationOrConfigOrDefault}} could be that one place. {{DataSourceTask}} is a legacy DataSet API class. We can safely limit ourselves just to {{StreamTask}}.

I don't like that we would have to pass the restore mode to implement such temporary check, but I don't know what's the alternative? Can we actually fix this properly before the release? As this is a bug, we could fix it after the feature freeze.;;;","11/Feb/22 08:47;roman;[~dwysakowicz]
{quote}Do I understand it correctly, that the use case that breaks is basically changing the state backend from a non-changelog to a changelog state backend?
{quote}
Yes. Recovering from a non-changelog checkpoint (not savepoint) is desirable. The motivation is to reduce downtime.
----
[~pnowojski]
{quote}DataSourceTask is a legacy DataSet API class. We can safely limit ourselves just to StreamTask.
StreamTask#createStateBackend or StateBackendLoader#fromApplicationOrConfigOrDefault could be that one place. 
{quote}
You're right regarding the DataSourceTask, I mistook it for FLIP-27 task.
However, state backend is also created by StreamOperatorContextBuilder (called by operators). Shouldn't the check be there as well?
{quote}I don't like that we would have to pass the restore mode to implement such temporary check, but I don't know what's the alternative?
{quote}
No, me neither. I'm not sure we should implement the validation.

I see the following alternatives:
1. Fix the original issue
2. Only document the limitation without enforcing it
3. Disallow recovery from non-changelog checkpoints (only allow savepoints as Dawid mentioned)
 

----
As for fixing the original issue (cc: [~yunta]):
1. Register all state with the SharedStateRegistry. This would require changing registerSharedStates() of at least KeyGroupsStateHandle and IncrementalRemoteKeyedStateHandle
2. Limit the above to only initial checkpoint and only recovery (CompletedCheckpoint.registerSharedStatesAfterRestored)
3. Wrap the materialized state with Changelog handles on JM, during recovery (not an option IMO because JM shouldn't be aware of that)
 ;;;","11/Feb/22 13:32;pnowojski;{quote}
I see the following alternatives:
1. Fix the original issue
2. Only document the limitation without enforcing it
3. Disallow recovery from non-changelog checkpoints (only allow savepoints as Dawid mentioned)
{quote}
I presume 3. would be easier to implement compared to the `LEGACY` check we discussed earlier? And you mean allowing recovery only from changelog checkpoints or canonical savepoints (native savepoints wouldn't be supported)?

If so I would be also fine with that (especially if we could implement such check in the changelog package) for the time being until we fix the original issue.;;;","11/Feb/22 14:33;roman;Yes, I also think (3) is the easiest validation option.

It can be implemented by checking handles types while [building|https://github.com/apache/flink/blob/3aa267bf511287e39b0a9d781b9aaf38843e1e91/flink-state-backends/flink-statebackend-changelog/src/main/java/org/apache/flink/state/changelog/ChangelogStateBackend.java#L271] the backend. Only  ChangelogStateBackendHandle and SavepointKeyedStateHandle should be allowed.

 

With such a check, the following will be allowed:
 # changelog checkpoint + CLAIM/LEGACY
 # canonical savepoint (any restore mode)

The following disallowed:
 # non-changelog checkpoint + CLAIM
 # non-changelog checkpoint + LEGACY
 # native savepoint
 # NO_CLAIM - by an existing check in StreamTask

Unnecessarily disallowing (2) ""non-changelog checkpoint + LEGACY"" is the downside of this approach.
;;;","11/Feb/22 14:39;pnowojski;So let's do that. Given the MVP status of the changelog I think that's good enough. ;;;","11/Feb/22 14:47;roman;Great, I'll create a PR.

And probably we'll manage to resolve the original issue after the feature freeze, it then can be easily reverted.;;;","14/Feb/22 15:26;ym;> But when recovering from the non-changelog checkpoint, it is first added to the Checkpoint store, and wrapping in subsequent checkpoints doesn't help.
 * shouldn't claim mode with changelog state backend wrap it first and then be added to the checkpoint store?
 * Conceptually, when claiming: shouldn't we claim as a changelog checkpoint if we are using changelog state-backend after restoring?
 * But I agree that's something we need to figure out after code freeze.;;;","14/Feb/22 16:43;roman;> shouldn't claim mode with changelog state backend wrap it first and then be added to the checkpoint store?

As per the discussion above, this doesn't seem a good option because JM should NOT be aware of how specific backends build their state handles.
> 3. Wrap the materialized state with Changelog handles on JM, during recovery (not an option IMO because JM shouldn't be aware of that)

 

> Conceptually, when claiming: shouldn't we claim as a changelog checkpoint if we are using changelog state-backend after restoring?
What do you mean by ""claim as a changelog checkpoint""?

 

> But I agree that's something we need to figure out after code freeze.
I personally meant that after the feature freeze we'll probably fix the root cause.
This issue of enforcing the limitation should be merged ASAP IMO, so that it can be tested as part of the release process. Although formally it can be merged after the feature freeze too.;;;","15/Feb/22 07:27;ym;>> shouldn't claim mode with changelog state backend wrap it first and then be added to the checkpoint store?

> As per the discussion above, this doesn't seem a good option because JM should NOT be aware of how specific backends build their state handles.

That is a fair statement, but from an implementation point of view, let me state this in a different way:

>> Conceptually, when claiming: shouldn't we claim as a changelog checkpoint if we are using changelog state-backend after restoring?
> What do you mean by ""claim as a changelog checkpoint""?

What I mean is: when we restore a job from a normal checkpoint (created by non-changelog state backend) and use changelog state backend with CLAIM mode, It's natural that we make/transfer/register the first checkpoint to/as a changelog-style checkpoint. 

 

> I personally meant that after the feature freeze we'll probably fix the root cause.

I do not disagree with that;;;","15/Feb/22 08:30;roman;>  What I mean is: when we restore a job from a normal checkpoint (created by non-changelog state backend) and use changelog state backend with CLAIM mode, It's natural that we make/transfer the first checkpoint to a changelog-style checkpoint. 

I think it is natural for TM (state backend) - and this is what it actually does. But it is not natural for JM (checkpoint store), because it is not aware of the changelog (and should not be in my opinion).

Implementation-wise, are you proposing to add a dependency on ChangelogStateBackend to flink-runtime and run the necessary logic in JM inside or around CompletedCheckpointStore?

Let's maybe move this discussion to FLINK-25872 which is about addressing the original issue, not the enforcing the limitation?;;;","15/Feb/22 08:48;ym;OK, let's move the discussion to the original ticket.

To the disallow config itself, my last question:

 

[~roman]  Would disallowing ""Non-changelog checkpoint + LEGACY"" break any existing ITCase + changelog randomization?;;;","15/Feb/22 08:54;roman;Thanks.

No, ""changelog.enabled"" is not changed in tests during the recovery (except the newly added ChangelogCompatibilityITCase).;;;","16/Feb/22 15:50;roman;Merged into master as 369088f0f94ae7732f8a65b6676f2187c871328a
..6fc4bad2c6443ef33ac86a286f815ecc9afba31c.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisFirehoseSinkITCase failed due to org.testcontainers.containers.ContainerLaunchException: Container startup failed,FLINK-26069,13427666,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,CrynetLogistics,gaoyunhaii,gaoyunhaii,10/Feb/22 07:38,28/Feb/22 14:56,13/Jul/23 08:08,28/Feb/22 14:56,1.15.0,,,,,,,,,,Connectors / Kinesis,,,,,0,test-stability,,,"
{code:java}
2022-02-09T20:52:36.6208358Z Feb 09 20:52:36 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-02-09T20:52:37.8270432Z Feb 09 20:52:37 [INFO] Running org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase
2022-02-09T20:54:08.9842331Z Feb 09 20:54:08 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 91.02 s <<< FAILURE! - in org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase
2022-02-09T20:54:08.9845140Z Feb 09 20:54:08 [ERROR] org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase  Time elapsed: 91.02 s  <<< ERROR!
2022-02-09T20:54:08.9847119Z Feb 09 20:54:08 org.testcontainers.containers.ContainerLaunchException: Container startup failed
2022-02-09T20:54:08.9848834Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:336)
2022-02-09T20:54:08.9850502Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
2022-02-09T20:54:08.9852012Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1066)
2022-02-09T20:54:08.9853695Z Feb 09 20:54:08 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
2022-02-09T20:54:08.9855316Z Feb 09 20:54:08 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-02-09T20:54:08.9856955Z Feb 09 20:54:08 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-09T20:54:08.9858330Z Feb 09 20:54:08 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-02-09T20:54:08.9859838Z Feb 09 20:54:08 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-02-09T20:54:08.9861123Z Feb 09 20:54:08 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-02-09T20:54:08.9862747Z Feb 09 20:54:08 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-02-09T20:54:08.9864691Z Feb 09 20:54:08 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-02-09T20:54:08.9866384Z Feb 09 20:54:08 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-02-09T20:54:08.9868138Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-09T20:54:08.9869980Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-09T20:54:08.9871255Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-09T20:54:08.9872602Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-09T20:54:08.9874126Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-09T20:54:08.9875899Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-09T20:54:08.9877109Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-09T20:54:08.9878367Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-09T20:54:08.9879761Z Feb 09 20:54:08 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-09T20:54:08.9881148Z Feb 09 20:54:08 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-09T20:54:08.9882768Z Feb 09 20:54:08 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-09T20:54:08.9884214Z Feb 09 20:54:08 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-09T20:54:08.9885475Z Feb 09 20:54:08 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-09T20:54:08.9886856Z Feb 09 20:54:08 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-09T20:54:08.9888037Z Feb 09 20:54:08 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-09T20:54:08.9889181Z Feb 09 20:54:08 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-02-09T20:54:08.9890330Z Feb 09 20:54:08 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
2022-02-09T20:54:08.9891485Z Feb 09 20:54:08 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
2022-02-09T20:54:08.9892704Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
2022-02-09T20:54:08.9893701Z Feb 09 20:54:08 	... 27 more
2022-02-09T20:54:08.9894927Z Feb 09 20:54:08 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
2022-02-09T20:54:08.9896924Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:525)
2022-02-09T20:54:08.9898077Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
2022-02-09T20:54:08.9900406Z Feb 09 20:54:08 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
2022-02-09T20:54:08.9902108Z Feb 09 20:54:08 	... 28 more
2022-02-09T20:54:08.9903113Z Feb 09 20:54:08 Caused by: org.rnorth.ducttape.TimeoutException: Timeout waiting for result with exception
2022-02-09T20:54:08.9904522Z Feb 09 20:54:08 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:54)
2022-02-09T20:54:08.9906132Z Feb 09 20:54:08 	at org.apache.flink.connector.aws.testutils.LocalstackContainer$ListBucketObjectsWaitStrategy.waitUntilReady(LocalstackContainer.java:70)
2022-02-09T20:54:08.9907813Z Feb 09 20:54:08 	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)
2022-02-09T20:54:08.9909611Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:929)
2022-02-09T20:54:08.9911042Z Feb 09 20:54:08 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:468)
2022-02-09T20:54:08.9912050Z Feb 09 20:54:08 	... 30 more
2022-02-09T20:54:08.9913241Z Feb 09 20:54:08 Caused by: java.util.concurrent.ExecutionException: software.amazon.awssdk.core.exception.SdkClientException: An exception was thrown and did not match any waiter acceptors
2022-02-09T20:54:08.9914974Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-02-09T20:54:08.9916308Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-02-09T20:54:08.9918149Z Feb 09 20:54:08 	at org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.createBucket(AWSServicesTestUtils.java:112)
2022-02-09T20:54:08.9919961Z Feb 09 20:54:08 	at org.apache.flink.connector.aws.testutils.LocalstackContainer$ListBucketObjectsWaitStrategy.list(LocalstackContainer.java:82)
2022-02-09T20:54:08.9921284Z Feb 09 20:54:08 	at org.rnorth.ducttape.ratelimits.RateLimiter.getWhenReady(RateLimiter.java:51)
2022-02-09T20:54:08.9922741Z Feb 09 20:54:08 	at org.apache.flink.connector.aws.testutils.LocalstackContainer$ListBucketObjectsWaitStrategy.lambda$waitUntilReady$0(LocalstackContainer.java:73)
2022-02-09T20:54:08.9924349Z Feb 09 20:54:08 	at org.rnorth.ducttape.unreliables.Unreliables.lambda$retryUntilSuccess$0(Unreliables.java:43)
2022-02-09T20:54:08.9925981Z Feb 09 20:54:08 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-02-09T20:54:08.9927176Z Feb 09 20:54:08 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-02-09T20:54:08.9929193Z Feb 09 20:54:08 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-02-09T20:54:08.9930239Z Feb 09 20:54:08 	at java.lang.Thread.run(Thread.java:748)
2022-02-09T20:54:08.9931306Z Feb 09 20:54:08 Caused by: software.amazon.awssdk.core.exception.SdkClientException: An exception was thrown and did not match any waiter acceptors
2022-02-09T20:54:08.9932460Z Feb 09 20:54:08 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:98)
2022-02-09T20:54:08.9933563Z Feb 09 20:54:08 	at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:43)
2022-02-09T20:54:08.9935042Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.waiters.WaiterExecutorHelper.lambda$noneMatchException$3(WaiterExecutorHelper.java:94)
2022-02-09T20:54:08.9936698Z Feb 09 20:54:08 	at java.util.Optional.map(Optional.java:215)
2022-02-09T20:54:08.9937704Z Feb 09 20:54:08 	at software.amazon.awssdk.utils.Either.lambda$map$0(Either.java:51)
2022-02-09T20:54:08.9939329Z Feb 09 20:54:08 	at java.util.Optional.orElseGet(Optional.java:267)
2022-02-09T20:54:08.9940462Z Feb 09 20:54:08 	at software.amazon.awssdk.utils.Either.map(Either.java:51)
2022-02-09T20:54:08.9941648Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.waiters.WaiterExecutorHelper.noneMatchException(WaiterExecutorHelper.java:92)
2022-02-09T20:54:08.9943161Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.waiters.AsyncWaiterExecutor.lambda$runAsyncPollingFunction$0(AsyncWaiterExecutor.java:107)
2022-02-09T20:54:08.9944739Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9946146Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9947402Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9948587Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9949929Z Feb 09 20:54:08 	at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:76)
2022-02-09T20:54:08.9951301Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9952522Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9954082Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9955332Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9956874Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallMetricCollectionStage.lambda$execute$0(AsyncApiCallMetricCollectionStage.java:54)
2022-02-09T20:54:08.9958241Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9959746Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9961024Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9962196Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9963560Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallTimeoutTrackingStage.lambda$execute$2(AsyncApiCallTimeoutTrackingStage.java:67)
2022-02-09T20:54:08.9965446Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9966855Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9968134Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9969400Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9970771Z Feb 09 20:54:08 	at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:76)
2022-02-09T20:54:08.9972127Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9973362Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9974823Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9976206Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9977607Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeAttemptExecute(AsyncRetryableStage.java:103)
2022-02-09T20:54:08.9979196Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:181)
2022-02-09T20:54:08.9980707Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.lambda$attemptExecute$1(AsyncRetryableStage.java:159)
2022-02-09T20:54:08.9982388Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9983980Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9985284Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9986699Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9988093Z Feb 09 20:54:08 	at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:76)
2022-02-09T20:54:08.9989391Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9990676Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9991931Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:08.9993243Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:08.9994817Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$null$0(MakeAsyncHttpRequestStage.java:104)
2022-02-09T20:54:08.9996420Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-02-09T20:54:08.9997660Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-02-09T20:54:08.9999218Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-02-09T20:54:09.0000400Z Feb 09 20:54:08 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-02-09T20:54:09.0002048Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage$WrappedErrorForwardingResponseHandler.onError(MakeAsyncHttpRequestStage.java:158)
2022-02-09T20:54:09.0003667Z Feb 09 20:54:08 	at software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.handleFailure(NettyRequestExecutor.java:300)
2022-02-09T20:54:09.0005435Z Feb 09 20:54:08 	at software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.makeRequestListener(NettyRequestExecutor.java:172)
2022-02-09T20:54:09.0006943Z Feb 09 20:54:08 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
2022-02-09T20:54:09.0008172Z Feb 09 20:54:08 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
2022-02-09T20:54:09.0009340Z Feb 09 20:54:08 	at io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)
2022-02-09T20:54:09.0010537Z Feb 09 20:54:08 	at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:502)
2022-02-09T20:54:09.0011739Z Feb 09 20:54:08 	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
2022-02-09T20:54:09.0013063Z Feb 09 20:54:08 	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
2022-02-09T20:54:09.0014384Z Feb 09 20:54:08 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
2022-02-09T20:54:09.0015694Z Feb 09 20:54:08 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
2022-02-09T20:54:09.0016895Z Feb 09 20:54:08 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2022-02-09T20:54:09.0017887Z Feb 09 20:54:08 	... 1 more
2022-02-09T20:54:09.0019066Z Feb 09 20:54:08 Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: The channel was closed before the protocol could be determined.
2022-02-09T20:54:09.0020598Z Feb 09 20:54:08 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:98)
2022-02-09T20:54:09.0021957Z Feb 09 20:54:08 	at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:43)
2022-02-09T20:54:09.0023384Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:204)
2022-02-09T20:54:09.0025162Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:200)
2022-02-09T20:54:09.0026874Z Feb 09 20:54:08 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:179)
2022-02-09T20:54:09.0028112Z Feb 09 20:54:08 	... 28 more
2022-02-09T20:54:09.0029066Z Feb 09 20:54:08 Caused by: java.io.IOException: The channel was closed before the protocol could be determined.
2022-02-09T20:54:09.0030456Z Feb 09 20:54:08 	at software.amazon.awssdk.http.nio.netty.internal.http2.Http2SettingsFrameHandler.channelUnregistered(Http2SettingsFrameHandler.java:58)
2022-02-09T20:54:09.0031909Z Feb 09 20:54:08 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:198)
2022-02-09T20:54:09.0033353Z Feb 09 20:54:08 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:184)
2022-02-09T20:54:09.0034853Z Feb 09 20:54:08 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelUnregistered(AbstractChannelHandlerContext.java:177)
2022-02-09T20:54:09.0036671Z Feb 09 20:54:08 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelUnregistered(DefaultChannelPipeline.java:1388)
2022-02-09T20:54:09.0038036Z Feb 09 20:54:08 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:198)
2022-02-09T20:54:09.0039531Z Feb 09 20:54:08 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelUnregistered(AbstractChannelHandlerContext.java:184)
2022-02-09T20:54:09.0040889Z Feb 09 20:54:08 	at io.netty.channel.DefaultChannelPipeline.fireChannelUnregistered(DefaultChannelPipeline.java:821)
2022-02-09T20:54:09.0042066Z Feb 09 20:54:08 	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:839)
2022-02-09T20:54:09.0043557Z Feb 09 20:54:08 	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
2022-02-09T20:54:09.0044950Z Feb 09 20:54:08 	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
2022-02-09T20:54:09.0046292Z Feb 09 20:54:08 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
2022-02-09T20:54:09.0047179Z Feb 09 20:54:08 	... 3 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31051&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=44195",,dannycranmer,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26300,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 28 14:56:15 UTC 2022,,,,,,,,,,"0|z0zfrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 06:27;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31971&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=44277;;;","28/Feb/22 14:56;dannycranmer;This is addressed in FLINK-26300 https://github.com/apache/flink/pull/18887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperJobGraphsStoreITCase.testPutAndRemoveJobGraph failed on azure,FLINK-26068,13427665,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,gaoyunhaii,gaoyunhaii,10/Feb/22 07:30,14/Feb/22 10:22,13/Jul/23 08:08,14/Feb/22 10:22,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"
{code:java}

Feb 09 13:41:24 org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /flink/default/testPutAndRemoveJobGraph/372cd3c2dc2c8b3071d3f8fec2285fb9
Feb 09 13:41:24 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:122)
Feb 09 13:41:24 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
Feb 09 13:41:24 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:2384)
Feb 09 13:41:24 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.SetDataBuilderImpl$7.call(SetDataBuilderImpl.java:398)
Feb 09 13:41:24 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.SetDataBuilderImpl$7.call(SetDataBuilderImpl.java:385)
Feb 09 13:41:24 	at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93)
Feb 09 13:41:24 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:382)
Feb 09 13:41:24 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:358)
Feb 09 13:41:24 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:36)
Feb 09 13:41:24 	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.setStateHandle(ZooKeeperStateHandleStore.java:268)
Feb 09 13:41:24 	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.replace(ZooKeeperStateHandleStore.java:232)
Feb 09 13:41:24 	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.replace(ZooKeeperStateHandleStore.java:86)
Feb 09 13:41:24 	at org.apache.flink.runtime.jobmanager.DefaultJobGraphStore.putJobGraph(DefaultJobGraphStore.java:226)
Feb 09 13:41:24 	at org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphsStoreITCase.testPutAndRemoveJobGraph(ZooKeeperJobGraphsStoreITCase.java:123)
Feb 09 13:41:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 09 13:41:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 09 13:41:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 09 13:41:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 09 13:41:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 09 13:41:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 09 13:41:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 09 13:41:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 09 13:41:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Feb 09 13:41:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Feb 09 13:41:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 09 13:41:24 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Feb 09 13:41:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Feb 09 13:41:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 09 13:41:24 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 09 13:41:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31007&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9438",,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26067,,,,,CURATOR-584,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 14 10:22:23 UTC 2022,,,,,,,,,,"0|z0zfrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 10:22;chesnay;master: 0cf7c3dedd3575cdfed57727e9712c28c013d7ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.table.api.PlanReference$ContentPlanReference $FilePlanReference $ResourcePlanReference violation the api rules,FLINK-26065,13427635,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,slinkydeveloper,gaoyunhaii,gaoyunhaii,10/Feb/22 02:50,10/Feb/22 12:14,13/Jul/23 08:08,10/Feb/22 12:14,1.15.0,,,,,,,,,,Table SQL / API,,,,,0,test-stability,,,"
{code:java}
Feb 09 21:10:32 [ERROR] Failures: 
Feb 09 21:10:32 [ERROR]   Architecture Violation [Priority: MEDIUM] - Rule 'Classes in API packages should have at least one API visibility annotation.' was violated (3 times):
Feb 09 21:10:32 org.apache.flink.table.api.PlanReference$ContentPlanReference does not satisfy: annotated with @Internal or annotated with @Experimental or annotated with @PublicEvolving or annotated with @Public or annotated with @Deprecated
Feb 09 21:10:32 org.apache.flink.table.api.PlanReference$FilePlanReference does not satisfy: annotated with @Internal or annotated with @Experimental or annotated with @PublicEvolving or annotated with @Public or annotated with @Deprecated
Feb 09 21:10:32 org.apache.flink.table.api.PlanReference$ResourcePlanReference does not satisfy: annotated with @Internal or annotated with @Experimental or annotated with @PublicEvolving or annotated with @Public or annotated with @Deprecated
Feb 09 21:10:32 [INFO] 
Feb 09 21:10:32 [ERROR] Tests run: 7, Failures: 1, Errors: 0, Skipped: 0
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31051&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=26427",,gaoyunhaii,roman,slinkydeveloper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25841,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 10 12:14:54 UTC 2022,,,,,,,,,,"0|z0zfko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/22 02:52;gaoyunhaii;Hi [~francescoguard] [~twalthr] could you have a look at this issue~?;;;","10/Feb/22 07:47;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31065&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=26350;;;","10/Feb/22 12:14;slinkydeveloper;Fixed by https://github.com/apache/flink/commit/74815407dae8c687dabdc62378905b8f3c143a77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisFirehoseSinkITCase IllegalStateException: Trying to access closed classloader,FLINK-26064,13427587,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,CrynetLogistics,pnowojski,pnowojski,09/Feb/22 20:49,21/Feb/22 17:29,13/Jul/23 08:08,21/Feb/22 15:34,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31044&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d

(shortened stack trace, as full is too large)
{noformat}
Feb 09 20:05:04 java.util.concurrent.ExecutionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
Feb 09 20:05:04 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Feb 09 20:05:04 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
(...)
Feb 09 20:05:04 Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
Feb 09 20:05:04 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:98)
Feb 09 20:05:04 	at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:43)
Feb 09 20:05:04 	at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:204)
Feb 09 20:05:04 	at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:200)
Feb 09 20:05:04 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:179)
Feb 09 20:05:04 	at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.lambda$attemptExecute$1(AsyncRetryableStage.java:159)
(...)
Feb 09 20:05:04 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
Feb 09 20:05:04 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
Feb 09 20:05:04 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResources(FlinkUserCodeClassLoaders.java:188)
Feb 09 20:05:04 	at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:348)
Feb 09 20:05:04 	at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
Feb 09 20:05:04 	at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
Feb 09 20:05:04 	at javax.xml.stream.FactoryFinder$1.run(FactoryFinder.java:352)
Feb 09 20:05:04 	at java.security.AccessController.doPrivileged(Native Method)
Feb 09 20:05:04 	at javax.xml.stream.FactoryFinder.findServiceProvider(FactoryFinder.java:341)
Feb 09 20:05:04 	at javax.xml.stream.FactoryFinder.find(FactoryFinder.java:313)
Feb 09 20:05:04 	at javax.xml.stream.FactoryFinder.find(FactoryFinder.java:227)
Feb 09 20:05:04 	at javax.xml.stream.XMLInputFactory.newInstance(XMLInputFactory.java:154)
Feb 09 20:05:04 	at software.amazon.awssdk.protocols.query.unmarshall.XmlDomParser.createXmlInputFactory(XmlDomParser.java:124)
Feb 09 20:05:04 	at java.lang.ThreadLocal$SuppliedThreadLocal.initialValue(ThreadLocal.java:284)
Feb 09 20:05:04 	at java.lang.ThreadLocal.setInitialValue(ThreadLocal.java:180)
Feb 09 20:05:04 	at java.lang.ThreadLocal.get(ThreadLocal.java:170)
(...)
{noformat}
",,dannycranmer,dmvk,gaoyunhaii,mapohl,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25945,,,,,,,,,FLINK-26256,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 21 15:34:02 UTC 2022,,,,,,,,,,"0|z0zfa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/22 07:22;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30979&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=44192;;;","10/Feb/22 07:31;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31012&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=43799;;;","10/Feb/22 07:35;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31035&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=44189;;;","10/Feb/22 07:54;dannycranmer;[~CrynetLogistics] is working on this test failure. ;;;","10/Feb/22 17:16;mapohl;This test is temporarily disabled on {{master}}: [6b17d319c4cfa37027e3ccfb00d742d52b2d794d|https://github.com/apache/flink/commit/6b17d319c4cfa37027e3ccfb00d742d52b2d794d];;;","10/Feb/22 19:18;dmvk;There is actually not a problem with reusing http clients, it just makes the root cause more visible for some reason.

The actual problem is that if the underlying netty event loop group is not configured, the SharedSdkEventLoopGroup is used. This means that clients that are used by the test suite and clients that are used by the actual pipeline are using the very same thread pool for handling HTTP communication.

By default the thread pool uses quite high number of threads (I think something like 2x number of CPUs, but would have to dive into Netty to confirm this).

Now the problem is that the Netty thread inherits a context classloader from the thread that has created it. This could either be the main thread (if thread has been created in the test suite) or FlinkUserClassLoader (if the thread has been created by the pipeline - in taskmanager).

There is a slight chance, that once the pipeline finishes (and user classloader is closed), we'll initiate a call on thread that has been created with this classloader attached.

That has couple of consequences:

    It will simply fail.
    If two jobs are running on the same TM and both are using AWS related clients, they will also eventually fail.

The fix if fairly simple, we need to set separate event loops for tests and for each sink executed by the TM (we can eventually try to find a way to reuse ELG within the TM -> some kind of reference counting scoped per job, if that causes too much overhead).

The proper place for fix would be AWSGeneralUtil#createAsyncHttpClient(software.amazon.awssdk.utils.AttributeMap, software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient.Builder).;;;","21/Feb/22 15:34;dannycranmer;Test is fixed, working on the root cause in FLINK-26256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Incorrect key group logged for PQ.poll and remove,FLINK-26063,13427583,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,09/Feb/22 19:49,14/Jul/22 23:11,13/Jul/23 08:08,15/Mar/22 17:14,1.15.0,,,,,,1.15.0,1.16.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,"Key group is logged so that state changes can be re-distributed or shuffled.
It is currently obtained from keyContext during poll() and remove() operations.
However, keyContext is not updated when dequeing processing time timers.

The impact is relatively small for remove(): in the worst case, the operation will be ignored.
poll() should probably be replaced with remove() anyways - see FLINK-26062.

One way to solve this problem is to extract key group from the polled element - if it is a timer.

cc: [~masteryhx], [~ym], [~yunta]",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25144,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 15 17:14:58 UTC 2022,,,,,,,,,,"0|z0zf94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/22 20:11;roman;{quote}
The impact is relatively small for remove(): in the worst case, the operation will be ignored.
{quote}
The operation is actually *not* ignored and fails the re-scaling (both heap and rocksdb queue impl.):
{code}
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_cdf5528fc65ae6b8b6b126cfdfcc40dd_(1/4) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
	... 11 more
Caused by: java.lang.IllegalArgumentException: key group from 0 to 32 does not contain 46
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:160)
	at org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.globalKeyGroupToLocalIndex(KeyGroupPartitionedPriorityQueue.java:191)
	at org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.computeKeyGroupIndex(KeyGroupPartitionedPriorityQueue.java:186)
	at org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.getKeyGroupSubHeapForElement(KeyGroupPartitionedPriorityQueue.java:179)
	at org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.remove(KeyGroupPartitionedPriorityQueue.java:129)
	at org.apache.flink.state.changelog.restore.PriorityQueueStateChangeApplier.apply(PriorityQueueStateChangeApplier.java:45)
	at org.apache.flink.state.changelog.restore.ChangelogBackendLogApplier.applyDataChange(ChangelogBackendLogApplier.java:222)
	at org.apache.flink.state.changelog.restore.ChangelogBackendLogApplier.applyOperation(ChangelogBackendLogApplier.java:96)
	at org.apache.flink.state.changelog.restore.ChangelogBackendLogApplier.apply(ChangelogBackendLogApplier.java:73)
	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:93)
	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:74)
	at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:225)
	at org.apache.flink.state.changelog.ChangelogStateBackend.createKeyedStateBackend(ChangelogStateBackend.java:148)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	... 13 more
{code}

Raising priority to Major and fixVersion to 1.15 (not Blocker to not block the release).;;;","11/Mar/22 14:11;roman;So far I've only been able to reproduce the issue on a cluster.

The same [program|https://github.com/rkhachatryan/flink/blob/c24c94261c23cbebd664ecf02c61e937bb7d1e36/flink-end-to-end-tests/flink-changelog-test/src/main/java/org/apache/flink/tests/ChangelogTestProgram.java#L53] on minicluster doesn't fail.;;;","15/Mar/22 17:14;roman;Merged into 1.15 as e31c1b0f88cbcd3f3a720948123b262fc21e4cd5 and into master as 092647cb3db6dcdf43fbf4a04b9f6cd5a95bee79.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] Non-deterministic recovery of PriorityQueue states,FLINK-26062,13427582,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,09/Feb/22 19:41,10/Feb/22 22:05,13/Jul/23 08:08,10/Feb/22 22:05,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"Currently, InternalPriorityQueue.poll() is logged as a separate operation, without specifying the element that has been polled. On recovery, this recorded poll() is replayed.

However, this is not deterministic because the order of PQ elements with equal priorityis not specified. For example, TimerHeapInternalTimer only compares timestamps, which are often equal. This results in polling timers from queue in wrong order => dropping timers => and not firing timers.

 

ProcessingTimeWindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow fails with materialization enabled and using heap state backend (both in-memory and fs-based implementations).

 

Proposed solution is to replace poll with remove operation (which is based on equality).
 
cc: [~masteryhx], [~ym], [~yunta]",,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,FLINK-23559,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26019,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 10 22:04:31 UTC 2022,,,,,,,,,,"0|z0zf8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/22 02:57;yunta;I think this is also true for InternalPriorityQueue#peek() operation as the order is non-deterministic on recovery. As peek is not a writing operation, changelog cannot record it. However, this might not affect the expected behaviors, right?;;;","10/Feb/22 08:12;roman;Good point, yes, peek() is also affected.
However:
1 When processing a timer, it is polled after peeking it. That poll() will be recorded and the result will be the same after recovery.
2 When peeking a timer and not removing it, only its timestamp is checked, which is the same for all timers at the head of the queue
3 PriorityQueue state is not exposed to the user, so no there can be no side-effects generated by a user program

Any side-effects resulting from processing a timer, such as writing to an external system, are subject to the existing constraints (i.e. any side-effects should only be committed after the checkpoint completion notification).

So I think this shouldn't be an issue.;;;","10/Feb/22 22:04;roman;Fix for poll() (replacement with remove()) merged into master as 9034b3c32875536b8b7d99aa31c7a2e6c942c811.

I'm going to close the ticket.

[~yunta] please reopen it if you think we should also approach peek(); or there are some other issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The tolerable-failed-checkpoints logic is invalid when checkpoint trigger failed,FLINK-26049,13427447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,09/Feb/22 10:12,07/Apr/22 13:09,13/Jul/23 08:08,04/Mar/22 09:21,1.13.5,1.14.3,,,,,1.14.5,1.15.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"After triggerCheckpoint, if checkpoint failed, flink will execute the tolerable-failed-checkpoints logic. But if triggerCheckpoint failed, flink won't execute the tolerable-failed-checkpoints logic.
h1. How to reproduce this issue?

In our online env, hdfs sre deletes the flink base dir by mistake, and flink job don't have permission to create checkpoint dir. So cause flink trigger checkpoint failed.

There are some didn't meet expectations:
 * JM just log _""Failed to trigger checkpoint for job 6f09d4a15dad42b24d52c987f5471f18 since Trigger checkpoint failure"" ._ Don't show the root cause or exception.
 * user set tolerable-failed-checkpoints=0, but if triggerCheckpoint failed, flink won't execute the tolerable-failed-checkpoints logic. 
 * When triggerCheckpoint failed, numberOfFailedCheckpoints is always 0
 * When triggerCheckpoint failed, we can't find checkpoint info in checkpoint history page.

 

!image-2022-02-09-18-08-17-868.png!

 

!image-2022-02-09-18-08-34-992.png!

!image-2022-02-09-18-08-42-920.png!

 
h3. *All metrics are normal, so the next day we found out that the checkpoint failed, and the checkpoint has been failing for a day. it's not acceptable to the flink user.*

I have some ideas:
 # Should tolerable-failed-checkpoints logic be executed when triggerCheckpoint fails?
 # When triggerCheckpoint failed, should increase numberOfFailedCheckpoints?
 # When triggerCheckpoint failed, should show checkpoint info in checkpoint history page?
 # JM just show ""Failed to trigger checkpoint"", should we show detailed exception to easy find the root cause?

 

Masters, could we do these changes? Please correct me if I'm wrong.",,akalashnikov,fanrui,jackwangcs,pnowojski,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24384,,,,,,,FLINK-26550,FLINK-26993,,,,,,,,,,,"09/Feb/22 10:08;fanrui;image-2022-02-09-18-08-17-868.png;https://issues.apache.org/jira/secure/attachment/13039825/image-2022-02-09-18-08-17-868.png","09/Feb/22 10:08;fanrui;image-2022-02-09-18-08-34-992.png;https://issues.apache.org/jira/secure/attachment/13039824/image-2022-02-09-18-08-34-992.png","09/Feb/22 10:08;fanrui;image-2022-02-09-18-08-42-920.png;https://issues.apache.org/jira/secure/attachment/13039823/image-2022-02-09-18-08-42-920.png","18/Feb/22 03:28;fanrui;image-2022-02-18-11-28-53-337.png;https://issues.apache.org/jira/secure/attachment/13040214/image-2022-02-18-11-28-53-337.png","18/Feb/22 03:33;fanrui;image-2022-02-18-11-33-28-232.png;https://issues.apache.org/jira/secure/attachment/13040215/image-2022-02-18-11-33-28-232.png","18/Feb/22 03:44;fanrui;image-2022-02-18-11-44-52-745.png;https://issues.apache.org/jira/secure/attachment/13040216/image-2022-02-18-11-44-52-745.png","22/Feb/22 02:27;fanrui;image-2022-02-22-10-27-43-731.png;https://issues.apache.org/jira/secure/attachment/13040309/image-2022-02-22-10-27-43-731.png","22/Feb/22 02:31;fanrui;image-2022-02-22-10-31-05-012.png;https://issues.apache.org/jira/secure/attachment/13040310/image-2022-02-22-10-31-05-012.png",,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 04 10:56:32 UTC 2022,,,,,,,,,,"0|z0zefc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/22 03:48;fanrui;I'm sorry, our prod env use Flink 1.13. I see some jiras have resolved this issue.

https://issues.apache.org/jira/browse/FLINK-23189

https://issues.apache.org/jira/browse/FLINK-24344

After I cherry-pick these commits, I think there are still some improvements we can make to facilitate troubleshooting. 
h2. 1. when create initializeLocation failure, JM don't show the root cause.

JM just show ""An Exception occurred while triggering the checkpoint. IO-problem detected."". Don't show the root cause. 

We should show throwable instead of throwable.getMessage(), and I have shown my code.

 

!image-2022-02-18-11-28-53-337.png|width=2790,height=248!

!image-2022-02-18-11-33-28-232.png|width=2115,height=574!
h2. 2. Can we initializeLocation after create PendingCheckpoint?

After create PendingCheckpoint, if there are some exception, we can see checkpoint info in History Page, and the numberOfFailedCheckpoints metric can be increased. 

They are useful for troubleshooting and monitor job. and initializeLocation isn't necessary for create PendingCheckpoint. So I think we can initializeLocation after create PendingCheckpoint.

 

!image-2022-02-18-11-44-52-745.png!

 

Masters [~akalashnikov]  [~pnowojski] , how do you think?;;;","18/Feb/22 11:31;fanrui;Hi [~akalashnikov] , could we increase numberOfFailedCheckpoints [here|https://github.com/apache/flink/blob/ac3ad139fbad02b2de241d5eef7b1e3ce6007b82/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L938]? It should be same bug with FLINK-24344.

 

User usually use the metric to monitor checkpoint status.

 ;;;","18/Feb/22 15:54;akalashnikov;Hi [~fanrui], Do you work on this task right now? I mean I am ready to take this task now if you are not working on it yet. About the question - it seems approximately the right place but I need to look deeper. So I will answer a little later.;;;","19/Feb/22 05:46;fanrui;Hi [~akalashnikov] . Actually, I'm working on this JIRA, I'm pleasant to do it. Could you assign to me, please? Thanks a lot.;;;","21/Feb/22 09:31;pnowojski;Hi [~fanrui], I would suggest maybe to slow down here a bit and think more about how do we want to treat failures on the {{CheckpointCoordinator}}. Is this really a bug? So far we only committed ourselves to check IOExceptions on the CheckpointCoordinator against the tolerable failed checkpoints counter. We have never claimed that any other types of exceptions will be treated the same way.;;;","21/Feb/22 11:07;fanrui;Hi, [~pnowojski] , thanks for your reply.

I think check IOExceptions should be enough. In our production environment, we met hdfs  permission problem. It's a sub class of IOException. So IOException can cover our Exception.

To summarize, this jira may need to do three things:
1. Optimize log and display root cause
2. initializeCheckpointLocation after create PendingCheckpoint
3. In onTriggerFailure, if checkpoint == null and CheckpointFailureReason==IO_EXCEPTION, increase the numberOfFailedCheckpoints metric

 

How do you think?;;;","21/Feb/22 15:00;pnowojski;Could you [~fanrui] post an example stack trace of an exception that caused this problem?;;;","22/Feb/22 02:38;fanrui;Hi [~pnowojski], this is exception stack. We meet the hdfs permission problem when initializeLocationForCheckpoint and org.apache.hadoop.security.AccessControlException is a subclass of IOException. 

This exception occurs before create PendingCheckpoint. So numberOfFailedCheckpoints can't be increased. And community flink version also does't print the Exception([code|https://github.com/apache/flink/blob/ac3ad139fbad02b2de241d5eef7b1e3ce6007b82/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L934]),  it's just show the throwable.getMessage() :""An Exception occurred while triggering the checkpoint. IO-problem detected.""

 

 

!image-2022-02-22-10-27-43-731.png!

 

!image-2022-02-22-10-31-05-012.png!

 ;;;","22/Feb/22 09:49;pnowojski;Ok, thanks for sharing the stack trace. Indeed we can treat this as a bug and I agree that this should have been checked against the failure counter just as other IOExceptions in the CheckpointCoordinator.;;;","23/Feb/22 08:14;fanrui;Hi [~pnowojski] [~akalashnikov] , I have updated the [PR|[https://github.com/apache/flink/pull/18852],] could you help to review in your free time, please? Thanks a lot.;;;","03/Mar/22 16:59;pnowojski;merged commit 5ce2e062cdb..ffe353a into apache:master
merged commit 970932bfe20..39afe944840 into apache:master;;;","04/Mar/22 09:21;pnowojski;Thanks for reporting, describing and fixing the issue [~fanrui] :);;;","04/Mar/22 10:56;fanrui;[~pnowojski]  [~akalashnikov]  Thank you for your advice and review. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlinkEmbeddedSubInterpreterTests. test_udf_without_arguments failed on azure,FLINK-26042,13427404,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,gaoyunhaii,gaoyunhaii,09/Feb/22 08:04,24/Feb/22 07:00,13/Jul/23 08:08,23/Feb/22 09:46,1.15.0,,,,,,1.15.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-02-08T02:55:16.0701246Z Feb 08 02:55:16 =================================== FAILURES ===================================
2022-02-08T02:55:16.0702483Z Feb 08 02:55:16 ________ PyFlinkEmbeddedSubInterpreterTests.test_udf_without_arguments _________
2022-02-08T02:55:16.0703190Z Feb 08 02:55:16 
2022-02-08T02:55:16.0703959Z Feb 08 02:55:16 self = <pyflink.table.tests.test_udf.PyFlinkEmbeddedSubInterpreterTests testMethod=test_udf_without_arguments>
2022-02-08T02:55:16.0704967Z Feb 08 02:55:16 
2022-02-08T02:55:16.0705639Z Feb 08 02:55:16     def test_udf_without_arguments(self):
2022-02-08T02:55:16.0706641Z Feb 08 02:55:16         one = udf(lambda: 1, result_type=DataTypes.BIGINT(), deterministic=True)
2022-02-08T02:55:16.0707595Z Feb 08 02:55:16         two = udf(lambda: 2, result_type=DataTypes.BIGINT(), deterministic=False)
2022-02-08T02:55:16.0713079Z Feb 08 02:55:16     
2022-02-08T02:55:16.0714866Z Feb 08 02:55:16         table_sink = source_sink_utils.TestAppendSink(['a', 'b'],
2022-02-08T02:55:16.0716495Z Feb 08 02:55:16                                                       [DataTypes.BIGINT(), DataTypes.BIGINT()])
2022-02-08T02:55:16.0717411Z Feb 08 02:55:16         self.t_env.register_table_sink(""Results"", table_sink)
2022-02-08T02:55:16.0718059Z Feb 08 02:55:16     
2022-02-08T02:55:16.0719148Z Feb 08 02:55:16         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 1)], ['a', 'b'])
2022-02-08T02:55:16.0719974Z Feb 08 02:55:16 >       t.select(one(), two()).execute_insert(""Results"").wait()
2022-02-08T02:55:16.0720697Z Feb 08 02:55:16 
2022-02-08T02:55:16.0721294Z Feb 08 02:55:16 pyflink/table/tests/test_udf.py:252: 
2022-02-08T02:55:16.0722119Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0722943Z Feb 08 02:55:16 pyflink/table/table_result.py:76: in wait
2022-02-08T02:55:16.0723686Z Feb 08 02:55:16     get_method(self._j_table_result, ""await"")()
2022-02-08T02:55:16.0725024Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1322: in __call__
2022-02-08T02:55:16.0726044Z Feb 08 02:55:16     answer, self.gateway_client, self.target_id, self.name)
2022-02-08T02:55:16.0726824Z Feb 08 02:55:16 pyflink/util/exceptions.py:146: in deco
2022-02-08T02:55:16.0727569Z Feb 08 02:55:16     return f(*a, **kw)
2022-02-08T02:55:16.0728326Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0728995Z Feb 08 02:55:16 
2022-02-08T02:55:16.0729717Z Feb 08 02:55:16 answer = 'x'
2022-02-08T02:55:16.0730447Z Feb 08 02:55:16 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7fadb5dc97f0>
2022-02-08T02:55:16.0731465Z Feb 08 02:55:16 target_id = 'o26503', name = 'await'
2022-02-08T02:55:16.0732045Z Feb 08 02:55:16 
2022-02-08T02:55:16.0732763Z Feb 08 02:55:16     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-02-08T02:55:16.0733699Z Feb 08 02:55:16         """"""Converts an answer received from the Java gateway into a Python object.
2022-02-08T02:55:16.0734508Z Feb 08 02:55:16     
2022-02-08T02:55:16.0735205Z Feb 08 02:55:16         For example, string representation of integers are converted to Python
2022-02-08T02:55:16.0736228Z Feb 08 02:55:16         integer, string representation of objects are converted to JavaObject
2022-02-08T02:55:16.0736974Z Feb 08 02:55:16         instances, etc.
2022-02-08T02:55:16.0737508Z Feb 08 02:55:16     
2022-02-08T02:55:16.0738185Z Feb 08 02:55:16         :param answer: the string returned by the Java gateway
2022-02-08T02:55:16.0739074Z Feb 08 02:55:16         :param gateway_client: the gateway client used to communicate with the Java
2022-02-08T02:55:16.0739994Z Feb 08 02:55:16             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-02-08T02:55:16.0740723Z Feb 08 02:55:16             list, map)
2022-02-08T02:55:16.0741491Z Feb 08 02:55:16         :param target_id: the name of the object from which the answer comes from
2022-02-08T02:55:16.0742350Z Feb 08 02:55:16             (e.g., *object1* in `object1.hello()`). Optional.
2022-02-08T02:55:16.0743175Z Feb 08 02:55:16         :param name: the name of the member from which the answer comes from
2022-02-08T02:55:16.0744315Z Feb 08 02:55:16             (e.g., *hello* in `object1.hello()`). Optional.
2022-02-08T02:55:16.0744973Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0745608Z Feb 08 02:55:16         if is_error(answer)[0]:
2022-02-08T02:55:16.0746484Z Feb 08 02:55:16             if len(answer) > 1:
2022-02-08T02:55:16.0747162Z Feb 08 02:55:16                 type = answer[1]
2022-02-08T02:55:16.0747943Z Feb 08 02:55:16                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-02-08T02:55:16.0748809Z Feb 08 02:55:16                 if answer[1] == REFERENCE_TYPE:
2022-02-08T02:55:16.0749555Z Feb 08 02:55:16                     raise Py4JJavaError(
2022-02-08T02:55:16.0750313Z Feb 08 02:55:16                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-02-08T02:55:16.0751291Z Feb 08 02:55:16                         format(target_id, ""."", name), value)
2022-02-08T02:55:16.0752004Z Feb 08 02:55:16                 else:
2022-02-08T02:55:16.0752642Z Feb 08 02:55:16                     raise Py4JError(
2022-02-08T02:55:16.0753484Z Feb 08 02:55:16                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-02-08T02:55:16.0754479Z Feb 08 02:55:16                         format(target_id, ""."", name, value))
2022-02-08T02:55:16.0755171Z Feb 08 02:55:16             else:
2022-02-08T02:55:16.0755914Z Feb 08 02:55:16                 raise Py4JError(
2022-02-08T02:55:16.0756575Z Feb 08 02:55:16                     ""An error occurred while calling {0}{1}{2}"".
2022-02-08T02:55:16.0757325Z Feb 08 02:55:16 >                   format(target_id, ""."", name))
2022-02-08T02:55:16.0758178Z Feb 08 02:55:16 E               py4j.protocol.Py4JError: An error occurred while calling o26503.await
2022-02-08T02:55:16.0758918Z Feb 08 02:55:16 
2022-02-08T02:55:16.0760185Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/protocol.py:336: Py4JError
2022-02-08T02:55:16.0761380Z Feb 08 02:55:16 ----------------------------- Captured stdout call -----------------------------
2022-02-08T02:55:16.0762060Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0762752Z Feb 08 02:55:16 # A fatal error has been detected by the Java Runtime Environment:
2022-02-08T02:55:16.0763425Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0764075Z Feb 08 02:55:16 #  SIGSEGV (0xb) at pc=0x00007f417f38bc06, pid=19997, tid=0x00007f4159d73700
2022-02-08T02:55:16.0764867Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0766133Z Feb 08 02:55:16 # JRE version: OpenJDK Runtime Environment (8.0_292-b10) (build 1.8.0_292-8u292-b10-0ubuntu1~16.04.1-b10)
2022-02-08T02:55:16.0767491Z Feb 08 02:55:16 # Java VM: OpenJDK 64-Bit Server VM (25.292-b10 mixed mode linux-amd64 compressed oops)
2022-02-08T02:55:16.0768305Z Feb 08 02:55:16 # Problematic frame:
2022-02-08T02:55:16.0769026Z Feb 08 02:55:16 # C  [libpython3.7m.so.1.0+0x1f5c06]  _PyObject_Malloc+0x76
2022-02-08T02:55:16.0769662Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0770699Z Feb 08 02:55:16 # Core dump written. Default location: /__w/2/s/flink-python/core or core.19997
2022-02-08T02:55:16.0771460Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0772156Z Feb 08 02:55:16 # An error report file with more information is saved as:
2022-02-08T02:55:16.0773210Z Feb 08 02:55:16 # /__w/2/s/flink-python/hs_err_pid19997.log
2022-02-08T02:55:16.0773840Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0774570Z Feb 08 02:55:16 # If you would like to submit a bug report, please visit:
2022-02-08T02:55:16.0775367Z Feb 08 02:55:16 #   http://bugreport.java.com/bugreport/crash.jsp
2022-02-08T02:55:16.0776264Z Feb 08 02:55:16 # The crash happened outside the Java Virtual Machine in native code.
2022-02-08T02:55:16.0777074Z Feb 08 02:55:16 # See problematic frame for where to report the bug.
2022-02-08T02:55:16.0777677Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0778665Z Feb 08 02:55:16 ------------------------------ Captured log call -------------------------------
2022-02-08T02:55:16.0779519Z Feb 08 02:55:16 ERROR    root:java_gateway.py:1056 Exception while sending command.
2022-02-08T02:55:16.0780512Z Feb 08 02:55:16 Traceback (most recent call last):
2022-02-08T02:55:16.0781811Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1224, in send_command
2022-02-08T02:55:16.0782801Z Feb 08 02:55:16     raise Py4JNetworkError(""Answer from Java side is empty"")
2022-02-08T02:55:16.0783648Z Feb 08 02:55:16 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2022-02-08T02:55:16.0784455Z Feb 08 02:55:16 
2022-02-08T02:55:16.0785200Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0786054Z Feb 08 02:55:16 
2022-02-08T02:55:16.0786654Z Feb 08 02:55:16 Traceback (most recent call last):
2022-02-08T02:55:16.0787982Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1038, in send_command
2022-02-08T02:55:16.0789321Z Feb 08 02:55:16     response = connection.send_command(command)
2022-02-08T02:55:16.0790649Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1229, in send_command
2022-02-08T02:55:16.0791618Z Feb 08 02:55:16     ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
2022-02-08T02:55:16.0792405Z Feb 08 02:55:16 py4j.protocol.Py4JNetworkError: Error while receiving
2022-02-08T02:55:16.0793242Z Feb 08 02:55:16 ______ PyFlinkStreamUserDefinedFunctionTests.test_execute_from_json_plan _______
2022-02-08T02:55:16.0793915Z Feb 08 02:55:16 
2022-02-08T02:55:16.0794707Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayClient object at 0x7fadb5dc97f0>
2022-02-08T02:55:16.0795354Z Feb 08 02:55:16 
2022-02-08T02:55:16.0796067Z Feb 08 02:55:16     def _get_connection(self):
2022-02-08T02:55:16.0796782Z Feb 08 02:55:16         if not self.is_connected:
2022-02-08T02:55:16.0797537Z Feb 08 02:55:16             raise Py4JNetworkError(""Gateway is not connected."")
2022-02-08T02:55:16.0798233Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0798882Z Feb 08 02:55:16 >           connection = self.deque.pop()
2022-02-08T02:55:16.0799589Z Feb 08 02:55:16 E           IndexError: pop from an empty deque
2022-02-08T02:55:16.0800212Z Feb 08 02:55:16 
2022-02-08T02:55:16.0801304Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:982: IndexError
2022-02-08T02:55:16.0802023Z Feb 08 02:55:16 
2022-02-08T02:55:16.0802720Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0803443Z Feb 08 02:55:16 
2022-02-08T02:55:16.0804235Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayConnection object at 0x7faecec6ecc0>
2022-02-08T02:55:16.0804911Z Feb 08 02:55:16 
2022-02-08T02:55:16.0805463Z Feb 08 02:55:16     def start(self):
2022-02-08T02:55:16.0806319Z Feb 08 02:55:16         """"""Starts the connection by connecting to the `address` and the `port`
2022-02-08T02:55:16.0807034Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0807593Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0808269Z Feb 08 02:55:16 >           self.socket.connect((self.address, self.port))
2022-02-08T02:55:16.0809116Z Feb 08 02:55:16 E           ConnectionRefusedError: [Errno 111] Connection refused
2022-02-08T02:55:16.0809804Z Feb 08 02:55:16 
2022-02-08T02:55:16.0810912Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1132: ConnectionRefusedError
2022-02-08T02:55:16.0811653Z Feb 08 02:55:16 
2022-02-08T02:55:16.0812378Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0813232Z Feb 08 02:55:16 pyflink/testing/test_case_utils.py:137: in setUp
2022-02-08T02:55:16.0814075Z Feb 08 02:55:16     self.t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
2022-02-08T02:55:16.0815068Z Feb 08 02:55:16 pyflink/table/environment_settings.py:267: in in_streaming_mode
2022-02-08T02:55:16.0816018Z Feb 08 02:55:16     get_gateway().jvm.EnvironmentSettings.inStreamingMode())
2022-02-08T02:55:16.0826059Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1712: in __getattr__
2022-02-08T02:55:16.0826592Z Feb 08 02:55:16     ""\n"" + proto.END_COMMAND_PART)
2022-02-08T02:55:16.0827339Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1036: in send_command
2022-02-08T02:55:16.0827853Z Feb 08 02:55:16     connection = self._get_connection()
2022-02-08T02:55:16.0828568Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:984: in _get_connection
2022-02-08T02:55:16.0829089Z Feb 08 02:55:16     connection = self._create_connection()
2022-02-08T02:55:16.0830171Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:990: in _create_connection
2022-02-08T02:55:16.0830673Z Feb 08 02:55:16     connection.start()
2022-02-08T02:55:16.0831220Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0831828Z Feb 08 02:55:16 
2022-02-08T02:55:16.0832255Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayConnection object at 0x7faecec6ecc0>
2022-02-08T02:55:16.0832661Z Feb 08 02:55:16 
2022-02-08T02:55:16.0833006Z Feb 08 02:55:16     def start(self):
2022-02-08T02:55:16.0833469Z Feb 08 02:55:16         """"""Starts the connection by connecting to the `address` and the `port`
2022-02-08T02:55:16.0833921Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0834369Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0834798Z Feb 08 02:55:16             self.socket.connect((self.address, self.port))
2022-02-08T02:55:16.0835281Z Feb 08 02:55:16             self.stream = self.socket.makefile(""rb"")
2022-02-08T02:55:16.0835988Z Feb 08 02:55:16             self.is_connected = True
2022-02-08T02:55:16.0836367Z Feb 08 02:55:16     
2022-02-08T02:55:16.0836734Z Feb 08 02:55:16             self._authenticate_connection()
2022-02-08T02:55:16.0837200Z Feb 08 02:55:16         except Py4JAuthenticationError:
2022-02-08T02:55:16.0837722Z Feb 08 02:55:16             logger.exception(""Cannot authenticate with gateway server."")
2022-02-08T02:55:16.0838161Z Feb 08 02:55:16             raise
2022-02-08T02:55:16.0838559Z Feb 08 02:55:16         except Exception as e:
2022-02-08T02:55:16.0839045Z Feb 08 02:55:16             msg = ""An error occurred while trying to connect to the Java ""\
2022-02-08T02:55:16.0839682Z Feb 08 02:55:16                 ""server ({0}:{1})"".format(self.address, self.port)
2022-02-08T02:55:16.0840168Z Feb 08 02:55:16             logger.exception(msg)
2022-02-08T02:55:16.0840581Z Feb 08 02:55:16 >           raise Py4JNetworkError(msg, e)
2022-02-08T02:55:16.0841178Z Feb 08 02:55:16 E           py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:41141)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30878&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24930",,dianfu,gaoyunhaii,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 23 09:46:14 UTC 2022,,,,,,,,,,"0|z0ze5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 05:33;dianfu;cc [~hxbks2ks] ;;;","18/Feb/22 07:48;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31807&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=25582;;;","23/Feb/22 09:46;hxbks2ks;Merged into master via d33ba9637a0a9eec4d3c7cb63f969ada04977a71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect value getter in map unnest table function,FLINK-26039,13427398,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fornaix,fornaix,fornaix,09/Feb/22 07:50,16/Feb/22 18:23,13/Jul/23 08:08,09/Feb/22 13:42,1.14.3,,,,,,1.13.7,1.14.4,1.15.0,,Table SQL / Runtime,,,,,0,pull-request-available,,,"Suppose we have a map field that needs to be expanded.

 
{code:java}
CREATE TABLE t (
    id INT,
    map_field MAP<STRING, INT>
) WITH (
    -- ...
);

SELECT id, k, v FROM t, unnest(map_field) as A(k, v);{code}
 

 

We will get the following runtime exception:
{code:java}
Caused by: java.lang.ClassCastException: org.apache.flink.table.data.binary.BinaryStringData cannot be cast to java.lang.Integer
    at org.apache.flink.table.data.GenericRowData.getInt(GenericRowData.java:149)
    at org.apache.flink.table.data.utils.JoinedRowData.getInt(JoinedRowData.java:149)
    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$6(RowData.java:245)
    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$25774257$1(RowData.java:296)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:80)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at org.apache.flink.table.runtime.util.StreamRecordCollector.collect(StreamRecordCollector.java:44)
    at org.apache.flink.table.runtime.collector.TableFunctionCollector.outputResult(TableFunctionCollector.java:68)
    at StreamExecCorrelate$10$TableFunctionCollector$4.collect(Unknown Source)
    at org.apache.flink.table.runtime.collector.WrappingCollector.outputResult(WrappingCollector.java:39)
    at StreamExecCorrelate$10$TableFunctionResultConverterCollector$8.collect(Unknown Source)
    at org.apache.flink.table.functions.TableFunction.collect(TableFunction.java:197)
    at org.apache.flink.table.runtime.functions.SqlUnnestUtils$MapUnnestTableFunction.eval(SqlUnnestUtils.java:169)
    at StreamExecCorrelate$10.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) {code}",,begginghard,fornaix,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 09 13:42:39 UTC 2022,,,,,,,,,,"0|z0ze4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/22 13:42;twalthr;Fixed in master: a8ba755da86c9e5429056c83045384e836dd010a
Fixed in 1.14: ca9edeb8bcf6b5108bbef231b296403ac25016c0
Fixed in 1.13: ace49aa807f831e209dacf0129240137fff266f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory timeout on azure,FLINK-26036,13427394,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,gaoyunhaii,gaoyunhaii,09/Feb/22 07:41,11/Feb/22 09:26,13/Jul/23 08:08,11/Feb/22 09:26,1.15.0,,,,,,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,"
{code:java}
022-02-09T02:18:17.1827314Z Feb 09 02:18:14 [ERROR] org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory  Time elapsed: 62.252 s  <<< ERROR!
2022-02-09T02:18:17.1827940Z Feb 09 02:18:14 java.util.concurrent.TimeoutException
2022-02-09T02:18:17.1828450Z Feb 09 02:18:14 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2022-02-09T02:18:17.1829040Z Feb 09 02:18:14 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-02-09T02:18:17.1829752Z Feb 09 02:18:14 	at org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory(LocalRecoveryITCase.java:115)
2022-02-09T02:18:17.1830407Z Feb 09 02:18:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-02-09T02:18:17.1830954Z Feb 09 02:18:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-02-09T02:18:17.1831582Z Feb 09 02:18:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-02-09T02:18:17.1832135Z Feb 09 02:18:14 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-02-09T02:18:17.1832697Z Feb 09 02:18:14 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-02-09T02:18:17.1833566Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-02-09T02:18:17.1834394Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-02-09T02:18:17.1835125Z Feb 09 02:18:14 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-02-09T02:18:17.1835875Z Feb 09 02:18:14 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-02-09T02:18:17.1836565Z Feb 09 02:18:14 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-02-09T02:18:17.1837294Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-02-09T02:18:17.1838007Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-02-09T02:18:17.1838743Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-02-09T02:18:17.1839499Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-02-09T02:18:17.1840224Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-02-09T02:18:17.1840952Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-02-09T02:18:17.1841616Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-02-09T02:18:17.1842257Z Feb 09 02:18:14 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-02-09T02:18:17.1842951Z Feb 09 02:18:14 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-02-09T02:18:17.1843681Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1844782Z Feb 09 02:18:14 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-02-09T02:18:17.1845603Z Feb 09 02:18:14 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-02-09T02:18:17.1846375Z Feb 09 02:18:14 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-02-09T02:18:17.1847084Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-02-09T02:18:17.1847785Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1848490Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-02-09T02:18:17.1849138Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-02-09T02:18:17.1849797Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-02-09T02:18:17.1850500Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1851169Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-02-09T02:18:17.1851834Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-02-09T02:18:17.1852396Z Feb 09 02:18:14 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2022-02-09T02:18:17.1853086Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2022-02-09T02:18:17.1853876Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-02-09T02:18:17.1854746Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1855633Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-02-09T02:18:17.1856371Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-02-09T02:18:17.1857033Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-02-09T02:18:17.1857722Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1858400Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-02-09T02:18:17.1859068Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-02-09T02:18:17.1859632Z Feb 09 02:18:14 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2022-02-09T02:18:17.1860318Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2022-02-09T02:18:17.1861122Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-02-09T02:18:17.1861818Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1862519Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-02-09T02:18:17.1863169Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-02-09T02:18:17.1863920Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-02-09T02:18:17.1864685Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-02-09T02:18:17.1865773Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-02-09T02:18:17.1866640Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-02-09T02:18:17.1867395Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2022-02-09T02:18:17.1868198Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2022-02-09T02:18:17.1868928Z Feb 09 02:18:14 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
2022-02-09T02:18:17.1869645Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-09T02:18:17.1870359Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-09T02:18:17.1871067Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-09T02:18:17.1871823Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-09T02:18:17.1872551Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-09T02:18:17.1873208Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-09T02:18:17.1873826Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-09T02:18:17.1874572Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-09T02:18:17.1875289Z Feb 09 02:18:14 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-09T02:18:17.1876343Z Feb 09 02:18:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-09T02:18:17.1877233Z Feb 09 02:18:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-09T02:18:17.1877928Z Feb 09 02:18:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-09T02:18:17.1878584Z Feb 09 02:18:14 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-09T02:18:17.1879206Z Feb 09 02:18:14 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-09T02:18:17.1879793Z Feb 09 02:18:14 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-09T02:18:17.1880381Z Feb 09 02:18:14 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=23106",,gaoyunhaii,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 11 09:26:44 UTC 2022,,,,,,,,,,"0|z0ze3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/22 07:45;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=31047;;;","09/Feb/22 07:46;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=23911;;;","09/Feb/22 07:48;guoyangze;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30962&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=21108;;;","09/Feb/22 07:49;gaoyunhaii;Perhaps cc [~trohrmann]~ ;;;","09/Feb/22 07:53;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30947&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=24198
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30941&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=30226
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30921&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=22458
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30927&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=29652
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30924&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=24310;;;","09/Feb/22 09:53;trohrmann;The problem seems to be that a restarted {{TaskManager}} process finds an empty {{slotAllocationSnapshots}} directory where actually the previously allocated slot snapshots should be contained.;;;","09/Feb/22 10:11;trohrmann;The problem seems to be the shutdown hook introduced with FLINK-25709. In some cases, the shutdown hook manages to delete the {{slotAllocationSnapshots}} so that the newly restarted process cannot recover.;;;","09/Feb/22 22:03;trohrmann;Fixed via f839f12e2e425d8c32279fb505a8c624bbc2a266;;;","10/Feb/22 07:58;gaoyunhaii;Hi [~trohrmann]~ there seems to be an occurrence of this issue after merged: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31071&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=21947 ~;;;","10/Feb/22 11:19;trohrmann;Thanks for the pointer [~gaoyunhaii]. I will take another look at the problem. I assume that there is still somehow a call that triggers {{TaskExectuor.freeSlotInternal}}.;;;","10/Feb/22 16:19;trohrmann;The problem seems to be the following: When shutting down the {{TaskExecutor}}, then we fail all running tasks and close the {{TaskSlotTableImpl}}. If now a terminal state of a {{Task}} is processed, then this removes the {{Task}} from the {{TaskSlotTableImpl}}. This in turn triggers the {{TaskExecutor.freeSlotInternal}} via the {{SlotActions.freeSlot}} call.;;;","10/Feb/22 16:21;trohrmann;I think the solution is to move the {{isRunning()}} check into the {{freeSlotInternal}} method. That way we should guarantee that we don't free slots while shutting down.;;;","10/Feb/22 19:36;pnowojski;https://dev.azure.com/pnowojski/Flink/_build/results?buildId=625&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1;;;","11/Feb/22 08:08;trohrmann;Another instance w/o the latest fix: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31211&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=22954;;;","11/Feb/22 09:18;gaoyunhaii;Very thanks Till for the investigation! 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31090&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=28577
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31146&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=29067;;;","11/Feb/22 09:26;trohrmann;Fixed via 314f91df7019bf1ce1ede52b8ba7f2d0a1c44aac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set FLINK_LIB_DIR to 'lib' under working dir in YARN containers,FLINK-26030,13427375,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bgeng777,bgeng777,bgeng777,09/Feb/22 06:01,16/Mar/22 06:50,13/Jul/23 08:08,16/Mar/22 06:50,,,,,,,1.16.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,"Currently, we utilize {{org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils#tryFindUserLibDirectory}} to locate usrlib in both flink client and cluster side. 
This method relies on the value of environment variable {{FLINK_LIB_DIR}} to find the {{{}usrlib{}}}.
It makes sense in client side since in {{{}bin/config.sh{}}}, {{FLINK_LIB_DIR}} will be set by default(i.e. {{FLINK_HOME/lib}} if not exists. But in YARN cluster's containers, when we want to reuse this method to find {{{}usrlib{}}}, as the YARN usually starts the process using commands like
{quote}/bin/bash -c /usr/lib/jvm/java-1.8.0/bin/java -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b ...
{quote}
{{FLINK_LIB_DIR}} is not guaranteed to be set in such case. Current codes will use current working dir to locate the {{usrlib}} which is correct in most cases. But bad things can happen if the machine which the YARN container resides in has already set {{FLINK_LIB_DIR}} to a different folder. In that case, codes will try to find {{usrlib}} in a undesired place.

One possible solution would be overriding the {{FLINK_LIB_DIR}} in YARN container env to the {{lib}} dir under YARN's working dir.",,bgeng777,huwh,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 16 06:50:02 UTC 2022,,,,,,,,,,"0|z0zdzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/22 07:35;wangyang0918;{{ClusterEntrypointUtils#tryFindUserLibDirectory}} will return a wrong usr lib if environment {{FLINK_LIB_DIR}} is pre-configured in the YARN cluster. So I think this ticket is a minor bug, not an improvement. Right?;;;","10/Feb/22 05:02;bgeng777;Yes, it should be a bug.

I believe to be consistent with the behavior in flink client side, in YARN cluster side, we should make the {{FLINK_LIB_DIR}} in YARN container env to the {{lib}} dir under YARN's working dir. If that is the case, I may start investigating on how to implement it properly. Let me know if above assumption make sense. Thanks!;;;","10/Feb/22 06:12;wangyang0918;Setting the {{FLINK_LIB_DIR}} to workDir/usrlib makes sense to me.;;;","16/Mar/22 06:50;wangyang0918;Fixed via:

master(1.16): 

5bf4b072c31e799c91f171f0dcd56319728aec5a

b95037b4d9c2be2e5eacd2205d9fcd033db117d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary late events when using the new KafkaSource,FLINK-26018,13427277,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,qinjunjerry,qinjunjerry,08/Feb/22 20:37,31/Mar/22 07:02,13/Jul/23 08:08,31/Mar/22 06:56,1.14.3,1.14.4,,,,,1.14.5,1.15.0,,,Connectors / Common,,,,,0,pull-request-available,,,"There is an issue with the new KafkaSource connector in Flink 1.14: when one task consumes messages from multiple topic partitions (statically created, timestamp are in order), it may start with one partition and advances watermarks before the data from other partitions come. In this case, the early messages in other partitions may unnecessarily be considered  as late ones.

I discussed with [~renqs], it seems that the new KafkaSource only adds a partition into {{WatermarkMultiplexer}} when it receives data from that partition. In contrast, FlinkKafkaConsumer adds all known partition before it fetch any data. 

Attached two files: the messages in Kafka and the corresponding TM logs.",,begginghard,danderson,dmvk,knaufk,leonard,martijnvisser,mason6345,nkruber,pnowojski,qinjunjerry,renqs,victorunique,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21871,,,,,,,,,,"08/Feb/22 20:35;qinjunjerry;message in kafka.txt;https://issues.apache.org/jira/secure/attachment/13039789/message+in+kafka.txt","08/Feb/22 20:35;qinjunjerry;taskmanager_10.28.0.131_33249-b3370c_log;https://issues.apache.org/jira/secure/attachment/13039788/taskmanager_10.28.0.131_33249-b3370c_log",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 31 06:56:47 UTC 2022,,,,,,,,,,"0|z0zde0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/22 08:04;martijnvisser;[~qinjunjerry] Just to double check, is this only a problem in Flink 1.14 or also in earlier version like 1.13? ;;;","09/Feb/22 08:39;qinjunjerry;I haven't tried with 1.13. [~renqs] could you comment?;;;","11/Feb/22 10:27;renqs;This issue exists for all sources extending SourceReaderBase, so I think KafkaSource in 1.13 also has this problem.

A solution in my mind is that we can create per-split output for a split once it's assigned to the source operator. For splits assigned before the main reader output is created (before the first {{SourceOperator#emitNext}} invocation), we can use a list to hold them temporarily in SourceOperator and create per-split outputs when the main output is ready. ;;;","18/Feb/22 10:48;pnowojski;I believe solution to this problem and FLINK-21781 (about broadcasting watermarks) might need to be considered together.;;;","18/Feb/22 13:59;danderson;[~pnowojski] Perhaps you meant FLINK-21871 rather than FLINK-21781?;;;","18/Feb/22 15:26;pnowojski;Yes [~danderson], you are right. Thanks for spotting the typo. ;;;","07/Mar/22 09:16;pnowojski;After an offline discussion we agreed for my team to take this over to try to fix it before 1.15.0 release.;;;","31/Mar/22 06:56;renqs;Fixed on master: b9593715f35640154615df0098ac7459f37911c3

release-1.15: c71582d5837b067f26f8aec610193bacf6ccb26b

release-1.14: 65047113eceefd377df7017b6b8d553dda9a8758;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystemLookupFunction does not produce correct results when hive table uses columnar storage,FLINK-26016,13427239,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,08/Feb/22 16:03,21/Mar/22 06:06,13/Jul/23 08:08,21/Mar/22 06:06,1.14.3,,,,,,1.14.5,1.15.0,,,Connectors / Hive,,,,,0,pull-request-available,,,"When I use the parquet hive table as the lookup table, there will be some records that cannot be joined. This can be reproduced by adding unit tests to HiveLookupJoinITCase.

{code:java}
  // create the hive table with columnar storage.
        tableEnv.executeSql(
                String.format(
                        ""create table columnar_table (x string) STORED AS PARQUET ""
                                + ""tblproperties ('%s'='5min')"",
                        HiveOptions.LOOKUP_JOIN_CACHE_TTL.key()));

    @Test
    public void testLookupJoinTableWithColumnarStorage() throws Exception {
        // constructs test data, as the DEFAULT_SIZE of VectorizedColumnBatch is 2048, we should
        // write as least 2048 records to the test table.
        List<Row> testData = new ArrayList<>(4096);
        for (int i = 0; i < 4096; i++) {
            testData.add(Row.of(String.valueOf(i)));
        }

        // constructs test data using values table
        TableEnvironment batchEnv = HiveTestUtils.createTableEnvInBatchMode(SqlDialect.DEFAULT);
        batchEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);
        batchEnv.useCatalog(hiveCatalog.getName());
        String dataId = TestValuesTableFactory.registerData(testData);
        batchEnv.executeSql(
                String.format(
                        ""create table value_source(x string, p as proctime()) with (""
                                + ""'connector' = 'values', 'data-id' = '%s', 'bounded'='true')"",
                        dataId));
        batchEnv.executeSql(""insert overwrite columnar_table select x from value_source"").await();
        TableImpl flinkTable =
                (TableImpl)
                        tableEnv.sqlQuery(
                                ""select t.x as x1, c.x as x2 from value_source t ""
                                        + ""left join columnar_table for system_time as of t.p c ""
                                        + ""on t.x = c.x where c.x is null"");
        List<Row> results = CollectionUtil.iteratorToList(flinkTable.execute().collect());
        assertTrue(results.size() == 0);
    }
{code}

The problem may be caused by the following code. 

{code:java}
RowData row;
while ((row = partitionReader.read(reuse)) != null) {
   count++;
   RowData key = extractLookupKey(row);
   List<RowData> rows = cache.computeIfAbsent(key, k -> new ArrayList<>());
   rows.add(serializer.copy(row));
}
{code}

         
It can be fixed with the following modification
{code:java}
RowData row;
while ((row = partitionReader.read(reuse)) != null) {
    count++;
    RowData rowData = serializer.copy(row);
    RowData key = extractLookupKey(rowData);
    List<RowData> rows = cache.computeIfAbsent(key, k -> new ArrayList<>());
    rows.add(rowData);
}
{code}
",,hackergin,luoyuxia,lzljs3620320,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 06:06:30 UTC 2022,,,,,,,,,,"0|z0zd5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/22 16:08;hackergin; cc [~lirui]  ;;;","15/Feb/22 02:32;hackergin;[~MartijnVisser]   [~luoyuxia]  Can you help take a look?    If it's a bug, I can help fix it;;;","15/Feb/22 03:18;luoyuxia;[~hackergin]Thanks for reporting it. It seems a bug. But I'm a little of confused about the root case. Would you like to explain it a bit more?;;;","15/Feb/22 05:51;hackergin;[~luoyuxia] ， Thanks for your reply.  I think the root cause is the ColumnarRowData::getString() method. 
{code:java}
//代码占位符

@Override
public StringData getString(int pos) {
Bytes byteArray = vectorizedColumnBatch.getByteArray(rowId, pos);
return StringData.{_}fromBytes{_}(byteArray.data, byteArray.offset, byteArray.len);
}
{code}

When reading StringData, the data is not copied. And vectorizedColumnBatch is reused when reading different batch. In the lookup join function, the bytes corresponding to the key read first in the cache will be overwritten by the bytes corresponding to the key read later.;;;","15/Feb/22 09:35;luoyuxia;[~hackergin] Thanks, make sense to me. Feel free to open a pr to us, we are glad to review it .;;;","16/Feb/22 02:03;hackergin;[~luoyuxia]  I have submitted a pr, please take the time to help review the code,  Thanks .;;;","21/Mar/22 06:06;lzljs3620320;master (1.15): faf9a8a1f6849e3cb0b9e074cc647c9d24915c6a

release-1.14: a19b44b3932c1799ff837a6ad78595df02a3d5f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisFirehoseSinkITCase leaks resources,FLINK-26006,13427167,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,dmvk,dmvk,08/Feb/22 11:11,08/Feb/22 18:11,13/Jul/23 08:08,08/Feb/22 18:11,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,Both KinesisFirehoseSinkITCase and KinesisFirehose sink in general don't close the underlying AWS resources.,,dmvk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24433,FLINK-25937,FLINK-25977,FLINK-18356,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 08 18:11:11 UTC 2022,,,,,,,,,,"0|z0zcqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/22 11:12;dmvk;Found during investigation;;;","08/Feb/22 18:11;chesnay;master: fd92e56bb2309415e11f0086465f64ff3c322e0b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobDispatcherITCase.testRecoverFromCheckpointAfterLosingAndRegainingLeadership fails on azure,FLINK-25992,13427030,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiangang,roman,roman,07/Feb/22 20:58,18/Feb/22 15:22,13/Jul/23 08:08,18/Feb/22 15:22,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30871&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9154

{code}
19:41:35,515 [flink-akka.actor.default-dispatcher-9] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - jobVertex (1/1)#0 (7efdea21f5f95490e02117063ce8a314) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Error while notify checkpoint ABORT.
	at org.apache.flink.runtime.taskmanager.Task.notifyCheckpoint(Task.java:1457)
	at org.apache.flink.runtime.taskmanager.Task.notifyCheckpointAborted(Task.java:1407)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.abortCheckpoint(TaskExecutor.java:1021)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.UnsupportedOperationException: notifyCheckpointAbortAsync not supported by org.apache.flink.runtime.dispatcher.JobDispatcherITCase$AtLeastOneCheckpointInvokable
	at org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.notifyCheckpointAbortAsync(AbstractInvokable.java:205)
	at org.apache.flink.runtime.taskmanager.Task.notifyCheckpoint(Task.java:1430)
	... 31 more

{code}",,Jiangang,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/22 07:55;roman;mvn-2.log;https://issues.apache.org/jira/secure/attachment/13039758/mvn-2.log",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 18 15:22:10 UTC 2022,,,,,,,,,,"0|z0zbwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/22 21:01;roman;[~Jiangang] could you please take a look?;;;","08/Feb/22 06:43;Jiangang;[~roman] Thanks for the report. Indeed, the class AtLeastOneCheckpointInvokable doesn't override the method notifyCheckpointAbortAsync. Since the code '

AtLeastOneCheckpointInvokable.atLeastOneCheckpointCompleted.await();

' is finished, there must be one complete checkpoint to recover later. I am confused with the AssertionError. The case is hard to reproduce in my local host. Can you show me the full log?;;;","08/Feb/22 07:56;roman;Sure [~Jiangang], I've attached the log file (you can also download it from the build main page following the artifacts link).;;;","09/Feb/22 07:15;Jiangang;[~roman] I am afraid that the reason is still uncertain from the log. From the log, the processing is as following:
 # When the first checkpoint expired, the UnsupportedOperationException is thrown and the job restarts.
 # The job is running and checkpoint 2 is done.
 # The leadership changes and the job restores from checkpoint 2.
 # We check that the restored checkpoint is not null.

I reproduce the processing in my local host but the test is still ok. So the UnsupportedOperationException will not cause the test fail. Other two things can be optimized as following:
 # Implement the method notifyCheckpointAbortAsync.
 # Tune the checkpoint timeout to 1000 to avoid the failed checkpoint.

But I am not sure whether the optimization can resolve the issue.;;;","10/Feb/22 08:47;roman;Thanks for looking into it [~Jiangang]. The conclusion and proposed changes make sense to me.

 

Regarding the test failure, probably the root cause is in obtaining the StatsSnapshot on [line 143|https://github.com/apache/flink/blob/f5819cec01eab914e8d4e7db41587b715a557e3b/flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/JobDispatcherITCase.java#L143] of JobDispatcherITCase: the check is made only once.

At [line 122|https://github.com/apache/flink/blob/f5819cec01eab914e8d4e7db41587b715a557e3b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointStatsTracker.java#L122] of CheckpointStatsTracker, it checks if there is no update in progress; if there IS then it doesn't update the snapshot, and client receives the initial snapshot (with empty restoredCheckpoint); which fails the test.

If that's the cause then JobDispatcherITCase should probably repeat this request until success or deadline.

WDYT?;;;","11/Feb/22 01:51;Jiangang;Thanks [~roman] . I have check the code and the log. When the job restores from the checkpoint 2, the method reportRestoredCheckpoint in CheckpointStatsTracker is called . So there is update in progress and the member dirty is set true. In method createSnapshot, only when statsReadWriteLock.tryLock() is false then snapshot will not be updated. Additionally, the method createSnapshot is also triggered when awaiting job status in line 138  of JobDispatcherITCase. 

Based on the above info, it is hard to get the root reason. Your suggestion is valuable. But I am not sure wether it can resolve the problem thoroughly. What do you think?;;;","14/Feb/22 14:23;roman;I think that the race condition is possible, and that race condition can be a root cause. 
It's hard to say whether that is actually what happened in that particular case, but I'd fix it anyways.
If the issue remains after that, we could re-open the ticket and continue the investigation.;;;","15/Feb/22 06:47;Jiangang;Thanks, [~roman]. Retry is a good way to avoid the race condition. If we have no other findings, this is an effect way to make the test stable. If so, I would like to fix it.;;;","15/Feb/22 07:45;roman;Thanks [~Jiangang], that would be great!

I've assigned the ticket to you.;;;","16/Feb/22 02:12;Jiangang;[~roman] Fixed. Please take time to review the code. Thanks.;;;","18/Feb/22 15:22;roman;Thanks for the fix [~Jiangang], merged into master as a060302379a7a53fe10f699625e36245e6a90d50.
I think it should prevent any further test failures and going to close the ticket.
If not, please feel free to reopen the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointRescaleITCase failed with exit code 137,FLINK-25988,13426961,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,07/Feb/22 15:46,01/Mar/22 10:31,13/Jul/23 08:08,01/Mar/22 10:31,1.14.3,1.15.0,,,,,1.14.5,1.15.0,,,Runtime / Checkpointing,,,,,0,test-stability,,,"
{code:java}
Feb 07 10:35:02 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale multi_input from 7 to 7, buffersPerChannel = 0].
Feb 07 10:35:02 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale multi_input from 7 to 7, buffersPerChannel = 0].
Feb 07 10:35:02 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale multi_input from 7 to 7, buffersPerChannel = 0].
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1003  -w /home/agent04_azpcontainer 54763e695d61cf1a80b5d4d6c397eec13218a705cddf2d5d659cec21533e1f8a /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
Finishing: Test - finegrained_resource_management
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30829&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12459
",,akalashnikov,dwysakowicz,gaoyunhaii,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25989,FLINK-25026,,FLINK-18356,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 01 10:30:55 UTC 2022,,,,,,,,,,"0|z0zbhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/22 15:53;gaoyunhaii;Perhaps cc [~pnowojski]~ 

With the findings when debugging https://issues.apache.org/jira/browse/FLINK-18356:
1. The reuseForks are set to false for flink-test module, indicating it should not related to other issues.
2. One possible issues might still be the network buffers could not be deallocated in time. For this case, each test would create separate mini-clusters and create network buffer pool, and the buffers in the pool would only be deallocated on the direct buffer get GCed. ;;;","11/Feb/22 09:31;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31188&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=4969 1.14;;;","23/Feb/22 03:29;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32030&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12571;;;","01/Mar/22 10:30;dwysakowicz;Closing for now as we introduced an improvement in https://issues.apache.org/jira/browse/FLINK-25026 that should help with memory control. Feel free to reopen if it reappears.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Changelog] IllegalArgumentException thrown from FsStateChangelogWriter.truncate,FLINK-25987,13426956,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,07/Feb/22 15:34,31/Mar/22 09:26,13/Jul/23 08:08,12/Feb/22 07:56,1.15.0,,,,,,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"{code}
java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.changelog.fs.FsStateChangelogWriter.truncate(FsStateChangelogWriter.java:278)
	at org.apache.flink.state.changelog.ChangelogKeyedStateBackend.updateChangelogSnapshotState(ChangelogKeyedStateBackend.java:702)
	at org.apache.flink.state.changelog.PeriodicMaterializationManager.lambda$null$2(PeriodicMaterializationManager.java:163)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750){code}
",,roman,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23559,,,,,,,,,,,,FLINK-25997,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Feb 12 07:56:40 UTC 2022,,,,,,,,,,"0|z0zbg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/22 07:56;roman;Merged as 66048d11c1b53b5c6b24cdb20a6d4d3348f87026.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close sink client and sink http client for KDS/KDF Sinks,FLINK-25977,13426896,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,CrynetLogistics,CrynetLogistics,CrynetLogistics,07/Feb/22 11:15,23/Feb/22 08:51,13/Jul/23 08:08,22/Feb/22 08:54,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,We are not closing the AWS SDK and HTTP clients for the new KDS/KDF async sink. The close method needs to be invoked when operator stops.,,CrynetLogistics,dmvk,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25976,,,FLINK-25948,,,,,,,,,,,,FLINK-26006,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 22 08:08:28 UTC 2022,,,,,,,,,,"0|z0zb2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/22 18:15;dmvk;[~CrynetLogistics] how comes this is closed? We've only fixed firehose related issues in FLINK-26006, the other AWS modules suffer from the very same problems. Is there anything I've missed?;;;","08/Feb/22 18:21;dmvk;Also apart from the sink issues, please make sure we're also closing any resources that are used for testing; leaky tests negatively affect the overall stability of the build infrastructure;;;","22/Feb/22 08:08;trohrmann;PR is reviewed. Waiting for CI to pass.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible class leaks in flink-table / sql modules,FLINK-25968,13426864,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,slinkydeveloper,gaoyunhaii,gaoyunhaii,07/Feb/22 08:22,10/Mar/22 08:41,13/Jul/23 08:08,10/Mar/22 08:37,1.15.0,,,,,,1.15.0,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,,,,"This is the umbrella issues for possible class leaks in flink-table / sql planner and runtimes.

Currently for a flink cluster, the flink classes are loaded by the system ClassLoader while each job would have separate user ClassLoaders. In this case, if some class loaded by the system ClassLoader has static variables that reference the classes loaded by the user  ClassLoaders, the user ClassLoaders would not be able to be released, which might cause class leak in some way. ",,aitozi,begginghard,gaoyunhaii,lincoln.86xy,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-02-07 08:22:45.0,,,,,,,,,,"0|z0zavs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink generated Avro schemas can't be parsed using Python,FLINK-25962,13426672,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,04/Feb/22 17:11,02/Aug/22 12:56,13/Jul/23 08:08,02/Aug/22 12:56,1.14.3,,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"Flink currently generates Avro schemas as records with the top-level name {{""record""}}

Unfortunately, there is some inconsistency between Avro implementations in different languages that may prevent this record from being read, notably Python, which generates the error:
*avro.schema.SchemaParseException: record is a reserved type name*
(See the comment on FLINK-18096 for the full stack trace).

The Java SDK accepts this name, and there's an [ongoing discussion|https://lists.apache.org/thread/0wmgyx6z69gy07lvj9ndko75752b8cn2] about what the expected behaviour should be.  This should be clarified and fixed in Avro, of course.

Regardless of the resolution, the best practice (which is used almost everywhere else in the Flink codebase) is to explicitly specify a top-level namespace for an Avro record.   We should use a default like: {{{}org.apache.flink.avro.generated{}}}.",,Mynttinen,rskraba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18096,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 12:56:28 UTC 2022,,,,,,,,,,"0|z0z9p4:",9223372036854775807,"Avro schemas generated by Flink now use the ""org.apache.flink.avro.generated"" namespace for compatibility with the Avro Python SDK.",,,,,,,,,,,,,,,,,,,"10/Feb/22 12:06;rskraba;I forgot to ask whether this could be assigned to me; the PR is done.  This would be a good candidate for the next releases, since these generated schemas are currently broken in all current versions of Avro's Python SDK :/;;;","11/Apr/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Apr/22 22:39;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/May/22 10:04;rskraba;Hello, this is probably more than a minor issue -- we're generating Avro data that is incompatible with Python readers.  The fix is pretty easy and having a namespace for Avro records is a pretty good practice.;;;","18/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/Jul/22 10:43;rskraba;Hello – I believe this is still a major issue.  The PR is available and the fix is fairly straightforward. ;;;","02/Aug/22 12:56;chesnay;master: 2c58dca500d0ec4f5d80852aa96ddb9c06ae4d61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOME Checkpoints & Savepoints were shown as COMPLETE in Flink UI,FLINK-25958,13426594,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,victorunique,victorunique,04/Feb/22 10:20,03/Mar/22 17:21,13/Jul/23 08:08,03/Mar/22 17:21,1.12.7,1.13.5,1.14.3,1.15.0,,,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"Flink job was running but the checkpoints & savepoints were failing all the time due to OOM Exception. However, the Flink UI showed COMPLETE for those checkpoints & savepoints.

For example (checkpoint 39 & 40):
{noformat}
2022-01-27 02:41:39,969 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 39 (type=CHECKPOINT) @ 1643251299952 for job ab2217e5ce144087bbddf6bd6c3
668eb.
2022-01-27 02:43:19,678 WARN  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Error while processing AcknowledgeCheckpoint message
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete the pending checkpoint 39. Failure reason: Failure to finalize checkpoint.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1227) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1072) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-s
tream2]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.IllegalArgumentException: Self-suppression not permitted
        at java.lang.Throwable.addSuppressed(Throwable.java:1054) ~[?:?]
        at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:627) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at com.ververica.platform.flink.ha.kubernetes.KubernetesHaCheckpointStore.serializeCheckpoint(KubernetesHaCheckpointStore.java:204) ~[vvp-flink-ha-kubernetes-flink113-1.4-20211013.09
1138-2.jar:?]
        at com.ververica.platform.flink.ha.kubernetes.KubernetesHaCheckpointStore.addCheckpoint(KubernetesHaCheckpointStore.java:83) ~[vvp-flink-ha-kubernetes-flink113-1.4-20211013.091138-2.
jar:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1209) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        ... 9 more
Caused by: java.lang.OutOfMemoryError: Java heap space
2022-01-27 03:41:39,970 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 40 (type=CHECKPOINT) @ 1643254899952 for job ab2217e5ce144087bbddf6bd6c3
668eb.
2022-01-27 03:43:22,326 WARN  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Error while processing AcknowledgeCheckpoint message
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete the pending checkpoint 40. Failure reason: Failure to finalize checkpoint.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1227) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1072) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-s
tream2]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.IllegalArgumentException: Self-suppression not permitted
        at java.lang.Throwable.addSuppressed(Throwable.java:1054) ~[?:?]
        at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:627) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        at com.ververica.platform.flink.ha.kubernetes.KubernetesHaCheckpointStore.serializeCheckpoint(KubernetesHaCheckpointStore.java:204) ~[vvp-flink-ha-kubernetes-flink113-1.4-20211013.09
1138-2.jar:?]
        at com.ververica.platform.flink.ha.kubernetes.KubernetesHaCheckpointStore.addCheckpoint(KubernetesHaCheckpointStore.java:83) ~[vvp-flink-ha-kubernetes-flink113-1.4-20211013.091138-2.jar:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1209) ~[flink-dist_2.12-1.13.5-stream2.jar:1.13.5-stream2]
        ... 9 more
Caused by: java.lang.OutOfMemoryError: Java heap space{noformat}
Please find attached a screenshot of the Flink UI (both 39 & 40 were COMPLETE).

 ","Ververica Platform 2.6.2

Flink 1.13.5",akalashnikov,jackwangcs,nkruber,pnowojski,RocMarshal,tanyuxin,victorunique,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-4410,,,,,,,,,,,,,,"04/Feb/22 10:18;victorunique;JIRA-1.jpg;https://issues.apache.org/jira/secure/attachment/13039662/JIRA-1.jpg",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 03 17:21:32 UTC 2022,,,,,,,,,,"0|z0z97s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/22 10:27;pnowojski;It looks like the problem is caused by pre-mature reporting that the checkpoint is completed via {{PendingCheckpointStats#reportCompletedCheckpoint}} call in {{PendingCheckpoint#finalizeCheckpoint}}. That's when {{PendingCheckpoint}} is converted to {{CompletedCheckpoint}}, however this doesn't mean the checkpoint will indeed completed. For example adding to checkpoint store can still fail.

It looks like there is no good reason behind this behaviour and it's just an unintentional artefact of FLINK-4410 changes, that added support for displaying failed/pending checkpoints stats.

The most naive solution might be just moving {{PendingCheckpointStats#reportCompletedCheckpoint}}  at the end of the checkpointing process. However one thing to consider is that this could indadvertedly create the opposite problem, where checkpoint has completed but failure while reporting fails to mark it as such.;;;","03/Mar/22 17:21;akalashnikov;merge to master between:

f57a5379ff9c108627d3c511414e7ea71a1a2a2f and fbfdb0e468356fe71826eb6b185ecda9bc8b1de3

 

Since the behavior was changed a bit and this bug is not so critical. We decided that it doesn't make sense to backport it to older version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint on S3 are not relocatable even if entropy injection is not enabled,FLINK-25952,13426461,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,03/Feb/22 16:14,07/Feb/22 16:47,13/Jul/23 08:08,07/Feb/22 16:47,1.13.5,1.14.3,1.15.0,,,,1.13.7,1.14.4,1.15.0,,FileSystems,Runtime / Checkpointing,,,,0,pull-request-available,,,"We have a limitation that if we create savepoints with an injected entropy, they are not relocatable (https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/savepoints/#triggering-savepoints).

However the check if we use the entropy is flawed. In {{FsCheckpointStreamFactory}} we check only if the used filesystem extends from {{EntropyInjectingFileSystem}}. {{FlinkS3FileSystem}} does, but it still may have the entropy disabled. {{FlinkS3FileSystem#getEntropyInjectionKey}} may still return {{null}}. We should check for that in {{org.apache.flink.core.fs.EntropyInjector#isEntropyInjecting}}",,danderson,dwysakowicz,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 07 16:47:35 UTC 2022,,,,,,,,,,"0|z0z8eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/22 16:47;dwysakowicz;Fixed in:
* master
** 27cd145cbf5a694aa81419e2472c9da84e137cfa
* 1.14.4
** e2a17adf9075ba5feb99a5c87a1b8d41254bbe9e
* 1.13.7
** e12ad75f0ecf364e9c99aa0ddc42c4dd0475c1b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-171] Kinesis Firehose sink builder falls back to wrong http protocol.,FLINK-25949,13426406,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,chalixar,chalixar,03/Feb/22 12:23,03/Feb/22 16:24,13/Jul/23 08:08,03/Feb/22 16:05,,,,,,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,"h2. Description
As captured by {{KinesisFirehoseITCase}} merged in [PR#18314|http://https://github.com/apache/flink/pull/18314] , Firehose client created using default values in {{KinesisFirehoseSinkBuilder}} is unstable. 

h2.Proposed changes
- Default protocol overriden in {{KinesisFirehoseSinkBuilder}} is instance of {{software.amazon.awssdk.http.Protocol}} while required type is {{String}}. 
Protocol inserted should be of type {{String}}.

- Unit tests should be added to capture the issue if reoccured. 
",,chalixar,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 03 16:05:00 UTC 2022,,,,,,,,,,"0|z0z828:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/22 16:05;dannycranmer;[~chalixar] please update description and issue links;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kinesis, Firehose Async Sink state serializer implementation",FLINK-25943,13426376,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chalixar,CrynetLogistics,CrynetLogistics,03/Feb/22 10:09,15/Feb/22 08:32,13/Jul/23 08:08,11/Feb/22 19:15,1.15.0,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,"AsyncSinkWriter implements snapshotState to write the pending request into state but none of the implementation (Kinesis, Firehose) provides a state serializer nor interacts with the recovered state.

 

* Implement 
{code:java}
getWriterStateSerializer{code}
 for the Kinesis/Firehose Sinks",,CrynetLogistics,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26119,FLINK-26118,FLINK-24228,FLINK-24227,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 11 19:15:40 UTC 2022,,,,,,,,,,"0|z0z7vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/22 19:15;dannycranmer;Merged to master at aab253a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint fails on AZP,FLINK-25941,13426367,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,trohrmann,trohrmann,03/Feb/22 09:43,22/Feb/22 12:42,13/Jul/23 08:08,22/Feb/22 12:42,1.15.0,,,,,,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,"The test {{KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint}} fails on AZP with

{code}
2022-02-02T17:22:29.5131631Z Feb 02 17:22:29 [ERROR] org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint  Time elapsed: 2.186 s  <<< FAILURE!
2022-02-02T17:22:29.5146972Z Feb 02 17:22:29 java.lang.AssertionError
2022-02-02T17:22:29.5148918Z Feb 02 17:22:29 	at org.junit.Assert.fail(Assert.java:87)
2022-02-02T17:22:29.5149843Z Feb 02 17:22:29 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-02-02T17:22:29.5150644Z Feb 02 17:22:29 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-02-02T17:22:29.5151730Z Feb 02 17:22:29 	at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint(KafkaSinkITCase.java:267)
2022-02-02T17:22:29.5152858Z Feb 02 17:22:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-02-02T17:22:29.5153757Z Feb 02 17:22:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-02-02T17:22:29.5155002Z Feb 02 17:22:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-02-02T17:22:29.5156464Z Feb 02 17:22:29 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-02-02T17:22:29.5157384Z Feb 02 17:22:29 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-02-02T17:22:29.5158445Z Feb 02 17:22:29 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-02-02T17:22:29.5159478Z Feb 02 17:22:29 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-02-02T17:22:29.5160524Z Feb 02 17:22:29 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-02-02T17:22:29.5161758Z Feb 02 17:22:29 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-02-02T17:22:29.5162775Z Feb 02 17:22:29 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-02-02T17:22:29.5163744Z Feb 02 17:22:29 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-02-02T17:22:29.5164913Z Feb 02 17:22:29 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-02-02T17:22:29.5166101Z Feb 02 17:22:29 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-02-02T17:22:29.5167030Z Feb 02 17:22:29 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-02-02T17:22:29.5167953Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-02T17:22:29.5168956Z Feb 02 17:22:29 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-02-02T17:22:29.5169936Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-02-02T17:22:29.5170903Z Feb 02 17:22:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-02-02T17:22:29.5171953Z Feb 02 17:22:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-02-02T17:22:29.5172919Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-02-02T17:22:29.5173811Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-02-02T17:22:29.5174874Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-02-02T17:22:29.5175917Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-02-02T17:22:29.5176851Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-02-02T17:22:29.5177816Z Feb 02 17:22:29 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-02-02T17:22:29.5178816Z Feb 02 17:22:29 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-02-02T17:22:29.5179929Z Feb 02 17:22:29 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-02-02T17:22:29.5180960Z Feb 02 17:22:29 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-02-02T17:22:29.5181827Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-02T17:22:29.5182733Z Feb 02 17:22:29 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-02-02T17:22:29.5183595Z Feb 02 17:22:29 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-02-02T17:22:29.5184586Z Feb 02 17:22:29 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-02-02T17:22:29.5185660Z Feb 02 17:22:29 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-02-02T17:22:29.5186712Z Feb 02 17:22:29 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-02-02T17:22:29.5187763Z Feb 02 17:22:29 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-02-02T17:22:29.5188837Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-02T17:22:29.5190020Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-02T17:22:29.5191469Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-02T17:22:29.5192712Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-02T17:22:29.5193946Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-02T17:22:29.5195325Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-02T17:22:29.5196488Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-02T17:22:29.5197609Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-02T17:22:29.5198802Z Feb 02 17:22:29 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-02T17:22:29.5199906Z Feb 02 17:22:29 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-02T17:22:29.5201084Z Feb 02 17:22:29 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-02T17:22:29.5202246Z Feb 02 17:22:29 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-02T17:22:29.5203357Z Feb 02 17:22:29 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-02T17:22:29.5204504Z Feb 02 17:22:29 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-02T17:22:29.5205490Z Feb 02 17:22:29 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-02T17:22:29.5206578Z Feb 02 17:22:29 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30642&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35949",,fpaul,gaoyunhaii,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 22 12:42:38 UTC 2022,,,,,,,,,,"0|z0z7tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/22 09:43;trohrmann;cc [~fpaul];;;","03/Feb/22 09:46;fpaul;I'll check if it caused by https://issues.apache.org/jira/browse/FLINK-25575;;;","07/Feb/22 07:49;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=35677;;;","14/Feb/22 07:46;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31305&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=36150;;;","21/Feb/22 15:31;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31930&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35597;;;","22/Feb/22 12:42;fpaul;Merged in master: de465995dc7e28bca71f1610579133b99f064e52;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state failed on AZP,FLINK-25940,13426365,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,trohrmann,trohrmann,03/Feb/22 09:41,14/Feb/22 06:44,13/Jul/23 08:08,14/Feb/22 06:44,1.15.0,,,,,,,,,,API / Python,,,,,0,pull-request-available,test-stability,,"The test {{pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state}} fails on AZP:

{code}
2022-02-02T17:44:12.1898582Z Feb 02 17:44:12 =================================== FAILURES ===================================
2022-02-02T17:44:12.1899860Z Feb 02 17:44:12 _____ StreamingModeDataStreamTests.test_keyed_process_function_with_state ______
2022-02-02T17:44:12.1900493Z Feb 02 17:44:12 
2022-02-02T17:44:12.1901218Z Feb 02 17:44:12 self = <pyflink.datastream.tests.test_data_stream.StreamingModeDataStreamTests testMethod=test_keyed_process_function_with_state>
2022-02-02T17:44:12.1901948Z Feb 02 17:44:12 
2022-02-02T17:44:12.1902745Z Feb 02 17:44:12     def test_keyed_process_function_with_state(self):
2022-02-02T17:44:12.1903722Z Feb 02 17:44:12         self.env.get_config().set_auto_watermark_interval(2000)
2022-02-02T17:44:12.1904473Z Feb 02 17:44:12         self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
2022-02-02T17:44:12.1906780Z Feb 02 17:44:12         data_stream = self.env.from_collection([(1, 'hi', '1603708211000'),
2022-02-02T17:44:12.1908034Z Feb 02 17:44:12                                                 (2, 'hello', '1603708224000'),
2022-02-02T17:44:12.1909166Z Feb 02 17:44:12                                                 (3, 'hi', '1603708226000'),
2022-02-02T17:44:12.1910122Z Feb 02 17:44:12                                                 (4, 'hello', '1603708289000'),
2022-02-02T17:44:12.1911099Z Feb 02 17:44:12                                                 (5, 'hi', '1603708291000'),
2022-02-02T17:44:12.1912451Z Feb 02 17:44:12                                                 (6, 'hello', '1603708293000')],
2022-02-02T17:44:12.1913456Z Feb 02 17:44:12                                                type_info=Types.ROW([Types.INT(), Types.STRING(),
2022-02-02T17:44:12.1914338Z Feb 02 17:44:12                                                                     Types.STRING()]))
2022-02-02T17:44:12.1914811Z Feb 02 17:44:12     
2022-02-02T17:44:12.1915317Z Feb 02 17:44:12         class MyTimestampAssigner(TimestampAssigner):
2022-02-02T17:44:12.1915724Z Feb 02 17:44:12     
2022-02-02T17:44:12.1916782Z Feb 02 17:44:12             def extract_timestamp(self, value, record_timestamp) -> int:
2022-02-02T17:44:12.1917621Z Feb 02 17:44:12                 return int(value[2])
2022-02-02T17:44:12.1918262Z Feb 02 17:44:12     
2022-02-02T17:44:12.1918855Z Feb 02 17:44:12         class MyProcessFunction(KeyedProcessFunction):
2022-02-02T17:44:12.1919363Z Feb 02 17:44:12     
2022-02-02T17:44:12.1919744Z Feb 02 17:44:12             def __init__(self):
2022-02-02T17:44:12.1920143Z Feb 02 17:44:12                 self.value_state = None
2022-02-02T17:44:12.1920648Z Feb 02 17:44:12                 self.list_state = None
2022-02-02T17:44:12.1921298Z Feb 02 17:44:12                 self.map_state = None
2022-02-02T17:44:12.1921864Z Feb 02 17:44:12     
2022-02-02T17:44:12.1922479Z Feb 02 17:44:12             def open(self, runtime_context: RuntimeContext):
2022-02-02T17:44:12.1923907Z Feb 02 17:44:12                 value_state_descriptor = ValueStateDescriptor('value_state', Types.INT())
2022-02-02T17:44:12.1924922Z Feb 02 17:44:12                 self.value_state = runtime_context.get_state(value_state_descriptor)
2022-02-02T17:44:12.1925741Z Feb 02 17:44:12                 list_state_descriptor = ListStateDescriptor('list_state', Types.INT())
2022-02-02T17:44:12.1926482Z Feb 02 17:44:12                 self.list_state = runtime_context.get_list_state(list_state_descriptor)
2022-02-02T17:44:12.1927465Z Feb 02 17:44:12                 map_state_descriptor = MapStateDescriptor('map_state', Types.INT(), Types.STRING())
2022-02-02T17:44:12.1927998Z Feb 02 17:44:12                 state_ttl_config = StateTtlConfig \
2022-02-02T17:44:12.1928444Z Feb 02 17:44:12                     .new_builder(Time.seconds(1)) \
2022-02-02T17:44:12.1928943Z Feb 02 17:44:12                     .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \
2022-02-02T17:44:12.1929462Z Feb 02 17:44:12                     .set_state_visibility(
2022-02-02T17:44:12.1929939Z Feb 02 17:44:12                         StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp) \
2022-02-02T17:44:12.1930601Z Feb 02 17:44:12                     .disable_cleanup_in_background() \
2022-02-02T17:44:12.1931032Z Feb 02 17:44:12                     .build()
2022-02-02T17:44:12.1931480Z Feb 02 17:44:12                 map_state_descriptor.enable_time_to_live(state_ttl_config)
2022-02-02T17:44:12.1932018Z Feb 02 17:44:12                 self.map_state = runtime_context.get_map_state(map_state_descriptor)
2022-02-02T17:44:12.1932610Z Feb 02 17:44:12     
2022-02-02T17:44:12.1933172Z Feb 02 17:44:12             def process_element(self, value, ctx):
2022-02-02T17:44:12.1933623Z Feb 02 17:44:12                 import time
2022-02-02T17:44:12.1934007Z Feb 02 17:44:12                 time.sleep(1)
2022-02-02T17:44:12.1934419Z Feb 02 17:44:12                 current_value = self.value_state.value()
2022-02-02T17:44:12.1934977Z Feb 02 17:44:12                 self.value_state.update(value[0])
2022-02-02T17:44:12.1935451Z Feb 02 17:44:12                 current_list = [_ for _ in self.list_state.get()]
2022-02-02T17:44:12.1935921Z Feb 02 17:44:12                 self.list_state.add(value[0])
2022-02-02T17:44:12.1936401Z Feb 02 17:44:12                 map_entries = {k: v for k, v in self.map_state.items()}
2022-02-02T17:44:12.1936862Z Feb 02 17:44:12                 keys = sorted(map_entries.keys())
2022-02-02T17:44:12.1937649Z Feb 02 17:44:12                 map_entries_string = [str(k) + ': ' + str(map_entries[k]) for k in keys]
2022-02-02T17:44:12.1938404Z Feb 02 17:44:12                 map_entries_string = '{' + ', '.join(map_entries_string) + '}'
2022-02-02T17:44:12.1938906Z Feb 02 17:44:12                 self.map_state.put(value[0], value[1])
2022-02-02T17:44:12.1939350Z Feb 02 17:44:12                 current_key = ctx.get_current_key()
2022-02-02T17:44:12.1939889Z Feb 02 17:44:12                 yield ""current key: {}, current value state: {}, current list state: {}, "" \
2022-02-02T17:44:12.1940521Z Feb 02 17:44:12                       ""current map state: {}, current value: {}"".format(str(current_key),
2022-02-02T17:44:12.1941111Z Feb 02 17:44:12                                                                         str(current_value),
2022-02-02T17:44:12.1941645Z Feb 02 17:44:12                                                                         str(current_list),
2022-02-02T17:44:12.1942254Z Feb 02 17:44:12                                                                         map_entries_string,
2022-02-02T17:44:12.1942796Z Feb 02 17:44:12                                                                         str(value))
2022-02-02T17:44:12.1943369Z Feb 02 17:44:12     
2022-02-02T17:44:12.1943761Z Feb 02 17:44:12             def on_timer(self, timestamp, ctx):
2022-02-02T17:44:12.1944178Z Feb 02 17:44:12                 pass
2022-02-02T17:44:12.1944503Z Feb 02 17:44:12     
2022-02-02T17:44:12.1944898Z Feb 02 17:44:12         watermark_strategy = WatermarkStrategy.for_monotonous_timestamps() \
2022-02-02T17:44:12.1945537Z Feb 02 17:44:12             .with_timestamp_assigner(MyTimestampAssigner())
2022-02-02T17:44:12.1946018Z Feb 02 17:44:12         data_stream.assign_timestamps_and_watermarks(watermark_strategy) \
2022-02-02T17:44:12.1946525Z Feb 02 17:44:12             .key_by(lambda x: x[1], key_type=Types.STRING()) \
2022-02-02T17:44:12.1947019Z Feb 02 17:44:12             .process(MyProcessFunction(), output_type=Types.STRING()) \
2022-02-02T17:44:12.1947465Z Feb 02 17:44:12             .add_sink(self.test_sink)
2022-02-02T17:44:12.1948146Z Feb 02 17:44:12         self.env.execute('test time stamp assigner with keyed process function')
2022-02-02T17:44:12.1948637Z Feb 02 17:44:12         results = self.test_sink.get_results()
2022-02-02T17:44:12.1949166Z Feb 02 17:44:12         expected = [""current key: hi, current value state: None, current list state: [], ""
2022-02-02T17:44:12.1949957Z Feb 02 17:44:12                     ""current map state: {}, current value: Row(f0=1, f1='hi', ""
2022-02-02T17:44:12.1950624Z Feb 02 17:44:12                     ""f2='1603708211000')"",
2022-02-02T17:44:12.1951234Z Feb 02 17:44:12                     ""current key: hello, current value state: None, ""
2022-02-02T17:44:12.1951822Z Feb 02 17:44:12                     ""current list state: [], current map state: {}, current value: Row(f0=2,""
2022-02-02T17:44:12.1952596Z Feb 02 17:44:12                     "" f1='hello', f2='1603708224000')"",
2022-02-02T17:44:12.1953292Z Feb 02 17:44:12                     ""current key: hi, current value state: 1, current list state: [1], ""
2022-02-02T17:44:12.1954134Z Feb 02 17:44:12                     ""current map state: {1: hi}, current value: Row(f0=3, f1='hi', ""
2022-02-02T17:44:12.1954799Z Feb 02 17:44:12                     ""f2='1603708226000')"",
2022-02-02T17:44:12.1955331Z Feb 02 17:44:12                     ""current key: hello, current value state: 2, current list state: [2], ""
2022-02-02T17:44:12.1956145Z Feb 02 17:44:12                     ""current map state: {2: hello}, current value: Row(f0=4, f1='hello', ""
2022-02-02T17:44:12.1956826Z Feb 02 17:44:12                     ""f2='1603708289000')"",
2022-02-02T17:44:12.1957362Z Feb 02 17:44:12                     ""current key: hi, current value state: 3, current list state: [1, 3], ""
2022-02-02T17:44:12.1958156Z Feb 02 17:44:12                     ""current map state: {1: hi, 3: hi}, current value: Row(f0=5, f1='hi', ""
2022-02-02T17:44:12.1958845Z Feb 02 17:44:12                     ""f2='1603708291000')"",
2022-02-02T17:44:12.1959382Z Feb 02 17:44:12                     ""current key: hello, current value state: 4, current list state: [2, 4],""
2022-02-02T17:44:12.1960011Z Feb 02 17:44:12                     "" current map state: {2: hello, 4: hello}, current value: Row(f0=6, ""
2022-02-02T17:44:12.1960715Z Feb 02 17:44:12                     ""f1='hello', f2='1603708293000')""]
2022-02-02T17:44:12.1961159Z Feb 02 17:44:12 >       self.assert_equals_sorted(expected, results)
2022-02-02T17:44:12.1961533Z Feb 02 17:44:12 
2022-02-02T17:44:12.1961906Z Feb 02 17:44:12 pyflink/datastream/tests/test_data_stream.py:683: 
2022-02-02T17:44:12.1962464Z Feb 02 17:44:12 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-02T17:44:12.1963186Z Feb 02 17:44:12 pyflink/datastream/tests/test_data_stream.py:62: in assert_equals_sorted
2022-02-02T17:44:12.1963670Z Feb 02 17:44:12     self.assertEqual(expected, actual)
2022-02-02T17:44:12.1964685Z Feb 02 17:44:12 E   AssertionError: Lists differ: [""cur[719 chars]te: {1: hi, 3: hi}, current value: Row(f0=5, f[172 chars]0')""] != [""cur[719 chars]te: {3: hi}, current value: Row(f0=5, f1='hi',[165 chars]0')""]
2022-02-02T17:44:12.1965369Z Feb 02 17:44:12 E   
2022-02-02T17:44:12.1965731Z Feb 02 17:44:12 E   First differing element 4:
2022-02-02T17:44:12.1966428Z Feb 02 17:44:12 E   ""curr[80 chars]te: {1: hi, 3: hi}, current value: Row(f0=5, f[23 chars]00')""
2022-02-02T17:44:12.1967192Z Feb 02 17:44:12 E   ""curr[80 chars]te: {3: hi}, current value: Row(f0=5, f1='hi',[16 chars]00')""
2022-02-02T17:44:12.1967860Z Feb 02 17:44:12 E   
2022-02-02T17:44:12.1968268Z Feb 02 17:44:12 E   Diff is 1211 characters long. Set self.maxDiff to None to see it.
2022-02-02T17:44:12.1968783Z Feb 02 17:44:12 =============================== warnings summary ===============================
2022-02-02T17:44:12.1969374Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_classpaths
2022-02-02T17:44:12.1970541Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/future/standard_library/__init__.py:65: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2022-02-02T17:44:12.1971219Z Feb 02 17:44:12     import imp
2022-02-02T17:44:12.1971530Z Feb 02 17:44:12 
2022-02-02T17:44:12.1972027Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_classpaths
2022-02-02T17:44:12.1973535Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/typehints/typehints.py:693: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
2022-02-02T17:44:12.1974526Z Feb 02 17:44:12     if not isinstance(type_params, collections.Iterable):
2022-02-02T17:44:12.1974930Z Feb 02 17:44:12 
2022-02-02T17:44:12.1975407Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_classpaths
2022-02-02T17:44:12.1976680Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/typehints/typehints.py:532: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
2022-02-02T17:44:12.1977509Z Feb 02 17:44:12     if not isinstance(type_params, (collections.Sequence, set)):
2022-02-02T17:44:12.1977939Z Feb 02 17:44:12 
2022-02-02T17:44:12.1978432Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_python_archive
2022-02-02T17:44:12.1979475Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/_pytest/threadexception.py:75: PytestUnhandledThreadExceptionWarning: Exception in thread read_grpc_client_inputs
2022-02-02T17:44:12.1980064Z Feb 02 17:44:12   
2022-02-02T17:44:12.1980434Z Feb 02 17:44:12   Traceback (most recent call last):
2022-02-02T17:44:12.1981152Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/dev/.conda/envs/3.8/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
2022-02-02T17:44:12.1981642Z Feb 02 17:44:12       self.run()
2022-02-02T17:44:12.1982368Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/dev/.conda/envs/3.8/lib/python3.8/threading.py"", line 870, in run
2022-02-02T17:44:12.1982884Z Feb 02 17:44:12       self._target(*self._args, **self._kwargs)
2022-02-02T17:44:12.1983865Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/data_plane.py"", line 598, in <lambda>
2022-02-02T17:44:12.1984471Z Feb 02 17:44:12       target=lambda: self._read_inputs(elements_iterator),
2022-02-02T17:44:12.1985299Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/data_plane.py"", line 581, in _read_inputs
2022-02-02T17:44:12.1985881Z Feb 02 17:44:12       for elements in elements_iterator:
2022-02-02T17:44:12.1986760Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/grpc/_channel.py"", line 426, in __next__
2022-02-02T17:44:12.1987262Z Feb 02 17:44:12       return self._next()
2022-02-02T17:44:12.1987946Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/grpc/_channel.py"", line 826, in _next
2022-02-02T17:44:12.1988425Z Feb 02 17:44:12       raise self
2022-02-02T17:44:12.1989023Z Feb 02 17:44:12   grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
2022-02-02T17:44:12.1990067Z Feb 02 17:44:12   	status = StatusCode.CANCELLED
2022-02-02T17:44:12.1990653Z Feb 02 17:44:12   	details = ""Multiplexer hanging up""
2022-02-02T17:44:12.1991849Z Feb 02 17:44:12   	debug_error_string = ""{""created"":""@1643823819.576493566"",""description"":""Error received from peer ipv4:127.0.0.1:33091"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1074,""grpc_message"":""Multiplexer hanging up"",""grpc_status"":1}""
2022-02-02T17:44:12.1993432Z Feb 02 17:44:12   >
2022-02-02T17:44:12.1993889Z Feb 02 17:44:12   
2022-02-02T17:44:12.1994521Z Feb 02 17:44:12     warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))
2022-02-02T17:44:12.1995279Z Feb 02 17:44:12 
2022-02-02T17:44:12.1996037Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_python_file
2022-02-02T17:44:12.1997435Z Feb 02 17:44:12   /__w/1/s/flink-python/pyflink/table/table_environment.py:1997: DeprecationWarning: Deprecated in 1.12. Use from_data_stream(DataStream, *Expression) instead.
2022-02-02T17:44:12.1998269Z Feb 02 17:44:12     warnings.warn(
2022-02-02T17:44:12.1998594Z Feb 02 17:44:12 
2022-02-02T17:44:12.1999075Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_execute
2022-02-02T17:44:12.2000008Z Feb 02 17:44:12   /__w/1/s/flink-python/pyflink/table/table_environment.py:538: DeprecationWarning: Deprecated in 1.10. Use create_table instead.
2022-02-02T17:44:12.2000823Z Feb 02 17:44:12     warnings.warn(""Deprecated in 1.10. Use create_table instead."", DeprecationWarning)
2022-02-02T17:44:12.2001522Z Feb 02 17:44:12 
2022-02-02T17:44:12.2002614Z Feb 02 17:44:12 -- Docs: https://docs.pytest.org/en/stable/warnings.html
2022-02-02T17:44:12.2003603Z Feb 02 17:44:12 ============================= slowest 20 durations =============================
2022-02-02T17:44:12.2004618Z Feb 02 17:44:12 10.16s call     pyflink/datastream/tests/test_connectors.py::ConnectorTests::test_stream_file_sink
2022-02-02T17:44:12.2005726Z Feb 02 17:44:12 9.83s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_process_function_with_state
2022-02-02T17:44:12.2006511Z Feb 02 17:44:12 8.79s call     pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state
2022-02-02T17:44:12.2007232Z Feb 02 17:44:12 6.78s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_python_file
2022-02-02T17:44:12.2007961Z Feb 02 17:44:12 5.52s call     pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_execute_and_collect
2022-02-02T17:44:12.2009001Z Feb 02 17:44:12 5.44s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_execute_and_collect
2022-02-02T17:44:12.2010033Z Feb 02 17:44:12 5.26s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_basic_co_operations_with_output_type
2022-02-02T17:44:12.2011152Z Feb 02 17:44:12 5.25s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_basic_co_operations
2022-02-02T17:44:12.2012377Z Feb 02 17:44:12 4.53s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_co_process
2022-02-02T17:44:12.2013701Z Feb 02 17:44:12 4.35s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_set_requirements_with_cached_directory
2022-02-02T17:44:12.2014884Z Feb 02 17:44:12 4.32s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_reduce_with_state
2022-02-02T17:44:12.2015900Z Feb 02 17:44:12 4.26s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_flat_map
2022-02-02T17:44:12.2016970Z Feb 02 17:44:12 4.21s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_co_map
2022-02-02T17:44:12.2018270Z Feb 02 17:44:12 4.06s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_filter
2022-02-02T17:44:12.2019463Z Feb 02 17:44:12 3.90s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_set_requirements_without_cached_directory
2022-02-02T17:44:12.2020715Z Feb 02 17:44:12 3.90s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_aggregating_state
2022-02-02T17:44:12.2021749Z Feb 02 17:44:12 3.87s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_co_flat_map
2022-02-02T17:44:12.2022862Z Feb 02 17:44:12 3.84s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_multi_key_by
2022-02-02T17:44:12.2024078Z Feb 02 17:44:12 3.83s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_time_window
2022-02-02T17:44:12.2024912Z Feb 02 17:44:12 3.83s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_count_window
2022-02-02T17:44:12.2025750Z Feb 02 17:44:12 =========================== short test summary info ============================
2022-02-02T17:44:12.2026370Z Feb 02 17:44:12 FAILED pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state
2022-02-02T17:44:12.2027008Z Feb 02 17:44:12 ======= 1 failed, 154 passed, 1 skipped, 6 warnings in 235.76s (0:03:55) =======
2022-02-02T17:44:12.5428501Z Feb 02 17:44:12 test module /__w/1/s/flink-python/pyflink/datastream failed
2022-02-02T17:44:12.5431151Z Feb 02 17:44:12 ERROR: InvocationError for command /bin/bash ./dev/integration_test.sh (exited with code 1)
2022-02-02T17:44:12.5432097Z Feb 02 17:44:12 py38 finish: run-test  after 999.77 seconds
2022-02-02T17:44:12.5436171Z Feb 02 17:44:12 py38 start: run-test-post 
2022-02-02T17:44:12.5437071Z Feb 02 17:44:12 py38 finish: run-test-post  after 0.00 seconds
2022-02-02T17:44:12.5438162Z Feb 02 17:44:12 ___________________________________ summary ____________________________________
2022-02-02T17:44:12.5453873Z Feb 02 17:44:12 ERROR:   py38: commands failed
2022-02-02T17:44:12.5455066Z Feb 02 17:44:12 cleanup /__w/1/s/flink-python/.tox/.tmp/package/1/apache-flink-1.15.dev0.zip
2022-02-02T17:44:12.6013749Z Feb 02 17:44:12 ============tox checks... [FAILED]============
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30642&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24759",,gaoyunhaii,hxbks2ks,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25967,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 14 06:44:27 UTC 2022,,,,,,,,,,"0|z0z7t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/22 07:31;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23725;;;","07/Feb/22 07:33;gaoyunhaii;Perhaps cc [~hxbks2ks]~;;;","09/Feb/22 07:47;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24773;;;","10/Feb/22 07:27;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31000&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24147;;;","11/Feb/22 09:38;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31190&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=26884;;;","14/Feb/22 06:44;hxbks2ks;Merged into master via c92cda97108c6d6818e0fd6c22dba274884f3479;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test e2e fails on AZP,FLINK-25937,13426360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,trohrmann,trohrmann,03/Feb/22 09:21,11/Feb/22 08:46,13/Jul/23 08:08,11/Feb/22 08:46,1.15.0,,,,,,1.15.0,,,,API / Core,API / DataStream,Runtime / Coordination,Table SQL / API,,0,pull-request-available,test-stability,,"The {{SQL Client end-to-end test}} e2e tests fails on AZP when using the {{AdaptiveScheduler}} because the scheduler expects that the parallelism is set for all vertices:

{code}
Feb 03 03:45:13 org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
Feb 03 03:45:13 	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
Feb 03 03:45:13 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Feb 03 03:45:13 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Feb 03 03:45:13 	at java.lang.Thread.run(Thread.java:748)
Feb 03 03:45:13 Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: The adaptive scheduler expects the parallelism being set for each JobVertex (violated JobVertex: f74b775b58627a33e46b8c155b320255).
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
Feb 03 03:45:13 	... 3 more
Feb 03 03:45:13 Caused by: java.lang.IllegalStateException: The adaptive scheduler expects the parallelism being set for each JobVertex (violated JobVertex: f74b775b58627a33e46b8c155b320255).
Feb 03 03:45:13 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
Feb 03 03:45:13 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.assertPreconditions(AdaptiveScheduler.java:296)
Feb 03 03:45:13 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.<init>(AdaptiveScheduler.java:230)
Feb 03 03:45:13 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerFactory.createInstance(AdaptiveSchedulerFactory.java:122)
Feb 03 03:45:13 	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:115)
Feb 03 03:45:13 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:345)
Feb 03 03:45:13 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:322)
Feb 03 03:45:13 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:106)
Feb 03 03:45:13 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:94)
Feb 03 03:45:13 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
Feb 03 03:45:13 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
Feb 03 03:45:13 	... 3 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30662&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=5782",,dmvk,gaoyunhaii,pltbkd,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25575,,,,,,,FLINK-26006,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 11 08:46:36 UTC 2022,,,,,,,,,,"0|z0z7s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/22 09:21;trohrmann;cc [~dmvk], [~twalthr];;;","03/Feb/22 09:27;trohrmann;Same problem for the {{UpsertKafkaTableITCase}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30662&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=36323;;;","07/Feb/22 07:19;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=5704;;;","07/Feb/22 07:57;gaoyunhaii;UpsertKafkaTableITCase: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=36182;;;","07/Feb/22 12:54;gaoyunhaii;SQL Client end-to-end test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30806&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=5762;;;","07/Feb/22 15:35;gaoyunhaii;UpsertKafkaTableITCase: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30806&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=36037;;;","09/Feb/22 07:45;gaoyunhaii;SQL Client end-to-end test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=5005;;;","09/Feb/22 08:05;gaoyunhaii;SQL Client end-to-end test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30878&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=5805;;;","09/Feb/22 08:09;gaoyunhaii;UpsertKafkaITCase: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30878&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=35621;;;","09/Feb/22 09:55;dmvk;The AdaptiveScheduler assumes that the submitted JobGraph has the parallelism set for each vertex (I'm missing some historical context here, maybe [~rmetzger] could provide an additional insight).

There is a preconditions that guards this contract (https://github.com/apache/flink/commit/da77da365f0f5175be71d8e0f8e9b20ed2515038).

After migration of the table API to new sinks (FLINK-25575), this contract has been violated, but unfortunately we don't run a full test suite against the AS on the PRs.

[~fpaul] Can you please take this over?;;;","09/Feb/22 09:57;dmvk;This will fail on every single test run.;;;","09/Feb/22 09:59;gaoyunhaii;Very thanks [~dmvk] for the investigation! I'll also take a look~;;;","10/Feb/22 02:47;gaoyunhaii;Hi~ [~pltbkd] would also like to have a look at this issue~ I first assigned the issue to him~;;;","10/Feb/22 07:28;pltbkd;The reason has been identified. 

Parallelism of a transformation with default parallelism(-1) is set when transforming, using the default parallelism set in the environment. However, in SinkExpander#expand, the environment parallelism is set to -1 at the entrance, to verify if the parallelism of a expanded transformation is set. The environment parallelism will be restored when exiting the method, but at present the transform is called within this scope. If the parallelism of a sink is not set, the parallelism of the sink transformation and all transformations expanded from it will not be handled, so the JobGraph generated will have vertices with -1 parallelism, causing the assertion failure in AdaptiveScheduler.

We can fix the bug by putting the restoring of the environment parallelism ahead of transforming the sink transformations. The pull request has been created, and has been verified with UpsertKafkaTableITCase.;;;","10/Feb/22 13:25;rmetzger;[~dmvk] The only thing that I can add as additional context is the commit message of the commit you've mentioned: ""This check is necessary since AdaptiveScheduler.calculateDesiredResources expects the parallelism to be set for each JobVertex.""
I think the calculations in calculateDesiredResources will be wrong when a parallelism is negative.;;;","11/Feb/22 08:46;chesnay;master: edcf27a0c0da274f9d3ffb5d1b72e25da5ee9d4e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterTest.testJobMasterAcceptsExcessSlotsWhenJobIsRestarting,FLINK-25925,13426172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,02/Feb/22 11:53,03/Feb/22 08:50,13/Jul/23 08:08,03/Feb/22 08:50,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"The test {{JobMasterTest.testJobMasterAcceptsExcessSlotsWhenJobIsRestarting}} fails on AZP with

{code}
Feb 02 02:49:46 [ERROR]   JobMasterTest.testJobMasterAcceptsExcessSlotsWhenJobIsRestarting:1944 
Feb 02 02:49:46 Expected: is <RUNNING>
Feb 02 02:49:46      but: was <CREATED>
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30598&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9114",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 03 08:50:25 UTC 2022,,,,,,,,,,"0|z0z6m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/22 13:37;trohrmann;The problem is that for the {{AdaptiveScheduler}} the test case fails. For this, the problem is that we don't accept excess slots with the {{AdaptiveScheduler}}. We only accept the same set of resources that we've accepted before.;;;","02/Feb/22 13:56;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30545&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8993;;;","03/Feb/22 08:50;trohrmann;Fixed via 2317ab4f29180951712d3606ec7da156dc00d3ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using upsert-kafka with a flush buffer results in Null Pointer Exception,FLINK-25916,13426085,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,heaje,heaje,01/Feb/22 21:10,12/Apr/23 17:00,13/Jul/23 08:08,12/Apr/23 17:00,1.14.3,1.15.0,,,,,kafka-3.0.0,,,,Connectors / Kafka,Table SQL / Runtime,,,,3,pull-request-available,,,"Flink Version: 1.14.3

upsert-kafka version: 1.14.3

 

I have been trying to buffer output from the upsert-kafka connector using the documented parameters {{sink.buffer-flush.max-rows}} and {{sink.buffer-flush.interval}}

Whenever I attempt to run an INSERT query with buffering, I receive the following error (shortened for brevity):
{code:java}
Caused by: java.lang.NullPointerException
        at org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.flush(ReducingUpsertWriter.java:145) 
        at org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.lambda$registerFlush$3(ReducingUpsertWriter.java:124) 
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1693) 
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1684) 
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) 
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) 
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338) 
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324) 
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) 
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) 
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) 
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) 
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) 
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) 
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) 
        at java.lang.Thread.run(Thread.java:829) [?:?] {code}
 

If I remove the parameters related to flush buffering, then everything works as expected with no problems at all.  For reference, here is the full setup with source, destination, and queries.  Yes, I realize the INSERT could use an overhaul, but that's not the issue at hand :).
{code:java}
CREATE TABLE `source_topic` (
    `timeGMT` INT,
    `eventtime` AS TO_TIMESTAMP(FROM_UNIXTIME(`timeGMT`)),
    `visIdHigh` BIGINT,
    `visIdLow` BIGINT,
    `visIdStr` AS CONCAT(IF(`visIdHigh` IS NULL, '', CAST(`visIdHigh` AS STRING)), IF(`visIdLow` IS NULL, '', CAST(`visIdLow` AS STRING))),
    WATERMARK FOR eventtime AS eventtime - INTERVAL '25' SECONDS
) WITH (
    'connector' = 'kafka',
    'properties.group.id' = 'flink_metrics',
    'properties.bootstrap.servers' = 'brokers.example.com:9093',
    'topic' = 'source_topic',
    'scan.startup.mode' = 'earliest-offset',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://schema.example.com',
    'value.fields-include' = 'EXCEPT_KEY'
);


 CREATE TABLE dest_topic (
    `messageType` VARCHAR,
    `observationID` BIGINT,
    `obsYear` BIGINT,
    `obsMonth` BIGINT,
    `obsDay` BIGINT,
    `obsHour` BIGINT,
    `obsMinute` BIGINT,
    `obsTz` VARCHAR(5),
    `value` BIGINT,
    PRIMARY KEY (observationID, messageType) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'key.format' = 'json',
    'properties.bootstrap.servers' = 'brokers.example.com:9092',
    'sink.buffer-flush.max-rows' = '50000',
    'sink.buffer-flush.interval' = '1000',
    'topic' = 'dest_topic ',
    'value.format' = 'json'
);

INSERT INTO adobenow_metrics
    SELECT `messageType`, `observationID`, obsYear, obsMonth, obsDay, obsHour, obsMinute, obsTz, SUM(`value`) AS `value` FROM (
        SELECT `messageType`, `observationID`, obsYear, obsMonth, obsDay, obsHour, obsMinute, '-0000' AS obsTz, 1 AS `value`, `visIdStr` FROM (
            SELECT
                'visit' AS `messageType`,
                CAST(DATE_FORMAT(window_start, 'yyyyMMddHHmm') AS BIGINT) AS `observationID`,
                year(window_start) AS obsYear,
                month(window_start) AS obsMonth,
                dayofmonth(window_start) AS obsDay,
                hour(window_start) AS obsHour,
                minute(window_start) AS obsMinute,
                '-0000' AS obsTz,
                visIdStr
            FROM TABLE(TUMBLE(TABLE `adobenow_sparkweb`, DESCRIPTOR(`eventtime`), INTERVAL '60' SECONDS))
            WHERE visIdStr IS NOT NULL
            GROUP BY window_start, window_end, visIdStr
        )
        GROUP BY messageType, observationID, obsYear, obsMonth, obsDay, obsHour, obsMinute, `visIdStr`
    )
    GROUP BY messageType, observationID, obsYear, obsMonth, obsDay, obsHour, obsMinute, obsTz;{code}
 

 ","CentOS 7.9 x64

Intel Xeon Gold 6140 CPU",begginghard,chengbing.liu,danderson,fpaul,heaje,jackwangcs,jjimenezMM,martijnvisser,mason6345,mhv,okowr,paul8263,qingyue,slinkydeveloper,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 23:12:09 UTC 2023,,,,,,,,,,"0|z0z62w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/22 12:28;fpaul;[~heaje] I think your problems lays somewhere different. We have a test to ensure that the configurations work [1]. Actually, I suspect the buffering was never triggered if none of the options was set. Flushing of the internal buffer only happens either on a checkpoint or if one of the options you described is used.


 

I am including [~slinkydeveloper] to check the validity of the SQL query that it extracts the timestamp/watermark correctly.

 

[1] https://github.com/apache/flink/blob/98997ea37ba08eae0f9aa6dd34823238097d8e0d/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaTableITCase.java#L160;;;","03/Feb/22 12:50;slinkydeveloper;Everything seems correct to me. [~heaje] are you sure the input {{`timeGMT` INT}} is always non null? NULLs could cause issues to watermark computations;;;","07/Feb/22 23:04;heaje;I can confirm that `timeGMT` is always non null.  As mentioned in the description, everything works completely fine with no problems of any kind as long as I don't use `{{sink.buffer-flush.max-rows`}} and {{`sink.buffer-flush.interval`}}.  The moment I add those configuration options (using any values for them I've tried) I immediately start getting the Null Pointer Exception as mentioned.  

I'm not using checkpoints currently (as I'm still just testing), so flushing wouldn't be related to a checkpoint yet.;;;","08/Feb/22 20:33;heaje;I have done further testing and enabled checkpoints.  Checkpoints function just fine without the config options mentioned related to buffer flushes.  Once I enable the ""buffer-flush"" config options, I still get the exact same NPE as before.;;;","14/Feb/22 14:22;fpaul;Maybe I am missing something here but I am wondering how line 145 in the ReducingUpsertWriter  [1]can ever throw such exception.

 

I see two potential points
 * wrappedContext is null should not be impossible because the variable is initialized as an instance block that should run before the constructor
 * value is null AFAICT the only place we insert in the reducingBuffer is this  [2] where the value is never null

 

 

[1] [https://github.com/apache/flink/blob/98997ea37ba08eae0f9aa6dd34823238097d8e0d/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L145]

[2] https://github.com/apache/flink/blob/98997ea37ba08eae0f9aa6dd34823238097d8e0d/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L109;;;","15/Mar/22 08:33;paul8263;Hi all,

I think one potential point might be the timestamp of one record in reduceBuffer is null. Null value is legal for Long type but if it was unboxed a NPE would be thrown.

The ReducingUpsertWriter::setTimestamp accepts a timestamp with primitive long type.
{code:java}
public void setTimestamp(long timestamp) {
    this.timestamp = timestamp;
} {code}
And I tracked the call stack and finally got SinkWriterOperator.Context::getTimestamp:
{code:java}
@Override
public Long timestamp() {
    if (element.hasTimestamp()
            && element.getTimestamp() != TimestampAssigner.NO_TIMESTAMP) {
        return element.getTimestamp();
    }
    return null;
} {code}
Timestamp could be null if the element(StreamRecord) did not have a timestamp.

Correct me if I am wrong.;;;","15/Mar/22 09:43;fpaul;[~paul8263] yes your analysis is correct but the timestamp could only be null if either the SQL watermark extraction did not work [~slinkydeveloper] ruled that out or the `timeGMT` column has null values and [~heaje] also ruled that out. 

So I am still unsure about the issue.;;;","04/Apr/22 20:07;arvid;This looks like FLINK-24608 to me except this was fixed in 1.14.3. [~heaje] can you double-check if you use a newer version? I also see 1.15.0 in the ticket description, did anyone actually verify it with 1.15.0?;;;","17/May/22 08:23;jjimenezMM;We have been experiencing the same issue with our Flink jobs. 
If we add {color:#172b4d}{{sink.buffer-flush.interval}} & {{sink.buffer-flush.max-rows}} into our SQL Flink jobs (using {{{}upsert-kafka{}}}), they start raising {color}{{NullpointerException}}{color:#172b4d} in versions 1.14.3 & 1.15. We had to rollback to Flink 1.13.2 to make them work again{color};;;","07/Jun/22 07:18;paul8263;Hi [~fpaul],

After further investigation I found that if we did not define water mark in the source table or just insert plain data into an upsert_kafka table, a NPE will be thrown.

But in [~heaje] 's case, I am still not sure why watermark could be null.

I think that throwing a NPE if the timestamp is null might not be a good practice.

In ReducingUpsertWriter#WrappedContext，is it necessary to ensure timestamp must not be null?
{code:java}
@Override
public Long timestamp() {
    checkNotNull(timestamp, ""timestamp must to be set before retrieving it."");
    return timestamp;
} 
{code}
If not, maybe we can allow null value as timestamp in WrappedContext. I would like to fix it.

Looking forward to your suggestions. Thanks.;;;","22/Jul/22 02:01;okowr;Hi team,

We also encountered the same bug and that blocks we stay on 1.13 and cannot move forward to adapt new features in 1.14+. May I know if this issue on the path of fix? ;;;","22/Jul/22 08:50;paul8263;Hi all,

As ProducerRecord allows null timestamp and DynamicKafkaRecordSerializationSchema::serialize method does not do anything with the parameter timestamp, I plan to remove the not-null restriction for ReducingUpsertWriter#WrappedContext.

Correct me if I am wrong.;;;","12/Sep/22 12:48;jjimenezMM;Hi, I see there's a MR to fix this [https://github.com/apache/flink/pull/20343], but it's not being merged, anyone knows why?;;;","14/Sep/22 12:15;mhv;Hi all,

do you know when this MR will be merged? I'm currently facing this issue

Thanks ;;;","05/Oct/22 19:21;martijnvisser;[~jjimenezMM] [~mhv] That's because no maintainers has yet had to opportunity to review it. ;;;","11/Jan/23 08:20;paul8263;Hi all，

Could someone help review the code please? Thanks.;;;","06/Apr/23 23:12;tzulitai;Merged to apache/flink-connector-kafka
 * main: 16172a00211317702f5c057e98fae5ca30f1b7f2
 * v3.0: 2beab3451a57bf3b93de15357e937b3dc697c500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullArgumentException when accessing checkpoint stats on standby JobManager,FLINK-25904,13425932,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,Sergey Nuyanzin,Sergey Nuyanzin,01/Feb/22 09:03,08/Jul/23 08:20,13/Jul/23 08:08,21/Mar/22 17:34,1.14.3,,,,,,1.14.5,1.15.0,,,Runtime / Metrics,,,,,0,pull-request-available,,,"We have a job running on one node
after increasing number of nodes to e.g. 3 on a new nodes job starts failing with 
{noformat}ERROR Unhandled exception. (org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler:260)
 org.apache.commons.math3.exception.NullArgumentException: input array
         at org.apache.commons.math3.util.MathArrays.verifyValues(MathArrays.java:1650) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.commons.math3.stat.descriptive.AbstractUnivariateStatistic.test(AbstractUnivariateStatistic.java:158) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:272) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:241) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics$CommonMetricsSnapshot.getPercentile(DescriptiveStatisticsHistogramStatistics.java:158) >
         at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.getQuantile(DescriptiveStatisticsHistogramStatistics.java:52) ~[flink-dist_2.12-1.14.3.>
         at org.apache.flink.runtime.checkpoint.StatsSummarySnapshot.getQuantile(StatsSummarySnapshot.java:108) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.flink.runtime.rest.messages.checkpoints.StatsSummaryDto.valueOf(StatsSummaryDto.java:81) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.createCheckpointingStatistics(CheckpointingStatisticsHandler.java:129) ~[fli>
         at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:84) ~[flink-dist_2.12-1.14>
         at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:58) ~[flink-dist_2.12-1.14>
         at org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.handleRequest(AbstractAccessExecutionGraphHandler.java:68) ~[flink-dist_2.12-1.14.3>
         at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:87) ~[flink-dist_2.12-1.14.3.ja>
         at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) [?:?]
         at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) [?:?]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
         at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
         at java.lang.Thread.run(Thread.java:829) [?:?]
{noformat}",,maccamlc,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MATH-1642,,,,FLINK-26780,,,FLINK-32535,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 21 13:58:27 UTC 2022,,,,,,,,,,"0|z0z55s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/22 11:41;chesnay;Are you using the adaptive scheduler?;;;","15/Feb/22 07:41;Sergey Nuyanzin;no, we do not use adaptive scheduler

Could it be a reason that we are using a standalone mode and the calls in the REST API don't always go to the leading Job Manager?;;;","18/Mar/22 08:16;maccamlc;I am seeing something similar on 1.14.4. Getting this error

{code}
2022-03-18 08:07:21,281 ERROR org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler [] - Unhandled exception.
org.apache.commons.math3.exception.NullArgumentException: input array
	at org.apache.commons.math3.util.MathArrays.verifyValues(MathArrays.java:1650) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.commons.math3.stat.descriptive.AbstractUnivariateStatistic.test(AbstractUnivariateStatistic.java:158) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:272) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:241) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics$CommonMetricsSnapshot.getPercentile(DescriptiveStatisticsHistogramStatistics.java:158) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.getQuantile(DescriptiveStatisticsHistogramStatistics.java:52) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.checkpoint.StatsSummarySnapshot.getQuantile(StatsSummarySnapshot.java:108) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.rest.messages.checkpoints.StatsSummaryDto.valueOf(StatsSummaryDto.java:81) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.createCheckpointingStatistics(CheckpointingStatisticsHandler.java:129) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:84) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:58) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.handleRequest(AbstractAccessExecutionGraphHandler.java:68) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:87) ~[flink-dist_2.12-1.14.4.jar:1.14.4]
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) [?:1.8.0_322]
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) [?:1.8.0_322]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_322]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_322]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_322]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_322]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_322]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]
{code}

I have 2 Job Managers running on different Nodes, using https://github.com/lyft/flinkk8soperator.

This is occuring on the Job Manager that is not currently the lead.

It doesn't seem to affect operation though, as if I kill the lead JM, this will pick up the job.;;;","18/Mar/22 12:55;chesnay;It's a serialization issue in the {{DescriptiveStatisticsHistogramStatistics}}.;;;","21/Mar/22 13:58;chesnay;master: 9302b313fab8ffb4ee918d1b048585aa03cc4b35
1.15: 559ed4c8e5bf21b525f775d39de767cc87016ed5 
1.14: f44982b719457af56a03f7ae39e9bbf271fa2de4 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update project configuration gradle doc to 7.x version,FLINK-25897,13425807,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,matriv,slinkydeveloper,slinkydeveloper,31/Jan/22 15:57,06/Apr/22 17:26,13/Jul/23 08:08,06/Apr/22 17:26,,,,,,,1.15.0,,,,Documentation,,,,,0,pull-request-available,stale-assigned,,Update the gradle build script and its doc page to 7.x,,dmvk,martijnvisser,slinkydeveloper,,,,,,,,,,,,,,,,,,,,,FLINK-25129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 06 17:26:18 UTC 2022,,,,,,,,,,"0|z0z4eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/22 10:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","04/Apr/22 12:05;dmvk;asf-site: 16607c8e4ec87382b9dfa411c75b6f229f7e0f4f;;;","06/Apr/22 14:51;martijnvisser;master: 9b2d03a2a4e075d9e307644af36bbd888be7f4ed and 986820632ed9d7b14d50c471c9734f5fcb36cd41;;;","06/Apr/22 17:26;martijnvisser;release-1.15: 6588ae7249f7c40ce362bff8d35d3c1220f02c37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceManagerServiceImpl's lifecycle can lead to exceptions,FLINK-25893,13425746,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,trohrmann,trohrmann,31/Jan/22 10:22,21/Feb/22 10:56,13/Jul/23 08:08,21/Feb/22 10:55,1.14.3,1.15.0,,,,,1.14.4,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,"The {{ResourceManagerServiceImpl}} lifecycle can lead to exceptions when calling {{ResourceManagerServiceImpl.deregisterApplication}}. The problem arises when the {{DispatcherResourceManagerComponent}} is shutdown before the {{ResourceManagerServiceImpl}} gains leadership or while it is starting the {{ResourceManager}}.

One problem is that {{deregisterApplication}} returns an exceptionally completed future if there is no leading {{ResourceManager}}.

Another problem is that if there is a leading {{ResourceManager}}, then it can still be the case that it has not been started yet. If this is the case, then [ResourceManagerGateway.deregisterApplication|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManagerServiceImpl.java#L143] will be discarded. The reason for this behaviour is that we create a {{ResourceManager}} in one {{Runnable}} and only start it in another. Due to this there can be the {{deregisterApplication}} call that gets the {{lock}} in between.

I'd suggest to correct the lifecycle and contract of the {{ResourceManagerServiceImpl.deregisterApplication}}.

Please note that due to this problem, the error reporting of this method has been suppressed. See FLINK-25885 for more details.",,gaborgsomogyi,huwh,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23240,FLINK-25885,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 21 03:42:08 UTC 2022,,,,,,,,,,"0|z0z40w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/22 10:23;trohrmann;[~xtsong] could you take a look at this problem. I think you know this code part the best.;;;","03/Feb/22 02:44;xtsong;Hi [~trohrmann],

For problem 2, I think it is indeed a problem. {{ResourceManagerServiceImpl}} should not only check whether there is a leading {{ResourceManager}}, but also make sure the leading {{ResourceManager}} is fully started, before calling the {{ResourceManagerGateway#deregisterApplication}}.

For problem 1, I think it's kind of expected. When there's no leading {{ResourceManager}}, the {{ResourceManagerServiceImpl}} can respond to a `deregisterApplication` call by either ignoring the call or report an exception. When there's a leading {{ResourceManager}} in another process, it is desired that the non-leading process ignores the `deregisterApplication` call. On the other hand, if there is no leading {{ResourceManager}} in any process of the cluster, it would be desired that the failure of `deregisterApplication` is reported. Since {{ResourceManagerServiceImpl}} cannot know whether there's a leading {{ResourceManager}} in another process, I think a false alarm in the non-leading process when there is another leading process is probably better than failing the `deregisterApplication` silently when there's no leading process, as in the latter case Kubernetes / Yarn may unexpectedly bring the master process up again.;;;","03/Feb/22 08:33;trohrmann;Hmm, I think we had a similar discussion before. The problem seems to be that a {{RM}} is needed for the application deregistration. However, the decision whether to deregister or not will be made by another component that might run in a different process. I think this problem should be solved in one way or another. I could see two ideas:

1) Make the (de)registration independent of the {{RM}}.
2) Change the contract of the {{ClusterEntrypoint}} to not deregister the application if the {{RM}} is not running. In this case the process running eventually the {{RM}} should gain leadership and initiate the shut down.;;;","07/Feb/22 03:32;xtsong;I also have the impression of a similar discussion, just couldn't remember whom I discussed with. :P

I'm leaning towards to option 2).

I'm afraid your option 1) would not work. 
- Any component that is responsible for the deregistering would need a leader election. Otherwise, we may accidentally deregister the application from a non-leading master process while there is another leading master process. Thus, we still face the same problem that deregistering is called while no leader is elected.
- It exposes the Kubernetes/Yarn client to either {{ClusterEntrypoint}} or {{Dispatcher}}, which complicates the system.

For option 2), changing the contract means the process will exit with code 0 when there's no leading RM. That should not affect the native Kubernetes & Yarn deployment (neither of them relies on the exit code for process restarting), but will help the standalone Kubernetes deployment (which performs nothing in deregistering and relies on the exit code for restarting).

If we have consensus, I can work on this ticket and make the following changes:
- Makes {{ResourceManagerGateway#deregisterApplication}} wait for the leading RM being fully started.
- Change the contract to not deregister the application if there's no leading RM in the process.;;;","07/Feb/22 10:46;trohrmann;For option 1) I think a leading {{Dispatcher}} would decide that it is now time to shut down and to deregister the application. Then the {{ClusterEntrypoint}} would get the signal and initiate the deregistration.

For option 2): Assuming that eventually a {{Dispatcher}} becomes leader that is running in the same process as the leading {{RM}} which then triggers the shut down, I think this can work. Moreover with FLINK-24038 the problem of a leading RM and {{Dispatcher}} running in different processes should no longer happen.;;;","07/Feb/22 12:25;xtsong;For option 1): Do we have the guarantee that the deregistration only happens when there's a leading \{{Dispatcher}}? I think it can also happen e.g., in the catch block of \{{ClusterEntrypoint#startCluster}}. This may not be a perfect example, as the process will exit with non-zero code anyway. My point is we may still need to face the problem when shutting down the cluster without a leading \{{Dispatcher}}.;;;","21/Feb/22 03:42;xtsong;Fixed via
- master (1.15): cb478fb751dbe28405152707040f9126b5a5269b
- release-1.14: 451c5aa98b516bc7dde2dedfe01a6d3ae8d9c8dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoClassDefFoundError AsyncSSLPrivateKeyMethod in benchmark,FLINK-25891,13425742,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,akalashnikov,akalashnikov,31/Jan/22 10:11,31/Jan/22 16:27,13/Jul/23 08:08,31/Jan/22 16:27,1.15.0,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,"After FLINK-25016(upgrading Netty) StreamNetworkThroughputBenchmark benchmark fails when it tries to connect via SSL with error:

{noformat}
java.lang.NoClassDefFoundError: org/apache/flink/shaded/netty4/io/netty/internal/tcnative/AsyncSSLPrivateKeyMethod
	at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSslX509KeyManagerFactory$OpenSslKeyManagerFactorySpi.engineInit(OpenSslX509KeyManagerFactory.java:129)
	at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:256)
	at org.apache.flink.runtime.net.SSLUtils.getKeyManagerFactory(SSLUtils.java:279)
	at org.apache.flink.runtime.net.SSLUtils.createInternalNettySSLContext(SSLUtils.java:324)
	at org.apache.flink.runtime.net.SSLUtils.createInternalNettySSLContext(SSLUtils.java:303)
	at org.apache.flink.runtime.net.SSLUtils.createInternalClientSSLEngineFactory(SSLUtils.java:119)
	at org.apache.flink.runtime.io.network.netty.NettyConfig.createClientSSLEngineFactory(NettyConfig.java:147)
	at org.apache.flink.runtime.io.network.netty.NettyClient.init(NettyClient.java:115)
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.start(NettyConnectionManager.java:64)
	at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start(NettyShuffleEnvironment.java:329)
	at org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.setUp(StreamNetworkBenchmarkEnvironment.java:133)
	at org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmark.setUp(StreamNetworkThroughputBenchmark.java:108)
	at org.apache.flink.benchmark.StreamNetworkThroughputBenchmarkExecutor$MultiEnvironment.setUp(StreamNetworkThroughputBenchmarkExecutor.java:117)
	at org.apache.flink.benchmark.generated.StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest._jmh_tryInit_f_multienvironment1_1(StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest.java:351)
	at org.apache.flink.benchmark.generated.StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest.networkThroughput_Throughput(StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.netty4.io.netty.internal.tcnative.AsyncSSLPrivateKeyMethod
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 27 more
{noformat}
 ",,akalashnikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25016,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 31 16:27:26 UTC 2022,,,,,,,,,,"0|z0z400:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/22 10:28;akalashnikov;[~chesnay], Maybe do you have an idea how to fix it quickly?;;;","31/Jan/22 16:27;akalashnikov;merged to flink-benchmarks master: 253bc6455201ac49fe1c821bc629c7c79b37c4d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterEntrypointTest.testWorkingDirectoryIsDeletedIfApplicationCompletes  failed on azure,FLINK-25885,13425703,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,gaoyunhaii,gaoyunhaii,31/Jan/22 06:52,03/Feb/22 13:09,13/Jul/23 08:08,03/Feb/22 13:09,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-01-31T05:00:07.3113870Z Jan 31 05:00:07 java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.
2022-01-31T05:00:07.3115008Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-01-31T05:00:07.3115778Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-01-31T05:00:07.3116527Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-01-31T05:00:07.3117267Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-01-31T05:00:07.3118011Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-01-31T05:00:07.3118770Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-01-31T05:00:07.3119608Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:251)
2022-01-31T05:00:07.3120425Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-01-31T05:00:07.3121199Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-01-31T05:00:07.3121957Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-01-31T05:00:07.3122716Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-01-31T05:00:07.3123457Z Jan 31 05:00:07 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)
2022-01-31T05:00:07.3124241Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-01-31T05:00:07.3125106Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-01-31T05:00:07.3126063Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-01-31T05:00:07.3127207Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-01-31T05:00:07.3127982Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-01-31T05:00:07.3128741Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-01-31T05:00:07.3129497Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-01-31T05:00:07.3130385Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)
2022-01-31T05:00:07.3131092Z Jan 31 05:00:07 	at akka.dispatch.OnComplete.internal(Future.scala:299)
2022-01-31T05:00:07.3131695Z Jan 31 05:00:07 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-01-31T05:00:07.3132310Z Jan 31 05:00:07 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-01-31T05:00:07.3132943Z Jan 31 05:00:07 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-01-31T05:00:07.3133577Z Jan 31 05:00:07 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-01-31T05:00:07.3134340Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-01-31T05:00:07.3135149Z Jan 31 05:00:07 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-01-31T05:00:07.3135898Z Jan 31 05:00:07 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-01-31T05:00:07.3136692Z Jan 31 05:00:07 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-01-31T05:00:07.3137454Z Jan 31 05:00:07 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-01-31T05:00:07.3138127Z Jan 31 05:00:07 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-01-31T05:00:07.3138726Z Jan 31 05:00:07 	at akka.actor.ActorRef.tell(ActorRef.scala:131)
2022-01-31T05:00:07.3139391Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.sendErrorIfSender(AkkaRpcActor.java:501)
2022-01-31T05:00:07.3140173Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:173)
2022-01-31T05:00:07.3140882Z Jan 31 05:00:07 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-01-31T05:00:07.3141535Z Jan 31 05:00:07 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-01-31T05:00:07.3142177Z Jan 31 05:00:07 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-01-31T05:00:07.3142822Z Jan 31 05:00:07 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-01-31T05:00:07.3143467Z Jan 31 05:00:07 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-01-31T05:00:07.3144145Z Jan 31 05:00:07 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-01-31T05:00:07.3145019Z Jan 31 05:00:07 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-01-31T05:00:07.3145744Z Jan 31 05:00:07 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-01-31T05:00:07.3146421Z Jan 31 05:00:07 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-01-31T05:00:07.3147053Z Jan 31 05:00:07 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-01-31T05:00:07.3147714Z Jan 31 05:00:07 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-01-31T05:00:07.3148417Z Jan 31 05:00:07 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-01-31T05:00:07.3149072Z Jan 31 05:00:07 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-01-31T05:00:07.3149725Z Jan 31 05:00:07 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-01-31T05:00:07.3231566Z Jan 31 05:00:07 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-01-31T05:00:07.3232417Z Jan 31 05:00:07 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-01-31T05:00:07.3233367Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-01-31T05:00:07.3234208Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-01-31T05:00:07.3234909Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-01-31T05:00:07.3235609Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-01-31T05:00:07.3237023Z Jan 31 05:00:07 Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.
2022-01-31T05:00:07.3238316Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:175)
2022-01-31T05:00:07.3238886Z Jan 31 05:00:07 	... 20 more
2022-01-31T05:00:07.3239220Z Jan 31 05:00:07 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30491&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12987",,gaoyunhaii,nsemmler,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21667,,,,,,,FLINK-25893,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 03 13:09:36 UTC 2022,,,,,,,,,,"0|z0z3s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/22 08:50;trohrmann;I think the problem is that the {{ResourceManagerServiceImpl.deregisterApplication}} does not properly wait for the {{ResourceManager}} to be started. This has been introduced with FLINK-21667.;;;","31/Jan/22 10:24;trohrmann;I think there are some deeper problems with the lifecycle of the {{ResourceManagerServiceImpl}}. I'd suggest to disable the error reporting for the {{ResourceManagerServiceImpl.deregisterApplication}} in order to stabilize the test and to fix the true problem with FLINK-25893.;;;","01/Feb/22 09:55;nsemmler;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30491&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12987;;;","03/Feb/22 13:09;trohrmann;Fixed via 59d2d84d83696d4775b6ff3ec97f8274ff6e371f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The value of DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S is too large ,FLINK-25883,13425685,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dianfu,Karganak,Karganak,30/Jan/22 23:55,27/Apr/22 08:28,13/Jul/23 08:08,14/Feb/22 03:57,,,,,,,1.12.8,1.13.7,1.14.4,1.15.0,,,,,,0,,,,"In [this line|https://github.com/apache/flink/blob/fb38c99a38c63ba8801e765887f955522072615a/flink-python/pyflink/fn_execution/beam/beam_sdk_worker_main.py#L30], the value of DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S is set to 3153600000. This is more than the default value of threading.TIMEOUT_MAX on Windows Python, which is 4294967. Due to this, ""OverflowError: timeout value is too large"" error is produced.

Full traceback:
{code:java}
 File ""G:\PycharmProjects\PyFlink\venv_from_scratch\lib\site-packages\apache_beam\runners\worker\data_plane.py"", line 218, in run
  while not self._finished.wait(next_call - time.time()):
 File ""C:\Python38\lib\threading.py"", line 558, in wait
  signaled = self._cond.wait(timeout)
 File ""C:\Python38\lib\threading.py"", line 306, in wait
  gotit = waiter.acquire(True, timeout)
OverflowError: timeout value is too large{code}","Windows, Python 3.8",dianfu,Karganak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25207,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 14 03:58:24 UTC 2022,,,,,,,,,,"0|z0z3o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 03:57;dianfu;Fixed in
- master via 26bde8bacbc9050ea1b1e2e4c739b8d21623443b
- release-1.14 via 77e65db61aeccb35a6015e74de3f647db796774c
- release-1.13 via f71cbb9a8a6349411a158ee0d9faf5253fa15d35
- release-1.12 via b1e7b892cc9241f568150135b8bcf7bcd9f0c125;;;","14/Feb/22 03:58;dianfu;[~Karganak] Good catch. Have changed the value to 86400 * 30 (30 days).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move states of AbstractAvroBulkFormat into its reader,FLINK-25861,13425302,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,28/Jan/22 03:27,29/Jan/22 02:35,13/Jul/23 08:08,29/Jan/22 02:35,,,,,,,1.15.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,FLINK-24565 ports avro format to {{BulkReaderFormatFactory}}. However the implementation leaves some states into the format factory itself. These states should be in the readers.,,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24565,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jan 29 02:35:02 UTC 2022,,,,,,,,,,"0|z0z1bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/22 02:35;lzljs3620320;master: b2011e59221aa40728748676c3a947f6f7ed3a60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix use of UserDefinedType in from_elements,FLINK-25856,13425157,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,27/Jan/22 12:58,22/Feb/22 08:51,13/Jul/23 08:08,22/Feb/22 01:57,1.14.3,1.15.0,,,,,1.14.4,1.15.0,,,API / Python,,,,,0,pull-request-available,,,"If we define a new UserDefinedType, and use it in `from_elements`, it will failed.

{code:python}
class VectorUDT(UserDefinedType):
    @classmethod
    def sql_type(cls):
        return DataTypes.ROW(
            [
                DataTypes.FIELD(""type"", DataTypes.TINYINT()),
                DataTypes.FIELD(""size"", DataTypes.INT()),
                DataTypes.FIELD(""indices"", DataTypes.ARRAY(DataTypes.INT())),
                DataTypes.FIELD(""values"", DataTypes.ARRAY(DataTypes.DOUBLE())),
            ]
        )

    @classmethod
    def module(cls):
        return ""pyflink.ml.core.linalg""

    def serialize(self, obj):
        if isinstance(obj, SparseVector):
            indices = [int(i) for i in obj._indices]
            values = [float(v) for v in obj._values]
            return 0, obj.size(), indices, values
        elif isinstance(obj, DenseVector):
            values = [float(v) for v in obj._values]
            return 1, None, None, values
        else:
            raise TypeError(""Cannot serialize %r of type %r"".format(obj, type(obj)))
{code}

{code:python}
self.t_env.from_elements([
            (Vectors.dense([1, 2, 3, 4]), 0., 1.),
            (Vectors.dense([2, 2, 3, 4]), 0., 2.),
            (Vectors.dense([3, 2, 3, 4]), 0., 3.),
            (Vectors.dense([4, 2, 3, 4]), 0., 4.),
            (Vectors.dense([5, 2, 3, 4]), 0., 5.),
            (Vectors.dense([11, 2, 3, 4]), 1., 1.),
            (Vectors.dense([12, 2, 3, 4]), 1., 2.),
            (Vectors.dense([13, 2, 3, 4]), 1., 3.),
            (Vectors.dense([14, 2, 3, 4]), 1., 4.),
            (Vectors.dense([15, 2, 3, 4]), 1., 5.),
        ],
            DataTypes.ROW([
                DataTypes.FIELD(""features"", VectorUDT()),
                DataTypes.FIELD(""label"", DataTypes.DOUBLE()),
                DataTypes.FIELD(""weight"", DataTypes.DOUBLE())]))
{code}
",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 22 01:57:03 UTC 2022,,,,,,,,,,"0|z0z0fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 01:57;hxbks2ks;Merged into master via 6056680e148cb32323aabb307ed6f71b3eb3aec4
Merged into release-1.14 via cfc54caf4882a7339b02d06b3c43df64feb02ad5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-171] KDS Sink does not fast fail when invalid configuration supplied,FLINK-25848,13425118,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,CrynetLogistics,dannycranmer,dannycranmer,27/Jan/22 09:31,24/Feb/22 18:54,13/Jul/23 08:08,24/Feb/22 18:54,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,"h4. Description 

KDS sink does not fail job when invalid configuration provided.

h4. Reproduction Steps

- Start a job using an Async Sink implementation, for example KDS
- Specify an invalid credential provider configuration, for example

{code}
CREATE TABLE orders (
  `code` STRING,
  `quantity` BIGINT
) WITH (
  'connector' = 'kinesis',
  'stream' = 'source',
  'aws.credentials.provider' = 'ASSUME_ROLE',
  'aws.region' = 'us-east-1',
  'format' = 'json'
);
{code}


h4. Actual Results

- Sink operator transitions to running, consistently retrying

{code}
2022-01-27 08:29:31,582 WARN  org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter [] - KDS Sink failed to persist 5 entries to KDS
java.util.concurrent.CompletionException: org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.model.StsException: 2 validation errors detected: Value null at 'roleArn' failed to satisfy constraint: Member must not be null; Value null at 'roleSessionName' failed to satisfy constraint: Member must not be null (Service: Sts, Status Code: 400, Request ID: af8f2176-aafa-4230-805b-72d90e418810, Extended Request ID: null)
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331) ~[?:?]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346) ~[?:?]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:870) ~[?:?]
	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883) ~[?:?]
	at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251) ~[?:?]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.kinesis.DefaultKinesisAsyncClient.putRecords(DefaultKinesisAsyncClient.java:2112) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.submitRequestEntries(KinesisDataStreamsSinkWriter.java:122) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.flush(AsyncSinkWriter.java:311) ~[flink-connector-files-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.prepareCommit(AsyncSinkWriter.java:391) ~[flink-connector-files-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.operators.sink.SinkOperator.endInput(SinkOperator.java:192) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:517) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:802) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:751) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:829) [?:?]
{code}

h4. Expected Results

- Job fails fast

h4. Suggested Resolution

- Validate configuration and cancel job when fails (legacy connector does this)",,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25944,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 18:54:43 UTC 2022,,,,,,,,,,"0|z0z06o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 18:54;dannycranmer;Merged into master https://github.com/apache/flink/commit/ec83c087a53cf081cb8d0a01c7d6c310ce755c2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesHighAvailabilityRecoverFromSavepointITCase. testRecoverFromSavepoint failed on azure,FLINK-25847,13425117,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,gaoyunhaii,gaoyunhaii,27/Jan/22 09:30,07/Feb/22 13:27,13/Jul/23 08:08,31/Jan/22 17:49,1.15.0,,,,,,1.15.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,test-stability,,"
{code:java}
2022-01-27T06:08:57.7214748Z Jan 27 06:08:57 [INFO] Running org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase
2022-01-27T06:10:23.2568324Z Jan 27 06:10:23 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 85.553 s <<< FAILURE! - in org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase
2022-01-27T06:10:23.2572289Z Jan 27 06:10:23 [ERROR] org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint  Time elapsed: 84.078 s  <<< ERROR!
2022-01-27T06:10:23.2573945Z Jan 27 06:10:23 java.util.concurrent.TimeoutException
2022-01-27T06:10:23.2574625Z Jan 27 06:10:23 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2022-01-27T06:10:23.2575381Z Jan 27 06:10:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-01-27T06:10:23.2576428Z Jan 27 06:10:23 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:104)
2022-01-27T06:10:23.2578437Z Jan 27 06:10:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-27T06:10:23.2579141Z Jan 27 06:10:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-27T06:10:23.2579893Z Jan 27 06:10:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-27T06:10:23.2594686Z Jan 27 06:10:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-27T06:10:23.2595622Z Jan 27 06:10:23 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-27T06:10:23.2596397Z Jan 27 06:10:23 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-27T06:10:23.2597158Z Jan 27 06:10:23 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-27T06:10:23.2597900Z Jan 27 06:10:23 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-27T06:10:23.2598630Z Jan 27 06:10:23 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-27T06:10:23.2599335Z Jan 27 06:10:23 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-27T06:10:23.2600044Z Jan 27 06:10:23 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-27T06:10:23.2600736Z Jan 27 06:10:23 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-27T06:10:23.2601408Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-27T06:10:23.2602124Z Jan 27 06:10:23 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-27T06:10:23.2602831Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-27T06:10:23.2603531Z Jan 27 06:10:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-27T06:10:23.2604270Z Jan 27 06:10:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-27T06:10:23.2604975Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-27T06:10:23.2605641Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-27T06:10:23.2606313Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-27T06:10:23.2607713Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-27T06:10:23.2608497Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-27T06:10:23.2609049Z Jan 27 06:10:23 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-27T06:10:23.2609623Z Jan 27 06:10:23 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-27T06:10:23.2610165Z Jan 27 06:10:23 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-27T06:10:23.2610700Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-27T06:10:23.2611621Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-27T06:10:23.2612145Z Jan 27 06:10:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-27T06:10:23.2612644Z Jan 27 06:10:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-27T06:10:23.2613196Z Jan 27 06:10:23 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-27T06:10:23.2613967Z Jan 27 06:10:23 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-27T06:10:23.2614595Z Jan 27 06:10:23 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-27T06:10:23.2615259Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-27T06:10:23.2615978Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-27T06:10:23.2616717Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-27T06:10:23.2617485Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-27T06:10:23.2618235Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-27T06:10:23.2618907Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-27T06:10:23.2619515Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-27T06:10:23.2620205Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-27T06:10:23.2620927Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-27T06:10:23.2621627Z Jan 27 06:10:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-27T06:10:23.2622339Z Jan 27 06:10:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-27T06:10:23.2623439Z Jan 27 06:10:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-01-27T06:10:23.2624147Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-27T06:10:23.2624773Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-27T06:10:23.2625353Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-27T06:10:23.2625943Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-01-27T06:10:23.2626383Z Jan 27 06:10:23 
2022-01-27T06:10:23.6317559Z Jan 27 06:10:23 [INFO] 
2022-01-27T06:10:23.6318426Z Jan 27 06:10:23 [INFO] Results:
2022-01-27T06:10:23.6318913Z Jan 27 06:10:23 [INFO] 
2022-01-27T06:10:23.6320466Z Jan 27 06:10:23 [ERROR] Errors: 
2022-01-27T06:10:23.6321907Z Jan 27 06:10:23 [ERROR]   KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint:104 » Timeout
2022-01-27T06:10:23.6322588Z Jan 27 06:10:23 [INFO] 
2022-01-27T06:10:23.6323353Z Jan 27 06:10:23 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30266&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5230",,gaoyunhaii,pnowojski,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24038,,,FLINK-25981,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 02 14:09:05 UTC 2022,,,,,,,,,,"0|z0z06g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/22 07:46;wangyang0918;This might be related with FLINK-24038.

I have run the test successfully more than 100 times with {{high-availability.use-old-ha-services}} set true. And it could be reproduced after few times when using the new multiple component HA mode.;;;","31/Jan/22 10:35;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30494&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5342;;;","31/Jan/22 11:28;trohrmann;I think the problem is a deadlock in the {{DefaultMultipleComponentLeaderElectionService.notifyAllKnownLeaderInformation}}. The problem is that we don't use the {{leadershipOperationExecutor}} to notify the {{LeaderElectionEventHandler}} in the {{notifyAllKnownLeaderInformation}}.;;;","31/Jan/22 17:49;trohrmann;Fixed via 0290715a57b8d243586ab747b0cd2416c8081012;;;","01/Feb/22 11:27;pnowojski;[~trohrmann], could this deadlock be an explanation for this failed build?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30536&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14

(this build didn't have your fix);;;","02/Feb/22 14:09;trohrmann;I don't think so [~pnowojski] because it is not using the K8s HA services.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-171] Async Sink does not gracefully shutdown on Cancel,FLINK-25846,13425116,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,CrynetLogistics,dannycranmer,dannycranmer,27/Jan/22 09:15,24/Feb/22 18:55,13/Jul/23 08:08,24/Feb/22 18:55,,,,,,,1.15.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"h4. Description

Async Sink does not react gracefully to cancellation signal

h4. Reproduction Steps

- Start a job using an Async Sink implementation, for example KDS
- Navigate to Flink Dashboard 
- Click Job > Cancel

h4. Actual Results

- Sink operator stuck in Cancelling, retrying 

{code}
2022-01-27 08:33:40,301 WARN  org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter [] - KDS Sink failed to persist 5 entries to KDS
java.util.concurrent.CompletionException: java.lang.IllegalStateException: Interrupted waiting to refresh the value.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331) ~[?:?]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346) ~[?:?]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:870) ~[?:?]
	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883) ~[?:?]
	at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251) ~[?:?]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.kinesis.DefaultKinesisAsyncClient.putRecords(DefaultKinesisAsyncClient.java:2112) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.submitRequestEntries(KinesisDataStreamsSinkWriter.java:122) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.flush(AsyncSinkWriter.java:311) ~[flink-connector-files-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.prepareCommit(AsyncSinkWriter.java:391) ~[flink-connector-files-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.operators.sink.SinkOperator.endInput(SinkOperator.java:192) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:517) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:802) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:751) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.IllegalStateException: Interrupted waiting to refresh the value.
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.handleInterruptedException(CachedSupplier.java:146) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.refreshCache(CachedSupplier.java:140) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.get(CachedSupplier.java:89) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsCredentialsProvider.resolveCredentials(StsCredentialsProvider.java:91) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsAssumeRoleCredentialsProvider.resolveCredentials(StsAssumeRoleCredentialsProvider.java:41) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.internal.AwsExecutionContextBuilder.resolveCredentials(AwsExecutionContextBuilder.java:165) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.internal.AwsExecutionContextBuilder.invokeInterceptorsAndCreateExecutionContext(AwsExecutionContextBuilder.java:102) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.client.handler.AwsAsyncClientHandler.invokeInterceptorsAndCreateExecutionContext(AwsAsyncClientHandler.java:65) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseAsyncClientHandler.lambda$execute$1(BaseAsyncClientHandler.java:77) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseAsyncClientHandler.measureApiCallSuccess(BaseAsyncClientHandler.java:282) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseAsyncClientHandler.execute(BaseAsyncClientHandler.java:75) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.client.handler.AwsAsyncClientHandler.execute(AwsAsyncClientHandler.java:52) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.kinesis.DefaultKinesisAsyncClient.putRecords(DefaultKinesisAsyncClient.java:2107) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	... 16 more
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireNanos(AbstractQueuedSynchronizer.java:1286) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.tryLock(ReentrantLock.java:424) ~[?:?]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.refreshCache(CachedSupplier.java:126) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.cache.CachedSupplier.get(CachedSupplier.java:89) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsCredentialsProvider.resolveCredentials(StsCredentialsProvider.java:91) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.sts.auth.StsAssumeRoleCredentialsProvider.resolveCredentials(StsAssumeRoleCredentialsProvider.java:41) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.internal.AwsExecutionContextBuilder.resolveCredentials(AwsExecutionContextBuilder.java:165) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.internal.AwsExecutionContextBuilder.invokeInterceptorsAndCreateExecutionContext(AwsExecutionContextBuilder.java:102) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.client.handler.AwsAsyncClientHandler.invokeInterceptorsAndCreateExecutionContext(AwsAsyncClientHandler.java:65) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseAsyncClientHandler.lambda$execute$1(BaseAsyncClientHandler.java:77) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseAsyncClientHandler.measureApiCallSuccess(BaseAsyncClientHandler.java:282) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.handler.BaseAsyncClientHandler.execute(BaseAsyncClientHandler.java:75) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.awscore.client.handler.AwsAsyncClientHandler.execute(AwsAsyncClientHandler.java:52) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.services.kinesis.DefaultKinesisAsyncClient.putRecords(DefaultKinesisAsyncClient.java:2107) ~[blob_p-a167cef2d4088e526c4e209bb2b4e5a83f8ab359-fc0375a6ef7545bc316b770d110d4564:1.15-SNAPSHOT]
	... 16 more
{code}

h4. Expected Results

- Sink operator closes

h4. Suggested Resolution

- Async Sink should treat `InterruptedException` as stop signal",,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25944,,FLINK-24041,,,,,,,FLINK-25944,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 18:55:23 UTC 2022,,,,,,,,,,"0|z0z068:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 18:55;dannycranmer;Merged to master https://github.com/apache/flink/commit/ec83c087a53cf081cb8d0a01c7d6c310ce755c2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_keyed_process_function_with_state of BatchModeDataStreamTests  faild in PyFlink,FLINK-25836,13425074,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,27/Jan/22 04:58,27/Jan/22 06:49,13/Jul/23 08:08,27/Jan/22 06:49,1.15.0,,,,,,1.15.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30264&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901,,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 27 06:49:43 UTC 2022,,,,,,,,,,"0|z0yzww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/22 06:49;hxbks2ks;Merged into master via 13c8d4d4e5a210df63f08507d342bde57d32d690;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkUserCodeClassLoader is not registered as parallel capable,FLINK-25833,13425067,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,akalashnikov,billleecn,billleecn,27/Jan/22 03:38,03/Mar/22 08:28,13/Jul/23 08:08,03/Mar/22 08:28,,,,,,,1.15.0,,,,API / Core,,,,,0,pull-request-available,,,"A classloader can be registered as parallel capable only if all of its super classes are registered as parallel capable[1]. It must be registered to make ChildFirstClassLoader and ParentFirstClassLoader parallel capable.

[1] https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html#registerAsParallelCapable--
",,billleecn,dwysakowicz,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18773,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 03 08:28:09 UTC 2022,,,,,,,,,,"0|z0yzvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/22 12:04;chesnay;How can you verify that a CL is properly regarded as parallel capable?;;;","04/Feb/22 12:09;chesnay;Ah you can just check the return value of {{ClassLoader.registerAsParallelCapable()}}.;;;","03/Mar/22 08:28;dwysakowicz;Fixed in f57a5379ff9c108627d3c511414e7ea71a1a2a2f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential memory leaks in SourceOperator,FLINK-25827,13424920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,26/Jan/22 13:59,04/Feb/22 11:42,13/Jul/23 08:08,04/Feb/22 11:42,1.14.3,1.15.0,,,,,1.14.4,1.15.0,,,Runtime / Task,,,,,0,pull-request-available,,,"{{SourceOperator.SourceOperatorAvailabilityHelper}} is prone to the same type of memory leak as FLINK-25728. Every time new CompletableFuture.any is created:
{code:java}
                currentCombinedFuture =
                        CompletableFuture.anyOf(forcedStopFuture, sourceReaderFuture);
                return currentCombinedFuture;
{code} 
Such future is never garbage collected, because {{forcedStopFuture}} will keep a reference to it. This will eventually lead to a memory leak, or force stopping might take very long time to complete.  ",,mason6345,pedrosbs,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25728,FLINK-24300,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 04 11:42:57 UTC 2022,,,,,,,,,,"0|z0yyyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/22 11:42;pnowojski;merged to master as d95dc864959~4..d95dc864959
merged to release-1.14 as 9365510e8b0~4..9365510e8b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MySqlCatalogITCase fails on azure,FLINK-25825,13424907,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,RocMarshal,roman,roman,26/Jan/22 13:26,15/Feb/22 18:46,13/Jul/23 08:08,15/Feb/22 11:11,1.15.0,,,,,,1.15.0,,,,Connectors / JDBC,Table SQL / API,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30189&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13677
 
{code}
2022-01-26T06:04:42.8019913Z Jan 26 06:04:42 [ERROR] org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.testFullPath  Time elapsed: 2.166 *s  <<< FAILURE!
2022-01-26T06:04:42.8025522Z Jan 26 06:04:42 java.lang.AssertionError: expected: java.util.ArrayList<[+I[1, -1, 1, null, true, null, hello, 2021-0 8-04, 2021-08-04T01:54:16, -1, 1, -1.0, 1.0, enum2, -9.1, 9.1, -1, 1, -1, 1, \{""k1"": ""v1""}, null, col_longtext, null, -1, 1, col_mediumtext, -99, 9 9, -1.0, 1.0, set_ele1, -1, 1, col_text, 10:32:34, 2021-08-04T01:54:16, col_tinytext, -1, 1, null, col_varchar, 2021-08-04T01:54:16.463, 09:33:43,  2021-08-04T01:54:16.463, null], +I[2, -1, 1, null, true, null, hello, 2021-08-04, 2021-08-04T01:53:19, -1, 1, -1.0, 1.0, enum2, -9.1, 9.1, -1, 1,  -1, 1, \{""k1"": ""v1""}, null, col_longtext, null, -1, 1, col_mediumtext, -99, 99, -1.0, 1.0, set_ele1,set_ele12, -1, 1, col_text, 10:32:34, 2021-08- 04T01:53:19, col_tinytext, -1, 1, null, col_varchar, 2021-08-04T01:53:19.098, 09:33:43, 2021-08-04T01:53:19.098, null]]> but was: java.util.ArrayL ist<[+I[1, -1, 1, null, true, null, hello, 2021-08-04, 2021-08-04T01:54:16, -1, 1, -1.0, 1.0, enum2, -9.1, 9.1, -1, 1, -1, 1, \{""k1"": ""v1""}, null,  col_longtext, null, -1, 1, col_mediumtext, -99, 99, -1.0, 1.0, set_ele1, -1, 1, col_text, 10:32:34, 2021-08-04T01:54:16, col_tinytext, -1, 1, null , col_varchar, 2021-08-04T01:54:16.463, 09:33:43, 2021-08-04T01:54:16.463, null], +I[2, -1, 1, null, true, null, hello, 2021-08-04, 2021-08-04T01: 53:19, -1, 1, -1.0, 1.0, enum2, -9.1, 9.1, -1, 1, -1, 1, \{""k1"": ""v1""}, null, col_longtext, null, -1, 1, col_mediumtext, -99, 99, -1.0, 1.0, set_el e1,set_ele12, -1, 1, col_text, 10:32:34, 2021-08-04T01:53:19, col_tinytext, -1, 1, null, col_varchar, 2021-08-04T01:53:19.098, 09:33:43, 2021-08-0 4T01:53:19.098, null]]>
2022-01-26T06:04:42.8029336Z Jan 26 06:04:42    at org.junit.Assert.fail(Assert.java:89)
2022-01-26T06:04:42.8029824Z Jan 26 06:04:42    at org.junit.Assert.failNotEquals(Assert.java:835)
2022-01-26T06:04:42.8030319Z Jan 26 06:04:42    at org.junit.Assert.assertEquals(Assert.java:120)
2022-01-26T06:04:42.8030815Z Jan 26 06:04:42    at org.junit.Assert.assertEquals(Assert.java:146)
2022-01-26T06:04:42.8031419Z Jan 26 06:04:42    at org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.testFullPath(MySqlCatalogITCase.java*:306)
{code}
 
{code}
2022-01-26T06:04:43.2899378Z Jan 26 06:04:43 [ERROR] Failures:
2022-01-26T06:04:43.2907942Z Jan 26 06:04:43 [ERROR]   MySqlCatalogITCase.testFullPath:306 expected: java.util.ArrayList<[+I[1, -1, 1, null, true,
2022-01-26T06:04:43.2914065Z Jan 26 06:04:43 [ERROR]   MySqlCatalogITCase.testGetTable:253 expected:<(
2022-01-26T06:04:43.2983567Z Jan 26 06:04:43 [ERROR]   MySqlCatalogITCase.testSelectToInsert:323 expected: java.util.ArrayList<[+I[1, -1, 1, null,
2022-01-26T06:04:43.2997373Z Jan 26 06:04:43 [ERROR]   MySqlCatalogITCase.testWithoutCatalog:291 expected: java.util.ArrayList<[+I[1, -1, 1, null,
2022-01-26T06:04:43.3010450Z Jan 26 06:04:43 [ERROR]   MySqlCatalogITCase.testWithoutCatalogDB:278 expected: java.util.ArrayList<[+I[1, -1, 1, nul
{code}
 ",,gaoyunhaii,martijnvisser,RocMarshal,roman,trohrmann,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15352,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 15 11:11:05 UTC 2022,,,,,,,,,,"0|z0yyw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/22 13:50;martijnvisser;[~RocMarshal] Any idea why this fails? ;;;","27/Jan/22 07:49;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30261&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13480;;;","28/Jan/22 07:42;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30345&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13700;;;","28/Jan/22 07:44;gaoyunhaii;Perhaps also cc [~twalthr]~ ;;;","29/Jan/22 12:36;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30443&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14319;;;","31/Jan/22 06:43;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30466&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14010;;;","31/Jan/22 06:54;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30491&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14011;;;","02/Feb/22 10:31;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30598&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14726;;;","02/Feb/22 13:55;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30545&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14591;;;","03/Feb/22 09:23;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30662&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307;;;","03/Feb/22 09:24;trohrmann;[~roman], [~twalthr] do you have time looking into this problem?;;;","06/Feb/22 10:44;martijnvisser;[~RocMarshal] Can you please look into this? ;;;","07/Feb/22 05:07;RocMarshal;Sorry for not checking the issue in time.  [~MartijnVisser]
I looked over some examples of error reporting, I found this phenomenon accidental.

And in these cases, the output of all test data meets the expectation. Are there something related with the test framework or test-container? ;;;","07/Feb/22 07:47;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14489;;;","07/Feb/22 07:48;gaoyunhaii;Hi [~RocMarshal] Very thanks for the investigation! Might it because some elements in the list does not implement the `equals` method properly? ;;;","07/Feb/22 08:25;martijnvisser;[~RocMarshal] This is happening when running/compiling on Java 11. I expect that you can also reproduce this locally by running it on Java 11. ;;;","07/Feb/22 12:56;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30806&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14404;;;","07/Feb/22 14:24;RocMarshal;Thanks [~MartijnVisser] [~gaoyunhaii] .

I verified carefully this error which was caused by dependency mariadb-java-client with test scope.

So I plan to use *test-container* to refactor `UnsignedTypeConversionITCase` and remove dependencies about mariadb.

Please let me know what's your opinion.;;;","07/Feb/22 14:48;martijnvisser;[~RocMarshal] Sounds good. Please keep us posted when you think you can open a PR to address this instability. ;;;","08/Feb/22 10:30;RocMarshal;[~gaoyunhaii] [~MartijnVisser] Could you help me to open the switch of jdk11 ci-test for test on the PR branch? Thank you.;;;","08/Feb/22 10:42;martijnvisser;[~RocMarshal] The JDK11 tests only run every night on the master branch in the Flink CI pipeline. Have you tried running the tests locally with JDK11? If so, we should just do the review and see if it indeed resolves the issue. ;;;","08/Feb/22 14:25;RocMarshal;[~MartijnVisser] I have verified it locally on ubuntu 20 with jdk 11.;;;","08/Feb/22 14:46;martijnvisser;I believe [~twalthr] will have a look;;;","09/Feb/22 07:46;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14836;;;","09/Feb/22 08:08;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30878&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14841;;;","10/Feb/22 07:46;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31071&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14836;;;","14/Feb/22 07:22;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14504;;;","14/Feb/22 07:29;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31323&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14906;;;","15/Feb/22 11:11;twalthr;Fixed in master: 52a40180a513dafc36d094456ba4f7f2a6aeadf8;;;",,,,,,,,,,,,,,,,
NetworkBufferPoolTest.testIsAvailableOrNotAfterRequestAndRecycleMultiSegments fails on AZP,FLINK-25819,13424858,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,trohrmann,trohrmann,26/Jan/22 10:00,05/Apr/23 15:27,13/Jul/23 08:08,28/Feb/22 08:04,1.14.3,,,,,,1.14.5,1.15.0,,,Runtime / Network,,,,,0,pull-request-available,stale-critical,test-stability,"The {{NetworkBufferPoolTest.testIsAvailableOrNotAfterRequestAndRecycleMultiSegments}} fails on AZP with:

{code}
Jan 26 07:57:03 [ERROR] testIsAvailableOrNotAfterRequestAndRecycleMultiSegments  Time elapsed: 10.028 s  <<< ERROR!
Jan 26 07:57:03 org.junit.runners.model.TestTimedOutException: test timed out after 10 seconds
Jan 26 07:57:03 	at sun.misc.Unsafe.park(Native Method)
Jan 26 07:57:03 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
Jan 26 07:57:03 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:426)
Jan 26 07:57:03 	at java.util.concurrent.FutureTask.get(FutureTask.java:204)
Jan 26 07:57:03 	at org.junit.internal.runners.statements.FailOnTimeout.getResult(FailOnTimeout.java:167)
Jan 26 07:57:03 	at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:128)
Jan 26 07:57:03 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
Jan 26 07:57:03 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Jan 26 07:57:03 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Jan 26 07:57:03 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 26 07:57:03 	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30187&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7350",,akalashnikov,dwysakowicz,gaoyunhaii,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 09 09:33:08 UTC 2022,,,,,,,,,,"0|z0yyl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/22 12:01;chesnay;When I run this locally the thread requesting buffers crashes with an IOException:

{code}
java.io.IOException: Insufficient number of network buffers: required 5, but only 0 available. The total number of network buffers is currently set to 10 of 128 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.

	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.tryRedistributeBuffers(NetworkBufferPool.java:552)
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestUnpooledMemorySegments(NetworkBufferPool.java:216)
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPoolTest$4.go(NetworkBufferPoolTest.java:634)
	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:67)
{code}

As such it appears as if {{globalPool.requestUnpooledMemorySegments}} isn't blocking at all.;;;","02/Feb/22 08:02;chesnay;Better error reporting added:
master: 90a824c27cb52d3565a2fd22191e6bea42873638;;;","17/Feb/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Feb/22 08:07;akalashnikov;[~chesnay], are you working on this task right now? or have you just changed the reporting error and I can take this task?;;;","25/Feb/22 14:30;dwysakowicz;Fixed in:
* master
** f4e58369e6a26c94067c733ec971e3e0d89cdec4..44da40f1f7cc61cb6b696e520ac5686de6da2191
* 1.14.5
** 7a6c3c1b978a75898ff3f09a715e25e7e9e91690..e4c99b19e9fa876157a98d23b5b5bc498da7fac9;;;","08/Mar/22 06:45;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32654&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=5932] The issue happend also on 1.13;;;","09/Mar/22 07:21;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32697&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=5932

Hi [~dwysakowicz] could you also pick the fix to 1.13~? It seems to reproduce on 1.13 with some probability. ;;;","09/Mar/22 07:52;dwysakowicz;Hey [~akalashnikov], do you mind creating a backport for 1.13?;;;","09/Mar/22 09:33;akalashnikov;[~dwysakowicz] , yes, sure. PR - https://github.com/apache/flink/pull/19018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog keyed state backend would come across NPE during notification,FLINK-25816,13424819,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,26/Jan/22 03:33,11/Feb/22 16:26,13/Jul/23 08:08,28/Jan/22 12:35,1.15.0,,,,,,1.15.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,"Instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30158&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7

{code:java}
Caused by: java.lang.NullPointerException
 	at org.apache.flink.state.changelog.ChangelogKeyedStateBackend.notifyCheckpointAborted(ChangelogKeyedStateBackend.java:536)
 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.notifyCheckpointAborted(StreamOperatorStateHandler.java:298)
 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.notifyCheckpointAborted(AbstractStreamOperator.java:383)
 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointAborted(AbstractUdfStreamOperator.java:132)
 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointAborted(RegularOperatorChain.java:158)
 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:406)
 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:352)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$15(StreamTask.java:1327)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1350)
 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353)
 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317)
 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:802)
 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:751)
 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
 	at java.lang.Thread.run(Thread.java:748)

{code}
",,begginghard,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25902,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 28 12:35:38 UTC 2022,,,,,,,,,,"0|z0yycg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/22 09:17;roman;I think the bug confirms my previous [apprehension|https://github.com/apache/flink/pull/18382#discussion_r788780719] that abortion notification adds risk and complexity while being absolutely not necessary.
 
So I'd propose to just remove them (for the nested state backend).;;;","28/Jan/22 12:35;roman;8c4e9a5540e468e92829be32de41545eab7a8cba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableITCase.testCollectWithClose failed on azure,FLINK-25813,13424812,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,TsReaper,gaoyunhaii,gaoyunhaii,26/Jan/22 02:58,27/Mar/23 07:25,13/Jul/23 08:08,14/Feb/23 02:30,1.15.0,1.16.0,1.17.0,,,,1.16.2,1.17.0,,,Table SQL / Runtime,,,,,0,pull-request-available,stale-assigned,test-stability,"
{code:java}
2022-01-25T08:35:25.3735884Z Jan 25 08:35:25 [ERROR] TableITCase.testCollectWithClose  Time elapsed: 0.377 s  <<< FAILURE!
2022-01-25T08:35:25.3737127Z Jan 25 08:35:25 java.lang.AssertionError: Values should be different. Actual: RUNNING
2022-01-25T08:35:25.3738167Z Jan 25 08:35:25 	at org.junit.Assert.fail(Assert.java:89)
2022-01-25T08:35:25.3739085Z Jan 25 08:35:25 	at org.junit.Assert.failEquals(Assert.java:187)
2022-01-25T08:35:25.3739922Z Jan 25 08:35:25 	at org.junit.Assert.assertNotEquals(Assert.java:163)
2022-01-25T08:35:25.3740846Z Jan 25 08:35:25 	at org.junit.Assert.assertNotEquals(Assert.java:177)
2022-01-25T08:35:25.3742302Z Jan 25 08:35:25 	at org.apache.flink.table.api.TableITCase.testCollectWithClose(TableITCase.scala:135)
2022-01-25T08:35:25.3743327Z Jan 25 08:35:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-25T08:35:25.3744343Z Jan 25 08:35:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-25T08:35:25.3745575Z Jan 25 08:35:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-25T08:35:25.3746840Z Jan 25 08:35:25 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-25T08:35:25.3747922Z Jan 25 08:35:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-25T08:35:25.3749151Z Jan 25 08:35:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-25T08:35:25.3750422Z Jan 25 08:35:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-25T08:35:25.3751820Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-25T08:35:25.3753196Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-25T08:35:25.3754253Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-25T08:35:25.3755441Z Jan 25 08:35:25 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
2022-01-25T08:35:25.3756656Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3757778Z Jan 25 08:35:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-25T08:35:25.3758821Z Jan 25 08:35:25 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-25T08:35:25.3759840Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-25T08:35:25.3760919Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-25T08:35:25.3762249Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-25T08:35:25.3763322Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-25T08:35:25.3764436Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-25T08:35:25.3765907Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-25T08:35:25.3766957Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-25T08:35:25.3768104Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-25T08:35:25.3769128Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-25T08:35:25.3770125Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-25T08:35:25.3771118Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-25T08:35:25.3772264Z Jan 25 08:35:25 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-01-25T08:35:25.3773118Z Jan 25 08:35:25 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-01-25T08:35:25.3774092Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-25T08:35:25.3775056Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-25T08:35:25.3776144Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-25T08:35:25.3777125Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-25T08:35:25.3778190Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-25T08:35:25.3779234Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3780354Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3781583Z Jan 25 08:35:25 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-25T08:35:25.3782721Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-25T08:35:25.3783724Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-25T08:35:25.3784663Z Jan 25 08:35:25 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-25T08:35:25.3785542Z Jan 25 08:35:25 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-25T08:35:25.3786641Z Jan 25 08:35:25 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-25T08:35:25.3787854Z Jan 25 08:35:25 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-25T08:35:25.3789028Z Jan 25 08:35:25 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-25T08:35:25.3790347Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-25T08:35:25.3791934Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-25T08:35:25.3793503Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-25T08:35:25.3794936Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-25T08:35:25.3796384Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-25T08:35:25.3797588Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-25T08:35:25.3798765Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-25T08:35:25.3799989Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-25T08:35:25.3801441Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-25T08:35:25.3802916Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-01-25T08:35:25.3804358Z Jan 25 08:35:25 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-01-25T08:35:25.3805461Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-01-25T08:35:25.3806909Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-25T08:35:25.3808260Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-01-25T08:35:25.3809487Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-25T08:35:25.3810614Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-25T08:35:25.3811838Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-25T08:35:25.3813068Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-01-25T08:35:25.3813890Z Jan 25 08:35:25 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30101&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9693",,gaoyunhaii,hxbks2ks,leonard,mapohl,martijnvisser,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 07:25:31 UTC 2023,,,,,,,,,,"0|z0yyaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/22 03:30;gaoyunhaii;Perhaps cc [~twalthr]~ ;;;","10/Feb/22 10:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Feb/22 10:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Feb/22 02:55;TsReaper;Hi [~gaoyunhaii], thanks for raising this issue.

This test is to ensure that the job is canceled for finished after closing the result iterator. However the actual implementation only sends a signal to cancel the job and returns without waiting. So we should either change the test or change the behavior of the iterator.

I've talked with the creator of this test case [~godfreyhe] and we agree that it'll be better to add a limited number of retries to the test. I'll fix this.;;;","23/Feb/22 08:10;gaoyunhaii;Very thanks [~TsReaper] for the investigation! I assigned the issue to you~;;;","03/Mar/22 07:46;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32459&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10369;;;","03/Apr/22 09:14;mapohl;[https://dev.azure.com/mapohl/flink/_build/results?buildId=926&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&l=10710];;;","06/Jun/22 11:52;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36310&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f another instance.

Hi [~TsReaper] , what's the status of the this issue?;;;","21/Jun/22 11:21;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36981&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10616

[~TsReaper] Any update from your end?;;;","07/Jul/22 09:09;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","12/Jul/22 06:36;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10729

[~TsReaper] [~jark] Could you provide an update?;;;","01/Aug/22 11:51;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38933&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10898;;;","31/Aug/22 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Nov/22 09:08;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43012&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13895;;;","24/Nov/22 07:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43436&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13086;;;","25/Nov/22 07:23;mapohl;[~TsReaper] any update on this issue? The PR seems to hanging around for quite some time?;;;","25/Jan/23 08:31;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12306;;;","25/Jan/23 08:32;mapohl;[~qafro] [~tiwalter] Do we have someone who could have a look at this one?;;;","07/Feb/23 06:23;TsReaper;Hi [~mapohl]. Sorry for this late reply as I've almost forgotten this ticket. I've submitted a pull request and zentol reviewed. I asked him a question but he didn't reply so I guess things just stop there.

This ticket is just about an unstable test and it shouldn't have any impact on the users, so this is not a blocker for releases. If we want to resolve this ticket I'd like someone to help for the review. Thanks.;;;","07/Feb/23 08:56;mapohl;I see, thanks for clarification. Could we update the PR and finalize it? I answered your question in the PR.;;;","09/Feb/23 06:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45907&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10822;;;","10/Feb/23 07:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45979&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10822;;;","14/Feb/23 02:30;TsReaper;master: 3a647d6bec7d0413bcbc71668ed45be57bb4bfe6
release-1.17: bb66b989188c661d9d7847b6a5728697a3a2dcd2
release-1.16: c0d52c09340cb865c01c1590dd2803dab48e097a;;;","14/Feb/23 08:29;mapohl;Thanks for taking care of it [~TsReaper] (y);;;","27/Mar/23 07:25;mapohl;1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47582&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10824

We didn't provide a backport because 1.15. That's ok because 1.15 is deprecated with the release of 1.17.;;;",,,,,,,,,,,,,,,,,,,,
[FLIP-171] Fix generic AsyncSinkWriter retrying requests in reverse order,FLINK-25811,13424746,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,chalixar,chalixar,25/Jan/22 17:23,27/Jan/22 09:18,13/Jul/23 08:08,26/Jan/22 09:12,,,,,,,1.15.0,,,,Connectors / Common,,,,,0,pull-request-available,,,"h2. Motivation

{{AsyncSinkWriter}} retries failed request in reverse order.

*Scope:*
* change order of retry in {{AsyncSinkWriter}} in {{flink-connector-base}} module.

h2. References

More details to be found [https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink]",,chalixar,dannycranmer,mason6345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24041,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 26 09:13:14 UTC 2022,,,,,,,,,,"0|z0yxw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/22 09:13;dannycranmer;Also related to https://github.com/apache/flink/pull/18488/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonTypeInfo property should be valid java identifier,FLINK-25808,13424717,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,25/Jan/22 15:04,28/Jan/22 12:05,13/Jul/23 08:08,28/Jan/22 12:05,1.15.0,,,,,,1.15.0,,,,Documentation,Runtime / REST,,,,0,pull-request-available,,,"Some REST classes use the JsonTypeInfo with the property being named {{@class}}. This causes invalid java code to be generated by swagger.
Rename it to {{clazz}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 28 12:05:24 UTC 2022,,,,,,,,,,"0|z0yxps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/22 12:05;chesnay;master: bf70c8494a3e45c6dc69a3106502eb992cb74b34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated SubtaskExecutionAttemptDetailsInfo contains duplicate property,FLINK-25807,13424700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,25/Jan/22 14:16,28/Jan/22 16:16,13/Jul/23 08:08,28/Jan/22 16:16,1.15.0,,,,,,1.15.0,,,,Documentation,Runtime / REST,,,,0,pull-request-available,,,"Swaggers converts both start-time and start_time (kept for backwards-compatibility) into a startTime property, resulting in 2 properties with the same name in one class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 28 16:16:26 UTC 2022,,,,,,,,,,"0|z0yxm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/22 16:16;chesnay;master: e2f609c92918d669f2a1c0b69184878d691e3097;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix high throughput rate of AsyncSinkWriter for throttled destination,FLINK-25793,13424491,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,CrynetLogistics,CrynetLogistics,24/Jan/22 19:12,10/Mar/22 15:06,13/Jul/23 08:08,24/Feb/22 15:56,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,"h2. Bug:

The Async Sink Base sink is not limiting throughput to the destination and therefore exceeding rate limits

*Cause:*

We are not throttling our requests downstream at all.

We should monitor for requests that have failed with ThroughputExceeded exceptions and reduce the throughput accordingly.",,CrynetLogistics,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25792,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 24 15:56:12 UTC 2022,,,,,,,,,,"0|z0ywcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/22 15:56;dannycranmer;Merged to master https://github.com/apache/flink/commit/f5335664dff304ece318a34a9cf939656ee3a2e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Async Sink Base is too being flushed too frequently resulting in backpressure even when buffer is near empty,FLINK-25792,13424489,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,CrynetLogistics,CrynetLogistics,CrynetLogistics,24/Jan/22 19:06,01/Mar/22 11:22,13/Jul/23 08:08,01/Mar/22 11:22,,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,"h2. Bug:

Async Sink Base is too being flushed too frequently resulting in backpressure even when buffer is near empty

*Cause:*

During a write(), flushIfAble() is called, which checks if the number of buffered elements is greater than a batch size, and if so, insists that the sink flushes immediately, even if the number of inFlightRequests is greater than the maximum allowed number of inFlightRequests, resulting in a yield of the current mailbox thread, and hence blocks.

Notice that this can occur even if the buffer is near empty, so the blocking behaviour is unnecessary and undesirable, since we would like the element to be written to the buffer and no blocking to occur.",,CrynetLogistics,dannycranmer,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25976,,,,,,,,,FLINK-24905,FLINK-24041,,,FLINK-25793,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 01 11:22:29 UTC 2022,,,,,,,,,,"0|z0ywc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/22 11:22;dannycranmer;Merged to master https://github.com/apache/flink/commit/e16ef6bb7ced7c66bd00b7dfe2c7199d7303a54c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraConnectorITCase.testRetrialAndDropTables timeouts on AZP,FLINK-25771,13424291,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,echauchot,trohrmann,trohrmann,24/Jan/22 08:29,29/Mar/22 07:33,13/Jul/23 08:08,15/Mar/22 10:07,1.13.5,1.14.3,1.15.0,,,,1.13.7,1.14.5,1.15.0,,Connectors / Cassandra,,,,,0,pull-request-available,test-stability,,"The test {{CassandraConnectorITCase.testRetrialAndDropTables}} fails on AZP with

{code}
Jan 23 01:02:52 com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /172.17.0.1:59220 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/172.17.0.1] Timed out waiting for server response))
Jan 23 01:02:52 	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
Jan 23 01:02:52 	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:37)
Jan 23 01:02:52 	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
Jan 23 01:02:52 	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)
Jan 23 01:02:52 	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)
Jan 23 01:02:52 	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:39)
Jan 23 01:02:52 	at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.testRetrialAndDropTables(CassandraConnectorITCase.java:554)
Jan 23 01:02:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 23 01:02:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 23 01:02:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 23 01:02:52 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 23 01:02:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jan 23 01:02:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jan 23 01:02:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jan 23 01:02:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jan 23 01:02:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jan 23 01:02:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jan 23 01:02:52 	at org.apache.flink.testutils.junit.RetryRule$RetryOnExceptionStatement.evaluate(RetryRule.java:196)
Jan 23 01:02:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jan 23 01:02:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Jan 23 01:02:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Jan 23 01:02:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Jan 23 01:02:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jan 23 01:02:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jan 23 01:02:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jan 23 01:02:52 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Jan 23 01:02:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Jan 23 01:02:52 	at org.junit.runners.Suite.runChild(Suite.java:128)
Jan 23 01:02:52 	at org.junit.runners.Suite.runChild(Suite.java:27)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jan 23 01:02:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Jan 23 01:02:52 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Jan 23 01:02:52 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Jan 23 01:02:52 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Jan 23 01:02:52 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Jan 23 01:02:52 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Jan 23 01:02:52 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Jan 23 01:02:52 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jan 23 01:02:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jan 23 01:02:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jan 23 01:02:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jan 23 01:02:52 Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /172.17.0.1:59220 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/172.17.0.1] Timed out waiting for server response))
Jan 23 01:02:52 	at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:218)
Jan 23 01:02:52 	at com.datastax.driver.core.RequestHandler.access$1000(RequestHandler.java:43)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29945&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13601",,echauchot,fpaul,gaoyunhaii,mapohl,mzuehlke,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 07:33:47 UTC 2022,,,,,,,,,,"0|z0yv40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/22 08:29;trohrmann;cc [~echauchot].;;;","25/Jan/22 09:28;echauchot;[~trohrmann] thanks for pointing out ! I was busy on another ASF project, I'm taking a look at this ticket now, can you assign me ?;;;","25/Jan/22 11:40;pnowojski;Done [~echauchot]

Another instance:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30050&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d;;;","25/Jan/22 15:26;echauchot;[~pnowojski] It is a different error (not a timeout). PR just submitted for the timeout issue.;;;","26/Jan/22 03:25;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30107&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14558;;;","26/Jan/22 09:37;echauchot;[~gaoyunhaii] thanks for the pointer. This issue should be fixed by the above PR;;;","26/Jan/22 10:34;echauchot;I'm starting to take a look at the error sent by [~pnowojski] ;;;","26/Jan/22 13:35;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30210&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11823 (1.15);;;","26/Jan/22 13:44;echauchot;[~roman]  thanks for pointing out but it is still the same timeout issue fixed by pr linked above;;;","26/Jan/22 13:56;roman;[~echauchot], yes, I know  :)
Additional link provides some hint on failure frequency, and also clarifies which branches are affected (before, the ticket only mentioned 1.13);;;","26/Jan/22 14:39;echauchot;[~roman]  yes indeed, thanks for the additional info;;;","26/Jan/22 15:06;echauchot;Regarding the error linked by [~pnowojski], it is strange as the whole keyspace is dropped in a _BeforeClass_ method and the table noticed in the stacktrace is dropped in an _After_ method. And this after method is executed even in case of retrials through the _Rule._ I'll investigate and update the PR above along with addressing your comments. Thanks.;;;","27/Jan/22 10:53;chesnay;[~echauchot] Maybe the test failed in the _After_ method (e.g., when trying to delete the table), and then the retry kicked in.;;;","27/Jan/22 10:54;echauchot;Extracted the error raised by  [~pnowojski] in [another ticket|https://issues.apache.org/jira/browse/FLINK-25851] because it is different ;;;","27/Jan/22 11:17;echauchot;??Etienne Chauchot Maybe the test failed in the After method (e.g., when trying to delete the table), and then the retry kicked in.??

[~chesnay] thanks. But in that case we should receive a _RuntimeException (QueryExecutionException or NoHostAvailableException)_  thrown by _session.execute()_ in the after method.;;;","31/Jan/22 08:33;fpaul;Merged in master: 3144fae0dc8f3ef4b2ed6a8da4cdff920bcc4128;;;","01/Feb/22 11:31;echauchot;reopen for backports to 1.13 and 1.14;;;","01/Feb/22 12:41;fpaul;Merged in release-1.14: 498ee85f970cb25fb905374cb6c577e9984adc30

Merged in release-1.13: 10a30f6d8585d8cfc7d06ed582bc462b9c07b029;;;","01/Feb/22 13:45;echauchot;thanks [~fpaul] for merging the backports !;;;","14/Feb/22 08:37;gaoyunhaii;Hi~ sorry the issue seems reproduced after the fix is merged : https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31223&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12625
;;;","18/Feb/22 08:30;mapohl;Same here: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31774&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12654

{{3144fae0dc8f3ef4b2ed6a8da4cdff920bcc4128}} was included in the code;;;","21/Feb/22 15:32;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31935&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12590;;;","22/Feb/22 10:09;echauchot;Yes I think I need to check if the connection timeout is included in the _miscellaneous_ timeouts that were raised.;;;","24/Feb/22 07:31;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32098&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12194;;;","28/Feb/22 17:50;gaoyunhaii;Similar issue for CassandraConnectorITCase.testCassandraTupleAtLeastOnceSink https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32244&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12591;;;","02/Mar/22 15:24;mapohl;Same for {{CassandraConnectorITCase.testScalingUp}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32369&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12590;;;","03/Mar/22 07:48;gaoyunhaii;Same for {{{}CassandraConnectorITCase.testScalingUp{}}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32459&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12569;;;","03/Mar/22 14:32;echauchot;It seems to happen with various requests: select, insert, create table. In the last PR I multiplied by 3 the timeout of all requests in the testContainer Cassandra conf. Maybe it is not enough or I can do some conf at the client/driver side. Investigating ...

 ;;;","03/Mar/22 14:48;echauchot;I just raised the Cassandra driver timeouts at the Cluster level. PR available. Let's see on loaded AZP;;;","06/Mar/22 15:51;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32512&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=12755;;;","08/Mar/22 06:35;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32633&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12572;;;","09/Mar/22 11:27;chesnay;Timeouts raised:
master: d0d97cb5c12b1dae6e9b9bcd40161fb67510a93e
1.14: af019cc5dad5811925a51e906b9de8a8d2c7014e 
1.13: 7a0f5ea94d2a80234ff509040caeb4ad39b7306b ;;;","09/Mar/22 13:16;echauchot;Thanks [~chesnay] for merging to master and also for the backports.;;;","10/Mar/22 06:48;mapohl;I'm going to reopen this issue because we experienced the same build failure in [this build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32791&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=14591] once more in {{release-1.14}}:
{code}
Mar 10 02:44:32 [ERROR] Tests run: 20, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 234.114 s <<< FAILURE! - in org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase
Mar 10 02:44:32 [ERROR] testRetrialAndDropTables  Time elapsed: 25.5 s  <<< FAILURE!
Mar 10 02:44:32 org.opentest4j.MultipleFailuresError: 
Mar 10 02:44:32 Multiple Failures (2 failures)
Mar 10 02:44:32 	com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (no host was tried)
Mar 10 02:44:32 	com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /172.17.0.1:50767 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/172.17.0.1] Timed out waiting for server response))
Mar 10 02:44:32 	at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:196)
Mar 10 02:44:32 	at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:226)
Mar 10 02:44:32 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:192)
Mar 10 02:44:32 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:79)
[...]
{code}

I verified that the fix was included in this build:
{code}
$ git log --oneline 32c7067f | grep -m1 af019cc5dad
af019cc5dad [FLINK-25771][cassandra][tests] Raise client timeouts
{code}

May you have another look [~echauchot]?;;;","10/Mar/22 09:26;echauchot;Thanks [~mapohl] for raising this ! Flakiness issues are definitely very hard to solve: we always have to try something, merge to master, wait for a given period to see if it is fixed and iterate over and over again if not :).  I'll think about it and hopefully I'll come with (yet :)) another try.;;;","14/Mar/22 05:42;gaoyunhaii;1.13: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32960&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=13939;;;","14/Mar/22 05:42;gaoyunhaii;1.13: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32960&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13782;;;","15/Mar/22 06:37;gaoyunhaii;1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33010&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12574;;;","15/Mar/22 09:05;echauchot;[~gaoyunhaii] thanks for reporting the failures, they should be fixed by [this PR|https://github.com/apache/flink/pull/19059];;;","15/Mar/22 10:01;chesnay;Cluster load reduced:
master: 3bb6fa31fbeed5319c9526105ffe26c3ae525ed7
1.15: 5e160cc7b59f16484557a4747e6a75b373c56d22 
1.14: e0aae1159ff83cf1717387fae5ced8555084d832 
1.13: 2a1580239b148b8314b61e4482bf19741d177857 ;;;","15/Mar/22 10:24;echauchot;thanks [~chesnay] for the backports too !

Let's see for a week if the flakiness is reduced. Please continue reporting the logs here if you see a failure. In that case we would reopen the ticket.

Thanks guys !;;;","18/Mar/22 14:25;echauchot;I've Monitored the failures and so far there was no cassandra failure since the PR was merged. Let's hope flakiness is fixed.;;;","18/Mar/22 14:38;mapohl;{quote}I've Monitored the failures and so far there was no cassandra failure since the PR was merged. Let's hope flakiness is fixed.{quote}
Thanks for being stubborn on that issue, [~echauchot] ;;;","21/Mar/22 10:26;echauchot;[~mapohl] I took a look this morning, still no flakiness for 6 days. I guess it is fixed.;;;","29/Mar/22 07:33;gaoyunhaii;It perhaps re-occured on 1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33820&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12177

Perhaps it is another case with the same issue?;;;"
Delete file is not correct in MergeTreeWriter,FLINK-25770,13424285,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Jan/22 07:57,25/Jan/22 17:02,13/Jul/23 08:08,25/Jan/22 01:59,,,,,,,table-store-0.1.0,,,,Table Store,,,,,0,pull-request-available,,,"The deletion in MergeTreeWriter.updateCompactResult dose not consider upgrade case, the upgrade file is required by previous snapshot and following snapshot, we should ensure:
1. This file is not the output of upgraded.
2. This file is not the input of upgraded.

Otherwise, the file will be deleted incorrectly.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 25 01:59:39 UTC 2022,,,,,,,,,,"0|z0yv2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/22 01:59;lzljs3620320;master: eb9fdfb05cf68ed641124c6b5fe5164d7fcf927f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCS Filesystem implementation fails on Java 11 tests due to licensing issues,FLINK-25758,13424128,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,galenwarren,martijnvisser,martijnvisser,22/Jan/22 08:27,25/Jan/22 17:09,13/Jul/23 08:08,25/Jan/22 03:29,1.15.0,,,,,,1.15.0,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,"{code}
00:33:45,410 DEBUG org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Dependency io.netty:netty-common:4.1.51.Final is mentioned in NOTICE file /__w/2/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency
00:33:45,411 ERROR org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Could not find dependency javax.annotation:javax.annotation-api:1.3.2 in NOTICE file /__w/2/s/flink-filesystems/flink-gs-fs-hadoop/src/main/resources/META-INF/NOTICE
00:33:45,536 INFO  org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - Checking directory /tmp/flink-validation-deployment with a total of 197 jar files.
00:34:18,554 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - File '/javax/annotation/security/package.html' in jar '/tmp/flink-validation-deployment/org/apache/flink/flink-gs-fs-hadoop/1.15-SNAPSHOT/flink-gs-fs-hadoop-1.15-20220122.001944-1.jar' contains match with forbidden regex 'gnu ?\R?[\s/#]*general ?\R?[\s/#]*public ?\R?[\s/#]*license'.
00:34:18,555 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - File '/javax/annotation/package.html' in jar '/tmp/flink-validation-deployment/org/apache/flink/flink-gs-fs-hadoop/1.15-SNAPSHOT/flink-gs-fs-hadoop-1.15-20220122.001944-1.jar' contains match with forbidden regex 'gnu ?\R?[\s/#]*general ?\R?[\s/#]*public ?\R?[\s/#]*license'.
00:35:46,612 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - Found a total of 3 severe license issues
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29932&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1",,galenwarren,gaoyunhaii,martijnvisser,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25772,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 25 03:29:37 UTC 2022,,,,,,,,,,"0|z0yu3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/22 08:29;martijnvisser;[~galenwarren] Can you have a look at this? The GCS filesystem implementation fails on the Java 11 CI tests (these don't run during the Github PR, but in our daily cron jobs) due to licensing issues. 

You can build it locally for Java 11 by running {{-Djdk11 -Pjava11-target}} as Maven parameters (next to having Java 11 as JDK active of course);;;","22/Jan/22 15:43;galenwarren;Sure, I'll take a look. Is there a way to run the license check locally?;;;","23/Jan/22 17:11;galenwarren;[~MartijnVisser] PR created: https://github.com/apache/flink/pull/18452;;;","23/Jan/22 20:12;galenwarren;PR created: [https://github.com/apache/flink/pull/18452]

 ;;;","24/Jan/22 07:36;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29932&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1&l=26699]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29932&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=a7382ec4-87d2-5a9d-7c53-a2f93e317458&l=27949]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29932&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=dffc2faa-5b48-5b4e-0797-dec1b1f74872&l=27949]

 

 ;;;","25/Jan/22 03:29;xtsong;Fixed via:
- master (1.15): 1a6e3387fdf974a05488f1bc6c7293fb6e32cb0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testStartFromGroupOffsetsLatest fails on AZP,FLINK-25753,13424008,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ruanhang1993,trohrmann,trohrmann,21/Jan/22 12:51,01/Feb/22 09:50,13/Jul/23 08:08,28/Jan/22 12:58,1.15.0,,,,,,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,"The test {{KafkaTableITCase.testStartFromGroupOffsetsLatest}} fails on AZP with

{code}
2022-01-21T08:48:26.7044016Z Jan 21 08:48:26 [ERROR] KafkaTableITCase.testStartFromGroupOffsetsLatest  Time elapsed: 5.308 s  <<< ERROR!
2022-01-21T08:48:26.7044630Z Jan 21 08:48:26 java.util.concurrent.TimeoutException: Can not get the expected result.
2022-01-21T08:48:26.7047268Z Jan 21 08:48:26 	at org.apache.flink.core.testutils.CommonTestUtils.waitUtil(CommonTestUtils.java:214)
2022-01-21T08:48:26.7048108Z Jan 21 08:48:26 	at org.apache.flink.core.testutils.CommonTestUtils.waitUtil(CommonTestUtils.java:230)
2022-01-21T08:48:26.7048836Z Jan 21 08:48:26 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
2022-01-21T08:48:26.7049632Z Jan 21 08:48:26 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testStartFromGroupOffsets(KafkaTableITCase.java:942)
2022-01-21T08:48:26.7050427Z Jan 21 08:48:26 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testStartFromGroupOffsetsLatest(KafkaTableITCase.java:831)
2022-01-21T08:48:26.7051077Z Jan 21 08:48:26 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T08:48:26.7051638Z Jan 21 08:48:26 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T08:48:26.7052284Z Jan 21 08:48:26 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T08:48:26.7053008Z Jan 21 08:48:26 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T08:48:26.7053681Z Jan 21 08:48:26 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T08:48:26.7054328Z Jan 21 08:48:26 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T08:48:26.7054976Z Jan 21 08:48:26 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T08:48:26.7055621Z Jan 21 08:48:26 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-21T08:48:26.7056251Z Jan 21 08:48:26 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T08:48:26.7056904Z Jan 21 08:48:26 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T08:48:26.7057520Z Jan 21 08:48:26 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T08:48:26.7058445Z Jan 21 08:48:26 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T08:48:26.7059155Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T08:48:26.7059779Z Jan 21 08:48:26 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T08:48:26.7060382Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T08:48:26.7061036Z Jan 21 08:48:26 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T08:48:26.7061670Z Jan 21 08:48:26 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T08:48:26.7062369Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T08:48:26.7063456Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T08:48:26.7064218Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T08:48:26.7064805Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T08:48:26.7065375Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T08:48:26.7065919Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T08:48:26.7066437Z Jan 21 08:48:26 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-01-21T08:48:26.7066934Z Jan 21 08:48:26 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-01-21T08:48:26.7067448Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T08:48:26.7067979Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T08:48:26.7068601Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T08:48:26.7069167Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T08:48:26.7069745Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T08:48:26.7070398Z Jan 21 08:48:26 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-01-21T08:48:26.7071065Z Jan 21 08:48:26 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T08:48:26.7071644Z Jan 21 08:48:26 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T08:48:26.7072328Z Jan 21 08:48:26 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T08:48:26.7072930Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T08:48:26.7073561Z Jan 21 08:48:26 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T08:48:26.7074080Z Jan 21 08:48:26 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T08:48:26.7074580Z Jan 21 08:48:26 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T08:48:26.7075162Z Jan 21 08:48:26 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T08:48:26.7075811Z Jan 21 08:48:26 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T08:48:26.7076442Z Jan 21 08:48:26 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T08:48:26.7077116Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T08:48:26.7077973Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T08:48:26.7078904Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T08:48:26.7079693Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T08:48:26.7080712Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T08:48:26.7081383Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T08:48:26.7082019Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T08:48:26.7082949Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T08:48:26.7083840Z Jan 21 08:48:26 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T08:48:26.7084561Z Jan 21 08:48:26 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T08:48:26.7085276Z Jan 21 08:48:26 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T08:48:26.7086184Z Jan 21 08:48:26 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T08:48:26.7086873Z Jan 21 08:48:26 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T08:48:26.7087545Z Jan 21 08:48:26 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T08:48:26.7088159Z Jan 21 08:48:26 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-21T08:48:26.7088755Z Jan 21 08:48:26 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29867&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35254",,fpaul,gaoyunhaii,mapohl,nsemmler,roman,ruanhang1993,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 01 09:50:52 UTC 2022,,,,,,,,,,"0|z0ytd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/22 12:51;trohrmann;cc [~fpaul].;;;","22/Jan/22 07:02;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=618&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013;;;","24/Jan/22 21:05;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30040&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35167;;;","25/Jan/22 10:57;fpaul;[~ruanhang1993] can you have a look at this because it seems to be caused by the new feature you have added?;;;","25/Jan/22 11:22;ruanhang1993;Yes, I will take a look at this error.;;;","26/Jan/22 06:35;ruanhang1993;This test contains 3 phases:
 # insert data
 # flink job starts to read and set to latest offsets
 # add new data and check

All these three phases are submitted by `executeSql`.

But it seems like the third phase will finish before the set to latest offsets operation in the second phase. It causes that the new data will not be read. 

So I need add some sleep time before adding new data. I have tried to use `CommonTestUtils.waitForAllTaskRunning`, but it does not work.;;;","26/Jan/22 13:36;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30210&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35581;;;","27/Jan/22 07:45;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30261&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30261&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=35569;;;","27/Jan/22 07:48;gaoyunhaii;Hi [~ruanhang1993] ~ If possible we'd better not use sleep since it could not ensure the order of events happen~ Is it possible to use some-kind of in-memory coordination if possible?;;;","27/Jan/22 10:20;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30274&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35107;;;","28/Jan/22 06:23;ruanhang1993;Thanks, [~gaoyunhaii] ~ Using sleep is not a good idea. I have changed this code to stay watching the committed offset in the kafka broker. And we should add new data after all the offset committed to the kafka.;;;","28/Jan/22 07:45;gaoyunhaii;Very thanks [~ruanhang1993] for the update! 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30347&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35616;;;","28/Jan/22 11:50;nsemmler;https://dev.azure.com/NiklasFlink/NiklasFlink/_build/results?buildId=43&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013;;;","28/Jan/22 12:58;fpaul;Fixed in master: 567440115bcacb5aceaf3304e486281c7da8c14f;;;","01/Feb/22 09:50;nsemmler;https://dev.azure.com/NiklasFlink/NiklasFlink/_build/results?buildId=43&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOSecuredITCase.testDetachedMode fails on AZP,FLINK-25749,13423956,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,21/Jan/22 09:10,24/Jan/22 08:53,13/Jul/23 08:08,24/Jan/22 08:53,1.15.0,,,,,,,,,,Deployment / YARN,,,,,0,test-stability,,,"The test {{YARNSessionFIFOSecuredITCase.testDetachedMode}} fails on AZP:

{code}
2022-01-21T03:28:18.3712993Z Jan 21 03:28:18 java.lang.AssertionError: 
2022-01-21T03:28:18.3715115Z Jan 21 03:28:18 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1642735639007_0002/container_1642735639007_0002_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2022-01-21T03:28:18.3716389Z Jan 21 03:28:18 [
2022-01-21T03:28:18.3717531Z Jan 21 03:28:18 2022-01-21 03:27:56,921 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2022-01-21T03:28:18.3720496Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@11c5f741db81:37697/user/rpc/dispatcher_0.
2022-01-21T03:28:18.3722401Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Interrupted while waiting for queue
2022-01-21T03:28:18.3723661Z Jan 21 03:28:18 java.lang.InterruptedException: null
2022-01-21T03:28:18.3724529Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) ~[?:1.8.0_292]
2022-01-21T03:28:18.3725450Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048) ~[?:1.8.0_292]
2022-01-21T03:28:18.3726239Z Jan 21 03:28:18 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) ~[?:1.8.0_292]
2022-01-21T03:28:18.3727618Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:323) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3729147Z Jan 21 03:28:18 2022-01-21 03:27:56,927 WARN  org.apache.hadoop.ipc.Client                                 [] - Failed to connect to server: 11c5f741db81/172.25.0.2:39121: retries get failed due to exceeded maximum allowed retries number: 0
2022-01-21T03:28:18.3730293Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3730834Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3731499Z Jan 21 03:28:18 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_292]
2022-01-21T03:28:18.3732203Z Jan 21 03:28:18 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658) ~[?:1.8.0_292]
2022-01-21T03:28:18.3733478Z Jan 21 03:28:18 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3734470Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3735432Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3736414Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3737734Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3738853Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3739752Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1381) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3740638Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3741589Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3742621Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3743549Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3744684Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3745594Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3746221Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
2022-01-21T03:28:18.3746937Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
2022-01-21T03:28:18.3747615Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
2022-01-21T03:28:18.3748595Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3749706Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3750820Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3751915Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3753193Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3753988Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy52.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3754973Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainerInternal(NMClientImpl.java:316) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3756059Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainer(NMClientImpl.java:271) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3757457Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:541) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3758840Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:532) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3760149Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3761466Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3762578Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3763987Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3765348Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer.handle(NMClientAsyncImpl.java:617) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3766590Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor.run(NMClientAsyncImpl.java:676) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3767443Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
2022-01-21T03:28:18.3768153Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
2022-01-21T03:28:18.3768778Z Jan 21 03:28:18 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
2022-01-21T03:28:18.3769835Z Jan 21 03:28:18 2022-01-21 03:27:56,930 WARN  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Error while calling YARN Node Manager to stop container container_1642735639007_0002_01_000002.
2022-01-21T03:28:18.3770910Z Jan 21 03:28:18 java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""11c5f741db81/172.25.0.2""; destination host is: ""11c5f741db81"":39121; 
2022-01-21T03:28:18.3772073Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3773188Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3774186Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3775072Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3776041Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3777094Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3777769Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) ~[?:?]
2022-01-21T03:28:18.3778869Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) ~[hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3779720Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3780182Z Jan 21 03:28:18 ]
2022-01-21T03:28:18.3780584Z Jan 21 03:28:18 	at org.junit.Assert.fail(Assert.java:89)
2022-01-21T03:28:18.3781169Z Jan 21 03:28:18 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:591)
2022-01-21T03:28:18.3782025Z Jan 21 03:28:18 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:82)
2022-01-21T03:28:18.3782677Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T03:28:18.3783557Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T03:28:18.3784250Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T03:28:18.3784878Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T03:28:18.3785487Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T03:28:18.3786168Z Jan 21 03:28:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T03:28:18.3786840Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T03:28:18.3787505Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-01-21T03:28:18.3788173Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-01-21T03:28:18.3788909Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3789557Z Jan 21 03:28:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T03:28:18.3790179Z Jan 21 03:28:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T03:28:18.3790755Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3791399Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T03:28:18.3792038Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T03:28:18.3792667Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T03:28:18.3793646Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T03:28:18.3794282Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T03:28:18.3794889Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T03:28:18.3795483Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T03:28:18.3796096Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T03:28:18.3796697Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T03:28:18.3797324Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T03:28:18.3797967Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T03:28:18.3798600Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3799223Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3799806Z Jan 21 03:28:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T03:28:18.3800367Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3800963Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T03:28:18.3801528Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T03:28:18.3802074Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T03:28:18.3802689Z Jan 21 03:28:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T03:28:18.3803675Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T03:28:18.3804487Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T03:28:18.3805205Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T03:28:18.3805990Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T03:28:18.3806794Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T03:28:18.3807616Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T03:28:18.3808420Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T03:28:18.3809106Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T03:28:18.3809759Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T03:28:18.3810463Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T03:28:18.3835763Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T03:28:18.3836503Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T03:28:18.3837248Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T03:28:18.3838212Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T03:28:18.3838953Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T03:28:18.3839649Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T03:28:18.3840279Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-21T03:28:18.3840921Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

The test {{YARNSessionFIFOSecuredITCase.testQueryCluster}} failed with:

{code}
2022-01-21T03:28:18.3842564Z Jan 21 03:28:18 java.lang.AssertionError: 
2022-01-21T03:28:18.3844597Z Jan 21 03:28:18 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1642735639007_0002/container_1642735639007_0002_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2022-01-21T03:28:18.3845483Z Jan 21 03:28:18 [
2022-01-21T03:28:18.3846666Z Jan 21 03:28:18 2022-01-21 03:27:56,921 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2022-01-21T03:28:18.3848108Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@11c5f741db81:37697/user/rpc/dispatcher_0.
2022-01-21T03:28:18.3849225Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Interrupted while waiting for queue
2022-01-21T03:28:18.3849824Z Jan 21 03:28:18 java.lang.InterruptedException: null
2022-01-21T03:28:18.3850480Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) ~[?:1.8.0_292]
2022-01-21T03:28:18.3851311Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048) ~[?:1.8.0_292]
2022-01-21T03:28:18.3852206Z Jan 21 03:28:18 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) ~[?:1.8.0_292]
2022-01-21T03:28:18.3853638Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:323) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3855047Z Jan 21 03:28:18 2022-01-21 03:27:56,927 WARN  org.apache.hadoop.ipc.Client                                 [] - Failed to connect to server: 11c5f741db81/172.25.0.2:39121: retries get failed due to exceeded maximum allowed retries number: 0
2022-01-21T03:28:18.3855925Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3856444Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3857075Z Jan 21 03:28:18 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_292]
2022-01-21T03:28:18.3857778Z Jan 21 03:28:18 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658) ~[?:1.8.0_292]
2022-01-21T03:28:18.3858858Z Jan 21 03:28:18 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3859778Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3860702Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3861692Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3862939Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3863944Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3864820Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1381) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3865660Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3866604Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3867586Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3868233Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3869278Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3870212Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3870850Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
2022-01-21T03:28:18.3871580Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
2022-01-21T03:28:18.3872230Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
2022-01-21T03:28:18.3873375Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3874477Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3875544Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3876609Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3877809Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3878470Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy52.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3879381Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainerInternal(NMClientImpl.java:316) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3880442Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainer(NMClientImpl.java:271) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3881623Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:541) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3883111Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:532) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3884553Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3885610Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3886619Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3887692Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3888827Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer.handle(NMClientAsyncImpl.java:617) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3890014Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor.run(NMClientAsyncImpl.java:676) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3890841Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
2022-01-21T03:28:18.3891528Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
2022-01-21T03:28:18.3892129Z Jan 21 03:28:18 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
2022-01-21T03:28:18.3893275Z Jan 21 03:28:18 2022-01-21 03:27:56,930 WARN  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Error while calling YARN Node Manager to stop container container_1642735639007_0002_01_000002.
2022-01-21T03:28:18.3894308Z Jan 21 03:28:18 java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""11c5f741db81/172.25.0.2""; destination host is: ""11c5f741db81"":39121; 
2022-01-21T03:28:18.3895436Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3896356Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3897214Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3898060Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3898994Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3899987Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3900639Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) ~[?:?]
2022-01-21T03:28:18.3901798Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) ~[hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3902618Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3903137Z Jan 21 03:28:18 ]
2022-01-21T03:28:18.3903592Z Jan 21 03:28:18 	at org.junit.Assert.fail(Assert.java:89)
2022-01-21T03:28:18.3904183Z Jan 21 03:28:18 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:591)
2022-01-21T03:28:18.3904901Z Jan 21 03:28:18 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:82)
2022-01-21T03:28:18.3905526Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T03:28:18.3906105Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T03:28:18.3906769Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T03:28:18.3907469Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T03:28:18.3908059Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T03:28:18.3908700Z Jan 21 03:28:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T03:28:18.3909350Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T03:28:18.3909993Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-01-21T03:28:18.3910632Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-01-21T03:28:18.3911243Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3911869Z Jan 21 03:28:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T03:28:18.3912465Z Jan 21 03:28:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T03:28:18.3913092Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3913775Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T03:28:18.3914387Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T03:28:18.3914995Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T03:28:18.3915645Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T03:28:18.3916252Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T03:28:18.3916823Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T03:28:18.3917405Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T03:28:18.3917976Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T03:28:18.3918554Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T03:28:18.3919152Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T03:28:18.3919778Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T03:28:18.3920386Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3920980Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3921545Z Jan 21 03:28:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T03:28:18.3922166Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3922727Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T03:28:18.3923398Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T03:28:18.3923921Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T03:28:18.3924505Z Jan 21 03:28:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T03:28:18.3925163Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T03:28:18.3926090Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T03:28:18.3926927Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T03:28:18.3927700Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T03:28:18.3928487Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T03:28:18.3929388Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T03:28:18.3930162Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T03:28:18.3930858Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T03:28:18.3931520Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T03:28:18.3932245Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T03:28:18.3933105Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T03:28:18.3933889Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T03:28:18.3934645Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T03:28:18.3935380Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T03:28:18.3936073Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T03:28:18.3936725Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T03:28:18.3937348Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29841&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461",,dmvk,trohrmann,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 21 18:06:15 UTC 2022,,,,,,,,,,"0|z0yt1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/22 09:13;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29841&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","21/Jan/22 12:52;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29867&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30719;;;","21/Jan/22 12:54;trohrmann;Here the {{YARNSessionFIFOITCase.testDetachedMode}} failed but it is probably the same reason.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29871&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30735;;;","21/Jan/22 14:53;zuston;All failure above looks lost connection with mini-yarn, maybe due to unstable network.

Maybe we could set config of ipc client to solve, like as follows:
{code:java}
yarnClusterConf.setInt(""ipc.client.connection.maxidletime"", 1000);
yarnClusterConf.setInt(""ipc.client.connect.max.retries"", 3);
yarnClusterConf.setInt(""ipc.client.connect.retry.interval"", 10);
yarnClusterConf.setInt(""ipc.client.connect.timeout"", 1000);
yarnClusterConf.setInt(""ipc.client.connect.max.retries.on.timeouts"", 3);
{code}
 

[~trohrmann] Do you think so? Maybe I can take over this ticket to improve test stability.;;;","21/Jan/22 17:51;trohrmann;[~dmvk] raised the suspicion that the instability could be caused by https://github.com/apache/flink/commit/dd6069fabf8a7ff65fbd9ff8dd7b0c47f492288f#diff-5ff30e09fc23978573250c9d95969a549be12648c085bd581696cf0b84da3a0b. Let me quickly double check whether I can reproduce it. Since I am responsible for this change, let me first try to clean up my mess.;;;","21/Jan/22 17:56;dmvk;Merging https://github.com/apache/flink/pull/18446 after the CI passes should fix the issue;;;","21/Jan/22 18:06;trohrmann;The problem is indeed caused by https://github.com/apache/flink/commit/dd6069fabf8a7ff65fbd9ff8dd7b0c47f492288f#diff-5ff30e09fc23978573250c9d95969a549be12648c085bd581696cf0b84da3a0b because due to the introduce shut down hook, it can happen that the TM deregisters from the RM which will queue up an operation in the {{NMClientAsync}}. Now if the RM stops and closes the {{NMClientAsync}} this can lead to exceptions that are logged. Luckily, https://github.com/apache/flink/pull/18169 will solve this problem properly ([~dmvk] correct me if I have told incorrect things).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website misses some Repositories,FLINK-25748,13423955,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,knaufk,knaufk,knaufk,21/Jan/22 09:10,21/Jan/22 21:49,13/Jul/23 08:08,21/Jan/22 21:49,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,"https://flink.apache.org/community.html should list 

* https://github.com/apache/flink-table-store
* https://github.com/apache/flink-ml
* https://github.com/apache/flink-benchmarks
* https://github.com/apache/flink-statefun-playground
* https://github.com/apache/flink-training
* https://github.com/apache/flink-playgrounds
* https://github.com/apache/flink-jira-bot
* https://github.com/apache/flink-connectors

As repositories of the project.",,knaufk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 21 09:19:29 UTC 2022,,,,,,,,,,"0|z0yt1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/22 09:16;chesnay;I'm wondering if we should just link to https://gitbox.apache.org/repos/asf#flink to avoid the maintenance overhead.;;;","21/Jan/22 09:18;chesnay;Especially since we're gonna be adding a whole bunch of additional repos for the connectors soon...;;;","21/Jan/22 09:19;knaufk;Good point. I'd say we list the main repository and link to gitbox for the full list. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dispatcher#requestMultipleJobDetails returns non-serialiable collection,FLINK-25732,13423747,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,20/Jan/22 12:35,26/Mar/22 10:16,13/Jul/23 08:08,24/Jan/22 10:02,1.13.6,1.14.3,1.15.0,,,,1.13.6,1.14.4,1.15.0,,Runtime / Coordination,,,,,0,pull-request-available,,,HashMap#values() returns a non-serializable collection.,,gvauvert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26558,FLINK-25837,FLINK-26869,,,FLINK-20195,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 25 10:41:52 UTC 2022,,,,,,,,,,"0|z0yrr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/22 10:02;chesnay;master: edcf814ed2b4007969cfe72393c09d96a092a40d
1.14: e358ac67d2d619fee0c12bf875c17728224dd2a5
1.13: 1152293be4e2ce24874caedfef51f1a168cbf86a;;;","25/Mar/22 10:41;gvauvert;This issue is impacting all deployments with 2 JobManagers or more (HA mode), because in this case serialization is used (well, depending on the JobManager who is responding, the Leader or a Follower).
It prevents:
 * usage of Flink UI
 * usage of Flink command ""flink.sh list""
 * usage of Flink REST API ""/jobs/overview"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential memory leaks in StreamMultipleInputProcessor,FLINK-25728,13423716,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wpc009,wpc009,wpc009,20/Jan/22 10:25,31/Jan/22 16:36,13/Jul/23 08:08,31/Jan/22 16:36,1.12.5,1.13.5,1.14.2,1.15.0,,,1.13.6,1.14.4,1.15.0,,Runtime / Task,,,,,0,pull-request-available,,,"We have an application that contains a broadcast process stage. The none-broadcast input has roughly 10 million messages per second, and the broadcast side is some kind of control stream, rarely has message follow through. 

After several hours of running, the TaskManager will run out of heap memory and restart. We reviewed the application code without finding any relevant issues.

We found that the running to crash time was roughly the same. Then we make a heap dump before the crash and found mass `CompletableFuture$UniRun` instances. 

These `CompletableFuture$UniRun` instances consume several gigabytes memories.

 

The following pic is from the heap dump we get from a mock testing stream with the same scenario.

!image-2022-01-20-18-43-32-816.png|width=1161,height=471!

 

After some source code research. We found that it might be caused by the *StreamMultipleInputProcessor.getAvailableFuture()*.

*StreamMultipleInputProcessor* has multiple *inputProcessors* , it's *availableFuture* got completed when any of it's input's *availableFuture* is complete. 
The current implementation create a new *CompletableFuture* and a new *CompletableFuture$UniRun* append to delegate inputProcessor's *avaiableFuture*.
The issue is caused by the stacking of *CompletableFuture$UniRun* on the slow inputProcessor's *avaiableFuture*. 
See the source code below.
[StreamMultipleInputProcessor.java#L65|https://github.com/wpc009/flink/blob/d33c39d974f08a5ac520f220219ecb0796c9448c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/StreamMultipleInputProcessor.java#L65]

Because the *UniRun* holds the reference of outside *StreamMultipleInputProcessor*'s avaiableFuture, that cause mass *CompletableFuture* instance which can not be recycled.

We made some modifications to the *StreamMultipleInputProcessor*.*getAvaiableFuture* function, and verify that the issue is gone on our modified version. 

We are willing to make a PR for this fix.

 Heap Dump File [^flink-completablefuture-issue.tar.xz] 
PS: This is a YourKit heap dump. may be not compatible HPROF files.

[Sample Code to reproduce the issue|https://github.com/wpc009/flink/blob/FLINK-25728/flink-end-to-end-tests/flink-datastream-allround-test/src/main/java/org/apache/flink/streaming/tests/MultipleInputStreamMemoryIssueTest.java]
 ",,gaoyunhaii,kevin.cyj,neighborhood,pnowojski,ram_krish,Thesharing,trohrmann,wpc009,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24300,,,FLINK-25827,FLINK-25869,,,,,,"20/Jan/22 12:27;wpc009;flink-completablefuture-issue.tar.xz;https://issues.apache.org/jira/secure/attachment/13039154/flink-completablefuture-issue.tar.xz","20/Jan/22 10:43;wpc009;image-2022-01-20-18-43-32-816.png;https://issues.apache.org/jira/secure/attachment/13039143/image-2022-01-20-18-43-32-816.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 31 16:36:27 UTC 2022,,,,,,,,,,"0|z0yrk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/22 12:26;wpc009;The issue has the same symptoms as FLINK-24300, beside the momery issue, the mass *CompletableFuture$UniRun* will also cause a long period of CPU busy after a long idle.;;;","24/Jan/22 06:57;wpc009;[~gaoyunhaii] hello, could you please take a look at this issue?
Thanks.;;;","24/Jan/22 07:02;gaoyunhaii;Thanks [~wpc009] for reporting the issue! I'll have a look~ ;;;","24/Jan/22 10:19;gaoyunhaii;Hi [~wpc009] ~ I roughly got the issue, have you already opened a PR for this issue?;;;","24/Jan/22 10:54;wpc009;Thanks.
You can reproduce this issue using this [Test|https://github.com/wpc009/flink-mirror/blob/3ade9719b565beeacf7761ab71d5abb7ba62e62a/flink-end-to-end-tests/flink-datastream-allround-test/src/main/java/org/apache/flink/streaming/tests/MultipleInputStreamMemoryIssueTest.java];;;","24/Jan/22 11:03;wpc009;Hi [~gaoyunhaii],
I'm working on a fix for the issue. Currently, it's working on my test environment.
I'm ready to make a PR, if it is ok with you.
;;;","24/Jan/22 12:53;gaoyunhaii;Thanks [~wpc009] for the PR~ By the way, is it [https://github.com/flink-ci/flink-mirror/pull/13] ? It should be targeted at apache/flink ~  ;;;","24/Jan/22 13:08;wpc009;I know. I'm using the *_flink-mirror_* repo to run some tests on Azure Pipeline.
I'll submit a formal PR at apache/flink repo.;;;","25/Jan/22 07:58;trohrmann;cc [~pnowojski];;;","25/Jan/22 15:53;pnowojski;Thanks for reporting and analysing the issue [~wpc009]. Could you maybe rephrase why 
{code:java}
                        inputProcessors[i]
                                .getAvailableFuture()
                                .thenRun(() -> anyInputAvailable.complete(null))
{code}
is causing this memory leak?

Or maybe let me rephrase it. Let's assume we have two inputs, one is flipping between available/unavailable status, the other is continuously unavailable. Now per each {{StreamMultipleInputProcessor#getAvailableFuture}} call, we will create one {{CompletableFuture<?> anyInputAvailable}}, referenced by TWO instances {{CompletableFuture$UniRun}}, for {{inputProcessors[i].getAvailableFuture()}} from each of the inputs. {{anyInputAvailable}} will be returned, and it will be completed by first input sooner or later, waking up {{SteamTask}} in the process. That's fine. Now the problem is that each of those {{anyInputAvailable}} instance will be in this scenario referenced forever by the second input's {{inputProcessors[i].getAvailableFuture()}}. As long as the second input is not available, we are keep building up the memory leak. 

Did I understand the problem correctly?
;;;","25/Jan/22 16:39;wpc009;That's correct. A bunch of *CompletableFuture* instances and *CompletableFuture$UniRun* instances were held by the _inputProcessors[i].getAvailableFuture()_  from the idle input.
Besides the memory issues, when the data was eventually available on the idle input, then the huge amount of *CompletableFuture$UniRun* will cause CPU spike.;;;","25/Jan/22 16:48;pnowojski;Thanks for the confirmation. Good catch! (let's move the discussion to the PR about how to fix it);;;","25/Jan/22 16:52;wpc009;Ok.
;;;","28/Jan/22 14:04;pnowojski;After an offline discussion with [~chesnay], we agreed to merge a fix for this bug without a test coverage. After the feature freeze, we will try to provide stress test coverage for this issue. I've created FLINK-25869 for this purpose.;;;","31/Jan/22 16:36;pnowojski;Merged to master as a7eadf57e42^ and a7eadf57e42
Merged to release-1.14 as 761a4623dda^ and 761a4623dda
Merged to release-1.13 as 9653999a27b^ and 9653999a27b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL convert constant string to char type which cause hive udtf json_tuple not work,FLINK-25727,13423708,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,,syntomic,syntomic,20/Jan/22 09:49,30/Jun/22 03:55,13/Jul/23 08:08,30/Jun/22 03:55,1.14.2,,,,,,,,,,Connectors / Hive,,,,,0,features,pull-request-available,,"Flink SQL(use default dialect) is:
{code:java}
SELECT
    a.`log`,
    b.`role_id`
FROM
    tmp_kafka a, lateral table(json_tuple(`log`, 'role_id')) AS b(`role_id`); {code}
Exception is:
{code:java}
org.apache.flink.table.api.ValidationException: SQL validation failed. java.lang.reflect.InvocationTargetException
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:164)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:215)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:716)
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:106)
	at org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callInnerSelect(FlinkStreamSqlInterpreter.java:86)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.callSelect(FlinkSqlInterpreter.java:494)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.callCommand(FlinkSqlInterpreter.java:257)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.runSqlList(FlinkSqlInterpreter.java:151)
	at org.apache.zeppelin.flink.FlinkSqlInterpreter.internalInterpret(FlinkSqlInterpreter.java:109)
	at org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:55)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:860)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
	at org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeGetResultType(HiveFunctionUtils.java:83)
	at org.apache.flink.table.planner.functions.utils.HiveTableSqlFunction.getRowType(HiveTableSqlFunction.java:116)
	at org.apache.flink.table.planner.functions.utils.TableSqlFunction$$anon$1.inferReturnType(TableSqlFunction.scala:89)
	at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:69)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3085)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3133)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3076)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:159)
	... 20 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeGetResultType(HiveFunctionUtils.java:76)
	... 40 more
Caused by: org.apache.flink.table.functions.hive.FlinkHiveUDFException: org.apache.hadoop.hive.ql.exec.UDFArgumentException: json_tuple()'s arguments have to be string type
	at org.apache.flink.table.functions.hive.HiveGenericUDTF.getHiveResultType(HiveGenericUDTF.java:146)
	... 45 more
Caused by: org.apache.hadoop.hive.ql.exec.UDFArgumentException: json_tuple()'s arguments have to be string type
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.initialize(GenericUDTFJSONTuple.java:118)
	at org.apache.flink.table.functions.hive.HiveGenericUDTF.getHiveResultType(HiveGenericUDTF.java:144)
	... 45 more
 {code}
 ","Flink 1.14.2

Hive 2.3.9",luoyuxia,syntomic,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15854,,,,,,,,,,,,,,,,,,,,,"20/Jan/22 09:44;syntomic;image-2022-01-20-17-44-58-478.png;https://issues.apache.org/jira/secure/attachment/13039138/image-2022-01-20-17-44-58-478.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,,Mon Jun 06 03:00:16 UTC 2022,,,,,,,,,,"0|z0yrig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/22 01:48;luoyuxia;It's a issue about using hive function in flink sql. It can be reproduced by the following code in HiveDialectITCase
{code:java}
tableEnv.executeSql(""create table foo (x int, y int)"");
tableEnv.executeSql(
                ""select foo.x, b.role_id from foo, lateral table(json_tuple('{\""a\"":\""0\"",\""b\"":\""1\""}', 'role')) AS b(role_id)"");
{code}
The reason is that the parameters of json_tuple will be considered as char in Flink SQl instead of string in the implementation. I'll try to fix it to make such sql can work in Flink.

Currently, can use 
{code:java}
json_tuple(repeat('{\""a\"":\""0\"",\""b\"":\""1\""}',1),"" "" repeat('a', 1)))
{code}
 as a work around.

;;;","24/Jan/22 02:46;luoyuxia;[~jark] Could you please like to move it to Connectors / Hive? And I'm glad to take it.;;;","24/Jan/22 02:50;syntomic;done;;;","28/Jan/22 04:01;luoyuxia;I have opened a pr to fix it.;;;","25/Feb/22 02:08;luoyuxia;I will fix it in [FLINK-15854|https://issues.apache.org/jira/browse/FLINK-15854].;;;","06/Jun/22 03:00;luoyuxia;[~syntomic] This problem should be fixed in FLINK-15854. I think you can close this issue now~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlueSchemaRegistryAvroKinesisITCase fails on AZP,FLINK-25709,13423571,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,19/Jan/22 18:29,20/Jan/22 20:37,13/Jul/23 08:08,20/Jan/22 20:37,1.15.0,,,,,,1.15.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,test-stability,,"The {{GlueSchemaRegistryAvroKinesisITCase}} fails on AZP with 

{code}
Jan 19 18:04:11 java.lang.IllegalStateException: Check failed: Docker environment should have more than 2GB free disk space
Jan 19 18:04:11 	at org.testcontainers.DockerClientFactory.check(DockerClientFactory.java:312)
Jan 19 18:04:11 	at org.testcontainers.DockerClientFactory.checkDiskSpace(DockerClientFactory.java:301)
Jan 19 18:04:11 	at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:238)
Jan 19 18:04:11 	at org.testcontainers.DockerClientFactory$1.getDockerClient(DockerClientFactory.java:101)
Jan 19 18:04:11 	at com.github.dockerjava.api.DockerClientDelegate.authConfig(DockerClientDelegate.java:107)
Jan 19 18:04:11 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:316)
Jan 19 18:04:11 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1066)
Jan 19 18:04:11 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
Jan 19 18:04:11 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Jan 19 18:04:11 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Jan 19 18:04:11 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 19 18:04:11 	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29722&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15816",,dannycranmer,martijnvisser,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25404,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 20 20:37:04 UTC 2022,,,,,,,,,,"0|z0yqo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/22 18:29;trohrmann;cc [~fpaul];;;","19/Jan/22 18:29;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29725&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15061;;;","19/Jan/22 18:37;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29723&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15678;;;","19/Jan/22 19:13;martijnvisser;[~dannycranmer] My first thought was that this could be caused by the dependency update, what do you think?;;;","19/Jan/22 20:28;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29731&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14892]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29728&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14863;;;","19/Jan/22 21:23;dannycranmer;[~MartijnVisser] I do not think it is to do with the Glue dependency update from 1.1.5 to 1.1.8 (FLINK-25656). I checked the diff and it looks like there were no actual changes, this is a module of a multi module project which has been versioned. There were no changes to the \{{avro-flink-serde}} or \{{schema-registry-common}} besides a bump in log4j version in a dependency management block. I will check the KCL diff too.;;;","19/Jan/22 21:46;dannycranmer;The KCL update (FLINK-25657) looks an unlikely candidate too. I believe it first occurred after [this commit|https://github.com/flink-ci/flink-mirror/commit/d31471e708b0e80976cf53c8d9ea1ebed66acd07]. Given this [changes the blob server file system clean-up|https://github.com/flink-ci/flink-mirror/commit/d31471e708b0e80976cf53c8d9ea1ebed66acd07#diff-d0243e01372c690149fdba90c49bceb9df88326666636f5d8b98c3564e991990L340], and the error is ""out of disk space"", it is possible there is a bug here? Maybe we have some ""borrowed"" files that are no longer being deleted, and building up. ([~trohrmann] );;;","20/Jan/22 08:07;trohrmann;Yes, I think you are right [~dannycranmer]. Let me check how the Docker processes clean up their things.;;;","20/Jan/22 09:03;trohrmann;The problem seems to be that we no longer clean up the blob directory by default because it is now contained in the working directory. However, the working directory is behaving in the tests like any other temp directory that is not intended to be reused. Hence, this is a regression compared to before. I would suggest to restore the old behaviour unless a deterministic working directory has been configured.;;;","20/Jan/22 11:11;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29763&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14758;;;","20/Jan/22 20:37;trohrmann;Fixed via

035b05516d9c7aeba9563d09c9e0e86c07927d4b
dd6069fabf8a7ff65fbd9ff8dd7b0c47f492288f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler never transitions executions into SCHEDULED state,FLINK-25707,13423549,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,19/Jan/22 16:40,28/Jan/22 09:17,13/Jul/23 08:08,28/Jan/22 09:17,1.14.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"The AdaptiveScheduler violates the Execution state machine by directly transitioning executions from CREATED to DEPLOYING.
As part of this ticket we should look into why Executions allow such a transition in the first place, and whether we can change that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 28 09:17:02 UTC 2022,,,,,,,,,,"0|z0yqj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/22 09:17;chesnay;master:
59d0a643eae8cc95dac198eb7f1cf65048e77f3e
1704b1336bcbcdd42405f5e6e3e09c0176925210
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on 18.01.2022 in batch network benchmarks,FLINK-25704,13423503,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,pnowojski,pnowojski,19/Jan/22 12:29,15/Feb/22 08:16,13/Jul/23 08:08,15/Feb/22 08:16,1.15.0,,,,,,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,"http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=compressedFilePartition&env=2&revs=200&equid=off&quarts=on&extr=on
http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=uncompressedFilePartition&env=2&revs=200&equid=off&quarts=on&extr=on
http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=uncompressedMmapPartition&env=2&revs=200&equid=off&quarts=on&extr=on

Suspected range:
{code}
git ls eeec246677..f5c99c6f26
f5c99c6f26 [5 weeks ago] [FLINK-17321][table] Add support casting of map to map and multiset to multiset [Sergey Nuyanzin]
745cfec705 [24 hours ago] [hotfix][table-common] Fix InternalDataUtils for MapData tests [Timo Walther]
ed699b6ee6 [6 days ago] [FLINK-25637][network] Make sort-shuffle the default shuffle implementation for batch jobs [kevin.cyj]
4275525fed [6 days ago] [FLINK-25638][network] Increase the default write buffer size of sort-shuffle to 16M [kevin.cyj]
e1878fb899 [6 days ago] [FLINK-25639][network] Increase the default read buffer size of sort-shuffle to 64M [kevin.cyj]
{code}

It looks [~kevin.cyj], that most likely your change has caused that?",,akalashnikov,kevin.cyj,pnowojski,Thesharing,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 15 08:15:45 UTC 2022,,,,,,,,,,"0|z0yq8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/22 04:12;kevin.cyj;After some analysis, I found that these tests is for BoundedBlockingResultPartition, but they does not choose to use which blocking shuffle explicitly, that is, they are using the default implementation and configuration. FLINK-25636 changed the default blocking shuffle implementation, so the tests are now testing SortMergeResultPartition instead of the BoundedBlockingResultPartition. There is should be no regression for BoundedBlockingResultPartition.

The reason that BoundedBlockingResultPartition is better than SortMergeResultPartition is that SortMergeResultPartition Introduce some record copy overhead. For big records, this overhead is not a big problem (no regression on TPC-DS), for small records, the copy overhead is non-negligible and these tests are using small records (8bytes long value). I created a new ticket FLINK-25796 to improve this scenario.

As a summary, the followup actions are as follows:
 # Make the existing benchmark tests still test BoundedBlockingResultPartition;
 # Add new benchmark tests for SortMergeResultPartition;
 # Try to optimize SortMergeResultPartition for small records;
 # Document this default blocking shuffle change in both release notes and user doc.

Any suggestions?;;;","26/Jan/22 08:54;pnowojski;Thanks for the investigation [~kevin.cyj]
{quote}
Make the existing benchmark tests still test BoundedBlockingResultPartition;
Add new benchmark tests for SortMergeResultPartition;
Try to optimize SortMergeResultPartition for small records;
Document this default blocking shuffle change in both release notes and user doc.
{quote}
This plan makes sense to me, +1;;;","01/Feb/22 10:56;pnowojski;Changes to the settings merged to flink-benchmark master as 5e5b35b;;;","11/Feb/22 10:08;kevin.cyj;Sort-shuffle micro benchmark merged via ffcdbb45c88e9453bd815229eebaec1e776722cb.;;;","12/Feb/22 03:54;kevin.cyj;Sort-shuffle optimization merged via 3be35d9c64e4b28cc73e157325c935d005286d99.

 ;;;","15/Feb/22 08:08;kevin.cyj;Document merged via 5a854a31aab7daa5f0240d1cc05cc102d2ff4574.;;;","15/Feb/22 08:15;kevin.cyj;As we already see performance gain after merging the sort-shuffle optimization on the sort-shuffle benchmark and the sort-shuffle benchmarks and hash-shuffle benchmarks now have similar performance, so closing this issue. Please feel free to reopen it if there is any other concern.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DropDelete is incorrect in CompactManager when outputLevel is zero,FLINK-25687,13423185,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,18/Jan/22 07:21,18/Jan/22 09:06,13/Jul/23 08:08,18/Jan/22 09:06,,,,,,,table-store-0.1.0,,,,Table Store,,,,,0,pull-request-available,,,"When output level is zero, there may be have other files in level 0, we can not drop delete.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 18 09:06:44 UTC 2022,,,,,,,,,,"0|z0yoag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/22 09:06;lzljs3620320;master: 632044f56f7fa2c07e9be6050ace7d5fe68302f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestClusterClient gets stuck on submitting job with local user artifact that has a scheme,FLINK-25685,13423173,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuannan,xuannan,xuannan,18/Jan/22 05:47,03/Feb/22 11:36,13/Jul/23 08:08,03/Feb/22 11:36,1.14.3,1.15.0,,,,,1.15.0,,,,Runtime / REST,,,,,0,pull-request-available,,,"I found that a job submission gets stuck if StreamExecutionEnvironment#registerCachedFile is called with a local file. After some digging, I found that it gets stuck when the RestClusterClient sends the job-submission request to the JobManager. 

Below is the unit test added to the `RestClusterClientTest` to reproduce the problem on my local machine.


{code:java}
    @Test
    public void testJobSubmissionWithUserArtifact() throws Exception {
        try (final TestRestServerEndpoint restServerEndpoint =
                     createRestServerEndpoint(new TestJobSubmitHandler())) {
            try (RestClusterClient<?> restClusterClient =
                         createRestClusterClient(restServerEndpoint.getServerAddress().getPort())) {

                TemporaryFolder temporaryFolder = new TemporaryFolder();
                temporaryFolder.create();
                File file = temporaryFolder.newFile();
                Files.write(file.toPath(), ""hello world"".getBytes(ConfigConstants.DEFAULT_CHARSET));
                jobGraph.addUserArtifact(""file"",
                        new DistributedCache.DistributedCacheEntry(file.toURI().toString(),
                                false));

                restClusterClient
                        .submitJob(jobGraph)
                        .get();
            }
        }
    }
{code}

The test can pass if the `jobGraph.addUserArtifact` is not called.  ",,wangyang0918,xuannan,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 03 11:36:04 UTC 2022,,,,,,,,,,"0|z0yo7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/22 09:04;chesnay;This test passes for me locally, both with 1.14.3 and 1.15.0.;;;","18/Jan/22 16:37;xuannan;I create a pr that adds the unit tests with and without `jobGraph.addUserArtifact`. The test with `jobGraph.addUserArtifact` fail while the other pass.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29653&view=results

[~chesnay] Could you have a look at the CI run?;;;","18/Jan/22 17:48;chesnay;I have seen it. I still can't reproduce the issue locally.;;;","21/Jan/22 03:23;xuannan;[~chesnay], I created a PR with unit test and the fix. May you assign the ticket to me and help review the PR?;;;","21/Jan/22 06:39;wangyang0918;I believe the test will pass if you use {{file.getAbsolutePath()}} instead of {{{}file.toURI().toString(){}}}.

The root cause might be as following.

 
{code:java}
filesToUpload.add(
        new FileUpload(
                Paths.get(artifacts.getValue().filePath),// here we will treat file:///path/of/a.tmp as a file path, not URI.
                RestConstants.CONTENT_TYPE_BINARY));{code}
Instead, we could use {{artifactFilePath.getPath()}} to replace {{artifacts.getValue().filePath}} for creating {{{}FileUpload{}}}.

 ;;;","21/Jan/22 07:42;xuannan;[~wangyang0918] You are right. (y) The fix is to use artifactFilePath.getPath() ;;;","03/Feb/22 11:36;chesnay;master: 46bb03848ea67fa7b8952f757d57a2cda6ab16aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong result if table transfrom to DataStream then window process in batch mode,FLINK-25683,13423166,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,zhangzihan,zhangzihan,18/Jan/22 04:28,24/Jan/22 16:03,13/Jul/23 08:08,24/Jan/22 08:38,1.14.2,,,,,,1.13.6,1.14.4,1.15.0,,Table SQL / API,Table SQL / Runtime,,,,0,pull-request-available,,,"I have 5 line datas,
i first need to transform current data with SQL
then mix current data and historical data which is batch get from hbase
for some special reason the program must run in batch mode
i think the correct result should be like this：
(BOB,1)
(EMA,1)
(DOUG,1)
(ALICE,1)
(CENDI,1)
but the result is :
(EMA,1)

 

if i set different parallelism ,the result is different.","mac book pro m1 

jdk 8 

scala 2.11

flink 1.14.2

idea 2020",dwysakowicz,martijnvisser,paul8263,twalthr,zhangzihan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25471,,,,,,,,,,"18/Jan/22 05:30;zhangzihan;TableToDataStreamBatchWindowTest.scala;https://issues.apache.org/jira/secure/attachment/13038980/TableToDataStreamBatchWindowTest.scala","18/Jan/22 04:28;zhangzihan;pom.xml;https://issues.apache.org/jira/secure/attachment/13038973/pom.xml",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 24 08:32:33 UTC 2022,,,,,,,,,,"0|z0yo68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/22 05:34;zhangzihan;hi @[~paul8263]  ：
I found a new problem similar to FLINK-25471。

 can you help me？;;;","19/Jan/22 01:12;paul8263;Hi [~zhangzihan] ,

Thank you very much for reporting this issue. I have successfully reproduced it. ;;;","19/Jan/22 06:45;paul8263;Hi [~twalthr] ,

This issue and FLINK-25471 is quite similar.  The root cause is BatchExecutionKeyedStateBackend calls 

notifyKeySelected only if it receives the data with different key. As a result, last elements(right before Task Manager exits) stored in BatchExecutionKeyedStateBackend the will never have a chance to be collected by the downstream. I think it is the root cause.

I plan to change AbstractStreamOperator by extending BoundedMultiInput and set the key to a designed value when the input reaches END_OF_DATA. By doing this, in batch mode it will trigger notifyKeySelected and finally all elements will be collected. It might not have any side effect in streaming mode. Both FLINK-25471 and this ticket can be solved by this change. Correct me if I am wrong.

Could you please assign this ticket to me?;;;","19/Jan/22 08:05;martijnvisser;[~paul8263] I've assigned it to you. ;;;","19/Jan/22 08:18;twalthr;Thanks for investigating [~paul8263]. I will loop in [~dwysakowicz] who knows this part of the code better.;;;","19/Jan/22 12:00;dwysakowicz;Hi [~paul8263]. This is a very good investigation. Moreover I think your suggested solution goes in the right direction. However, I'd put that logic inside of {{AbstractStreamOperator#finish}} (or {{AbstractStreamOperator#close}} in older versions). Would you like to create a PR for it?;;;","20/Jan/22 07:48;dwysakowicz;Sorry, I have not spotted that before. Actually, I am not sure anymore, the described situation is the root problem. In pure {{DataStream}} program, the trailing keys are supposed to be flushed by the {{MAX_WATERMARK}} emitted from sources. However, as I looked into the code in the {{InputConversionOperator}} we swallow all watermarks for the given program. See {{InputConversionOperator#processWatermark}}. [~twalthr] As I am not 100% familiar with that logic. Would it be possible to forward at least the {{MAX_WATERMARK}} there?;;;","20/Jan/22 08:59;twalthr;Yes, this might be indeed a bug. A max watermark should always be forwarded. However, I hope you agree with me that it doesn't make sense to forward other watermarks if there is no time attribute in SQL for it. I wanted to be conservative here and let the user decide whether it makes sense to rely on DataStream API or not. We should not let watermarks flow throw the SQL engine without a clear purpose, at least this was the original thought behind it.;;;","20/Jan/22 09:50;paul8263;Hi [~dwysakowicz] and [~twalthr] ,

Thank you very much for your reply. I retested this issue and finally got that the problem only exists if the DataStream is converted from table API. If we use pure Stream API everything works smoothly. It is not a good practice to change AbstractStreamOperator. The problem indeed lies in InputConversionOperator.
{code:java}
@Override
public void processWatermark(Watermark mark) throws Exception {
    if (propagateWatermark) {
        super.processWatermark(mark);
    }
} {code}
I debugged and it turned out that the value of propagateWatermark was false. No watermarks will be forwarded to the downstream.

 ;;;","20/Jan/22 10:08;dwysakowicz;[~twalthr] I can't think of any other case, where watermarks would be necessary if there is no corresponding time attribute.

Ok, now that we're on the same page [~paul8263] do you still want to create a PR for it? In the PR, we could propagate if it is the {{MAX_WATERMARK}}. Therefore the method could become:
{code}
@Override
public void processWatermark(Watermark mark) throws Exception {
    if (propagateWatermark || || mark.equals(Watermark.MAX_WATERMARK)) {
        super.processWatermark(mark);
    }
} 
{code};;;","20/Jan/22 10:46;paul8263;Hi [~dwysakowicz] ,

I still can't figure out what actually controls the boolean value propagateWatermark in InputConversionOperator. Does the watermark clause in SQL create statement controls it?

Currently I will follow you instruction and updated the PR, together with the tests. Thank you very much.;;;","20/Jan/22 13:14;dwysakowicz;I can't explain you where the flag comes from, as I don't know this part of code. However as Timo explained:

{quote}
that it doesn't make sense to forward other watermarks if there is no time attribute in SQL for it. I wanted to be conservative here and let the user decide whether it makes sense to rely on DataStream API or not. We should not let watermarks flow throw the SQL engine without a clear purpose, at least this was the original thought behind it.
{quote}

So the idea is that if the table API does not expect watermarks from the datastream API, it cuts off any unexpected watermarks.;;;","20/Jan/22 13:31;twalthr;[~paul8263] The propagate watermark is forwarded from the DDL using {{SOURCE_WATERMARK()}}. Maybe we need to have another pass over the docs to make this clearer but the content is actually there:

https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/data_stream_api/#examples-for-fromdatastream
{quote}
The virtual DataStream table source implements SupportsSourceWatermark and thus allows calling the SOURCE_WATERMARK() built-in function as a watermark strategy to adopt watermarks from the DataStream API.

// === EXAMPLE 4 ===

// derive all physical columns automatically
// but access the stream record's timestamp for creating a rowtime attribute column
// also rely on the watermarks generated in the DataStream API

// we assume that a watermark strategy has been defined for `dataStream` before
// (not part of this example)
Table table =
    tableEnv.fromDataStream(
        dataStream,
        Schema.newBuilder()
            .columnByMetadata(""rowtime"", ""TIMESTAMP_LTZ(3)"")
            .watermark(""rowtime"", ""SOURCE_WATERMARK()"")
            .build());
{quote}

+1 to Dawid's fix for this issue.;;;","24/Jan/22 08:32;dwysakowicz;Fixed in:
* master
** 9aa879f50c32de862831e82613f1cf1bc4d760f9
* 1.14.4
** 2cca3a6609a030b6f03fedbaab51da3dce2962cd
* 1.13.6
** 16feba605c87645a658947805be50f6c410fde67;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodeSplitITCase produces large stacktraces and potentially out of memory errors,FLINK-25681,13423047,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,TsReaper,martijnvisser,martijnvisser,17/Jan/22 12:48,21/Jan/22 05:32,13/Jul/23 08:08,21/Jan/22 05:32,,,,,,,1.15.0,,,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,"There's currently an open PR https://github.com/apache/flink/pull/18368 to enforce memory limits for Docker containers to mitigate or resolve [https://issues.apache.org/jira/browse/FLINK-18356|https://issues.apache.org/jira/browse/FLINK-18356?focusedCommentId=17476183&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17476183]

As mentioned in the above Flink ticket, the table tests fail. A run can be found at https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=29461

When looking into the logs, I noticed that there are large stacktraces reported on org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase. 

The logs show numerous errors for 

* {{testSelectManyColumns(org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase)}}
* {{testManyAggregations(org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase)}} 
* {{testManyOrsInCondition(org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase)}}

Some examples:

{code:java}
08:43:03,192 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - HashAggregate[30254] -> Sink: Collect table sink (1/1) (c515430ab543bf59471fce8c77f83ad7) switched from INITIALIZING to FAILED on efa7d996-3752-473e-87ff-66b403a09f38 @ localhost (dataPort=-1).
java.lang.RuntimeException: Could not instantiate generated class 'NoGroupingAggregateWithoutKeys$363111'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:85) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:201) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:60) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:653) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:641) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:86) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 11 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 11 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:99) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 11 more
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""NoGroupingAggregateWithoutKeys$363111"": Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""NoGroupingAggregateWithoutKeys$363111"" grows beyond 64 KB
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[janino-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[commons-compiler-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[commons-compiler-3.0.11.jar:?]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:96) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 11 more
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V"" of class ""NoGroupingAggregateWithoutKeys$363111"" grows beyond 64 KB
	at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.CodeContext.write(CodeContext.java:940) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:12282) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11941) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11926) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4465) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$8000(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4408) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4400) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:4274) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4461) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$7500(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4403) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4400) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:3988) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$6300(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$14$1.visitAmbiguousName(UnitCompiler.java:3942) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$14$1.visitAmbiguousName(UnitCompiler.java:3939) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$14.visitLvalue(UnitCompiler.java:3939) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$14.visitLvalue(UnitCompiler.java:3935) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:3935) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4006) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$6500(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$14.visitUnaryOperation(UnitCompiler.java:3956) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$14.visitUnaryOperation(UnitCompiler.java:3935) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$UnaryOperation.accept(Java.java:4730) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:3935) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2475) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[janino-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[commons-compiler-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[commons-compiler-3.0.11.jar:?]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:96) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-14.0.jar:30.1.1-jre-14.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:84) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 11 more
{code}
",,gaoyunhaii,guoyangze,lzljs3620320,martijnvisser,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 21 05:32:18 UTC 2022,,,,,,,,,,"0|z0ynfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/22 12:48;martijnvisser;[~TsReaper] Will you take a look?;;;","19/Jan/22 08:56;TsReaper;Hi [~MartijnVisser]! Thanks for looking into this issue.

This test will first run a large SQL script without code splitting (to make sure that this SQL script fails) and then runs with code splitting (to make sure it succeed). I guess it is the first part that produces a deep stack trace. I’ll remove the first part (we can determine the failure by hand when constructing the test case) and let’s see if this issue is still happening.;;;","21/Jan/22 05:32;lzljs3620320;master: 8c8e658636de1874b2abcf28827f34043bc49aff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorStateChangelogStoragesManager.shutdown is not thread-safe,FLINK-25678,13423016,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,17/Jan/22 10:54,22/Jan/22 11:18,13/Jul/23 08:08,22/Jan/22 11:18,1.14.2,1.15.0,,,,,1.14.4,1.15.0,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,"[https://github.com/apache/flink/pull/18169#discussion_r785741977]

The method is called from the shutdown hook and therefore should be thread-safe.

cc: [~Zakelly] , [~dmvk] ",,dmvk,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jan 22 11:18:57 UTC 2022,,,,,,,,,,"0|z0yn8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/22 11:18;roman;Merged as 11a406e67057ca9260c16c08054c209e3452a291 into 1.14,
as b72a5e0ef237ad02ed074bace1f7cb3aa09631e4 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond hangs on AZP,FLINK-25673,13422992,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,trohrmann,trohrmann,17/Jan/22 08:57,29/Mar/22 07:29,13/Jul/23 08:08,26/Jan/22 10:57,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"The test {{AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond}} hangs on AZP:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29512&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=18044",,gaoyunhaii,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24216,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 07:29:13 UTC 2022,,,,,,,,,,"0|z0yn3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/22 16:15;trohrmann;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29699&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=28390;;;","24/Jan/22 08:36;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29946&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=28462;;;","26/Jan/22 10:57;chesnay;master: d7bc8e29520907a2a7ac3b741d49784579d83316;;;","29/Mar/22 07:29;gaoyunhaii;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33808&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12500
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CPUResourceTest does not work with German Locale,FLINK-25633,13422288,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,12/Jan/22 16:51,19/Jan/22 10:56,13/Jul/23 08:08,19/Jan/22 10:56,1.13.5,1.14.2,,,,,1.13.6,1.14.4,1.15.0,,Tests,,,,,0,pull-request-available,,,"The {{CPUResourceTest}} does not work with a German Locale because it expects decimals to be formatted with a dot (e.g. {{0.00}} instead of {{0,00}}). 

I propose to fix the Locale in the {{pom.xml}} to US in order to fix this problem.",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21177,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 19 10:56:01 UTC 2022,,,,,,,,,,"0|z0yirc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/22 10:56;trohrmann;Fixed via

master: 4a756059b724488021184f55073a50d72166b14a
1.14.4: e7102f5f0c7ac136c67fd602f628d931951bf158
1.13.6: 5e15418cb25816810acf39d19737ab7b4e287919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure failed on the azure,FLINK-25585,13421607,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,gaoyunhaii,gaoyunhaii,10/Jan/22 06:44,21/Feb/23 12:47,13/Jul/23 08:08,21/Feb/23 12:47,1.14.2,1.15.0,,,,,,,,,Runtime / Coordination,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,test-stability,"
{code:java}
2022-01-08T02:39:54.7103772Z Jan 08 02:39:54 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 349.282 s <<< FAILURE! - in org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase
2022-01-08T02:39:54.7105233Z Jan 08 02:39:54 [ERROR] testDispatcherProcessFailure[ExecutionMode BATCH]  Time elapsed: 302.006 s  <<< FAILURE!
2022-01-08T02:39:54.7106478Z Jan 08 02:39:54 java.lang.AssertionError: The program encountered a RuntimeException : java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized
2022-01-08T02:39:54.7107409Z Jan 08 02:39:54 	at org.junit.Assert.fail(Assert.java:89)
2022-01-08T02:39:54.7108084Z Jan 08 02:39:54 	at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure(JobManagerHAProcessFailureRecoveryITCase.java:383)
2022-01-08T02:39:54.7108952Z Jan 08 02:39:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-08T02:39:54.7109491Z Jan 08 02:39:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-08T02:39:54.7110107Z Jan 08 02:39:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-08T02:39:54.7110772Z Jan 08 02:39:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-08T02:39:54.7111594Z Jan 08 02:39:54 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-08T02:39:54.7112510Z Jan 08 02:39:54 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-08T02:39:54.7113734Z Jan 08 02:39:54 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-08T02:39:54.7114673Z Jan 08 02:39:54 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-08T02:39:54.7115423Z Jan 08 02:39:54 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-08T02:39:54.7116011Z Jan 08 02:39:54 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-08T02:39:54.7116586Z Jan 08 02:39:54 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-08T02:39:54.7117154Z Jan 08 02:39:54 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-08T02:39:54.7117686Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-08T02:39:54.7118448Z Jan 08 02:39:54 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-08T02:39:54.7119020Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-08T02:39:54.7119571Z Jan 08 02:39:54 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-08T02:39:54.7120180Z Jan 08 02:39:54 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-08T02:39:54.7120754Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-08T02:39:54.7121286Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-08T02:39:54.7121832Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-08T02:39:54.7122376Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-08T02:39:54.7123179Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-08T02:39:54.7123796Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-08T02:39:54.7124304Z Jan 08 02:39:54 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-01-08T02:39:54.7125001Z Jan 08 02:39:54 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-01-08T02:39:54.7125753Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-08T02:39:54.7126595Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-08T02:39:54.7127354Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-08T02:39:54.7127911Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-08T02:39:54.7128456Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-08T02:39:54.7129019Z Jan 08 02:39:54 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-08T02:39:54.7129593Z Jan 08 02:39:54 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-08T02:39:54.7130155Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-08T02:39:54.7130682Z Jan 08 02:39:54 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-08T02:39:54.7131183Z Jan 08 02:39:54 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-08T02:39:54.7131826Z Jan 08 02:39:54 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-08T02:39:54.7132375Z Jan 08 02:39:54 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
2022-01-08T02:39:54.7133122Z Jan 08 02:39:54 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-01-08T02:39:54.7133866Z Jan 08 02:39:54 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-01-08T02:39:54.7134676Z Jan 08 02:39:54 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-01-08T02:39:54.7135476Z Jan 08 02:39:54 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2022-01-08T02:39:54.7136127Z Jan 08 02:39:54 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-01-08T02:39:54.7136711Z Jan 08 02:39:54 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-01-08T02:39:54.7137304Z Jan 08 02:39:54 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-01-08T02:39:54.7137912Z Jan 08 02:39:54 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-01-08T02:39:54.7138473Z Jan 08 02:39:54 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-01-08T02:39:54.7139036Z Jan 08 02:39:54 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-01-08T02:39:54.7139911Z Jan 08 02:39:54 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
2022-01-08T02:39:54.7140861Z Jan 08 02:39:54 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
2022-01-08T02:39:54.7141786Z Jan 08 02:39:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2022-01-08T02:39:54.7142701Z Jan 08 02:39:54 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2022-01-08T02:39:54.7143705Z Jan 08 02:39:54 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2022-01-08T02:39:54.7144644Z Jan 08 02:39:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2022-01-08T02:39:54.7145404Z Jan 08 02:39:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2022-01-08T02:39:54.7146351Z Jan 08 02:39:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2022-01-08T02:39:54.7147366Z Jan 08 02:39:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-01-08T02:39:54.7148318Z Jan 08 02:39:54 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2022-01-08T02:39:54.7149246Z Jan 08 02:39:54 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2022-01-08T02:39:54.7150166Z Jan 08 02:39:54 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2022-01-08T02:39:54.7151077Z Jan 08 02:39:54 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2022-01-08T02:39:54.7151764Z Jan 08 02:39:54 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29121&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5359",,dmvk,gaoyunhaii,knaufk,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31168,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 21 12:47:48 UTC 2023,,,,,,,,,,"0|z0yekg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/22 06:47;gaoyunhaii;perhaps cc [~dmvk] ~;;;","12/Jan/22 08:39;dmvk;Hi [~gaoyunhaii], I did a short pass on this one.

- Doesn't happen locally, over 100+ runs
- It might be related to a temporary ZK connection issues
- Need to dig deeper to understand the actual issue
- Also there is a weird gap in logs, for 20+ seconds; the actual failure is then caused by timeout on resolving a leader for webmonitor in order to make a REST call;;;","13/Jan/22 13:09;gaoyunhaii;Very thanks [~dmvk] for the investigation! Is the am in normal status or blocked during the period of 20+ seconds? If it is not blocked and reported the address normally, then it would indeed more like a ZK connection issue~ ;;;","24/Jan/22 13:02;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29993&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=11163;;;","28/Jan/22 07:41;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30343&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=4892;;;","09/Feb/22 08:13;knaufk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30949&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","16/Feb/22 18:57;roman;Similar failure, likely the same root cause:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31663&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5572]
{code:java}
java.util.concurrent.TimeoutException: Listener was not notified about a new leader within 268953ms
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:167)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:152)
	at org.apache.flink.runtime.leaderelection.TestingRetrievalBase.waitForNewLeader(TestingRetrievalBase.java:53)
	at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testDispatcherProcessFailure(JobManagerHAProcessFailureRecoveryITCase.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code};;;","17/Feb/22 07:44;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31707&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12446;;;","04/Mar/22 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Mar/22 10:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","14/Mar/22 10:36;roman;Probably, related issue: FLINK-26632;;;","06/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jul/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Feb/23 12:47;mapohl;Closing this one again in favor of FLINK-31168. It looks like this is the same error. But it's hard to verify because the builds are already cleaned up on the Azure;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerProcessFailureStreamingRecoveryITCase>AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure fails on AZP,FLINK-25564,13421245,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,nsemmler,trohrmann,trohrmann,07/Jan/22 08:31,02/Feb/22 08:21,13/Jul/23 08:08,02/Feb/22 08:21,1.15.0,,,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"The test {{TaskManagerProcessFailureStreamingRecoveryITCase>AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure}} fails on AZP with

{code}
Jan 07 05:07:22 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 31.057 s <<< FAILURE! - in org.apache.flink.test.recovery.TaskManagerProcessFailureStreamingRecoveryITCase
Jan 07 05:07:22 [ERROR] org.apache.flink.test.recovery.TaskManagerProcessFailureStreamingRecoveryITCase.testTaskManagerProcessFailure  Time elapsed: 31.012 s  <<< FAILURE!
Jan 07 05:07:22 java.lang.AssertionError: The program encountered a IOExceptionList : /tmp/junit2133275241637829858/junit7793757951823298127
Jan 07 05:07:22 	at org.junit.Assert.fail(Assert.java:89)
Jan 07 05:07:22 	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:205)
Jan 07 05:07:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 07 05:07:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 07 05:07:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 07 05:07:22 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 07 05:07:22 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jan 07 05:07:22 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jan 07 05:07:22 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jan 07 05:07:22 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jan 07 05:07:22 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 07 05:07:22 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 07 05:07:22 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 07 05:07:22 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 07 05:07:22 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jan 07 05:07:22 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jan 07 05:07:22 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jan 07 05:07:22 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jan 07 05:07:22 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jan 07 05:07:22 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jan 07 05:07:22 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Jan 07 05:07:22 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Jan 07 05:07:22 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Jan 07 05:07:22 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Jan 07 05:07:22 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Jan 07 05:07:22 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Jan 07 05:07:22 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Jan 07 05:07:22 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Jan 07 05:07:22 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
Jan 07 05:07:22 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Jan 07 05:07:22 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Jan 07 05:07:22 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29069&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=16224",,nsemmler,trohrmann,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 02 08:21:48 UTC 2022,,,,,,,,,,"0|z0ycc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/22 16:38;zuston;I can't reproduce this in my local machine, and you? [~trohrmann];;;","10/Jan/22 09:27;trohrmann;I didn't try to reproduce it yet. Maybe a good step could be to take a look at the logs in https://artprodsu6weu.artifacts.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/artifact/cGlwZWxpbmVhcnRpZmFjdDovL2FwYWNoZS1mbGluay9wcm9qZWN0SWQvOTg0NjM0OTYtMWFmMi00NjIwLThlYWItYTJlY2MxYTJlNmZlL2J1aWxkSWQvMjkwNjkvYXJ0aWZhY3ROYW1lL2xvZ3MtY3Jvbl9henVyZS10ZXN0X2Nyb25fYXp1cmVfZmluZWdyYWluZWRfcmVzb3VyY2VfbWFuYWdlbWVudC0xNjQxNTI3ODgz0/content?format=zip.;;;","27/Jan/22 14:15;nsemmler;The error is caused by an exception while [cleaning up the checkpoint directory|#L105]. The ""_metadata"" file is removed during the clean up process. (To be exact while the process is trying to ensure that the file is not read-only).

I am not sure, why the file disappears. I'd guess that the Flink Cluster does a separate clean-up. Possibly [here|https://github.com/apache/flink/blob/08794a6b2188268c3de6b134f399228415c7fc8f/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CompletedCheckpoint.java#L252] and deletes the ""_metadata"" file at the same time. The likelihood of the two clean-ups to interlock is small, so this could be a reason why this doesn't come up often.

Two options to solve this would be to:
 # Wait for a signal that cleanup is done
 # Wait till the directory is empty;;;","27/Jan/22 18:19;nsemmler;Option 1: 
- We may have this option in the future with the JobResultStore. Currently this is not possible.


Optiion 2:
- The Checkpoint Coordinator does not clean up the ""taskowned"" and ""shared"" subfolders of the checkpoint directory. We could use a partial scan for files, but then it becomes very quickly very complicated.

New Option3:

- Maybe, the easiest solution is to just not leave the directory intact. Yes, this is a leak of files, but from what I see locally, the files are cleaned up anyway.;;;","28/Jan/22 10:29;nsemmler;Let's go with option 3 and leave the cleanup to the Rule.;;;","02/Feb/22 08:21;chesnay;master: 18db2f2a8aa7ff2bd423489ece12d0c5488bd2da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL JOIN causes data loss,FLINK-25559,13421189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Ashulin,Ashulin,07/Jan/22 02:14,13/Jan/22 07:52,13/Jul/23 08:08,13/Jan/22 07:14,1.13.2,1.13.3,1.13.5,1.14.0,1.14.2,,1.13.6,1.14.3,1.15.0,,Table SQL / API,,,,,0,,,,"{code:java}
//sink table,omits some physical fields
CREATE TABLE kd_product_info (
  productId BIGINT COMMENT '产品编号',
  productSaleId BIGINT COMMENT '商品编号',
  PRIMARY KEY (productSaleId) NOT ENFORCED
)

// sql omits some selected fields
INSERT INTO kd_product_info
SELECT
 ps.product AS productId,
 ps.productsaleid AS productSaleId,
 CAST(p.complex AS INT) AS complex,
 p.createtime AS createTime,
 p.updatetime AS updateTime,
 p.ean AS ean,
 ts.availablequantity AS totalAvailableStock,
IF
 (
  ts.availablequantity - ts.lockoccupy - ts.lock_available_quantity > 0,
  ts.availablequantity - ts.lockoccupy - ts.lock_available_quantity,
  0
 ) AS sharedStock
 ,rps.purchase AS purchase
 ,v.`name` AS vendorName
 FROM
 product_sale ps
 JOIN product p ON ps.product = p.id
 LEFT JOIN rate_product_sale rps ON ps.productsaleid = rps.id
 LEFT JOIN pss_total_stock ts ON ps.productsaleid = ts.productsale
 LEFT JOIN vendor v ON ps.merchant_id = v.merchant_id AND ps.vendor = v.vendor
 LEFT JOIN mccategory mc ON ps.merchant_id = mc.merchant_id AND p.mccategory = mc.id
 LEFT JOIN new_mccategory nmc ON p.mccategory = nmc.mc
 LEFT JOIN product_sale_grade_plus psgp ON ps.productsaleid = psgp.productsale
 LEFT JOIN product_sale_extend pse359 ON ps.productsaleid = pse359.product_sale AND pse359.meta = 359
 LEFT JOIN product_image_url piu ON ps.product = piu.product {code}
All table sources are upsert-kafka，I have ensured that the associated columns are of the same type:
{code:java}
CREATE TABLE product_sale (
  id BIGINT COMMENT '主键',
  productsaleid BIGINT COMMENT '商品编号',
  product BIGINT COMMENT '产品编号',
  merchant_id DECIMAL(20, 0) COMMENT '商户id',
  vendor STRING COMMENT '供应商',
  PRIMARY KEY (productsaleid) NOT ENFORCED
)
// No computed columns
// Just plain physical columns
WITH (
'connector' = 'upsert-kafka',
'topic' = 'XXX',
'group.id' = '%s',
'properties.bootstrap.servers' = '%s',
'key.format' = 'json',
'value.format' = 'json'
) 

CREATE TABLE product (
  id BIGINT,
  mccategory STRING,
  PRIMARY KEY (id) NOT ENFORCED
)

CREATE TABLE rate_product_sale ( 
  id BIGINT COMMENT '主键',
  PRIMARY KEY (id) NOT ENFORCED
)

CREATE TABLE pss_total_stock (
  id INT COMMENT 'ID',
  productsale BIGINT COMMENT '商品编码',
  PRIMARY KEY (id) NOT ENFORCED
)

CREATE TABLE vendor (
  merchant_id DECIMAL(20, 0) COMMENT '商户id',
  vendor STRING COMMENT '供应商编码',
  PRIMARY KEY (merchant_id, vendor) NOT ENFORCED
)

CREATE TABLE mccategory (
  id STRING COMMENT 'mc编号',
  merchant_id DECIMAL(20, 0) COMMENT '商户id',
  PRIMARY KEY (merchant_id, id) NOT ENFORCED
)

CREATE TABLE new_mccategory (
  mc STRING,
  PRIMARY KEY (mc) NOT ENFORCED
)

CREATE TABLE product_sale_grade_plus (
  productsale BIGINT,
  PRIMARY KEY (productsale) NOT ENFORCED
)

CREATE TABLE product_sale_extend (
  id BIGINT,
  product_sale BIGINT,
  meta BIGINT,
  PRIMARY KEY (id) NOT ENFORCED
)

CREATE TABLE product_image_url(
  product BIGINT,
  PRIMARY KEY (product) NOT ENFORCED 
){code}
the data in each table is between 5 million and 10 million, parallelism: 24;

Not set ttl; In fact, we can notice data loss as soon as 30 minutes.

 

The data flow：

MySQL -> Flink CDC -> ODS (Upsert Kafka) -> the job -> sink

I'm sure the ODS data in Kafka is correct;

I have also tried to use the flink-cdc source directly, it didn't solve the problem;

 

We tested sinking to kudu, Kafka or ES;

Also tested multiple versions: 1.13.2, 1.13.3, 1.13.5, 1.14.0, 1.14.2;

Lost data appears out of order on kafka, guessed as a bug of retraction stream:

!image-2022-01-07-11-27-01-010.png!

 

After many tests, we found that when the left join table is more or the parallelism of the operator is greater, the data will be more easily lost.",,Ashulin,lincoln.86xy,monster#12,zhihao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20370,,,,,FLINK-22826,,,,,,,,,,,,,,,,"07/Jan/22 03:27;Ashulin;image-2022-01-07-11-27-01-010.png;https://issues.apache.org/jira/secure/attachment/13038373/image-2022-01-07-11-27-01-010.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 13 07:08:14 UTC 2022,,,,,,,,,,"0|z0ybzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/22 03:00;lincoln.86xy;[~Ashulin] Thanks for reporting this!
Could you offer more information about the source table definition? Does your job configure the state ttl(if configured ttl cause the data expiration, you may also see the dataloss)?;;;","07/Jan/22 03:16;Ashulin;[~lincoln.86xy] 
{code:java}
WITH (
'connector' = 'upsert-kafka',
'topic' = 'XXX',
'group.id' = 'XXX',
'properties.bootstrap.servers' = 's',
'key.format' = 'json',
'value.format' = 'json'
)
{code}
Not set TTL；

The associated fields are of the same type；;;;","07/Jan/22 03:35;Ashulin;[~lzljs3620320] Hi，Please take a look.;;;","07/Jan/22 04:01;lincoln.86xy;[~Ashulin] Does the sink table's primary key differs from your last join key (` LEFT JOIN product_image_url piu ON ps.product = piu.product `)? If so, maybe the case of FLINK-20370, you can try 1.14.3;;;","07/Jan/22 06:03;Ashulin;[~lincoln.86xy] Yes, the primary key of the Sink table is different from the last join key.

And I updated the SQL related to primary and join keys for all tables.;;;","13/Jan/22 04:13;monster#12;Hi [~Ashulin]    I am very interested in this，and I want do some job for flink，can I help to do that？;;;","13/Jan/22 07:08;Ashulin;[~monster#12] Following Lincoln's suggestion, I tried using flink 1.14.3-rc1 and so far the problem has not recurred.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableResult OK may be empty if returned by multiple queries,FLINK-25558,13421113,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Paul Lin,Paul Lin,06/Jan/22 16:41,25/May/22 08:52,13/Jul/23 08:08,25/May/22 08:52,1.14.0,,,,,,,,,,Table SQL / API,,,,,0,,,,"Many queries return `TABLE_RESULT_OK`, which is a static member for reducing memory overhead. However, TableResult contains an iterator that can be read once. That means if a query returns `TABLE_RESULT_OK` and it is read,  then`TABLE_RESULT_OK` returned by the following queries would be considered empty.

It can be reproduced by simply printing two query results.

```
tEnv.executeSql(""create table tbl_a (a string)"").print();
tEnv.executeSql(""create table tbl_b (a string)"").print();
```

The output would be

```
+--------+
| result |
+--------+
|     OK |
+--------+
1 row in set
Empty set

```

 

 ",,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 03 08:16:09 UTC 2022,,,,,,,,,,"0|z0ybiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/22 16:43;Paul Lin;Working on the fix. Please assign this issue to me.;;;","03/Mar/22 08:16;Paul Lin;Seems that the issue is fixed by FLINK-24685 in Flink 1.15.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extra waiting of final checkpoint in benchmarks,FLINK-25556,13421054,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,06/Jan/22 11:52,08/Jan/22 10:10,13/Jul/23 08:08,07/Jan/22 06:29,1.15.0,,,,,,1.15.0,,,,Benchmarks,Runtime / Checkpointing,,,,0,pull-request-available,,,"After finishing https://issues.apache.org/jira/browse/FLINK-25105, flink always waits for the final checkpoint after all tasks are finished which leads to prolongation of the test and incorrect benchmark results. The idea is to disable `ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH` for all benchmarks.",,akalashnikov,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25105,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 07 06:29:39 UTC 2022,,,,,,,,,,"0|z0yb5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/22 16:41;akalashnikov;[~gaoyunhaii], maybe you can help me with reviewing it?;;;","07/Jan/22 06:29;gaoyunhaii;Very thanks [~akalashnikov] for the fix! Fix on master via 312eef146cc3f736f49a505612fd10a4816aea15. 

We would be able to remove the settings after we fixed https://issues.apache.org/jira/browse/FLINK-25383 . ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DistributedCacheDfsTest.testSubmittingJobViaRestClusterClient fails on AZP,FLINK-25554,13421014,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,trohrmann,trohrmann,06/Jan/22 08:33,18/Oct/22 06:58,13/Jul/23 08:08,18/Oct/22 06:58,1.14.6,1.15.0,1.16.0,1.17.0,,,1.15.3,1.16.0,1.17.0,,Client / Job Submission,,,,,0,pull-request-available,test-stability,,"The test {{DistributedCacheDfsTest.testSubmittingJobViaRestClusterClient}} failed on AZP with 

{code}
Jan 06 03:46:14 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.939 s <<< FAILURE! - in org.apache.flink.hdfstests.DistributedCacheDfsTest
Jan 06 03:46:14 [ERROR] org.apache.flink.hdfstests.DistributedCacheDfsTest.testSubmittingJobViaRestClusterClient  Time elapsed: 30.009 s  <<< ERROR!
Jan 06 03:46:14 org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds
Jan 06 03:46:14 	at sun.misc.Unsafe.park(Native Method)
Jan 06 03:46:14 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 06 03:46:14 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 06 03:46:14 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Jan 06 03:46:14 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 06 03:46:14 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 06 03:46:14 	at org.apache.flink.hdfstests.DistributedCacheDfsTest.testSubmittingJobViaRestClusterClient(DistributedCacheDfsTest.java:155)
Jan 06 03:46:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 06 03:46:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 06 03:46:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 06 03:46:14 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 06 03:46:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jan 06 03:46:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jan 06 03:46:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jan 06 03:46:14 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jan 06 03:46:14 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Jan 06 03:46:14 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Jan 06 03:46:14 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 06 03:46:14 	at java.lang.Thread.run(Thread.java:748)
Jan 06 03:46:14 
Jan 06 03:46:17 Formatting using clusterid: testClusterID
Jan 06 03:46:20 Formatting using clusterid: testClusterID
Jan 06 03:46:25 Formatting using clusterid: testClusterID
Jan 06 03:46:32 Formatting using clusterid: testClusterID
Jan 06 03:46:47 Formatting using clusterid: testClusterID
Jan 06 03:46:50 Formatting using clusterid: testClusterID
Jan 06 03:46:52 Formatting using clusterid: testClusterID
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28994&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=15920",,gaoyunhaii,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 18 06:58:47 UTC 2022,,,,,,,,,,"0|z0yaww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/22 13:33;chesnay;The logs aren't conclusive at all. We start the job submission and then nothing happens for 30 seconds. The Flink cluster is active and the netty connection is active.

We could remove the timeout; a thread dump could be helpful if it happens again.;;;","06/Mar/22 16:07;gaoyunhaii;Happened also on 1.14: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32553&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=16280];;;","14/Oct/22 10:14;mapohl;Happened on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=46667;;;","14/Oct/22 10:19;mapohl;I'm reopening the issue to remove the timeout as suggested by [~chesnay] in a previous comment.;;;","18/Oct/22 06:58;mapohl;master: 1902699cddcafcfabf9166d4221caa3545b60943
1.16: da6db4237d93bf84baff39b39eb1a76739cf6e2d
1.15: 8181abe4bc0e4cb524ad9fa658f2244dc8ae3273;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
execute pre-job throws org.apache.flink.table.api.TableException: Failed to execute sql,FLINK-25534,13420948,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,jychen,jychen,06/Jan/22 01:31,06/Jan/22 02:53,13/Jul/23 08:08,06/Jan/22 02:53,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,"org.apache.flink.table.api.TableException: Failed to execute sql
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:777) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:742) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.table.api.internal.StatementSetImpl.execute(StatementSetImpl.java:99) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    at com.cig.cdp.flink.JobApplication.main(JobApplication.java:91) ~[flink-streaming-core.jar:1.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_312]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_312]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_312]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_312]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246) ~[flink-dist_2.12-1.13.3.jar:1.13.3]

...skipping 1 line
    at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_312]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_312]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) [hadoop-common-3.0.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132) [flink-dist_2.12-1.13.3.jar:1.13.3]
Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster.
    at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:481) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:81) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1957) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:137) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:55) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:759) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    ... 19 more
Caused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment. 
Diagnostics from YARN: Application application_1641278381631_0020 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1641278381631_0020_000002 exited with  exi
tCode: 1
Failing this attempt.Diagnostics: [2022-01-06 09:15:11.758]Exception from container-launch.
Container id: container_1641278381631_0020_02_000001
Exit code: 1

[2022-01-06 09:15:11.759]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :

[2022-01-06 09:15:11.760]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :

For more detailed output, check the application tracking page: http://master:8088/cluster/app/application_1641278381631_0020 Then click on links to logs of each attempt.
. Failing the application.
If log aggregation is enabled on your cluster, use this command to further investigate the issue:
yarn logs -applicationId application_1641278381631_0020
    at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1201) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:593) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:474) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:81) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1957) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:137) ~[flink-dist_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:55) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:759) ~[flink-table-blink_2.12-1.13.3.jar:1.13.3]
    ... 19 more
2022-01-06 01:15:23,722 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cancelling deployment from Deployment Failure Hook
2022-01-06 01:15:23,723 INFO  org.apache.hadoop.yarn.client.RMProxy                        [] - Connecting to ResourceManager at master/192.168.7.121:8032
2022-01-06 01:15:23,724 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Killing YARN application
2022-01-06 01:15:23,732 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1641278381631_0020
2022-01-06 01:15:23,832 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deleting files in hdfs://master:8020/user/root/.flink/application_1641278381631_0020.",,jychen,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 06 02:04:47 UTC 2022,,,,,,,,,,"0|z0yai8:",9223372036854775807,jobmanger ip error,,,,,,,,,,,,,,,,,,,"06/Jan/22 02:04;zuston;I think you could login to the appmaster container on yarn nodemanager to check the detailed logs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preferred AllocationIDs are not respected when fulfilling pending slot requests,FLINK-25533,13420889,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,05/Jan/22 16:12,15/Jan/22 10:12,13/Jul/23 08:08,15/Jan/22 10:12,1.13.5,1.14.2,1.15.0,,,,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"In order to make best use of local recovery, we have to forward the set of preferred allocations to the {{DeclarativeSlotPoolBridge}} where new slots are matched with pending slot requests. At the moment this is not the case and this means that whenever we try to recover locally while not having all slots available, we might do wrong scheduling decisions.

In order to improve the situation, I propose to forward the set of preferred allocations to the {{DeclarativeSlotPoolBridge}}.",,dmvk,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jan 15 10:12:38 UTC 2022,,,,,,,,,,"0|z0ya54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/22 10:12;trohrmann;Fixed via 4a98a10c931e4ff9750d38da5c260a35a54fb56a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The test testRetryCommittableOnRetriableError takes one hour before completing succesfully,FLINK-25531,13420808,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,alexanderpreuss,martijnvisser,martijnvisser,05/Jan/22 12:49,21/Mar/22 05:10,13/Jul/23 08:08,13/Jan/22 10:00,1.15.0,,,,,,1.15.0,,,,,,,,,0,pull-request-available,test-stability,,"When working on https://issues.apache.org/jira/browse/FLINK-25504 I noticed that the {{test_ci kafka_gelly}} run took more then 1:30 hours. 

When looking at the logs for this PR https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28866&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c5f0071e-1851-543e-9a45-9ac140befc32 I noticed that {{org.apache.flink.connector.kafka.sink.KafkaCommitterTest.testRetryCommittableOnRetriableError}} is running for an hour:


{code:java}
13:22:31,145 [kafka-producer-network-thread | producer-transactionalId] WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Connection to node -1 (localhost/127.0.0.1:1) could not be established. Broker may not be available.
13:22:31,145 [kafka-producer-network-thread | producer-transactionalId] WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Bootstrap broker localhost:1 (id: -1 rack: null) disconnected
13:22:31,347 [kafka-producer-network-thread | producer-transactionalId] WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Connection to node -1 (localhost/127.0.0.1:1) could not be established. Broker may not be available.

...

14:22:29,472 [kafka-producer-network-thread | producer-transactionalId] WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Bootstrap broker localhost:1 (id: -1 rack: null) disconnected
14:22:30,324 [kafka-producer-network-thread | producer-transactionalId] WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Connection to node -1 (localhost/127.0.0.1:1) could not be established. Broker may not be available.
14:22:30,324 [kafka-producer-network-thread | producer-transactionalId] WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Bootstrap broker localhost:1 (id: -1 rack: null) disconnected
14:22:31,144 [                main] INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Proceeding to force close the producer since pending requests could not be completed within timeout 3600000 ms.
14:22:31,145 [kafka-producer-network-thread | producer-transactionalId] INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Transiting to fatal error state due to org.apache.kafka.common.KafkaException: The producer closed forcefully
14:22:31,145 [kafka-producer-network-thread | producer-transactionalId] INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-transactionalId, transactionalId=transactionalId] Transiting to fatal error state due to org.apache.kafka.common.KafkaException: The producer closed forcefully
14:22:31,148 [                main] INFO  org.apache.flink.connectors.test.common.junit.extensions.TestLoggerExtension [] - 
--------------------------------------------------------------------------------
Test org.apache.flink.connector.kafka.sink.KafkaCommitterTest.testRetryCommittableOnRetriableError successfully run.
================================================================================
{code}
",,fpaul,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26754,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 13 10:00:13 UTC 2022,,,,,,,,,,"0|z0y9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/22 10:00;fpaul;Merged in master: f74f49cf710b3edc68ddae641303d364fb20968a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If enabled changelog, RocksDB incremental checkpoint would always be full",FLINK-25524,13420752,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,yunta,yunta,05/Jan/22 08:38,25/Jan/22 17:11,13/Jul/23 08:08,25/Jan/22 07:03,1.15.0,,,,,,1.15.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,"Once changelog is enabled, RocksDB incremental checkpoint would only be executed during materialization. During this phase, it will leverage the {{materization id}} as the checkpoint id for RocksDB state backend's snapshot method.

However, current incremental checkpoint mechanism heavily depends on the checkpoint id. And {{SortedMap<Long, Set<StateHandleID>> uploadedStateIDs}} with checkpoint id as the key within {{RocksIncrementalSnapshotStrategy}} is the kernel for incremental checkpoint. Once we notify checkpoint complete of previous checkpoint, it will then remove the uploaded stateIds of that checkpoint, leading to we cannot get proper checkpoint information on the next RocksDBKeyedStateBackend#snapshot. That is to say, we will always upload all RocksDB artifacts.",,roman,ym,yunta,,,,,,,,,,,,,,,,,,,,,,FLINK-21352,,,,,,,,,,,,,,,,,,,,,FLINK-25144,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 25 07:03:50 UTC 2022,,,,,,,,,,"0|z0y9aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/22 07:03;roman;Merged as d0927dd41e2f0441e4e5825ff423dd0e903713f3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoFlatMapFunction requires both two flat_maps to yield something,FLINK-25513,13420539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,atptour2017,atptour2017,atptour2017,04/Jan/22 08:36,06/Jan/22 11:36,13/Jul/23 08:08,06/Jan/22 11:36,1.13.0,,,,,,1.13.6,1.14.3,1.15.0,,API / Python,,,,,0,pull-request-available,,,"When I used CoFlatMapFunction in pyflink, I found out that I must put yield something in both flat_map1 and flat_map2. Otherwise, it will raise an exception: TypeError: 'NoneType' object is not iterable.

In pyflink source code: datastream.py,  the process_element2 of KeyedCoMapCoProcessFunction class in ConnectedStreams class has only one sentence: 

{color:#cc7832}yield {color}{color:#94558d}self{color}._underlying.map2(value). if the flat_map2 has not yield something it will result in exception. So it should judge whether the {color:#94558d}self{color}._underlying.map2(value) is None or not firstly.",,ana4,atptour2017,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 06 11:36:45 UTC 2022,,,,,,,,,,"0|z0y7zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/22 05:54;dianfu;[~atptour2017] Thanks for reporting this issue. Would you like to take it?;;;","06/Jan/22 11:36;dianfu;Fixed in
- master via ed814c1a6feeda5fcbc1e50fc47820a64efe7e1c
- release-1.14 via 69dc8ba865560596d5c7248f6128612852757fcd
- release-1.13 via 1dc9faf75907c9a6b4051335b5a3083802ea053a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
npm ci hangs on azure,FLINK-25501,13420243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,02/Jan/22 03:08,26/Jan/22 13:35,13/Jul/23 08:08,26/Jan/22 13:35,1.13.6,,,,,,,,,,Runtime / Web Frontend,,,,,0,test-stability,,,"{code:java}
2022-01-01T00:08:29.3140259Z [INFO] 
2022-01-01T00:08:29.3141565Z [INFO] --- frontend-maven-plugin:1.6:npm (npm install) @ flink-runtime-web_2.11 ---
2022-01-01T00:08:29.3167736Z [INFO] Running 'npm ci --cache-max=0 --no-save' in /__w/1/s/flink-runtime-web/web-dashboard
2022-01-01T04:02:49.1063559Z ##[error]The operation was canceled.
2022-01-01T04:02:49.1077610Z ##[section]Finishing: Compile {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28820&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1&l=3560",,gaoyunhaii,paul8263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 26 13:35:30 UTC 2022,,,,,,,,,,"0|z0y660:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/22 01:12;paul8263;The unstable network connection to node repository might lead to this issue. How about rerun azure several times?;;;","04/Jan/22 02:45;gaoyunhaii;Hi [~paul8263] , thanks a lot for the investigating, perhaps we could only re-run the npm part? Also cc [~vthinkxie] ~;;;","26/Jan/22 13:35;chesnay;Likely fixed by the introduction of the npm cache.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
